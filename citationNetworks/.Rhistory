"A systematic study of the class imbalance problem: Automatically identifying empty camera trap images using convolutional neural networks",
"An automatic method for removing empty camera trap images using ensemble learning",
"Train Fast While Reducing False Positives: Improving Animal Classification Performance Using Convolutional Neural Networks",
"Desert bighorn sheep (Ovis canadensis) recognition from camera traps based on learned features",
"Iterative Human and Automated Identification of Wildlife Images",
"Robust ecological analysis of camera trap data labelled by a machine learning model",
"Automated Location Invariant Animal Detection In Camera Trap Images Using Publicly Available Data Sources",
"U-Infuse: Democratization of Customizable AI for Object Detection",
"Wildlife insights: A platform to maximize the potential of camera trap and other passive sensor wildlife data for the planet",
"Identification of Wild Species in Texas from Camera-trap Images using Deep Neural Network for Conservation Monitoring",
"Automated Image Recognition for Wildlife Camera Traps: Making it Work for You",
"Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2",
"Automated detection of European wild mammal species in camera trap images with an existing and pre-trained computer vision model",
"Camera settings and biome influence the accuracy of citizen science approaches to camera trap image classification",
"Sequence Information Channel Concatenation for Improving Camera Trap Image Burst Classification",
"Synthetic examples improve generalization for rare classes",
"Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2",
"“How many images do I need?” Understanding how sample size per class affects deep learning model performance metrics for balanced designs in autonomous wildlife monitoring",
"Three critical factors affecting automated image species recognition performance for camera traps",
"Dynamic Programming Selection of Object Proposals for Sequence-Level Animal Species Classification in the Wild",
"Fast human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification",
"Sorting camera trap images",
"Finding areas of motion in camera trap images",
"Individual identification of wild giant pandas from camera trap photos – a systematic and hierarchical approach",
"Animal identification in low quality camera-trap images using very deep convolutional neural networks and confidence thresholds",
"Efficient pipeline for camera trap image review"
)
search_dir_CT <- "../Data/goldStandardCTPapers/"
# think it works fine by just saving as a .txt, even with the xlsx still in dir (but nicer to remove maybe?)
priorCTLit <-
litsearchr::import_results(directory = search_dir_CT, verbose = TRUE)
priorCTLit <- priorCTLit %>% dplyr::filter(pubyear >= 2012)
nrow(priorCTLit)
priorTitles <- priorCTLit$title
flatTitleList <- c()
for (i in 1:length(priorTitles)) {
flatTitleList <- append(flatTitleList, priorTitles[i])
}
goldStandard <- append(flatTitleList, goldStandardML)
goldStandard <- as.data.frame(goldStandard)
names(goldStandard)[names(goldStandard) == "goldStandard"] <- "title"
########################## ML ##################
MLSCOPUS <-
litsearchr::import_results(directory = "../Data/LitSearches/ML/SCOPUSFinal/", verbose = TRUE)
nrow(MLSCOPUS)
MLSCOPUS <-
litsearchr::remove_duplicates(MLSCOPUS, field = "title", method = "string_osa")
nrow(MLSCOPUS)
#foundArticlesML <- litsearchr::check_recall(true_hits = goldStandardML,
#                                            retrieved = MLSCOPUSFinalImport$title)
#nrow(foundArticlesML)
mlDf <- as.data.frame(unique(MLSCOPUS))
# pull out titles and abstracts which have key words in them (very likely to be relevant)
relevantDfML <- mlDf[grep("camera", mlDf$title), ]
relevantDfML2 <- mlDf[grep("camera", mlDf$abstract), ]
relevantDfML3 <- mlDf[grep("computer", mlDf$title), ]
relevantDfML4 <- mlDf[grep("computer", mlDf$abstract), ]
relevantDfML5 <- mlDf[grep("vision", mlDf$title), ]
relevantDfML6 <- mlDf[grep("vision", mlDf$abstract), ]
relevantDfML7 <- mlDf[grep("machine", mlDf$title), ]
relevantDfML8 <- mlDf[grep("machine", mlDf$abstract), ]
relevantDfML9 <- mlDf[grep("learning", mlDf$title), ]
relevantDfML10 <- mlDf[grep("learning", mlDf$abstract), ]
relevantDfML11 <- mlDf[grep("neural", mlDf$title), ]
relevantDfML12 <- mlDf[grep("neural", mlDf$abstract), ]
relevantDfML13 <- mlDf[grep("network", mlDf$title), ]
relevantDfML14 <- mlDf[grep("network", mlDf$abstract), ]
relevantDfML15 <- mlDf[grep("camera", mlDf$author_keywords), ]
relevantDfML16 <- mlDf[grep("computer", mlDf$author_keywords), ]
relevantDfML17 <- mlDf[grep("vision", mlDf$author_keywords), ]
relevantDfML18 <- mlDf[grep("machine", mlDf$author_keywords), ]
relevantDfML19 <- mlDf[grep("learning", mlDf$author_keywords), ]
relevantDfML20 <- mlDf[grep("neural", mlDf$author_keywords), ]
relevantDfML21 <- mlDf[grep("network", mlDf$author_keywords), ]
relevantMLDf <- merge(relevantDfML, relevantDfML2, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML3, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML4, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML5, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML6, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML7, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML8, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML9, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML10, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML11, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML12, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML13, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML14, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML15, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML16, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML17, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML18, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML19, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML20, all = T)
relevantMLDf <- merge(relevantMLDf, relevantDfML21, all = T)
# and add the gold standard to this to prevent any further checks on those
inGoldStandard <- mlDf[mlDf$title %in% goldStandard$title, ]
relevantMLDf <- merge(relevantMLDf, inGoldStandard, all = T)
# 60% of titles automatically screened this way (obvs not acceptable for paper)
percentAutoRelevant <- nrow(relevantMLDf)/nrow(mlDf)*100
nrow(relevantMLDf)
toCheckMLDf <- mlDf
toCheckMLDf <- toCheckMLDf[!toCheckMLDf$title %in% relevantMLDf$title, ]
nrow(toCheckMLDf)
percentManualCheck <- nrow(toCheckMLDf)/nrow(mlDf)*100
percentAutoRelevant + percentManualCheck # wonderful that worked
write.csv(toCheckMLDf, "../Data/ML/relevanceCheckML.csv")
# ok thats a bit too long to do in one, maybe slice into 100 paper docs?
nrow(toCheckMLDf)
toCheckMLDf1 <- toCheckMLDf[1:100,]
toCheckMLDf2 <- toCheckMLDf[101:200,]
toCheckMLDf3 <- toCheckMLDf[201:300,]
toCheckMLDf4 <- toCheckMLDf[301:400,]
toCheckMLDf5 <- toCheckMLDf[401:500,]
toCheckMLDf6 <- toCheckMLDf[501:600,]
toCheckMLDf7 <- toCheckMLDf[601:700,]
toCheckMLDf8 <- toCheckMLDf[701:800,]
toCheckMLDf9 <- toCheckMLDf[801:900,]
toCheckMLDf10 <- toCheckMLDf[901:1000,]
toCheckMLDf11 <- toCheckMLDf[1001:1100,]
toCheckMLDf12 <- toCheckMLDf[1101:1200,]
toCheckMLDf13 <- toCheckMLDf[1201:1300,]
toCheckMLDf14 <- toCheckMLDf[1301:1340,]
write.csv(toCheckMLDf1, "../Data/ML/relevanceCheckML1.csv")
write.csv(toCheckMLDf2, "../Data/ML/relevanceCheckML2.csv")
write.csv(toCheckMLDf3, "../Data/ML/relevanceCheckML3.csv")
write.csv(toCheckMLDf4, "../Data/ML/relevanceCheckML4.csv")
write.csv(toCheckMLDf5, "../Data/ML/relevanceCheckML5.csv")
write.csv(toCheckMLDf6, "../Data/ML/relevanceCheckML6.csv")
write.csv(toCheckMLDf7, "../Data/ML/relevanceCheckML7.csv")
write.csv(toCheckMLDf8, "../Data/ML/relevanceCheckML8.csv")
write.csv(toCheckMLDf9, "../Data/ML/relevanceCheckML9.csv")
write.csv(toCheckMLDf10, "../Data/ML/relevanceCheckML10.csv")
write.csv(toCheckMLDf11, "../Data/ML/relevanceCheckML11.csv")
write.csv(toCheckMLDf12, "../Data/ML/relevanceCheckML12.csv")
write.csv(toCheckMLDf13, "../Data/ML/relevanceCheckML13.csv")
write.csv(toCheckMLDf14, "../Data/ML/relevanceCheckML14.csv")
################################## CT #################
CTSCOPUSImport <-
litsearchr::import_results(directory = "../Data/LitSearches/CT/SCOPUSFinal/", verbose = TRUE)
nrow(CTSCOPUSImport)
CTSCOPUSImport <-
litsearchr::remove_duplicates(CTSCOPUSImport, field = "title", method = "string_osa")
nrow(CTSCOPUSImport)
ctDf <- as.data.frame(unique(CTSCOPUSImport))
colnames((ctDf))
# If camera terms in title OR abstract, immediately keep as relevant? Filter
#relevantDf <- ctDf %>% filter(title contains "camera" || abstract contains "camera")
relevantDf <- ctDf[grep("camera", ctDf$title), ]
relevantDf2 <- ctDf[grep("camera", ctDf$abstract), ]
relevantDf3 <- ctDf[grep("trap", ctDf$title), ]
relevantDf4 <- ctDf[grep("trap", ctDf$abstract), ]
#not sure what other low hanging fruit there is for this one
relevantDf5 <- ctDf[grep("camera", ctDf$author_keywords), ]
relevantCTDf <- merge(relevantDf, relevantDf2, all = T)
relevantCTDf <- merge(relevantCTDf, relevantDf3, all = T)
relevantCTDf <- merge(relevantCTDf, relevantDf4, all = T)
relevantCTDf <- merge(relevantCTDf, relevantDf5, all = T)
# get 57% of articles automatically screened.
inGoldStandard <- ctDf[ctDf$title %in% goldStandard$title, ]
relevantCTDf <- merge(relevantCTDf, inGoldStandard, all = T)
nrow(relevantCTDf)
percentAutoRelevant <- nrow(relevantCTDf)/nrow(ctDf)*100
toCheckCTDf <- ctDf
toCheckCTDf <- toCheckCTDf[!toCheckCTDf$title %in% relevantCTDf$title, ]
percentManualCheck <- nrow(toCheckCTDf)/nrow(ctDf)*100
percentAutoRelevant + percentManualCheck # wonderful that worked
write.csv(toCheckCTDf, "../Data/CT/relevanceCheckCT.csv")
#### Check for duplicated between searches ####
# If found, should in all likelihood be assigned to CT group
# as most of the papers found by ML are being picked up for their
# analysis (Linear Model, Kernel Density), not on how they process the images
set.seed(1)
inBoth <- relevantMLDf[relevantMLDf$title %in% relevantCTDf$title, ]
# maybe do a sample of those too?
toScreenDupes <- inBoth %>% dplyr::slice_sample(prop = 0.1)
write.csv(toScreenDupes, "../Data/autoScreenInBoth.csv")
################ Pull out 10% of auto screened ones and manually verify ########
# ML
toScreenML <- relevantMLDf %>% dplyr::slice_sample(prop = 0.1)
nrow(relevantMLDf)
write.csv(toScreenML, "../Data/ML/autoScreenCheck.csv")
# write the full auto screened to a df and use that
write.csv(relevantMLDf, "../Data/ML/autoScreenedML.csv")
# CT
toScreenCT <- relevantCTDf %>% dplyr::slice_sample(prop = 0.1)
write.csv(toScreenCT, "../Data/CT/autoScreenCheck.csv")
# write the full auto screened to a df and use that
write.csv(relevantCTDf, "../Data/CT/autoScreenedCT.csv")
nrow(inGoldStandard)
nrow(priorCTLit)
nrow(goldStandard)
articlesFound <- litsearchr::check_recall(true_hits = goldStandardML,
retrieved = MLSCOPUS$title)
# write.csv(articles_found, "../Results/articlesFoundML.csv")
m <- articles_found
articlesFound <- litsearchr::check_recall(true_hits = goldStandardML,
retrieved = MLSCOPUS$title)
# write.csv(articles_found, "../Results/articlesFoundML.csv")
m <- articlesFound
badMatches <- m[m[,"Similarity"] < 1,]
badMatches
nrow(badMatches)
badMatches <- m[m[,"Similarity"] < 0.5,]
badMatches
nrow(badMatches)
badMatches[,"Title"]
goodMatches <- m[m[,"Similarity"] == 1,]
nrow(goodMatches)
goodMatches
# write.csv(articles_found, "../Results/articlesFoundML.csv")
m <- articlesFound
badMatches <- m[m[,"Similarity"] < 1,]
badMatches
nrow(badMatches)
badMatches <- m[m[,"Similarity"] < 0.5,]
badMatches
nrow(badMatches)
badMatches[,"Title"]
goodMatches <- m[m[,"Similarity"] == 1,]
nrow(goodMatches)
goodMatches
nrow(goodMatches) / length(goldStandardML)
articlesFoundCT <- litsearchr::check_recall(true_hits = flatTitleList,
retrieved = CTSCOPUSImport$title)
# write.csv(articles_found, "../Results/articlesFoundML.csv")
c <- articlesFoundCT
badMatches <- c[c[,"Similarity"] < 1,]
badMatches
nrow(badMatches)
badMatches <- c[c[,"Similarity"] < 0.5,]
badMatches
nrow(badMatches)
badMatches[,"Title"]
goodMatches <- c[c[,"Similarity"] == 1,]
nrow(goodMatches)
goodMatches
nrow(goodMatches) / length(flatTitleList)
# make edge list
edges <- data[complete.cases(data),] # edges = data with rows containing NAs removed
# write.csv(articles_found, "../Results/articlesFoundML.csv")
m <- articlesFound
badMatches <- m[m[,"Similarity"] < 1,]
badMatches
nrow(badMatches)
badMatches <- m[m[,"Similarity"] < 0.5,]
badMatches
nrow(badMatches)
badMatches[,"Title"]
goodMatches <- m[m[,"Similarity"] == 1,]
nrow(goodMatches)
goodMatches
nrow(goodMatches) / length(goldStandardML)
nrow(goodMatches) / length(flatTitleList)
badMatches <- c[c[,"Similarity"] < 1,]
badMatches
nrow(badMatches)
badMatches <- c[c[,"Similarity"] < 0.5,]
badMatches
nrow(badMatches)
badMatches[,"Title"]
goodMatches <- c[c[,"Similarity"] == 1,]
nrow(goodMatches)
goodMatches
# write.csv(articles_found, "../Results/articlesFoundML.csv")
c <- articlesFoundCT
# write.csv(articles_found, "../Results/articlesFoundML.csv")
c <- articlesFoundCT
badMatches <- c[c[,"Similarity"] < 1,]
badMatches
nrow(badMatches)
badMatches <- c[c[,"Similarity"] < 0.5,]
badMatches
nrow(badMatches)
badMatches[,"Title"]
goodMatches <- c[c[,"Similarity"] == 1,]
nrow(goodMatches)
goodMatches
nrow(goodMatches) / length(flatTitleList)
########### ----- This code calculates the proportion of camera trap papers that cite ML papers, overall, and by year
# read in data
set.seed(1)
setwd("C:/docNonNetwork/RProjects/CT-MLNetworks/citationNetworks/Code")
data <- read.csv("../Data/proportionsData.csv")
# total number of CT papers
CT_ids <- unique(data[data$citingSearch == "CT","citingID"]) # vector of CT papers = number of unique citing/cited IDs in the CT search subset
CT_ids <- CT_ids[!is.na(CT_ids)] # remove NA
n_CT <- length(CT_ids) # total number of CT papers in the data (including those with degree = 0)
# total number of CT papers BY YEAR
n_CT_byyear <- sapply(split(data, data$citingYear), function(x){ # split data by year and repeat the above code for year subset
CT_ids <- unique(x[x$citingSearch == "CT","citingID"]) # vector of CT papers = number of unique citing/cited IDs in the CT search subset
CT_ids <- CT_ids[!is.na(CT_ids)] # remove NA
length(CT_ids) # total number of CT papers in the data (including those with degree = 0)
})
# make edge list
edges <- data[complete.cases(data),] # edges = data with rows containing NAs removed
# all edges FROM ct papers TO ml papers
edges_ct2ml <- edges[(edges$citingSearch == "CT") & (edges$citedSearch == "ML"),]
# OVERALL proportion of CT papers that cite ML papers
length(unique(edges_ct2ml$citingID))/n_CT # number of ct papers that cite ML papers = number of unique IDs in the ct2ml edgelist; to get the proportion, divide by total number of CT papers
# BY YEAR proportion of CT papers that cite ML papers
years <- unique(edges_ct2ml$citingYear) # years in the edges_ct2ml data
proportion_byyear <- sapply(years, function(x){
edges_ct2ml_focalyear<- edges_ct2ml[edges_ct2ml$citingYear == x,]
length(unique(edges_ct2ml_focalyear$citingID))/n_CT_byyear[as.character(x)] # number of ct papers that cite ML papers = number of unique IDs in the ct2ml edgelist; to get the proportion, divide by total number of CT papers
})
plot(as.numeric(names(proportion_byyear)), proportion_byyear)
########### ----- Sense check to make sure numbers match up between overall and by year data
length(unique(edges_ct2ml$citingID)) # overall number of CT papers that cite ML papers (from above)
# BY YEAR number of CT papers that cite ML papers (same as above, just removing the n_CT_byyear denominator at the end)
number_byyear <- sapply(years, function(x){
edges_ct2ml_focalyear<- edges_ct2ml[edges_ct2ml$citingYear == x,]
length(unique(edges_ct2ml_focalyear$citingID)) # number of ct papers that cite ML papers = number of unique IDs in the ct2ml edgelist; to get the proportion, divide by total number of CT papers
})
sum(number_byyear) == length(unique(edges_ct2ml$citingID)) # TRUE - hooray!
library(ggplot2)
# ggplot this out
p <- ggplot(as.numeric(names(proportion_byyear)), proportion_byyear)
?ggplot
# ggplot this out
p <- ggplot(as.numeric(proportion_byyear, mapping = names(proportion_byyear)))
# ggplot this out
class(proportion_byyear)
proportion_byyear
p <- ggplot(as.numeric(proportion_byyear))
p <- ggplot(proportion_byyear)
prop <- as.data.frame(proportion_byyear)
p <- ggplot(prop)
p
prop
View(prop)
ggplot(prop)
# edgeList.R
set.seed(1)
setwd("C:/docNonNetwork/RProjects/CT-MLNetworks/citationNetworks/Code")
# imports
library(dplyr)
messyData <- read.csv("../Data/edges/matchedResults.csv")
# drop anything with conf greater than -0.55
# most values above this are consistently correct
# but not all
# and we loose some otherwise correct matches
# but time and general brutality happen here
colnames(messyData)
messyData <- messyData %>% filter(conf < -0.55)
nrow(messyData)
# make edgelist from remaining data
edgeList <- messyData[, c('CitingID', 'CitedID')]
write.csv(edgeList, "../Data/edges/edgeList.csv")
# networkAnalysis.R
set.seed(1)
setwd("C:/docNonNetwork/RProjects/CT-MLNetworks/citationNetworks/Code")
# imports
library(igraph)
library(RColorBrewer)
# read in edgeList data
edges <- read.csv("../Data/edges/edgeList.csv")
# read data in
MLData <- read.csv("../Data/ML/autoScreenedML.csv")
CTData <- read.csv("../Data/CT/autoScreenedCT.csv")
# MLData$search <- 2
# CTData$search <- 1
MLData$search <- 'ML'
CTData$search <- 'CT'
MLData$label <- "ML"
CTData$label <- "CT"
MLData$color <- "gold"
CTData$color <- "blue"
fullData <- merge(MLData, CTData, all = T)
names(fullData)[names(fullData) == "X"] <- "ID"
colnames(fullData)
fullData$ID <- 1:nrow(fullData)
# fullData$ID <- 0:nrow(fullData) ?
write.csv(fullData, "../Data/dataForPythonModules.csv")
# fullData$search <- as.numeric(fullData$search)
colnames(edges)
edges <- edges[c("CitingID", "CitedID")]
colnames(edges)
nodes <- fullData[c("ID")] # slice of just ID and title
# currently this is an undirected shared reference network I think
# or at least it should be as those are the edges
#edges <- fullData[c("ID", "cited_by")]
titles <- fullData[c("ID", "title")]
authors <- fullData[c("ID", "author")]
journal <- fullData[c("ID", "source_title")]
year <- fullData[c("ID", "year")]
search <- fullData[c("ID", "search")]
label <- fullData[c("ID", "label")]
color <- fullData[c("ID", "color")]
references <- fullData[c("ID", "references")]
class(edges)
edges <- as.matrix(edges)
class(edges)
edges
net <- igraph::graph_from_edgelist(edges, directed = TRUE)
net <- set_vertex_attr(net, "title", value = nodes)
net <- set_vertex_attr(net, "author", value = authors)
net <- set_vertex_attr(net, "journal", value = journal)
net <- set_vertex_attr(net, "year", value = year)
net <- set_vertex_attr(net, "search", value = search)
# bind the dfs
# communities <- read.csv("../Data/membership.csv")
communities <- read.csv("../Data/pythonCommunityData.csv")
communities
# names(communities)[names(communities) == "X"] <- "ID"
# names(communities)[names(communities) == "x"] <- "Community"
names(communities)[names(communities) == "community"] <- "Community"
communities <- communities[c("ID", "Community")]
fullData <- merge(fullData, communities, all = T, by = 'ID')
head(fullData)
# now can see what search each module contains
# ok maybe drop irrelevant stuff
partData <- fullData[c("ID", "search", "Community")]
groupedData <- partData %>% dplyr::group_split(Community)
groupedData[[7]]
# great, now code for proportion of each comm of each search?
tab <- groupedData[[7]]
counts <- dplyr::count(tab, search)
totRows <- nrow(tab)
counts
# count return col2 value 1 the col 2 value 2 over nrow for proportion
CTProp <- counts[1,2] / totRows
MLProp <- counts[2,2] / totRows
CTProp + MLProp
CTProportionList <- c()
for (i in 1:length(groupedData)) {
tab <- groupedData[[i]]
counts <- dplyr::count(tab, search)
totRows <- nrow(tab)
CTProp <- counts[1,2] / totRows
MLProp <- counts[2,2] / totRows
# CTProp + MLProp
CTProportionList <- append(CTProportionList, CTProp)
}
CTPropDf <- as.data.frame(unlist(CTProportionList))
names(CTPropDf)[names(CTPropDf) == "unlist(CTProportionList)"] <- "proportionCT"
# CTPropDf$Community <- 0:(nrow(CTPropDf)-1) # or
CTPropDf$Community <- 1:nrow(CTPropDf)
CTPropDf$proportionCT
meanCTPropPerComm <- mean(CTPropDf$proportionCT)
meanCTPropPerComm
# partData$proportionCT
partData <- merge(partData, CTPropDf, all=T, by='Community')
head(partData)
MLProportionList <- c()
for (i in 1:length(groupedData)) {
tab <- groupedData[[i]]
counts <- dplyr::count(tab, search)
totRows <- nrow(tab)
CTProp <- counts[1,2] / totRows
MLProp <- counts[2,2] / totRows
if (is.na(MLProp)) {
MLProp <- 0
}
# CTProp + MLProp
MLProportionList <- append(MLProportionList, MLProp)
}
MLProportionList
nrow(messyData)
removedNa <- na.rm(messyData)
removedNa <- drop.na(messyData)
removedNa <- dplyr::drop.na(messyData)
removedNa <- dplyr::drop_na(messyData)
removedNa <- messyData %>% dplyr::drop_na()
library(tidyr)
removedNa <- messyData %>% tidyr::drop_na()
nrow(removedNa)
nrow(unique(messyData))
########### ----- This code calculates the proportion of camera trap papers that cite ML papers, overall, and by year
# read in data
set.seed(1)
setwd("C:/docNonNetwork/RProjects/CT-MLNetworks/citationNetworks/Code")
library(ggplot2)
data <- read.csv("../Data/proportionsData.csv")
# total number of CT papers
CT_ids <- unique(data[data$citingSearch == "CT","citingID"]) # vector of CT papers = number of unique citing/cited IDs in the CT search subset
CT_ids <- CT_ids[!is.na(CT_ids)] # remove NA
n_CT <- length(CT_ids) # total number of CT papers in the data (including those with degree = 0)
# total number of CT papers BY YEAR
n_CT_byyear <- sapply(split(data, data$citingYear), function(x){ # split data by year and repeat the above code for year subset
CT_ids <- unique(x[x$citingSearch == "CT","citingID"]) # vector of CT papers = number of unique citing/cited IDs in the CT search subset
CT_ids <- CT_ids[!is.na(CT_ids)] # remove NA
length(CT_ids) # total number of CT papers in the data (including those with degree = 0)
})
# make edge list
edges <- data[complete.cases(data),] # edges = data with rows containing NAs removed
# all edges FROM ct papers TO ml papers
edges_ct2ml <- edges[(edges$citingSearch == "CT") & (edges$citedSearch == "ML"),]
# OVERALL proportion of CT papers that cite ML papers
length(unique(edges_ct2ml$citingID))/n_CT # number of ct papers that cite ML papers = number of unique IDs in the ct2ml edgelist; to get the proportion, divide by total number of CT papers
# BY YEAR proportion of CT papers that cite ML papers
years <- unique(edges_ct2ml$citingYear) # years in the edges_ct2ml data
proportion_byyear <- sapply(years, function(x){
edges_ct2ml_focalyear<- edges_ct2ml[edges_ct2ml$citingYear == x,]
length(unique(edges_ct2ml_focalyear$citingID))/n_CT_byyear[as.character(x)] # number of ct papers that cite ML papers = number of unique IDs in the ct2ml edgelist; to get the proportion, divide by total number of CT papers
})
plot(as.numeric(names(proportion_byyear)), proportion_byyear)
# ggplot this out
class(proportion_byyear)
proportion_byyear
prop <- as.data.frame(proportion_byyear)
prop
p <- ggplot(prop)
p
########### ----- Sense check to make sure numbers match up between overall and by year data
length(unique(edges_ct2ml$citingID)) # overall number of CT papers that cite ML papers (from above)
# BY YEAR number of CT papers that cite ML papers (same as above, just removing the n_CT_byyear denominator at the end)
number_byyear <- sapply(years, function(x){
edges_ct2ml_focalyear<- edges_ct2ml[edges_ct2ml$citingYear == x,]
length(unique(edges_ct2ml_focalyear$citingID)) # number of ct papers that cite ML papers = number of unique IDs in the ct2ml edgelist; to get the proportion, divide by total number of CT papers
})
sum(number_byyear) == length(unique(edges_ct2ml$citingID)) # TRUE - hooray!
mean(proportion_byyear)
