text = paste(results$title, results$abstract),
method = "fakerake",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
# authorKeywords <- tryCatch(litsearchr::extract_terms(
#   keywords = results$author_keywords,
#   method = "tagged",
#   min_freq = 2,
#   ngrams = TRUE,
#   min_n = 2,
#   language = "English"),
#   finally = print("No author keywords, trying index key words"))
#
# tryCatch(indexKeywords <- litsearchr::extract_terms(
#   keywords = results$index_keywords,
#   method = "tagged",
#   min_freq = 2,
#   ngrams = TRUE,
#   min_n = 2,
#   language = "English"
# ), finally = "No index key words, trying tagged keywords")
#
# tryCatch( taggedkeywords <-
#             litsearchr::extract_terms(
#               keywords = results$keywords,
#               method = "tagged",
#               min_freq = 2,
#               ngrams = TRUE,
#               min_n = 2,
#               language = "English"
#             ), finally("No tagged key words, likely SCOPUS document"))
differs between scopus and WoS, for WoS use results$keywords, for scopus
use both / either of results$index_keywords and results$author_keywords
if (scopus == TRUE) {
authorKeywords <- litsearchr::extract_terms(
keywords = results$author_keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
indexKeywords <- litsearchr::extract_terms(
keywords = results$index_keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
} else {
taggedkeywords <-
litsearchr::extract_terms(
keywords = results$keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
}
# tryCatch() # need to work out the try catches tbh
if (scopus == TRUE) {
all_keywords <- unique(append(rakedkeywords, authorKeywords))
all_keywords <- unique(append(all_keywords, indexKeywords))
} else {
all_keywords <- unique(append(taggedkeywords, rakedkeywords))
}
#print("Generating document feature matrix...")
dfm <-
litsearchr::create_dfm(
elements = paste(results$title, results$abstract),
features = all_keywords
)
#print("done")
graph <-
litsearchr::create_network(
search_dfm = dfm,
min_studies = numStudiesTermOccurs,
min_occ = numTimesTermOccurs
)
if (cutoffMethod == "changepoint") {
cutoff <-
litsearchr::find_cutoff(
graph,
method = "changepoint",
knot_num = numKnots,
imp_method = cutoffImpl
)
} else {
cutoff <-
litsearchr::find_cutoff(
graph,
method = "cumulative",
percent = numCumulative,
imp_method = cutoffImpl
)
}
reducedgraph <-
litsearchr::reduce_graph(graph, cutoff_strength = cutoff[1])
searchterms <- litsearchr::get_keywords(reducedgraph)
write.csv(searchterms, outFilePath)
}
compareAgainstGold <- function(inputDir, goldStandard)
{
retrieved_articles <-
litsearchr::import_results(directory = inputDir, verbose = TRUE)
retrieved_articles <- litsearchr::remove_duplicates(retrieved_articles, field="title", method="string_osa")
articles_found <- litsearchr::check_recall(true_hits = goldStandard,
retrieved = retrieved_articles$title)
return(articles_found)
}
######################################################
############# Setting the Gold Standard ##############
gold_standard_ML <-
c(
"Automated Image Recognition for Wildlife Camera Traps: Making it Work for You",
"Zilong: A tool to identify empty images in camera-trap data",
"ClassifyMe: A Field-Scouting Software for the Identification of Wildlife in Camera Trap Images",
"EventFinder: a program for screening remotely captured images",
"A deep active learning system for species identification and counting in camera trap images",
"Animal Scanner: Software for classifying humans, animals, and empty frames in camera trap images",
"Camera-trap images segmentation using multi-layer robust principal component analysis",
"Camera‐trapping version 3.0: current constraints and future priorities for development",
"Recognition in terra incognita",
"Identifying animal species in camera trap images using deep learning and citizen science",
"Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning",
"Machine learning to classify animal species in camera trap images: Applications in ecology",
"Towards automatic detection of animals in camera-trap images",
"Deep learning object detection methods for ecological camera trap data",
"Past, present and future approaches using computer vision for animal re-identification from camera trap data",
"Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks",
"AnimalFinder: A semi-automated system for animal detection in time-lapse camera trap images",
"Deep convolutional neural network based species recognition for wild animal monitoring",
"Automated identification of animal species in camera trap images",
"Application of deep learning to camera trap data for ecologists in planning/engineering--Can captivity imagery train a model which generalises to the wild?",
"Minimizing the Annotation Effort for Detecting Wildlife in Camera Trap Images with Active Learning",
"Automatic Camera Trap Classification Using Wildlife-Specific Deep Learning in Nilgai Management",
"Filtering Empty Camera Trap Images in Embedded Systems",
"A systematic study of the class imbalance problem: Automatically identifying empty camera trap images using convolutional neural networks",
"An automatic method for removing empty camera trap images using ensemble learning",
"Train Fast While Reducing False Positives: Improving Animal Classification Performance Using Convolutional Neural Networks",
"Desert bighorn sheep (Ovis canadensis) recognition from camera traps based on learned features",
"Iterative Human and Automated Identification of Wildlife Images",
"Robust ecological analysis of camera trap data labelled by a machine learning model",
"Automated Location Invariant Animal Detection In Camera Trap Images Using Publicly Available Data Sources",
"U-Infuse: Democratization of Customizable AI for Object Detection",
"Wildlife insights: A platform to maximize the potential of camera trap and other passive sensor wildlife data for the planet",
"Identification of Wild Species in Texas from Camera-trap Images using Deep Neural Network for Conservation Monitoring",
"Automated Image Recognition for Wildlife Camera Traps: Making it Work for You",
"Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2",
"Automated detection of European wild mammal species in camera trap images with an existing and pre-trained computer vision model",
"Camera settings and biome influence the accuracy of citizen science approaches to camera trap image classification",
"Sequence Information Channel Concatenation for Improving Camera Trap Image Burst Classification",
"Synthetic examples improve generalization for rare classes",
"Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2",
"“How many images do I need?” Understanding how sample size per class affects deep learning model performance metrics for balanced designs in autonomous wildlife monitoring",
"Three critical factors affecting automated image species recognition performance for camera traps",
"Dynamic Programming Selection of Object Proposals for Sequence-Level Animal Species Classification in the Wild",
"Fast human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification",
"Sorting camera trap images",
"Finding areas of motion in camera trap images",
"Individual identification of wild giant pandas from camera trap photos – a systematic and hierarchical approach",
"Animal identification in low quality camera-trap images using very deep convolutional neural networks and confidence thresholds",
"Efficient pipeline for camera trap image review"
)
## worrying about CT papers now ##
# generate terms using the papers #
generateTerms("../Data/LitSearches/CT/scopus/", "../Results/searchTerms/CT/ctSearchTerms.csv", scopus = TRUE)
## worrying about CT papers now ##
# generate terms using the papers #
generateTerms("../Data/LitSearches/CT/scopus/", "../Results/searchTerms/CT/ctSearchTerms.csv", scopus = TRUE)
## searchTerms.R
# Lifted demo scripts of use of the two titled packages
# Edited to use a novel data set
######################################################
################## Imports ###########################
library(litsearchr)
library(tidyr)
library(bibliometrix)
######################################################
################ Functions ###########################
setwd("C:/docNonNetwork/RProjects/citationNetworks/Code")
generateTerms <- function(inputDir, outFilePath, removedupeMethod = "string_osa",
numStudiesTermOccurs = 10, numTimesTermOccurs = 10,
cutoffMethod = "changepoint", numKnots = 3,
numCumulative = 0.8, cutoffImpl = "strength",
scopus = TRUE)
{
import <-
litsearchr::import_results(directory = inputDir, verbose = TRUE)
results <-
litsearchr::remove_duplicates(import, field = "title", method = removedupeMethod)
rakedkeywords <-
litsearchr::extract_terms(
text = paste(results$title, results$abstract),
method = "fakerake",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
# authorKeywords <- tryCatch(litsearchr::extract_terms(
#   keywords = results$author_keywords,
#   method = "tagged",
#   min_freq = 2,
#   ngrams = TRUE,
#   min_n = 2,
#   language = "English"),
#   finally = print("No author keywords, trying index key words"))
#
# tryCatch(indexKeywords <- litsearchr::extract_terms(
#   keywords = results$index_keywords,
#   method = "tagged",
#   min_freq = 2,
#   ngrams = TRUE,
#   min_n = 2,
#   language = "English"
# ), finally = "No index key words, trying tagged keywords")
#
# tryCatch( taggedkeywords <-
#             litsearchr::extract_terms(
#               keywords = results$keywords,
#               method = "tagged",
#               min_freq = 2,
#               ngrams = TRUE,
#               min_n = 2,
#               language = "English"
#             ), finally("No tagged key words, likely SCOPUS document"))
differs between scopus and WoS, for WoS use results$keywords, for scopus
use both / either of results$index_keywords and results$author_keywords
if (scopus == TRUE) {
authorKeywords <- litsearchr::extract_terms(
keywords = results$author_keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
indexKeywords <- litsearchr::extract_terms(
keywords = results$index_keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
} else {
taggedkeywords <-
litsearchr::extract_terms(
keywords = results$keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
}
# tryCatch() # need to work out the try catches tbh
if (scopus == TRUE) {
all_keywords <- unique(append(rakedkeywords, authorKeywords))
all_keywords <- unique(append(all_keywords, indexKeywords))
} else {
all_keywords <- unique(append(taggedkeywords, rakedkeywords))
}
#print("Generating document feature matrix...")
dfm <-
litsearchr::create_dfm(
elements = paste(results$title, results$abstract),
features = all_keywords
)
#print("done")
graph <-
litsearchr::create_network(
search_dfm = dfm,
min_studies = numStudiesTermOccurs,
min_occ = numTimesTermOccurs
)
if (cutoffMethod == "changepoint") {
cutoff <-
litsearchr::find_cutoff(
graph,
method = "changepoint",
knot_num = numKnots,
imp_method = cutoffImpl
)
} else {
cutoff <-
litsearchr::find_cutoff(
graph,
method = "cumulative",
percent = numCumulative,
imp_method = cutoffImpl
)
}
reducedgraph <-
litsearchr::reduce_graph(graph, cutoff_strength = cutoff[1])
searchterms <- litsearchr::get_keywords(reducedgraph)
write.csv(searchterms, outFilePath)
}
generateTerms <- function(inputDir, outFilePath, removedupeMethod = "string_osa",
numStudiesTermOccurs = 10, numTimesTermOccurs = 10,
cutoffMethod = "changepoint", numKnots = 3,
numCumulative = 0.8, cutoffImpl = "strength",
scopus = TRUE)
{
import <-
litsearchr::import_results(directory = inputDir, verbose = TRUE)
results <-
litsearchr::remove_duplicates(import, field = "title", method = removedupeMethod)
rakedkeywords <-
litsearchr::extract_terms(
text = paste(results$title, results$abstract),
method = "fakerake",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
# authorKeywords <- tryCatch(litsearchr::extract_terms(
#   keywords = results$author_keywords,
#   method = "tagged",
#   min_freq = 2,
#   ngrams = TRUE,
#   min_n = 2,
#   language = "English"),
#   finally = print("No author keywords, trying index key words"))
#
# tryCatch(indexKeywords <- litsearchr::extract_terms(
#   keywords = results$index_keywords,
#   method = "tagged",
#   min_freq = 2,
#   ngrams = TRUE,
#   min_n = 2,
#   language = "English"
# ), finally = "No index key words, trying tagged keywords")
#
# tryCatch( taggedkeywords <-
#             litsearchr::extract_terms(
#               keywords = results$keywords,
#               method = "tagged",
#               min_freq = 2,
#               ngrams = TRUE,
#               min_n = 2,
#               language = "English"
#             ), finally("No tagged key words, likely SCOPUS document"))
# differs between scopus and WoS, for WoS use results$keywords, for scopus
#  use both / either of results$index_keywords and results$author_keywords
if (scopus == TRUE) {
authorKeywords <- litsearchr::extract_terms(
keywords = results$author_keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
indexKeywords <- litsearchr::extract_terms(
keywords = results$index_keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
} else {
taggedkeywords <-
litsearchr::extract_terms(
keywords = results$keywords,
method = "tagged",
min_freq = 2,
ngrams = TRUE,
min_n = 2,
language = "English"
)
}
# tryCatch() # need to work out the try catches tbh
if (scopus == TRUE) {
all_keywords <- unique(append(rakedkeywords, authorKeywords))
all_keywords <- unique(append(all_keywords, indexKeywords))
} else {
all_keywords <- unique(append(taggedkeywords, rakedkeywords))
}
#print("Generating document feature matrix...")
dfm <-
litsearchr::create_dfm(
elements = paste(results$title, results$abstract),
features = all_keywords
)
#print("done")
graph <-
litsearchr::create_network(
search_dfm = dfm,
min_studies = numStudiesTermOccurs,
min_occ = numTimesTermOccurs
)
if (cutoffMethod == "changepoint") {
cutoff <-
litsearchr::find_cutoff(
graph,
method = "changepoint",
knot_num = numKnots,
imp_method = cutoffImpl
)
} else {
cutoff <-
litsearchr::find_cutoff(
graph,
method = "cumulative",
percent = numCumulative,
imp_method = cutoffImpl
)
}
reducedgraph <-
litsearchr::reduce_graph(graph, cutoff_strength = cutoff[1])
searchterms <- litsearchr::get_keywords(reducedgraph)
write.csv(searchterms, outFilePath)
}
compareAgainstGold <- function(inputDir, goldStandard)
{
retrieved_articles <-
litsearchr::import_results(directory = inputDir, verbose = TRUE)
retrieved_articles <- litsearchr::remove_duplicates(retrieved_articles, field="title", method="string_osa")
articles_found <- litsearchr::check_recall(true_hits = goldStandard,
retrieved = retrieved_articles$title)
return(articles_found)
}
View(grouped_terms)
# extract the camera terms from the csv
termsCamera <- ctTerms$term[grep("camera", ctTerms$group)]
# wow that took like 5 hours to run #
# but it did run! #
# read the grouped terms back in and generate the search #
ctTerms <- read.csv("../Results/searchTerms/CT/ctSearchTermsGrouped.csv")
# extract the camera terms from the csv
termsCamera <- ctTerms$term[grep("camera", ctTerms$group)]
# pull out other relevant terms here
termsML <- ctTerms$term[grep("ml", ctTerms$group)]
termsStudyType <- ctTerms$term[grep("studyType", ctTerms$group)]
termsLocation <- ctTerms$term[grep("location", ctTerms$group)]
termsSpecies <- ctTerms$term[grep("species", ctTerms$group)]
View(ctTerms)
termsEcologyML <- termsML$term[grep("ecology", termsML$group)]
# manually group ML terms then return to the next line #
termsML <- read.csv("../Results/searchTerms/ML/mlSearchTermsGrouped.csv")
termsEcologyML <- termsML$term[grep("ecology", termsML$group)]
# extract the camera terms from the csv
termsCTML <- termsML$term[grep("camera", termsML$group)]
termsMLML <- termsML$term[grep("ML", termsML$group)]
termsUnifiedML <- unique(append(termsML , termsMLML))
#################### Unifying terms across CT and ML searches ##################
termsUnifiedCT <- unique(append(cameraGroup, termsCTML))
#################### Unifying terms across CT and ML searches ##################
termsUnifiedCT <- unique(append(termsCTcameras, termsCTML))
# join together a list of manually generated woodpecker terms with the ones from the csv
termsCTcameras <- unique(append(c("camera-trap","camera-traps", "camera trap",
"infrared triggered camera", "trail camera",
"automatic camera", "photo trap", "remote camera",
"remotely triggered camera"), camera_terms)) ## add the original search terms here
#################### Unifying terms across CT and ML searches ##################
termsUnifiedCT <- unique(append(termsCTcameras, termsCTML))
# wow that took like 5 hours to run #
# but it did run! #
# read the grouped terms back in and generate the search #
ctTerms <- read.csv("../Results/searchTerms/CT/ctSearchTermsGrouped.csv")
# extract the camera terms from the csv
termsCamera <- ctTerms$term[grep("camera", ctTerms$group)]
# pull out other relevant terms here
termsML <- ctTerms$term[grep("ml", ctTerms$group)]
# join together a list of manually generated woodpecker terms with the ones from the csv
termsCTcameras <- unique(append(c("camera-trap","camera-traps", "camera trap",
"infrared triggered camera", "trail camera",
"automatic camera", "photo trap", "remote camera",
"remotely triggered camera"), termsCamera)) ## add the original search terms here
#################### Unifying terms across CT and ML searches ##################
termsUnifiedCT <- unique(append(termsCTcameras, termsCTML))
termsUnifiedML <- unique(append(termsML , termsMLML))
termsunifiedEcol <- unique(append(termsEcologyML, termsStudyType))
#################### Unifying terms across CT and ML searches ##################
termsUnifiedCT <- unique(append(termsCTcameras, termsCTML))
termsUnifiedML <- unique(append(termsML , termsMLML))
# possibly need ecology / study type unified terms for each too?
termsunifiedEcol <- unique(append(termsEcologyML, termsStudyType))
termsAllUnifiedML <- list(termsUnifiedCT, termsUnifiedML, termsunifiedEcol)
searchMLFinal <-
litsearchr::write_search(
groupdata = termsAllUnifiedML,
languages = "English",
stemming = TRUE,
closure = "none",
exactphrase = TRUE,
writesearch = FALSE,
verbose = TRUE
)
write(searchMLFinal, '../Data/ML/searchMLFinal.txt')
termsAllUnifiedCT <- list(termsUnifiedCT, termsunifiedEcol)
searchCTFinal <-
litsearchr::write_search(
groupdata = termsAllUnifiedCT,
languages = "English",
stemming = TRUE,
closure = "none",
exactphrase = TRUE,
writesearch = FALSE,
verbose = TRUE
)
write(searchCTFinal, '../Data/ML/searchCTFinal.txt')
write(searchCTFinal, '../Data/CT/searchCTFinal.txt')
