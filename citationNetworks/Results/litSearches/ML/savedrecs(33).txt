FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Willi, M
   Pitman, RT
   Cardoso, AW
   Locke, C
   Swanson, A
   Boyer, A
   Veldthuis, M
   Fortson, L
AF Willi, Marco
   Pitman, Ross T.
   Cardoso, Anabelle W.
   Locke, Christina
   Swanson, Alexandra
   Boyer, Amy
   Veldthuis, Marten
   Fortson, Lucy
TI Identifying animal species in camera trap images using deep learning and
   citizen science
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal identification; camera trap; citizen science; convolutional
   neural networks; deep learning; machine learning
AB Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2% and 98.0%, whereas accuracies for identifying specific species were between 88.7% and 92.7%. Transferring information from CNNs trained on large datasets ("transfer-learning") was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.
C1 [Willi, Marco; Fortson, Lucy] Univ Minnesota, Sch Phys & Astron, Minneapolis, MN 55455 USA.
   [Pitman, Ross T.] Panthera, New York, NY USA.
   [Pitman, Ross T.] Univ Cape Town, Inst Communities & Wildlife Africa, Dept Biol Sci, Cape Town, South Africa.
   [Cardoso, Anabelle W.] Univ Oxford, Sch Geog & Environm, Oxford, England.
   [Locke, Christina] Wisconsin Dept Nat Resources, Off Appl Sci, Madison, WI USA.
   [Locke, Christina; Swanson, Alexandra; Veldthuis, Marten] Univ Oxford, Dept Astrophys, Oxford, England.
   [Boyer, Amy] Adler Planetarium, Chicago, IL USA.
RP Willi, M (corresponding author), Univ Minnesota, Sch Phys & Astron, Minneapolis, MN 55455 USA.
EM will5448@umn.edu
OI Willi, Marco/0000-0002-0041-396X; Cardoso, Anabelle
   Williamson/0000-0002-4327-7259
FU National Science FoundationNational Science Foundation (NSF) [IIS
   1619177]
FX National Science Foundation, Grant/Award Number: IIS 1619177
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Bowyer A., 2015, THIS IMAGE INTENTION
   Branson S., 2017, CVPR
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Fortson L, 2012, CH CRC DATA MIN KNOW, P213
   Gal Y., 2015, BAYESIAN CONVOLUTION
   Gal Y., 2016, THESIS, P174
   Giraldo-Zuluaga JH, 2017, PROC INT C TOOLS ART, P53, DOI 10.1109/ICTAI.2017.00020
   Glorot X., 2010, PROC 13 INT C ARTIFI, V9, P249, DOI DOI 10.1038/S41593-021-00857-X
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Guo CA, 2017, PR MACH LEARN RES, V70
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Parham J., 2016, APPL COMP VIS WORKSH, P1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Simpson E., 2013, DECISION MAKING IMPE, P1, DOI [DOI 10.1007/978-3-642-36406-8_1, DOI 10.1007/978-3-642-36406-8]
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zevin M., 2016, CLASSICAL QUANTUM GR, V34, P1
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 28
TC 80
Z9 82
U1 12
U2 58
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JAN
PY 2019
VL 10
IS 1
BP 80
EP 91
DI 10.1111/2041-210X.13099
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HK2NU
UT WOS:000457750600008
OA Bronze
DA 2022-02-10
ER

PT J
AU Kutugata, M
   Baumgardt, J
   Goolsby, JA
   Racelis, AE
AF Kutugata, Matthew
   Baumgardt, Jeremy
   Goolsby, John A.
   Racelis, Alexis E.
TI Automatic Camera-Trap Classification Using Wildlife-Specific Deep
   Learning in Nilgai Management
SO JOURNAL OF FISH AND WILDLIFE MANAGEMENT
LA English
DT Article
DE Boselaphus tragocamelus; camera trap; cattle fever ticks; deep learning;
   nilgai; transfer learning
AB Camera traps provide a low-cost approach to collect data and monitor wildlife across large scales but hand-labeling images at a rate that outpaces accumulation is difficult. Deep learning, a subdiscipline of machine learning and computer science, can address the issue of automatically classifying camera-trap images with a high degree of accuracy. This technique, however, may be less accessible to ecologists or small-scale conservation projects, and has serious limitations. In this study, we trained a simple deep learning model using a dataset of 120,000 images to identify the presence of nilgai Boselaphus tragocamelus, a regionally specific nonnative game animal, in camera-trap images with an overall accuracy of 97%. We trained a second model to identify 20 groups of animals and one group of images without any animals present, labeled as "none,'' with an accuracy of 89%. Lastly, we tested the multigroup model on images collected of similar species, but in the southwestern United States, resulting in significantly lower precision and recall for each group. This study highlights the potential of deep learning for automating camera-trap image processing workflows, provides a brief overview of image-based deep learning, and discusses the often-understated limitations and methodological considerations in the context of wildlife conservation and species monitoring.
C1 [Kutugata, Matthew; Baumgardt, Jeremy] Univ Texas Rio Grande Valley, Sch Earth Environm & Marine Sci, 1201 W Univ Dr, Edinburg, TX 78539 USA.
   [Goolsby, John A.] Texas A&M Univ, Caesar Kleberg Wildlife Res Inst, Kingsville, TX 78363 USA.
   [Racelis, Alexis E.] ARS, USDA, Knipling Bushland US Livestock Insects Res Lab, Cattle Fever Tick Res Lab, Edinburg, TX 78541 USA.
RP Kutugata, M (corresponding author), Univ Texas Rio Grande Valley, Sch Earth Environm & Marine Sci, 1201 W Univ Dr, Edinburg, TX 78539 USA.
FU Integrated Pest Management of Cattle Fever Ticks [3094-32000-042-00-D];
   U.S. Department of Agriculture National Institute of Food and
   AgricultureUnited States Department of Agriculture (USDA)
   [2016-38422-25543]
FX Game camera images and initial processing was supported through
   appropriated research project 3094-32000-042-00-D, Integrated Pest
   Management of Cattle Fever Ticks. This article reports results of
   research only and mention of a proprietary product does not constitute
   an endorsement or recommendation by the U.S. Department of Agriculture
   for its use. U.S. Department of Agriculture is an equal opportunity
   provider and employer. Special thanks to Amelia Berle for data
   management, and research technicians who spent countless hours labeling
   images. Additional thanks to Dr. Rupesh Kariyat and Dr. Christofferson
   for providing access to computing equipment. We would also like to thank
   the journal reviewers and Associate Editor for their commitment to open
   access, which ensures applied conservation science remains accessible to
   all. Matthew Kutugata was supported by U.S. Department of Agriculture
   National Institute of Food and Agriculture Grant 2016-38422-25543.
CR Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Chollet F., 2018, DEEP LEARNING PYTHON
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Foley AM, 2017, PREV VET MED, V146, P166, DOI 10.1016/j.prevetmed.2017.08.002
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Goolsby J.G., 2019, SUBTROPICAL AGR ENV, V70, P1
   Guilford J.P., 1954, PSYCHOMETRIC METHODS, V2d ed
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Ivan JS, 2016, METHODS ECOL EVOL, V7, P499, DOI 10.1111/2041-210X.12503
   Leslie D.M., 2016, INT BORDERLAND CONCE, P136
   Leslie David M. Jr., 2008, Mammalian Species, DOI 10.1644/813.1
   Lohmeyer KH, 2018, J MED ENTOMOL, V55, P515, DOI 10.1093/jme/tjy004
   Azlan JM, 2006, RAFFLES B ZOOL, V54, P469
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Rovero Francesco, 2006, Journal of East African Natural History, V95, P111, DOI 10.2982/0012-8317(2006)95[111:APNGSE]2.0.CO;2
   Schmidly DJ, 2004, MAMMALS TEXAS
   Srbek-Araujo AC, 2005, J TROP ECOL, V21, P121, DOI 10.1017/S0266467404001956
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Toda Y, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/9237136
   Ueda K., 2021, INATURALIST RES GRAD, DOI [10.15468/ab3s5x, DOI 10.15468/AB3S5X]
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
NR 27
TC 0
Z9 0
U1 0
U2 0
PU U S FISH & WILDLIFE SERVICE
PI SHEPHERDSTOWN
PA NATL CONSERVATION TRAINING CENTER, CONSERVATION LIBRARY, 698
   CONSERVATION WAY, SHEPHERDSTOWN, WV 25443 USA
SN 1944-687X
J9 J FISH WILDL MANAG
JI J. Fish Wildl. Manag.
PD DEC
PY 2021
VL 12
IS 2
BP 412
EP 421
DI 10.3996/JFWM-20-076
PG 10
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA XL2ZP
UT WOS:000728017600001
OA gold, Green Submitted
DA 2022-02-10
ER

PT C
AU Islam, SB
   Valles, D
AF Islam, Sazida B.
   Valles, Damian
BE Charkrabarti, S
   Paul, R
TI Identification of Wild Species in Texas from Camera-trap Images using
   Deep Neural Network for Conservation Monitoring
SO 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE
   (CCWC)
LA English
DT Proceedings Paper
CT 10th Annual Computing and Communication Workshop and Conference (CCWC)
CY JAN 06-08, 2020
CL Univ Nevada, Las Vegas, CA
SP IEEE Reg 1, IEEE Reg 6, IEEE USA, Inst Engn & Management, Univ Engn & Management, UNLV
HO Univ Nevada
DE DCNN; image classification; species recognition; camera traps; wildlife
   monitoring
AB Protection of endangered species requires continuous monitoring and updated information about the existence, location, and behavioral alterations in their habitat. Remotely activated camera or "camera traps" is a reliable and effective method of photo documentation of local population size, locomotion, and predator-prey relationships of wild species. However, manual data processing from a large volume of images and captured videos is extremely laborious, time-consuming, and expensive. The recent advancement of deep learning methods has shown great outcomes for object and species identification in images. This paper proposes an automated wildlife monitoring system by image classification using computer vision algorithms and machine learning techniques. The goal is to train and validate a Convolutional Neural Network (CNN) that will be able to detect Snakes, Lizards and Toads/Frogs from camera trap images. The initial experiment implies building a flexible CNN architecture with labeled images accumulated from standard benchmark datasets of different citizen science projects. After accessing satisfactory accuracy, new camera-trap imagery data (collected from Bastrop County, Texas) will be implemented to the model to detect species. The performance will be evaluated based on the accuracy of prediction within their classification. The suggested hardware and software framework will offer an efficient monitoring system, speed up wildlife investigation analysis, and formulate resource management decisions.
C1 [Islam, Sazida B.; Valles, Damian] Texas Sate Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.
RP Islam, SB (corresponding author), Texas Sate Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.
EM s_b608@txstate.edu; dvalles@txstate.edu
CR Al Bashit A, 2018, 2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON), P438, DOI 10.1109/IEMCON.2018.8615076
   [Anonymous], ZOONIVERSE DATABASE
   [Anonymous], IMAGENET LARGE SCALE
   Bashit A. A, 2019, INT S MEAS CONTR ROB
   Brownlee J, INTRO IMAGENET CHALL
   Brownlee J, PREPARE DATA MACHINE
   Che Yong Yeo, 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing & its Applications (CSPA 2011), P198, DOI 10.1109/CSPA.2011.5759872
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Gomez A., 2016, ARXIV160306169
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Kays R., 2010, ARXIV10095718V1CSNI
   Mech LD., 2002, CRITIQUE WILDLIFE RA
   Moreaux M, TOAD IMAGE DATA
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh M. S., 2017, ARXIV170305830V5
   Rosebrock A., 2017, DEEP LEARNING COMPUT
   Sahu R, 2019, VISUAL OBJECT TRACKI, DOI [10.5772/intechopen.88437, DOI 10.5772/INTECHOPEN.88437]
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yim J., 2017, P INT C DIG IM COMP, DOI 10.1109/DICTA.2017.8227427
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
NR 22
TC 0
Z9 0
U1 1
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-3783-4
PY 2020
BP 537
EP 542
AR 1570613456
PG 6
WC Computer Science, Theory & Methods; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BR7NI
UT WOS:000668567200085
DA 2022-02-10
ER

PT C
AU Zualkernan, IA
   Dhou, S
   Judas, J
   Sajun, AR
   Gomez, BR
   Hussain, LA
   Sakhnini, D
AF Zualkernan, Imran A.
   Dhou, Salam
   Judas, Jacky
   Sajun, Ali Reza
   Gomez, Brylle Ryan
   Hussain, Lana Alhaj
   Sakhnini, Dara
GP IEEE
TI Towards an IoT-based Deep Learning Architecture for Camera Trap Image
   Classification
SO 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF
   THINGS (GCAIOT)
LA English
DT Proceedings Paper
CT IEEE Global Conference on Artificial Intelligence and Internet of Things
   (GCAIoT)
CY DEC 12-16, 2020
CL ELECTR NETWORK
SP IEEE
DE deep learning; transfer learning; convolutional neural networks; animal
   classification; camera trap; wildlife monitoring; edge computing;
   TensorFlow lite; raspberry pi; IoT
AB Maintaining biodiversity is a key component of the United Nations (UN) "Life on Land" sustainability goal. Remote camera traps monitoring animals' movements support research in biodiversity. However, images from these camera traps are currently labeled manually resulting in high processing costs and long delays. This paper proposes an IoT-based system that leverages deep learning and edge computing to automatically label camera trap images and transmit this information to scientists in a timely manner. Inception-V3, MobileNet-V2, ResNet-18, and DenseNet-121 were trained on data consisting of 33,984 images taken during day and night with 6 animal classes. Inception-V3 yielded the highest macro average Fl-score of 0.93 and an accuracy of 94%. An IoT-based system was developed that directly captures images from a commercial camera trap, does the inference on the edge using a Raspberry Pi (RPi), and sends the classification results back to a cloud database system. A mobile App is used to monitor the camera images classified on camera traps in real-time. The RPi could easily sustain a rate of processing 1 image every 2 seconds with an average latency of 1.8 second/image. After capture and pre-processing, each inference took an average of 0.2 Millisecond/image on a RPi Model 4B.
C1 [Zualkernan, Imran A.; Dhou, Salam; Sajun, Ali Reza; Gomez, Brylle Ryan; Hussain, Lana Alhaj; Sakhnini, Dara] Amer Univ Sharjah, Comp Sci & Engn, Sharjah, U Arab Emirates.
   [Judas, Jacky] Emirates Nat WWF, Conservat Unit, Duai, U Arab Emirates.
RP Zualkernan, IA (corresponding author), Amer Univ Sharjah, Comp Sci & Engn, Sharjah, U Arab Emirates.
EM izualkernan@aus.edu; sdhou@aus.edu; jjudas@enwwf.ae; b00068908@aus.edu;
   b00067871@aus.edu; g00071496@aus.edu; g00068368@aus.edu
RI Zualkernan, Imran/B-6994-2018
OI Zualkernan, Imran/0000-0002-1048-5633; Sajun, Ali
   Reza/0000-0003-1270-3005
CR Al Balushi T, 2019, IEEE INTL CONF IND I, P1035, DOI 10.1109/INDIN41052.2019.8972063
   Allken V, 2019, ICES J MAR SCI, V76, P342, DOI 10.1093/icesjms/fsy147
   [Anonymous], 2018, LIVING PLANET REPORT
   Ayanzadeh A., 2018, MODIFIED DEEP NEURAL, DOI [10.20944/preprints201812.0232.v1, DOI 10.20944/PREPRINTS201812.0232.V1]
   Ayoub W, 2019, IEEE COMMUN SURV TUT, V21, P1561, DOI 10.1109/COMST.2018.2877382
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Curtin BH, 2019, 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P82, DOI 10.1109/UEMCON47517.2019.8993061
   Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
   Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN)
   Haupt J., 2018, LARGE SCALE PLANT CL
   Mac Aodha O, 2019, IEEE I CONF COMP VIS, P9595, DOI 10.1109/ICCV.2019.00969
   Mathur A., 2019, REAL TIME WILDLIFE D
   Monburinon N, 2019, PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (INCIT), P294, DOI 10.1109/INCIT.2019.8912138
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Popat Param, 2019, Information and Communication Technology for Intelligent Systems. Proceedings of ICTIS 2018. Smart Innovation, Systems and Technologies (SIST 106), P319, DOI 10.1007/978-981-13-1742-2_31
   Shi W., 2018, DOG BREED IDENTIFICA
   Sreedevi C. K., 2019, 3447740 SSRN
   Tan M., 2019, ARXIV190511946 CS ST
   Teto J. Kamdem, 2018, THESIS
   Thomassen S., 2017, EMBEDDED ANALYTICS A
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Wang H, 2019, IEEE INT CONF INDUST, P1796, DOI 10.1109/ICIT.2019.8755153
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Zimmerman G., 2019, WYSS CAMPAIGN NA SEP
NR 26
TC 3
Z9 3
U1 2
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-8420-3
PY 2020
BP 111
EP 116
DI 10.1109/GCAIOT51063.2020.9345858
PG 6
WC Computer Science, Artificial Intelligence; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BR9FX
UT WOS:000675459100019
DA 2022-02-10
ER

PT J
AU Tabak, MA
   Norouzzadeh, MS
   Wolfson, DW
   Sweeney, SJ
   Vercauteren, KC
   Snow, NP
   Halseth, JM
   Di Salvo, PA
   Lewis, JS
   White, MD
   Teton, B
   Beasley, JC
   Schlichting, PE
   Boughton, RK
   Wight, B
   Newkirk, ES
   Ivan, JS
   Odell, EA
   Brook, RK
   Lukacs, PM
   Moeller, AK
   Mandeville, EG
   Clune, J
   Miller, RS
AF Tabak, Michael A.
   Norouzzadeh, Mohammad S.
   Wolfson, David W.
   Sweeney, Steven J.
   Vercauteren, Kurt C.
   Snow, Nathan P.
   Halseth, Joseph M.
   Di Salvo, Paul A.
   Lewis, Jesse S.
   White, Michael D.
   Teton, Ben
   Beasley, James C.
   Schlichting, Peter E.
   Boughton, Raoul K.
   Wight, Bethany
   Newkirk, Eric S.
   Ivan, Jacob S.
   Odell, Eric A.
   Brook, Ryan K.
   Lukacs, Paul M.
   Moeller, Anna K.
   Mandeville, Elizabeth G.
   Clune, Jeff
   Miller, Ryan S.
TI Machine learning to classify animal species in camera trap images:
   Applications in ecology
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE artificial intelligence; camera trap; convolutional neural network; deep
   neural networks; image classification; machine learning; r package;
   remote sensing
AB Motion-activated cameras ("camera traps") are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or "out-of-distribution" in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82% accuracy and correctly identified 94% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.
C1 [Tabak, Michael A.; Wolfson, David W.; Sweeney, Steven J.; Di Salvo, Paul A.; Miller, Ryan S.] USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO 80526 USA.
   [Tabak, Michael A.; Mandeville, Elizabeth G.] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
   [Norouzzadeh, Mohammad S.; Clune, Jeff] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Vercauteren, Kurt C.; Snow, Nathan P.; Halseth, Joseph M.] USDA, Natl Wildlife Res Ctr, Ft Collins, CO USA.
   [Lewis, Jesse S.] Arizona State Univ, Coll Integrat Sci & Arts, Mesa, AZ USA.
   [White, Michael D.; Teton, Ben] Tejon Ranch Conservancy, Lebec, CA USA.
   [Beasley, James C.; Schlichting, Peter E.] Univ Georgia, Savannah River Ecol Lab, Warnell Sch Forestry & Nat Resources, Aiken, SC USA.
   [Boughton, Raoul K.; Wight, Bethany] Univ Florida, Wildlife Ecol & Conservat, Range Cattle Res & Educ Ctr, Ona, FL USA.
   [Newkirk, Eric S.; Ivan, Jacob S.; Odell, Eric A.] Colorado Pk & Wildlife, Ft Collins, CO USA.
   [Brook, Ryan K.] Univ Saskatchewan, Dept Anim & Poultry Sci, Saskatoon, SK, Canada.
   [Lukacs, Paul M.; Moeller, Anna K.] Univ Montana, WA Franke Coll Forestry & Conservat, Dept Ecosyst & Conservat Sci, Wildlife Biol Program, Missoula, MT 59812 USA.
   [Mandeville, Elizabeth G.] Univ Wyoming, Dept Bot, Laramie, WY 82071 USA.
RP Tabak, MA; Miller, RS (corresponding author), USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO 80526 USA.; Tabak, MA (corresponding author), Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
EM tabakma@gmail.com; ryan.s.miller@aphis.usda.gov
RI Ivan, Jacob S/R-9359-2018; Wolfson, David/AAJ-3485-2020
OI Wolfson, David/0000-0003-1098-9206; Snow, Nathan/0000-0002-5171-6493;
   Tabak, Michael/0000-0002-2986-7885
FU U.S. Department of EnergyUnited States Department of Energy (DOE)
   [DE-EM0004391]; USDA Animal and Plant Health Inspection Service,
   National Wildlife Research Center and Center for Epidemiology and Animal
   Health; Canadian Natural Science and Engineering Research CouncilNatural
   Sciences and Engineering Research Council of Canada (NSERC); University
   of Saskatchewan; Idaho Department of Game and Fish
FX U.S. Department of Energy, Grant/Award Number: DE-EM0004391; USDA Animal
   and Plant Health Inspection Service, National Wildlife Research Center
   and Center for Epidemiology and Animal Health; Colorado Parks and
   Wildlife; Canadian Natural Science and Engineering Research Council;
   University of Saskatchewan; Idaho Department of Game and Fish
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Advanced Research Computing Center, 2012, MOUNT MOR IBM SYST 1
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Rovero F., 2013, HYSTRIX ITALIAN J MA, V24, P585
   Scott AB, 2018, AVIAN DIS, V62, P65, DOI 10.1637/11761-101917-Reg.1
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 17
TC 100
Z9 101
U1 21
U2 86
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD APR
PY 2019
VL 10
IS 4
BP 585
EP 590
DI 10.1111/2041-210X.13120
PG 6
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HR3KR
UT WOS:000463036400013
OA Green Submitted, Green Published, Bronze
DA 2022-02-10
ER

PT J
AU Wei, WD
   Luo, G
   Ran, JH
   Li, J
AF Wei, Weideng
   Luo, Gai
   Ran, Jianghong
   Li, Jing
TI Zilong: A tool to identify empty images in camera-trap data
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Image classification; Non-machine learning algorithm; Software; Wildlife
   management
ID POPULATIONS; TIGER
AB The use of camera traps to research and monitor wildlife results in a large number of images. Many of the images are the result of a false trigger, resulting in an empty photo. Manually removing empty images is time-intensive and costly. To increase image processing efficiency, we present a non-machine learning algorithm to identify empty images in camera-trap data, and developed freely available software, Zilong. We applied Zilong to 53,598 camera-trap images from 24 sites and compared the results to a CNN-based (Convolutional Neural Network) R package MLWIC (Machine Learning for Wildlife Image Classification). Zilong correctly identified 87% of animal images and correctly identified 85% of empty images, while MLWIC identified 65% and 69%, respectively. Our results suggest that Zilong performed better than MLWIC on identifying empty images. Zilong performed well for most of sites (22/24), with reduced performance identifying empty images when there was vegetation swinging significantly in front of camera (2/24). By using Zilong, wildlife researchers can reduce time and resources required to review camera-trap images.
C1 [Wei, Weideng; Luo, Gai; Ran, Jianghong; Li, Jing] Sichuan Univ, Coll Life Sci, Key Lab Bioresources & Ecoenvironm, Minist Educ, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R China.
RP Li, J (corresponding author), Sichuan Univ, Coll Life Sci, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R China.
EM ljtjf@126.com
CR Bradski G., 2016, LEARNING OPENCV 3 CO
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Coster M, 2001, CEMENT CONCRETE COMP, V23, P133, DOI 10.1016/S0958-9465(00)00058-5
   Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
   Garrote G, 2011, EUR J WILDLIFE RES, V57, P355, DOI 10.1007/s10344-010-0440-7
   Janecka JE, 2011, J MAMMAL, V92, P771, DOI 10.1644/10-MAMM-A-036.1
   Jumeau J, 2017, ECOL EVOL, V7, P7399, DOI 10.1002/ece3.3149
   Li S, 2010, IBIS, V152, P299, DOI 10.1111/j.1474-919X.2009.00989.x
   Luo G, 2019, AVIAN RES, V10, DOI 10.1186/s40657-019-0144-y
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien TG, 2008, BIRD CONSERV INT, V18, pS144, DOI 10.1017/S0959270908000348
   R Core Team, 2018, STATS PACK LANG ENV
   Singh P, 2017, J MAMMAL, V98, P1453, DOI 10.1093/jmammal/gyx104
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak M.A., 2018, METHODS ECOL EVOL, V2018, P1
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Tan CKW, 2017, BIOL CONSERV, V206, P65, DOI 10.1016/j.biocon.2016.12.012
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 22
TC 6
Z9 6
U1 5
U2 13
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD JAN
PY 2020
VL 55
AR 101021
DI 10.1016/j.ecoinf.2019.101021
PG 7
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA KH9RO
UT WOS:000510985900007
DA 2022-02-10
ER

PT J
AU Shepley, A
   Falzon, G
   Meek, P
   Kwan, P
AF Shepley, Andrew
   Falzon, Greg
   Meek, Paul
   Kwan, Paul
TI Automated location invariant animal detection in camera trap images
   using publicly available data sources
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal identification; artificial intelligence; camera trap images;
   camera trapping; deep convolutional neural networks; deep learning;
   infusion; location invariance; wildlife ecology; wildlife monitoring
AB A time-consuming challenge faced by camera trap practitioners is the extraction of meaningful data from images to inform ecological management. An increasingly popular solution is automated image classification software. However, most solutions are not sufficiently robust to be deployed on a large scale due to lack of location invariance when transferring models between sites. This prevents optimal use of ecological data resulting in significant expenditure of time and resources to annotate and retrain deep learning models.
   We present a method ecologists can use to develop optimized location invariant camera trap object detectors by (a) evaluating publicly available image datasets characterized by high intradataset variability in training deep learning models for camera trap object detection and (b) using small subsets of camera trap images to optimize models for high accuracy domain-specific applications.
   We collected and annotated three datasets of images of striped hyena, rhinoceros, and pigs, from the image-sharing websites FlickR and iNaturalist (FiN), to train three object detection models. We compared the performance of these models to that of three models trained on the Wildlife Conservation Society and Camera CATalogue datasets, when tested on out-of-sample Snapshot Serengeti datasets. We then increased FiN model robustness by infusing small subsets of camera trap images into training.
   In all experiments, the mean Average Precision (mAP) of the FiN trained models was significantly higher (82.33%-88.59%) than that achieved by the models trained only on camera trap datasets (38.5%-66.74%). Infusion further improved mAP by 1.78%-32.08%.
   Ecologists can use FiN images for training deep learning object detection solutions for camera trap image processing to develop location invariant, robust, out-of-the-box software. Models can be further optimized by infusion of 5%-10% camera trap images into training data. This would allow AI technologies to be deployed on a large scale in ecological applications. Datasets and code related to this study are open source and available on this repository: .
C1 [Shepley, Andrew] Univ New England, Sch Sci & Technol, Armidale, NSW, Australia.
   [Falzon, Greg] Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA, Australia.
   [Meek, Paul] NSW Dept Primary Ind, Vertebrate Pest Res Unit, Coffs Harbour, NSW, Australia.
   [Meek, Paul] Univ New England, Sch Environm & Rural Sci, Armidale, NSW, Australia.
   [Kwan, Paul] Melbourne Inst Technol, Sch IT & Engn, Melbourne, Vic, Australia.
RP Shepley, A (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW, Australia.
EM asheple2@une.edu.au
OI Falzon, Gregory/0000-0002-1989-9357; Shepley, Andrew/0000-0001-7511-4967
FU University of New England; Australian Department of Agriculture and
   Water ResourcesAustralian Government; NSW Department of Primary
   Industries; NSW Environmental Trust; Centre for Invasive Animals
   Solutions
FX Centre for Invasive Animals Solutions; University of New England;
   Australian Department of Agriculture and Water Resources; NSW Department
   of Primary Industries; NSW Environmental Trust
CR Aradhya H.V.R., 2018, 2018 INT C COMM SIGN
   Beery S, 2018, RECOGNITION TERRA IN
   Christensen JH, 2018, 2018 IEEE/OES AUTONOMOUS UNDERWATER VEHICLE WORKSHOP (AUV)
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Clune, 2017, P NATL ACAD SCI USA, V115
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Gibb R, 2019, METHODS ECOL EVOL, V10, P169, DOI 10.1111/2041-210X.13101
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Kellenberger B, 2017, JOINT URB REMOTE SEN
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Maurice, 2019, SURV STAT PANG CAM T
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28
   Rodin CD, 2018, IEEE IJCNN
   Rovero F., 2016, CAMERA TRAPPING WILD
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Singh P., 2020, 2020 IEEE SW S IM AN
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tambe M., 2020, 2020 IEEE WINT C APP
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   Vedaldi, 2017, LEARNING MULTIPLE VI, P506
   Wang G., 2017, 2017 IEEE WINT C APP
   Wang X., 2019, 2019 IEEE CVF C COMP
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wearn OR, 2017, WWF CONSERVATION TEC, V1
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xu BB, 2020, INT J REMOTE SENS, V41, P8121, DOI 10.1080/01431161.2020.1734245
   Yang XY, 2019, IEEE INT CONF COMP V, P255, DOI 10.1109/ICCVW.2019.00034
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zisserman, 2007, DATASET ISSUES OBJEC, V4170, P29
NR 48
TC 1
Z9 1
U1 9
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD MAY
PY 2021
VL 11
IS 9
BP 4494
EP 4506
DI 10.1002/ece3.7344
EA MAR 2021
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA RW6CD
UT WOS:000626984400001
PM 33976825
OA Green Published, gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Norouzzadeh, MS
   Morris, D
   Beery, S
   Joshi, N
   Jojic, N
   Clune, J
AF Norouzzadeh, Mohammad Sadegh
   Morris, Dan
   Beery, Sara
   Joshi, Neel
   Jojic, Nebojsa
   Clune, Jeff
TI A deep active learning system for species identification and counting in
   camera trap images
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE active learning; camera trap images; computer vision; deep learning;
   deep neural networks
AB A typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical conservation questions may be answered too slowly to support decision-making. Recent studies demonstrated the potential for computer vision to dramatically increase efficiency in image-based biodiversity surveys; however, the literature has focused on projects with a large set of labelled training images, and hence many projects with a smaller set of labelled images cannot benefit from existing machine learning techniques. Furthermore, even sizable projects have struggled to adopt computer vision methods because classification models overfit to specific image backgrounds (i.e. camera locations).
   In this paper, we combine the power of machine intelligence and human intelligence via a novel active learning system to minimize the manual work required to train a computer vision model. Furthermore, we utilize object detection models and transfer learning to prevent overfitting to camera locations. To our knowledge, this is the first work to apply an active learning approach to camera trap images.
   Our proposed scheme can match state-of-the-art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labelling effort by over 99.5%. Our trained models are also less dependent on background pixels, since they operate only on cropped regions around animals.
   The proposed active deep learning scheme can significantly reduce the manual labour required to extract information from camera trap images. Automation of information extraction will not only benefit existing camera trap projects, but can also catalyse the deployment of larger camera trap arrays.
C1 [Norouzzadeh, Mohammad Sadegh; Morris, Dan; Beery, Sara] Microsoft AI Earth, Redmond, WA 98052 USA.
   [Norouzzadeh, Mohammad Sadegh; Clune, Jeff] Univ Wyoming, Comp Sci Dept, Laramie, WY 82071 USA.
   [Beery, Sara] CALTECH, Comp Sci Dept, Pasadena, CA USA.
   [Joshi, Neel; Jojic, Nebojsa] Microsoft Res, Redmond, WA USA.
   [Clune, Jeff] OpenAI, San Francisco, CA USA.
RP Morris, D (corresponding author), Microsoft AI Earth, Redmond, WA 98052 USA.
EM dan@microsoft.com
OI Norouzzadeh, Mohammad Sadegh/0000-0002-2983-9374
CR AI for Earth Microsoft, 2018, DET MOD
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   Beery S., 2018, P EUR C COMP VIS
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cho K., 2014, ARXIV14061078
   Dasgupta S., 2008, P INT C MACH LEARN, P208
   Elith J, 2010, METHODS ECOL EVOL, V1, P330, DOI 10.1111/j.2041-210X.2010.00036.x
   eMammal, EMAMMAL PROJ
   Forrester Tavis, 2013, P 98 ESA ANN CONV 20
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guo YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P823
   Hagan M.T., 1996, NEURAL NETWORK DESIG, V20
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hecht-Nielsen R., 1989, INT JOINT C NEUR NET
   Hermans A., 2017, ARXIV ARXIV170307737
   Hinton G., 2012, COURSERA NEURAL NETW, DOI DOI 10.1007/BF00992698
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Koch G, 2015, RADICAL PHILOS, P28
   Krizhevsky A., 2012, 2012 ADV NEUR INF PR
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3
   Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Miao Z., 2019, SCI REPORTS, V90
   Mohri M., 2012, FDN MACHINE LEARNING
   Norouzzadeh M. S., 2020, NOROUZZADEH ET AL ME, DOI [10.5281/zenodo.4052020, DOI 10.5281/ZENODO.4052020]
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sener O., 2017, ARXIV170800489
   Settles B., 2008, C EMP METH NAT LANG, P1070
   Settles Burr, 2009, TECHNICAL REPORT
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Southwood T., 2009, ECOLOGICAL METHODS
   Sutskever I., 2014, NIPS, P3104
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tikhonov G, 2017, METHODS ECOL EVOL, V8, P443, DOI 10.1111/2041-210X.12723
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xu Z, 2003, LECT NOTES COMPUT SC, V2633, P393
   Yosinski J., 2014, 2014 ADV NEURAL INFO
NR 45
TC 9
Z9 9
U1 5
U2 21
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JAN
PY 2021
VL 12
IS 1
BP 150
EP 161
DI 10.1111/2041-210X.13504
EA NOV 2020
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA PU3OB
UT WOS:000590688700001
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Schneider, S
   Taylor, GW
   Kremer, SC
AF Schneider, Stefan
   Taylor, Graham W.
   Kremer, Stefan C.
GP IEEE
TI Deep Learning Object Detection Methods for Ecological Camera Trap Data
SO 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV)
LA English
DT Proceedings Paper
CT 15th Conference on Computer and Robot Vision (CRV)
CY MAY 08-11, 2018
CL Toronto, CANADA
SP Canadian Image Proc & Pattern Recognit Soc, Assoc Canadienne Traitement Images Reconnaissance Formes, MDA, Modiface, NextAI, SPORTLOGiQ, StradigiAI, York Univ Vis Sci Applicat Program, York Univ Ctr Vis Res, ElementAI, EPSON, Miovision, Trans Plan
ID IDENTIFICATION
AB Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem's animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0% and 76.7% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
   [Taylor, Graham W.] Univ Guelph, Sch Engn, Guelph, ON, Canada.
   [Taylor, Graham W.] Vector Inst Artificial Intelligence, Toronto, ON, Canada.
   [Taylor, Graham W.] Canadian Inst Adv Res, Toronto, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca; gwtaylor@uoguelph.ca; skremer@uoguelph.ca
CR Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   BALFOORT HW, 1992, J PLANKTON RES, V14, P575, DOI 10.1093/plankt/14.4.575
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   CHAO A, 1989, BIOMETRICS, V45, P427, DOI 10.2307/2531487
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Fukushima K., 1979, ELECTR COMMUN JPN, V62, P11
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   GYSEL LESLIE W., 1956, JOUR WILDLIFE MANAGEMENT, V20, P451, DOI 10.2307/3797161
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343
   JEFFRIES HP, 1984, MAR BIOL, V78, P329, DOI 10.1007/BF00393019
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kotsiantis S. B., 2007, SUPERVISED MACHINE L
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Norouzzadeh M. S., 2017, ARXIV170305830V5
   Nowozin S, 2014, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2014.77
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   ROBSON D. S., 1964, TRANS AMER FISH SOC, V93, P215, DOI 10.1577/1548-8659(1964)93[215:SSIPME]2.0.CO;2
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   SIMPSON R, 1991, IEEE CONFERENCE ON NEURAL NETWORKS FOR OCEAN ENGINEERING, P223, DOI 10.1109/ICNN.1991.163354
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 34
TC 30
Z9 30
U1 2
U2 12
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-6481-0
PY 2018
BP 321
EP 328
DI 10.1109/CRV.2018.00052
PG 8
WC Computer Science, Theory & Methods; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BL9NK
UT WOS:000457719100042
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Adam, M
   Tomasek, P
   Lehejcek, J
   Trojan, J
   Junek, T
AF Adam, Matyas
   Tomasek, Pavel
   Lehejcek, Jiri
   Trojan, Jakub
   Junek, Tomas
TI The Role of Citizen Science and Deep Learning in Camera Trapping
SO SUSTAINABILITY
LA English
DT Article
DE artificial intelligence; crowdsourcing; environmental monitoring;
   conceptual framework; wildlife
ID FUTURE; MANAGEMENT; SOFTWARE
AB Camera traps are increasingly one of the fundamental pillars of environmental monitoring and management. Even outside the scientific community, thousands of camera traps in the hands of citizens may offer valuable data on terrestrial vertebrate fauna, bycatch data in particular, when guided according to already employed standards. This provides a promising setting for Citizen Science initiatives. Here, we suggest a possible pathway for isolated observations to be aggregated into a single database that respects the existing standards (with a proposed extension). Our approach aims to show a new perspective and to update the recent progress in engaging the enthusiasm of citizen scientists and in including machine learning processes into image classification in camera trap research. This approach (combining machine learning and the input from citizen scientists) may significantly assist in streamlining the processing of camera trap data while simultaneously raising public environmental awareness. We have thus developed a conceptual framework and analytical concept for a web-based camera trap database, incorporating the above-mentioned aspects that respect a combination of the roles of experts' and citizens' evaluations, the way of training a neural network and adding a taxon complexity index. This initiative could well serve scientists and the general public, as well as assisting public authorities to efficiently set spatially and temporarily well-targeted conservation policies.
C1 [Adam, Matyas; Tomasek, Pavel; Lehejcek, Jiri] Tomas Bata Univ Zlin, Fac Logist & Crisis Management, Uherske Hradiste 68601, Czech Republic.
   [Trojan, Jakub] Czech Acad Sci, Inst Geon, Dept Environm Geog, Brno 60200, Czech Republic.
   [Junek, Tomas] Czech Univ Life Sci Prague, Fac Environm Sci, Prague 16500, Czech Republic.
RP Adam, M (corresponding author), Tomas Bata Univ Zlin, Fac Logist & Crisis Management, Uherske Hradiste 68601, Czech Republic.
EM madam@utb.cz; tomasek@utb.cz; lehejcek@utb.cz; jakub.trojan@ugn.cas.cz;
   tjunek@fzp.czu.cz
RI ; Trojan, Jakub/D-6643-2015
OI , Pavel/0000-0001-8404-3486; Adam, Matyas/0000-0003-3602-4955; Trojan,
   Jakub/0000-0002-6658-8586
FU TA C. R [TG03010052]; INTER-COST project Geographical Aspects of Citizen
   Science: mapping trends, scientific potential and societal impacts in
   the Czech Republic [LTC18067, CA15212A]
FX Development of the analytical model and the prototype of the Czech
   national CT database was supported by TA C. R grant TG03010052. The
   paper was also supported by the INTER-COST project Geographical Aspects
   of Citizen Science: mapping trends, scientific potential and societal
   impacts in the Czech Republic (No. LTC18067), conducted under the COST
   EU action CA15212A Framework in Science and Technology to promote
   creativity, scientific literacy, and innovation throughout Europe.
CR [Anonymous], 2007, ACCESS BIOLOGICAL CO
   Apps PJ, 2018, AFR J ECOL, V56, P702, DOI 10.1111/aje.12563
   Bauerfeind R., 2020, ZOONOSES INFECT DIS
   Berger-Wolf T.Y., 2017, ARXIV PREPRINT ARXIV
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Cadman M., 2014, PUBLISHING CAMERA TR
   Catlin-Groves C.L., 2012, INT J ZOOL, V2012, P1, DOI [10.1155/2012/349630, DOI 10.1155/2012/349630]
   Ceccaroni L., 2019, CITIZ SCI THEORY PRA, V4, P29, DOI [10.5334/cstp.241, DOI 10.5334/CSTP.241]
   Colbert-Lewis D, 2016, REF REV, V30, P29
   Deb D, 2018, INT CONF BIOMETR THE
   Dickman AJ, 2010, ANIM CONSERV, V13, P458, DOI 10.1111/j.1469-1795.2010.00368.x
   Distefano E., 2005, HUMAN WILDLIFE CONFL
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Franzen M., 2021, SCI CITIZEN SCI, P183, DOI [DOI 10.1007/978-3-030-58278-4_10, 10.1007/978-3-030-58278-4_10]
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Goldsmith F.B., 2012, MONITORING CONSERVAT
   Green SE, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010132
   Hampton SE, 2013, FRONT ECOL ENVIRON, V11, P156, DOI 10.1890/120103
   Heilbrun RD, 2006, WILDLIFE SOC B, V34, P69, DOI 10.2193/0091-7648(2006)34[69:EBAUAT]2.0.CO;2
   Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
   Korschens M., 2018, AUTOMATIC IDENTIFICA
   Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
   Lyons JE, 2008, J WILDLIFE MANAGE, V72, P1683, DOI 10.2193/2008-141
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek PD, 2019, REMOTE SENS ECOL CON, V5, P160, DOI 10.1002/rse2.96
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Nipko RB, 2020, WILDLIFE SOC B, V44, P424, DOI 10.1002/wsb.1086
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Parsons AW, 2018, PEERJ, V6, DOI 10.7717/peerj.4536
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Schipper J, 2008, SCIENCE, V322, P225, DOI 10.1126/science.1165115
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Sutherland WJ, 2015, BIOL J LINN SOC, V115, P779, DOI 10.1111/bij.12576
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Terry Andrew M.R., 2005, Frontiers in Zoology, V2, P1
   Vincent C, 2001, MAMMALIA, V65, P363, DOI 10.1515/mamm.2001.65.3.363
   Waits LP, 2005, J WILDLIFE MANAGE, V69, P1419, DOI 10.2193/0022-541X(2005)69[1419:NGSTFW]2.0.CO;2
   Wang SW, 2009, BIOL CONSERV, V142, P606, DOI 10.1016/j.biocon.2008.11.023
   Welbourne DJ, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0226913
   Welbourne DJ, 2016, REMOTE SENS ECOL CON, V2, P77, DOI 10.1002/rse2.20
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yang DQ, 2021, ECOL EVOL, V11, P7591, DOI 10.1002/ece3.7591
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
NR 50
TC 1
Z9 1
U1 6
U2 6
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2071-1050
J9 SUSTAINABILITY-BASEL
JI Sustainability
PD SEP
PY 2021
VL 13
IS 18
AR 10287
DI 10.3390/su131810287
PG 14
WC Green & Sustainable Science & Technology; Environmental Sciences;
   Environmental Studies
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Science & Technology - Other Topics; Environmental Sciences & Ecology
GA UZ2NO
UT WOS:000702047200001
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Carl, C
   Schonfeld, F
   Profft, I
   Klamm, A
   Landgraf, D
AF Carl, Christin
   Schoenfeld, Fiona
   Profft, Ingolf
   Klamm, Alisa
   Landgraf, Dirk
TI Automated detection of European wild mammal species in camera trap
   images with an existing and pre-trained computer vision model
SO EUROPEAN JOURNAL OF WILDLIFE RESEARCH
LA English
DT Article
DE Computer vision; Image analysis; Camera trap; Pre-trained model; Wild
   mammal species
AB The use of camera traps is a nonintrusive monitoring method to obtain valuable information about the appearance and behavior of wild animals. However, each study generates thousands of pictures and extracting information remains mostly an expensive, time-consuming manual task. Nevertheless, image recognition and analyzing technologies combined with machine learning algorithms, particularly deep learning models, improve and speed up the analysis process. Therefore, we tested the usability of a pre-trained deep learning model available on the TensorFlow hub-FasterRCNN+InceptionResNet V2 network applied to images of ten different European wild mammal species such as wild boar (Sus scrofa), roe deer (Capreolus capreolus), or red fox (Vulpes vulpes) in color as well as black and white infrared images. We found that the detection rate of the correct region of interest (region of the animal) was 94%. The classification accuracy was 71% for the correct species' name as mammals and 93% for the correct species or higher taxonomic ranks such as "carnivore" as order. In 7% of cases, the classification was incorrect as the wrong species' name was classified. In this technical note, we have shown the potential of an existing and pre-trained image classification model for wildlife animal detection, classification, and analysis. A specific training of the model on European wild mammal species could further increase the detection and classification accuracy of the models. Analysis of camera trap images could thus become considerably faster, less expensive, and more efficient.
C1 [Carl, Christin; Schoenfeld, Fiona; Landgraf, Dirk] Univ Appl Sci Erfurt, Forestry & Ecosyst Management, Leipziger Str 77, D-99085 Erfurt, Germany.
   [Profft, Ingolf] ThuringenForst AoR, Forstliches Forsch & Kompetenzzentrum, Jagerstr 1, D-99867 Gotha, Germany.
   [Klamm, Alisa] Natl Pk Verwaltung Hainich, Bei Marktkirche 9, D-99947 Bad Langensalza, Germany.
RP Carl, C (corresponding author), Univ Appl Sci Erfurt, Forestry & Ecosyst Management, Leipziger Str 77, D-99085 Erfurt, Germany.
EM christin.carl@fh-erfurt.de; fiona.schoenfeld@fh-erfurt.de;
   Ingolf.Profft@forst.thueringen.de; Alisa.Klamm@nnl.thueringen.de;
   dirk.landgraf@fh-erfurt.de
RI Landgraf, Dirk/AAD-8062-2020
OI Landgraf, Dirk/0000-0002-1891-3751
FU University of Applied Sciences Erfurt
FX This research was supported by the University of Applied Sciences Erfurt
   (FHE).
CR Abadi, 2015, TENSORFLOW LARGE SCA
   Beery S., 2019, ARXIV190405986
   Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
   Casaer J, 2019, BIODIVERSITY INFORM
   Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
   Google LLC, 2019, OP IM DAT V4 CC 4 0
   Google LLC Colaboratory, 2019, WELC COL
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Hui Jonathan, 2018, OBJECT DETECTION SPE
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Inik o, 2018, J NEW RESULTS SCI, V7, P9
   Jayakumar R., 2020, J CRIT REV, V7, P434
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kluyver T, 2016, POSITIONING AND POWER IN ACADEMIC PUBLISHING: PLAYERS, AGENTS AND AGENDAS, P87, DOI 10.3233/978-1-61499-649-1-87
   LILA BC, 2019, LAB INF LIB AL BIOL
   Lundh F, 2016, PILLOW PYTHON IMAGIN
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Nationalparkverwaltung Hainich FFK Gotha, 2019, SCHW HAIN
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Oliphant T. E, 2006, A GUIDE TO NUMPY, P85
   Python Software Foundation, 2019, PYTH STAND LIBR TEMP
   Python Software Foundation, 2019, PYTH STAND LIB IO CO
   Python Software Foundation, 2019, PYTH STAND LIB URLL
   Python Software Foundation, 2019, PYTH STAND LIB TIM T
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Weingarth K, 2011, GRENZUBERSCHREITENDE
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 29
TC 3
Z9 3
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1612-4642
EI 1439-0574
J9 EUR J WILDLIFE RES
JI Eur. J. Wildl. Res.
PD JUL 14
PY 2020
VL 66
IS 4
AR 62
DI 10.1007/s10344-020-01404-y
PG 7
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA MJ1WH
UT WOS:000547883800001
DA 2022-02-10
ER

PT J
AU Yang, DQ
   Ren, GP
   Tan, K
   Huang, ZP
   Li, DP
   Li, XW
   Wang, JM
   Chen, BH
   Xiao, W
AF Yang, Deng-Qi
   Ren, Guo-Peng
   Tan, Kun
   Huang, Zhi-Pang
   Li, De-Pin
   Li, Xiao-Wei
   Wang, Jian-Ming
   Chen, Ben-Hui
   Xiao, Wen
TI An Adaptive Automatic Approach to Filtering Empty Images from Camera
   Traps Using a Deep Learning Model
SO WILDLIFE SOCIETY BULLETIN
LA English
DT Article
DE Artificial intelligence; camera traps; deep learning; empty images;
   image recognition; wildlife monitoring
ID ASSOCIATIONS
AB Camera traps are widely used in wildlife surveys because they are non-invasive, low-cost, and highly efficient. Camera traps deployed in the wild often produce large datasets, making it increasingly difficult to manually classify images. Deep learning is a machine learning method that provides a tool to automatically identify images, but it requires labeled training samples and high-performance servers with multiple Graphics Processing Units (GPUs). However, manually preparing large-scale training images for training deep learning models is labor intensive, and the high-performance servers with multiple GPUs are often not available for wildlife management agencies and field researchers. Our study explores an adaptive deep learning method to use small-scale training sets and a commonly-available, desktop personal computer (PC) to achieve automatic filtering of empty camera images. Our results showed that by using 29,192 training samples, the overall error, commission error, and omission error of the proposed method on a PC were 2.69%, 6.82%, and 6.45%, respectively. Moreover, the accuracy of our method can be adaptively improved on PCs in actual ecological monitoring projects, which would benefit researchers in field settings when only a PC is available. (c) 2021 The Wildlife Society.
C1 [Yang, Deng-Qi; Wang, Jian-Ming; Chen, Ben-Hui] Dali Univ, Dept Math & Comp Sci, Dali 671003, Yunnan, Peoples R China.
   [Ren, Guo-Peng; Tan, Kun; Huang, Zhi-Pang; Li, De-Pin; Xiao, Wen] Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali 671003, Yunnan, Peoples R China.
   [Li, Xiao-Wei] Dali Univ, Data Secur & Applicat Innovat Team, Dali 671003, Yunnan, Peoples R China.
RP Ren, GP (corresponding author), Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali 671003, Yunnan, Peoples R China.
EM rengp@eastern-himalaya.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [31960119, 31860164, 31860168, 61902049];
   Yunnan Provincial Science and Technology Department University Joint
   Project [2017FH001-027, 2018FH001-063, 2018FH001-106]; Innovative
   Project of Dali University [ZKLX2020308]
FX We appreciate the support of the National Natural Science Foundation of
   China (31960119, 31860164, 31860168, 61902049), the Yunnan Provincial
   Science and Technology Department University Joint Project
   (2017FH001-027, 2018FH001-063,2018FH001-106) and the Innovative Project
   of Dali University (ZKLX2020308). We thank J. McRoberts (Associate
   Editor), A. Knipps (Editorial Assistant), and 2 reviewers for their
   comments, which improved the manuscript.
CR Dertien JS, 2017, J WILDLIFE MANAGE, V81, P1457, DOI 10.1002/jwmg.21308
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Forsyth DM, 2019, J WILDLIFE MANAGE, V83, P1090, DOI 10.1002/jwmg.21675
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   Myers N, 1988, Environmentalist, V8, P187, DOI 10.1007/BF02240252
   Myers N, 1990, Environmentalist, V10, P243, DOI 10.1007/BF02239720
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thorne ED, 2017, J WILDLIFE MANAGE, V81, P1042, DOI 10.1002/jwmg.21282
   Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
   Webb SM, 2016, J WILDLIFE MANAGE, V80, P1461, DOI 10.1002/jwmg.21137
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yang D., 2019, CAMERA TRAP IMAGES C
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 25
TC 0
Z9 0
U1 1
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2328-5540
J9 WILDLIFE SOC B
JI Wildl. Soc. Bull.
PD JUN
PY 2021
VL 45
IS 2
BP 230
EP 236
DI 10.1002/wsb.1176
EA MAY 2021
PG 7
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA TY3RZ
UT WOS:000650299700001
DA 2022-02-10
ER

PT J
AU Tabak, MA
   Norouzzadeh, MS
   Wolfson, DW
   Newton, EJ
   Boughton, RK
   Ivan, JS
   Odell, EA
   Newkirk, ES
   Conrey, RY
   Stenglein, J
   Iannarilli, F
   Erb, J
   Brook, RK
   Davis, AJ
   Lewis, J
   Walsh, DP
   Beasley, JC
   VerCauteren, KC
   Clune, J
   Miller, RS
AF Tabak, Michael A.
   Norouzzadeh, Mohammad S.
   Wolfson, David W.
   Newton, Erica J.
   Boughton, Raoul K.
   Ivan, Jacob S.
   Odell, Eric A.
   Newkirk, Eric S.
   Conrey, Reesa Y.
   Stenglein, Jennifer
   Iannarilli, Fabiola
   Erb, John
   Brook, Ryan K.
   Davis, Amy J.
   Lewis, Jesse
   Walsh, Daniel P.
   Beasley, James C.
   VerCauteren, Kurt C.
   Clune, Jeff
   Miller, Ryan S.
TI Improving the accessibility and transferability of machine learning
   algorithms for identification of animals in camera trap images: MLWIC2
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE computer vision; deep convolutional neural networks; image
   classification; machine learning; motion-activated camera; R package;
   remote sensing; species identification
AB Motion-activated wildlife cameras (or "camera traps") are frequently used to remotely and noninvasively observe animals. The vast number of images collected from camera trap projects has prompted some biologists to employ machine learning algorithms to automatically recognize species in these images, or at least filter-out images that do not contain animals. These approaches are often limited by model transferability, as a model trained to recognize species from one location might not work as well for the same species in different locations. Furthermore, these methods often require advanced computational skills, making them inaccessible to many biologists. We used 3 million camera trap images from 18 studies in 10 states across the United States of America to train two deep neural networks, one that recognizes 58 species, the "species model," and one that determines if an image is empty or if it contains an animal, the "empty-animal model." Our species model and empty-animal model had accuracies of 96.8% and 97.3%, respectively. Furthermore, the models performed well on some out-of-sample datasets, as the species model had 91% accuracy on species from Canada (accuracy range 36%-91% across all out-of-sample datasets) and the empty-animal model achieved an accuracy of 91%-94% on out-of-sample datasets from different continents. Our software addresses some of the limitations of using machine learning to classify images from camera traps. By including many species from several locations, our species model is potentially applicable to many camera trap studies in North America. We also found that our empty-animal model can facilitate removal of images without animals globally. We provide the trained models in an R package (MLWIC2: Machine Learning for Wildlife Image Classification in R), which contains Shiny Applications that allow scientists with minimal programming experience to use trained models and train new models in six neural network architectures with varying depths.
C1 [Tabak, Michael A.] Quantitat Sci Consulting LLC, Laramie, WY 82072 USA.
   [Tabak, Michael A.] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
   [Norouzzadeh, Mohammad S.] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Wolfson, David W.] Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, Minnesota Cooperat Fish & Wildlife Res Unit, St Paul, MN 55108 USA.
   [Newton, Erica J.] Ontario Minist Nat Resources & Forestry, Wildlife Res & Monitoring Sect, Peterborough, ON, Canada.
   [Boughton, Raoul K.] Univ Florida, Range Cattle Res & Educ Ctr, Wildlife Ecol & Conservat, Ona, FL USA.
   [Ivan, Jacob S.; Odell, Eric A.; Newkirk, Eric S.; Conrey, Reesa Y.] Colorado Pk & Wildlife, Ft Collins, CO USA.
   [Stenglein, Jennifer] Wisconsin Dept Nat Resources, Madison, WI USA.
   [Iannarilli, Fabiola] Univ Minnesota, Conservat Sci Grad Program, St Paul, MN 55108 USA.
   [Erb, John] Minnesota Dept Nat Resources, Forest Wildlife Populat & Res Grp, Grand Rapids, MN USA.
   [Brook, Ryan K.] Univ Saskatchewan, Dept Anim & Poultry Sci, Saskatoon, SK, Canada.
   [Davis, Amy J.] USDA, Natl Wildlife Res Ctr, Ft Collins, CO USA.
   [Lewis, Jesse] Arizona State Univ, Coll Integrat Sci & Arts, Mesa, AZ USA.
   [Walsh, Daniel P.] US Geol Survey, Natl Wildlife Hlth Ctr, Madison, WI USA.
   [Beasley, James C.] Univ Georgia, Savannah River Ecol Lab, Warnell Sch Forestry & Nat Resources, Aiken, SC USA.
   [VerCauteren, Kurt C.] US Anim & Plant Hlth Inspect Serv, Natl Wildlife Res Ctr, USDA, Ft Collins, CO USA.
   [Clune, Jeff] OpenAI, San Francisco, CA USA.
   [Miller, Ryan S.] USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO USA.
RP Tabak, MA (corresponding author), Quantitat Sci Consulting LLC, Laramie, WY 82072 USA.
EM tabakma@gmail.com
RI Iannarilli, Fabiola/AAG-7774-2021; Davis, Amy/ABE-2065-2021
OI Iannarilli, Fabiola/0000-0002-7018-3557; Tabak,
   Michael/0000-0002-2986-7885; Stenglein, Jennifer/0000-0003-4578-5908
FU DOEUnited States Department of Energy (DOE) [DE-EM0004391]; USFWS
   Pittman-Robertson Wildlife Restoration Program; Wisconsin Department of
   Natural Resources
FX Contributions of JCB were partially supported by the DOE under Award
   Number DE-EM0004391 to the University of Georgia Research Foundation.
   Support for this research was provided by the USFWS Pittman-Robertson
   Wildlife Restoration Program and Wisconsin Department of Natural
   Resources. For supplying camera trap images, we thank USDA Forest
   Service: Rocky Mountain Research station; Montana Fish, Wildlife and
   Parks; Wyoming Game and Fish Department; Washington Department of Fish
   and Wildlife; Idaho Department of Fish and Game; and Woodland Park Zoo.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Advanced Research Computing Center, 2018, TET COMP ENV, DOI [10.15786/M2FY47, DOI 10.15786/M2FY47, 10. 15786/M2FY47]
   Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
   Beery S., 2019, EFFICIENT PIPELINE C
   Beery S., 2018, P EUR C COMP VIS ECC, P456
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guillera-Arroita G, 2017, METHODS ECOL EVOL, V8, P1081, DOI 10.1111/2041-210X.12743
   Harvey P., 2016, EXIFTOOL
   Huang J., 2020, P IEEE CVF C COMP VI, P13075
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   McIntyre T, 2020, WILDLIFE RES, V47, P177, DOI 10.1071/WR19040
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Norouzzadeh M. S., 2019, ARXIV191009716CSEESS
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak M. A., 2020, DRYAD, DOI [10.5061/dryad.x95x69pfx, DOI 10.5061/DRYAD.X95X69PFX]
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yousif H., 2019, IEEE T CIRCUITS SYST
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 26
TC 5
Z9 5
U1 6
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD OCT
PY 2020
VL 10
IS 19
BP 10374
EP 10383
DI 10.1002/ece3.6692
EA SEP 2020
PG 10
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA NY7YW
UT WOS:000569520700001
PM 33072266
OA Green Submitted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Schneider, S
   Taylor, GW
   Linquist, S
   Kremer, SC
AF Schneider, Stefan
   Taylor, Graham W.
   Linquist, Stefan
   Kremer, Stefan C.
TI Past, present and future approaches using computer vision for animal
   re-identification from camera trap data
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE animal reidentification; camera traps; computer vision; convolutional
   networks; deep learning; density estimation; monitoring; object
   detection
ID PATTERN-MATCHING ALGORITHM; IDENTIFICATION
AB The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Dept Comp Sci, Guelph, ON, Canada.
   [Taylor, Graham W.] Univ Guelph, Dept Engn, Guelph, ON, Canada.
   [Linquist, Stefan] Univ Guelph, Dept Philosophy, Guelph, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Dept Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca
OI Kremer, Stefan C./0000-0002-3667-4379
CR Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Arzoumanian Z, 2005, J APPL ECOL, V42, P999, DOI 10.1111/j.1365-2664.2005.01117.x
   Barz B., 2018, ARXIV181204418
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Burghardt T., 2007, 6 INT PENG C BIRDS T
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Carter SJB, 2014, J EXP MAR BIOL ECOL, V452, P105, DOI 10.1016/j.jembe.2013.12.010
   Deb D., 2018, ARXIV180408790
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Fukushima K., 1979, ELECTR COMMUN JPN, V62, P11
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   GROTH EJ, 1986, ASTRON J, V91, P1244, DOI 10.1086/114099
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Hiby L., 1990, Reports of the International Whaling Commission Special Issue, P57
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Hillman G.R., 2003, Aquatic Mammals, V29, P117, DOI 10.1578/016754203101023960
   Hinton GE, 2017, NEURIPS, DOI DOI 10.1371/JOURNAL.PONE.0035195
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   Huele R, 1998, MAR MAMMAL SCI, V14, P143, DOI 10.1111/j.1748-7692.1998.tb00697.x
   Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
   Kaggle, 2018, HUMPB WHAL ID CHALL
   Kelly MJ, 2001, J MAMMAL, V82, P440, DOI 10.1644/1545-1542(2001)082<0440:CAPMIS>2.0.CO;2
   Keutzer, 2014, ARXIV14041869
   Krebs C.J., 1989, ECOLOGICAL METHODOLO
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Martinel N, 2015, IEEE T PATTERN ANAL, V37, P1656, DOI 10.1109/TPAMI.2014.2377748
   Meek P. D., 2013, WILDLIFE BIOL PRACTI, V9, P461
   Mizroch S., 1990, COMPUTER ASSISTED PH, V12, P63
   Norouzzadeh M. S., 2017, P NATL ACAD SCI US
   Nowozin S, 2014, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2014.77
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pitts W., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   Ravela S., 2004, P AS C COMP VIS KI S, P742
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Rodner E., 2016, ARXIV161006756
   Rodner E., 2015, ARXIV150700913
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Scheel D, 2017, MAR FRESHW BEHAV PHY, V50, P285, DOI 10.1080/10236244.2017.1369851
   Schneider S., 2018, C COMP ROB VIS
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sherley Richard B., 2010, Endangered Species Research, V11, P101, DOI 10.3354/esr00267
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Souri Y., 2015, P 3 WORKSH FIN GRAIN
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C., 2015, P IEEE C COMP VIS PA, P461
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Town C, 2013, ECOL EVOL, V3, P1902, DOI 10.1002/ece3.587
   Whitehead H., 1990, Reports of the International Whaling Commission Special Issue, P71
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
NR 61
TC 38
Z9 38
U1 5
U2 55
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD APR
PY 2019
VL 10
IS 4
BP 461
EP 470
DI 10.1111/2041-210X.13133
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HR3KR
UT WOS:000463036400002
OA Bronze, Green Submitted
DA 2022-02-10
ER

PT J
AU Schneider, S
   Greenberg, S
   Taylor, GW
   Kremer, SC
AF Schneider, Stefan
   Greenberg, Saul
   Taylor, Graham W.
   Kremer, Stefan C.
TI Three critical factors affecting automated image species recognition
   performance for camera traps
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera traps; computer vision; convolutional networks; deep learning;
   density estimation; monitoring; population dynamics; species
   classification
ID K-FOLD; REIDENTIFICATION; IDENTIFICATION; CAPABILITIES
AB Ecological camera traps are increasingly used by wildlife biologists to unobtrusively monitor an ecosystems animal population. However, manual inspection of the images produced is expensive, laborious, and time-consuming. The success of deep learning systems using camera trap images has been previously explored in preliminary stages. These studies, however, are lacking in their practicality. They are primarily focused on extremely large datasets, often millions of images, and there is little to no focus on performance when tasked with species identification in new locations not seen during training. Our goal was to test the capabilities of deep learning systems trained on camera trap images using modestly sized training data, compare performance when considering unseen background locations, and quantify the gradient of lower bound performance to provide a guideline of data requirements in correspondence to performance expectations. We use a dataset provided by Parks Canada containing 47,279 images collected from 36 unique geographic locations across multiple environments. Images represent 55 animal species and human activity with high-class imbalance. We trained, tested, and compared the capabilities of six deep learning computer vision networks using transfer learning and image augmentation: DenseNet201, Inception-ResNet-V3, InceptionV3, NASNetMobile, MobileNetV2, and Xception. We compare overall performance on "trained" locations where DenseNet201 performed best with 95.6% top-1 accuracy showing promise for deep learning methods for smaller scale research efforts. Using trained locations, classifications with <500 images had low and highly variable recall of 0.750 +/- 0.329, while classifications with over 1,000 images had a high and stable recall of 0.971 +/- 0.0137. Models tasked with classifying species from untrained locations were less accurate, with DenseNet201 performing best with 68.7% top-1 accuracy. Finally, we provide an open repository where ecologists can insert their image data to train and test custom species detection models for their desired ecological domain.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
   [Greenberg, Saul] Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
   [Taylor, Graham W.] Univ Guelph, Sch Engn, Vector Inst Artificial Intelligence, Guelph, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca
OI Greenberg, Saul/0000-0003-0174-9665; Kremer, Stefan
   C./0000-0002-3667-4379; Schneider, Stefan/0000-0002-6903-6605
CR Amodei D, 2016, PR MACH LEARN RES, V48
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   BALFOORT HW, 1992, J PLANKTON RES, V14, P575, DOI 10.1093/plankt/14.4.575
   Beery S., 2019, ARXIV191203538
   Beery S., 2019, ARXIV190405986
   Bengio Y, 2004, J MACH LEARN RES, V5, P1089
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   CHAO A, 1989, BIOMETRICS, V45, P427, DOI 10.2307/2531487
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Csurka Gabriela, 2017, ADV COMPUTER VISION
   Deb D., 2018, ARXIV180408790
   Eraslan G, 2019, NAT REV GENET, V20, P389, DOI 10.1038/s41576-019-0122-6
   Fukushima K., 1979, ELECTR COMMUN JPN, V62, P11
   Goldberg AB., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [DOI 10.2200/S00196ED1V01Y200906AIM006, 10.2200/S00196ED1V01Y200906AIM006]
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goutte C, 2005, LECT NOTES COMPUT SC, V3408, P345
   Greenberg S, 2019, ECOL EVOL, V9, P13706, DOI 10.1002/ece3.5767
   GYSEL LESLIE W., 1956, JOUR WILDLIFE MANAGEMENT, V20, P451, DOI 10.2307/3797161
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Howard A.G., 2013, SOME IMPROVEMENTS ON
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343
   JEFFRIES HP, 1984, MAR BIOL, V78, P329, DOI 10.1007/BF00393019
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Norouzzadeh M. S., 2019, ARXIV191009716
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   ROBSON D. S., 1964, TRANS AMER FISH SOC, V93, P215, DOI 10.1577/1548-8659(1964)93[215:SSIPME]2.0.CO;2
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   SIMPSON R, 1991, IEEE CONFERENCE ON NEURAL NETWORKS FOR OCEAN ENGINEERING, P223, DOI 10.1109/ICNN.1991.163354
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
NR 49
TC 19
Z9 19
U1 2
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD APR
PY 2020
VL 10
IS 7
BP 3503
EP 3517
DI 10.1002/ece3.6147
PG 15
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA LB1TE
UT WOS:000524417200029
PM 32274005
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Norouzzadeh, MS
   Nguyen, A
   Kosmala, M
   Swanson, A
   Palmer, MS
   Packer, C
   Clune, J
AF Norouzzadeh, Mohammad Sadegh
   Anh Nguyen
   Kosmala, Margaret
   Swanson, Alexandra
   Palmer, Meredith S.
   Packer, Craig
   Clune, Jeff
TI Automatically identifying, counting, and describing wild animals in
   camera-trap images with deep learning
SO PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA
LA English
DT Article
DE deep learning; deep neural networks; artificial intelligence;
   camera-trap images; wildlife ecology
ID MANAGEMENT; LANDSCAPE; SOFTWARE; MODEL; FEAR
AB Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into "big data" sciences. Motion-sensor "camera traps" enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with >93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving >8.4 y (i.e., >17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.
C1 [Norouzzadeh, Mohammad Sadegh; Clune, Jeff] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Anh Nguyen] Auburn Univ, Dept Comp Sci & Software Engn, Auburn, AL 36849 USA.
   [Kosmala, Margaret] Harvard Univ, Dept Organism & Evolutionary Biol, Cambridge, MA 02138 USA.
   [Swanson, Alexandra] Univ Oxford, Dept Phys, Oxford OX1 3RH, England.
   [Palmer, Meredith S.; Packer, Craig] Univ Minnesota, Dept Ecol Evolut & Behav, St Paul, MN 55108 USA.
   [Clune, Jeff] Uber Al Labs, San Francisco, CA 94103 USA.
RP Clune, J (corresponding author), Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.; Clune, J (corresponding author), Uber Al Labs, San Francisco, CA 94103 USA.
EM jeffclune@uwyo.edu
RI Palmer, Meredith S/D-5042-2014
OI Palmer, Meredith S/0000-0002-1416-1732
FU National Science Foundation CAREER AwardNational Science Foundation
   (NSF) [1453549]
FX We thank Sarah Benson-Amram, the SS volunteers, and the members of the
   Evolving AI Laboratory at the University of Wyoming for valuable
   feedback, especially Joost Huizinga, Tyler Jaszkowiak, Roby Velez, Henok
   Mengistu, and Nick Cheney. J.C. was supported by National Science
   Foundation CAREER Award 1453549.
CR Anderson TM, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0314
   Bahdanau D., 2016, ICASSP
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
   Bridle J. S., 1990, NEUROCOMPUTING, V4, P227, DOI DOI 10.1007/978-3-642-76153-9_28
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   Chattopadhyay P., 2016, ABS160403505 CORR, V1
   Chherawala Y, 2013, 2013 INT C DOC AN RE
   Cho K., 2014, ARXIV14090473
   Cho K., 2014, ARXIV14061078
   Collobert R, 2008, P 25 INT C MACH LEAR, P160, DOI 10.1145/1390156.1390177
   Dauphin YN, 2014, NIPS, P2933, DOI DOI 10.5555/2969033.2969154
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng L, 2013, IEEE INT NEW CIRC
   Donahue J, 2014, PR MACH LEARN RES, V32
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619
   Kashif MN, 2016, I S BIOMED IMAGING, P1029, DOI 10.1109/ISBI.2016.7493441
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin M., 2013, NETWORK NETWORK, V10, DOI DOI 10.1109/ASRU.2015.7404828
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohri M., 2012, FDN MACHINE LEARNING
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Onoro- Rubio D., 2016, ECCV
   Palmer MS, 2017, ECOL LETT, V20, P1364, DOI 10.1111/ele.12832
   Palmer MS, 2018, AFR J ECOL, V56, P882, DOI 10.1111/aje.12505
   Park SR, 2018, OPT EXPRESS, V26, P4004, DOI 10.1364/OE.26.004004
   Rampasek L, 2018, CELL, V172, P893, DOI 10.1016/j.cell.2018.02.013
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   SAMUEL AL, 1959, IBM J RES DEV, V3, P211, DOI 10.1147/rd.33.0210
   Sener O., 2018, ICLR
   Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI DOI 10.2200/S00429ED1V01Y201207AIM018
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Sorower M. S., 2010, LIT SURVEY ALGORITHM, V18
   Sutskever I, 2014, ADV NEUR IN, V27
   Swanson A, 2016, ECOL EVOL, V6, P8534, DOI 10.1002/ece3.2569
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1
   Wang HB, 2014, J MED IMAGING, V1, DOI 10.1117/1.JMI.1.3.034003
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang PJ, 2014, IEEE C ELEC DEVICES
NR 63
TC 263
Z9 278
U1 27
U2 123
PU NATL ACAD SCIENCES
PI WASHINGTON
PA 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA
SN 0027-8424
J9 P NATL ACAD SCI USA
JI Proc. Natl. Acad. Sci. U. S. A.
PD JUN 19
PY 2018
VL 115
IS 25
BP E5716
EP E5725
DI 10.1073/pnas.1719367115
PG 10
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA GJ7RA
UT WOS:000435585200013
PM 29871948
OA hybrid, Green Published, Green Submitted
DA 2022-02-10
ER

PT J
AU Yang, DQ
   Tan, K
   Huang, ZP
   Li, XW
   Chen, BH
   Ren, GP
   Xiao, W
AF Yang, Deng-Qi
   Tan, Kun
   Huang, Zhi-Pang
   Li, Xiao-Wei
   Chen, Ben-Hui
   Ren, Guo-Peng
   Xiao, Wen
TI An automatic method for removing empty camera trap images using ensemble
   learning
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE artificial intelligence; camera trap images; convolutional neural
   networks; deep learning; ensemble learning
ID ASSOCIATIONS; CLASSIFIERS
AB Camera traps often produce massive images, and empty images that do not contain animals are usually overwhelming. Deep learning is a machine-learning algorithm and widely used to identify empty camera trap images automatically. Existing methods with high accuracy are based on millions of training samples (images) and require a lot of time and personnel costs to label the training samples manually. Reducing the number of training samples can save the cost of manually labeling images. However, the deep learning models based on a small dataset produce a large omission error of animal images that many animal images tend to be identified as empty images, which may lead to loss of the opportunities of discovering and observing species. Therefore, it is still a challenge to build the DCNN model with small errors on a small dataset. Using deep convolutional neural networks and a small-size dataset, we proposed an ensemble learning approach based on conservative strategies to identify and remove empty images automatically. Furthermore, we proposed three automatic identifying schemes of empty images for users who accept different omission errors of animal images. Our experimental results showed that these three schemes automatically identified and removed 50.78%, 58.48%, and 77.51% of the empty images in the dataset when the omission errors were 0.70%, 1.13%, and 2.54%, respectively. The analysis showed that using our scheme to automatically identify empty images did not omit species information. It only slightly changed the frequency of species occurrence. When only a small dataset was available, our approach provided an alternative to users to automatically identify and remove empty images, which can significantly reduce the time and personnel costs required to manually remove empty images. The cost savings were comparable to the percentage of empty images removed by models.
C1 [Yang, Deng-Qi; Li, Xiao-Wei; Chen, Ben-Hui] Dali Univ, Dept Math & Comp Sci, Dali, Peoples R China.
   [Yang, Deng-Qi; Tan, Kun; Huang, Zhi-Pang; Ren, Guo-Peng; Xiao, Wen] Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali, Peoples R China.
   [Yang, Deng-Qi; Tan, Kun; Huang, Zhi-Pang; Ren, Guo-Peng; Xiao, Wen] Collaborat Innovat Ctr Biodivers Three Parallel R, Dali, Peoples R China.
   [Yang, Deng-Qi; Li, Xiao-Wei; Chen, Ben-Hui] Dali Univ, Data Secur & Applicat Innovat Team, Dali, Peoples R China.
RP Ren, GP (corresponding author), Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali, Peoples R China.
EM rengp@eastern-himalaya.cn
RI Ren, Guo-Peng/ABA-2138-2021
OI Ren, Guo-Peng/0000-0003-3381-3166; , dengqiyang/0000-0003-1437-3097
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [31960119, 31860164, 31860168]; Yunnan
   Provincial Science and Technology Department University Joint Project
   [2017FH001-027]
FX This study was partially supported by the National Natural Science
   Foundation of China (31960119, 31860164, 31860168) and Yunnan Provincial
   Science and Technology Department University Joint Project
   (2017FH001-027).
CR Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45
   Chen YS, 2019, IEEE J-STARS, V12, P1882, DOI 10.1109/JSTARS.2019.2915259
   Clark A, 2019, PIL PILLOW 5 2 0 FOR
   Dertien JS, 2017, J WILDLIFE MANAGE, V81, P1457, DOI 10.1002/jwmg.21308
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Faria FA, 2014, PATTERN RECOGN LETT, V39, P52, DOI 10.1016/j.patrec.2013.07.014
   Forsyth DM, 2019, J WILDLIFE MANAGE, V83, P1090, DOI 10.1002/jwmg.21675
   Galar M, 2012, IEEE T SYST MAN CY C, V42, P463, DOI 10.1109/TSMCC.2011.2161285
   Giraldo-Zuluaga J.-H., 2017, VISUAL COMPUT, P1
   GOMEZ A, 2017, ECOL INFORM, V41, P24, DOI DOI 10.1016/j.ecoinf.2017.07.004
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
   Hurt JA, 2019, INT GEOSCI REMOTE SE, P1326, DOI 10.1109/IGARSS.2019.8898596
   Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin M., 2013, ARXIV13124444
   Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Pathak Sudipta, 2018, 2018 IEEE 8th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS), DOI 10.1109/ICCABS.2018.8541985
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tabak M., 2019, APPL ECOLOGY, DOI [10.1101/346809, DOI 10.1101/346809]
   Thorne ED, 2017, J WILDLIFE MANAGE, V81, P1042, DOI 10.1002/jwmg.21282
   Huynh T, 2016, IEEE T MED IMAGING, V35, P174, DOI 10.1109/TMI.2015.2461533
   Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
   Vinyals O., 2014, P 31 INT C INT C MAC, V32
   Webb SM, 2016, J WILDLIFE MANAGE, V80, P1461, DOI 10.1002/jwmg.21137
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xia J, 2018, IEEE T GEOSCI REMOTE, V56, P202, DOI 10.1109/TGRS.2017.2744662
   Yang D.Q., 2019, CAMERA TRAP IMAGES M
   Yosinski J, 2014, ADV NEUR IN, V27
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 41
TC 1
Z9 1
U1 5
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD JUN
PY 2021
VL 11
IS 12
BP 7591
EP 7601
DI 10.1002/ece3.7591
EA MAY 2021
PG 11
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA SV8AX
UT WOS:000645977200001
PM 34188837
OA Green Published
DA 2022-02-10
ER

PT J
AU Yang, DQ
   Li, T
   Liu, MT
   Li, XW
   Chen, BH
AF Yang, Deng-Qi
   Li, Tao
   Liu, Meng-Tao
   Li, Xiao-Wei
   Chen, Ben-Hui
TI A systematic study of the class imbalance problem: Automatically
   identifying empty camera trap images using convolutional neural networks
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Camera trap images; Class imbalance; Convolutional neural networks; Deep
   learning; Image classification
AB Camera traps, which are widely used in wildlife surveys, often produce massive images, and many of them are empty images not contain animals. Using the deep learning model to automatically identify the empty camera trap images can reduce the workload of manual classification significantly. However, the performance of deep learning models is easily affected by the class imbalance problem of training datasets, which is a common problem for actual wildlife survey projects. Almost all previous studies on empty image recognition used down sampling or oversampling methods to eliminate the effect of class imbalance on the performance of deep learning classifiers. The class imbalance problem has been systematically studied in the field of traditional image recognition, yet very limited research is available in the context of identifying camera trap images taken from highly cluttered natural scenes. This study systematically studied the impact of class imbalance on model performance when using a deep learning model to identify empty camera trap images. Then we proposed the construction method of training sets of the deep learning model when the data set has different class imbalance levels. Based on results from our experiments we concluded that (i) the class imbalance showed little effect on the performance of the model when the empty image ratio (EIR) in the data set was between 10% and 70%, so the training sets can be randomly built without changing the class distribution; (ii) we recommended using over sampling to partially eliminate class imbalance to reduce omission errors when the EIR of the data set exceeded 70%; (iii) when the EIRs of the training set and the test set were close, the overall error, omission error, and commission error of the model were relatively smaller, and the model tended to achieve a better overall performance; (iv) the omission and commission errors can be adjusted by changing the percentage of empty images in the training set.
C1 [Yang, Deng-Qi; Li, Tao; Liu, Meng-Tao; Chen, Ben-Hui] Dali Univ, Dept Math & Comp Sci, Dali 671003, Yunnan, Peoples R China.
   [Li, Xiao-Wei] Dali Univ, Data Secur & Applicat Innovat Team, Dali 671003, Yunnan, Peoples R China.
RP Yang, DQ (corresponding author), Dali Univ, Dept Math & Comp Sci, Dali 671003, Yunnan, Peoples R China.
EM dqyang@dali.edu.cn
FU National Natural ScienceFoundation of ChinaNational Natural Science
   Foundation of China (NSFC) [31960119]; Yunnan Provincial Science and
   Technology Department University Joint Project [2017FH001-027];
   Innovative Project of Dali University [ZKLX2020308]
FX We appreciate the support of the National Natural ScienceFoundation of
   China (31960119) and the Yunnan Provincial Science and Technology
   Department University Joint Project (2017FH001-027) and the Innovative
   Project of Dali University (ZKLX2020308) .
CR abak M., 2019, MACHINE LEARNING CLA
   Bennin KE, 2018, PROCEEDINGS 2018 IEEE/ACM 40TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P699, DOI 10.1145/3180155.3182520
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Chang J.R., 2015, BATCH NORMALIZED MAX
   Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dertien JS, 2017, J WILDLIFE MANAGE, V81, P1457, DOI 10.1002/jwmg.21308
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Drummond C, 2003, P ICML WORKSH LEARN, V11, P1
   Duarte A, 2017, J WILDLIFE MANAGE, V81, P182, DOI 10.1002/jwmg.21146
   Frey S, 2017, REMOTE SENS ECOL CON, V3, P123, DOI 10.1002/rse2.60
   Guo H., 2004, ACM SIGKDD EXPLOR NE, V6, P30, DOI [10.1145/1007730.1007736, DOI 10.1145/1007730.1007736]
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Jaccard N, 2017, J X-RAY SCI TECHNOL, V25, P323, DOI 10.3233/XST-16199
   Japkowicz N, 2000, NEURAL COMPUT, V12, P531, DOI 10.1162/089976600300015691
   Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
   Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5
   Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A., 2009, 1 U TOR
   Kubat M., 1997, P 14 INT C MACH LEAR, P179, DOI [DOI 10.1016/J.INS.2014.08.051, 10.1016/j.ins.2014.08.051]
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H, 2006, LECT NOTES COMPUT SC, V4233, P21
   Li FM, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P761, DOI 10.1109/ICIVC.2017.7984657
   Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
   Marouf M., 2020, 2020 3 INT C COMP MA
   Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Rich LN, 2019, J WILDLIFE MANAGE, V83, P855, DOI 10.1002/jwmg.21654
   Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29
   Steenweg R, 2016, BIOL CONSERV, V201, P192, DOI 10.1016/j.biocon.2016.06.020
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yousif H, 2017, IEEE INT SYMP CIRC S
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhou ZH, 2006, IEEE T KNOWL DATA EN, V18, P63, DOI 10.1109/TKDE.2006.17
NR 43
TC 0
Z9 0
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD SEP
PY 2021
VL 64
AR 101350
DI 10.1016/j.ecoinf.2021.101350
PG 8
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA UK1WY
UT WOS:000691767700003
DA 2022-02-10
ER

PT C
AU Yousif, H
   Yuan, JH
   Kays, R
   He, ZH
AF Yousif, Hayder
   Yuan, Jianhe
   Kays, Roland
   He, Zhihai
GP IEEE
TI Fast Human-Animal Detection from Highly Cluttered Camera-Trap Images
   Using Joint Background Modeling and Deep Learning Classification
SO 2017 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
LA English
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-31, 2017
CL Baltimore, MD
SP IEEE, IEEE Circuits & Syst Soc
AB In this paper, we couple effective dynamic background modeling with deep learning classification to develop a fast and accurate scheme for human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification. Specifically, first, we develop an effective background modeling and subtraction scheme to generate region proposals for the foreground objects. We then develop a cross-frame image patch verification to reduce the number of foreground object proposals. Finally, we perform complexity-accuracy analysis of deep convolutional neural networks (DCNN) to develop a fast deep learning classification scheme to classify these region proposals into three categories: human, animals, and background patches. The optimized DCNN is able to maintain high level of accuracy while reducing the computational complexity by 14 times. Our experimental results demonstrate that the proposed method outperforms existing methods on the camera-trap dataset.
C1 [Yousif, Hayder; Yuan, Jianhe; He, Zhihai] Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC USA.
RP Yousif, H (corresponding author), Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM hyypp5@mail.missouri.edu; rokays@gmail.com; hezhi@missouri.edu
RI He, Zhihai/A-5885-2019; Yousif, Hayder/AAG-2259-2020
OI Yousif, Hayder/0000-0002-7638-9505
CR Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495
   Wright J., 2009, P NIPS, P2080
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 9
TC 3
Z9 3
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 0271-4302
BN 978-1-4673-6853-7
J9 IEEE INT SYMP CIRC S
PY 2017
PG 4
WC Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering
GA BJ4DH
UT WOS:000424890101251
DA 2022-02-10
ER

PT J
AU Duggan, MT
   Groleau, MF
   Shealy, EP
   Self, LS
   Utter, TE
   Waller, MM
   Hall, BC
   Stone, CG
   Anderson, LL
   Mousseau, TA
AF Duggan, Matthew T.
   Groleau, Melissa F.
   Shealy, Ethan P.
   Self, Lillian S.
   Utter, Taylor E.
   Waller, Matthew M.
   Hall, Bryan C.
   Stone, Chris G.
   Anderson, Layne L.
   Mousseau, Timothy A.
TI An approach to rapid processing of camera trap images with minimal human
   input
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera trap; deep learning; neural network; transfer learning; wildlife
   ecology
ID NETWORKS
AB Camera traps have become an extensively utilized tool in ecological research, but the manual processing of images created by a network of camera traps rapidly becomes an overwhelming task, even for small camera trap studies. We used transfer learning to create convolutional neural network (CNN) models for identification and classification. By utilizing a small dataset with an average of 275 labeled images per species class, the model was able to distinguish between species and remove false triggers. We trained the model to detect 17 object classes with individual species identification, reaching an accuracy up to 92% and an average F1 score of 85%. Previous studies have suggested the need for thousands of images of each object class to reach results comparable to those achieved by human observers; however, we show that such accuracy can be achieved with fewer images. With transfer learning and an ongoing camera trap study, a deep learning model can be successfully created by a small camera trap study. A generalizable model produced from an unbalanced class set can be utilized to extract trap events that can later be confirmed by human processors.
C1 [Duggan, Matthew T.; Groleau, Melissa F.; Shealy, Ethan P.; Self, Lillian S.; Utter, Taylor E.; Waller, Matthew M.; Mousseau, Timothy A.] Univ South Carolina UofSC, Dept Biol Sci, Columbia, SC 29208 USA.
   [Hall, Bryan C.; Stone, Chris G.; Anderson, Layne L.] South Carolina Army Natl Guard Environm Off, Eastover, SC USA.
RP Mousseau, TA (corresponding author), Univ South Carolina UofSC, Dept Biol Sci, Columbia, SC 29208 USA.
EM mousseau@sc.edu
OI Mousseau, Timothy/0000-0002-2235-4868
FU Samuel Freeman Charitable Trust; University of South Carolina Honors
   College; University of South Carolina Office of Research
FX Samuel Freeman Charitable Trust; University of South Carolina Honors
   College; South Carolina Army National Guard; University of South
   Carolina Office of Research
CR Abadi M., 2015, TENSORFLOW LARGE SCA, V1, P1, DOI 10.1016/0076-6879(83)01039-3
   Alexander JS, 2016, BIOL CONSERV, V197, P27, DOI 10.1016/j.biocon.2016.02.023
   Almond R. E. A., 2018, LIVING PLANET REPORT
   Chitwood MC, 2020, DIVERSITY-BASEL, V12, DOI 10.3390/d12090341
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deepak S, 2019, COMPUT BIOL MED, V111, DOI 10.1016/j.compbiomed.2019.103345
   Edwards S, 2016, J ARID ENVIRON, V124, P304, DOI 10.1016/j.jaridenv.2015.09.009
   Ferreira-Rodriguez N, 2019, MAMMAL RES, V64, P155, DOI 10.1007/s13364-018-00414-1
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Han DM, 2018, EXPERT SYST APPL, V95, P43, DOI 10.1016/j.eswa.2017.11.028
   Jiang, 2019, PATTERN RECOGN, P394
   Jiménez Carlos F., 2010, Rev. peru biol., V17, P191
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kolbert E., 2014, 6 EXTINCTION UNNATUR
   Krasin I, 2017, OPENIMAGES PUBLIC DA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Shi ZH, 2019, MULTIMED TOOLS APPL, V78, P1017, DOI 10.1007/s11042-018-6082-6
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swati ZNK, 2019, COMPUT MED IMAG GRAP, V75, P34, DOI 10.1016/j.compmedimag.2019.05.001
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tzutalin, 2015, LAB GIT COD
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Xie M., 2015, ARXIV151000098
NR 34
TC 0
Z9 0
U1 8
U2 8
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD SEP
PY 2021
VL 11
IS 17
BP 12051
EP 12063
DI 10.1002/ece3.7970
EA AUG 2021
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA UN4RM
UT WOS:000680141800001
PM 34522360
OA Green Published, Green Submitted, gold
DA 2022-02-10
ER

PT C
AU Evans, BC
   Tucker, A
   Wearn, OR
   Carbone, C
AF Evans, Benjamin C.
   Tucker, Allan
   Wearn, Oliver R.
   Carbone, Chris
BE Koprinska, I
   Kamp, M
   Appice, A
   Loglisci, C
   Antonie, L
   Zimmermann, A
   Guidotti, R
   Ozgobek, O
TI Reasoning About Neural Network Activations: An Application in Spatial
   Animal Behaviour from Camera Trap Classifications
SO ECML PKDD 2020 WORKSHOPS
SE Communications in Computer and Information Science
LA English
DT Proceedings Paper
CT European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECML PKDD)
CY SEP 14-18, 2020
CL ELECTR NETWORK
SP Fraunhofer IAIS, ASML, F Secure, Roche, Amazon, Science, EURA NOVA, Google, NEC, Internet & Data Lab, KNIME, Qualcomm, AI Res, imec, FWO, Ghent Univ, Springer, Visitgent, gentcongres, AI Growth
DE Animal behavior; Convolutional Neural Networks; Bayesian networks;
   Activation based reasoning
AB Camera traps are a vital tool for ecologists to enable them to monitor wildlife over large areas in order to determine population changes, habitat, and behaviour. As a result, camera-trap datasets are rapidly growing in size. Recent advancements in Artificial Neural Networks (ANN) have emerged in image recognition and detection tasks which are now being applied to automate camera-trap labelling. An ANN designed for species detection will output a set of activations, representing the observation of a particular species (an individual class) at a particular location and time and are often used as a way to calculate population sizes in different regions. Here we go one step further and explore how we can combine ANNs with probabilistic graphical models to reason about animal behaviour using the ANN outputs over different geographical locations. By using the output activations from ANNs as data along with the trap's associated spatial coordinates, we build spatial Bayesian networks to explore species behaviours (how they move and distribute themselves) and interactions (how they distribute in relation to other species). This combination of probabilistic reasoning and deep learning offers many advantages for large camera trap projects as well as potential for other remote sensing datasets that require automated labelling.
C1 [Evans, Benjamin C.; Tucker, Allan] Brunel Univ London, Uxbridge UB8 3PH, Middx, England.
   [Wearn, Oliver R.; Carbone, Chris] Zool Soc London, Inst Zool, London NW1 4RY, England.
RP Evans, BC (corresponding author), Brunel Univ London, Uxbridge UB8 3PH, Middx, England.
EM Benjamin.Evans@brunel.ac.uk; Allan.Tucker@brunel.ac.uk;
   Oliver.Wearn@gmail.com; Chris.Carbone@ioz.ac.uk
OI Carbone, Chris/0000-0002-9253-3765; Wearn, Oliver/0000-0001-8258-3534
FU NERC (The Natural Environment Research Council)
FX Benjamin C. Evans work is funded by NERC (The Natural Environment
   Research Council).
CR Beery S., 2019, ARXIV190706772
   Devlin J., 2019, BERT PRETRAINING DEE, DOI DOI 10.18653/V1/N19-1423
   Franco C, 2016, ENVIRON MODELL SOFTW, V80, P132, DOI 10.1016/j.envsoft.2016.02.029
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Kavukcuoglu K., 2016, ARXIV160903499, DOI DOI 10.1109/ICASSP.2009.4960364
   Maldonado AD, 2019, ENVIRON MODELL SOFTW, V118, P281, DOI 10.1016/j.envsoft.2019.04.011
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Spirtes P., 1993, CAUSATION PREDICTION, P238
   Trifonova N, 2015, ECOL INFORM, V30, P142, DOI 10.1016/j.ecoinf.2015.10.003
   Uusitalo L, 2007, ECOL MODEL, V203, P312, DOI 10.1016/j.ecolmodel.2006.11.033
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
NR 12
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 1865-0929
EI 1865-0937
BN 978-3-030-65965-3; 978-3-030-65964-6
J9 COMM COM INF SC
PY 2020
VL 1323
BP 26
EP 37
DI 10.1007/978-3-030-65965-3_2
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Computer
   Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS4YI
UT WOS:000724139600002
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Nguyen, H
   Maclagan, SJ
   Nguyen, TD
   Nguyen, T
   Flemons, P
   Andrews, K
   Ritchie, EG
   Phung, D
AF Nguyen, Hung
   Maclagan, Sarah J.
   Tu Dinh Nguyen
   Nguyen, Thin
   Flemons, Paul
   Andrews, Kylie
   Ritchie, Euan G.
   Dinh Phung
GP IEEE
TI Animal Recognition and Identification with Deep Convolutional Neural
   Networks for Automated Wildlife Monitoring
SO 2017 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED
   ANALYTICS (DSAA)
SE Proceedings of the International Conference on Data Science and Advanced
   Analytics
LA English
DT Proceedings Paper
CT 4th IEEE / ACM / ASA International Conference on Data Science and
   Advanced Analytics (DSAA)
CY OCT 19-21, 2017
CL Tokyo, JAPAN
SP IEEE, IEEE Computat Intelligence Soc, Kozo Keikaku Engn Inc, NEC Corp, Air Force Off Sci Res, Asian Off Aerosp Res & Dev, U S Army RDECOM, FEG, NS Solut Grp, KDDI Res, ACM, Amer Stat Assoc, Off Naval Res Global, Int Technol Ctr Pacific, Financial Engn Grp, Gunosy, FRONTEO, Automagi, Sansan, XCompass, Google, RECRUIT Holfdings, LIFULL, FINATEXT, Yahoo Japan Corp, Panasonic, Honda Res Inst Japan
DE deep learning; convolutional neural networks; large scale image
   classification; animal recognition; wildlife monitoring; citizen science
ID CITIZEN SCIENCE; TOOL
AB Efficient and reliable monitoring of wild animals in their natural habitats is essential to inform conservation and management decisions. Automatic covert cameras or "camera traps" are being an increasingly popular tool for wildlife monitoring due to their effectiveness and reliability in collecting data of wildlife unobtrusively, continuously and in large volume. However, processing such a large volume of images and videos captured from camera traps manually is extremely expensive, time-consuming and also monotonous. This presents a major obstacle to scientists and ecologists to monitor wildlife in an open environment. Leveraging on recent advances in deep learning techniques in computer vision, we propose in this paper a framework to build automated animal recognition in the wild, aiming at an automated wildlife monitoring system. In particular, we use a single-labeled dataset from Wildlife Spotter project, done by citizen scientists, and the state-of-the-art deep convolutional neural network architectures, to train a computational system capable of filtering animal images and identifying species automatically. Our experimental results achieved an accuracy at 96.6% for the task of detecting images containing animal, and 90.4% for identifying the three most common species among the set of images of wild animals taken in South-central Victoria, Australia, demonstrating the feasibility of building fully automated wildlife observation. This, in turn, can therefore speed up research findings, construct more efficient citizen science based monitoring systems and subsequent management decisions, having the potential to make significant impacts to the world of ecology and trap camera images analysis.
C1 [Nguyen, Hung; Tu Dinh Nguyen; Nguyen, Thin; Dinh Phung] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
   [Maclagan, Sarah J.; Ritchie, Euan G.] Deakin Univ, Ctr Integrat Ecol, Burwood, Australia.
   [Flemons, Paul] Australian Museum Res Inst, Sydney, NSW, Australia.
   [Andrews, Kylie] ABC Radio Natl, Ultimo, NSW, Australia.
RP Nguyen, H (corresponding author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
EM hung@deakin.edu.au; smaclaga@deakin.edu.au; tu.nguyen@deakin.edu.au;
   thin.nguyen@deakin.edu.au; paul.flemons@austmus.gov.au;
   andrews.kylie@abc.net.au; e.ritchie@deakin.edu.au;
   dinh.phung@deakin.edu.au
OI Phung, Dinh/0000-0002-9977-8247
FU Telstra-Deakin Centre of Excellence in Big Data and Machine Learning
FX This work is partially supported by the Telstra-Deakin Centre of
   Excellence in Big Data and Machine Learning.
CR Abadi Martin., 2016, ARXIV PREPRINT ARXIV
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bishop C. M., 2006, MACH LEARN, V128, P1, DOI DOI 10.1002/9780471740360.EBS0904
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Chollet F., 2020, KERAS DOCUMENTATION
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Collobert R, 2008, P 25 INT C MACH LEAR, P160, DOI 10.1145/1390156.1390177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Garrott R.A., 2012, ANAL WILDLIFE RADIO
   Gehring J, 2016, ARXIV PREPRINT ARXIV
   Gehring J., 2017, ARXIV E PRINTS
   Godley B. J., 2008, Endangered Species Research, V4, P3, DOI 10.3354/esr00060
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hulbert IAR, 2001, J APPL ECOL, V38, P869, DOI 10.1046/j.1365-2664.2001.00624.x
   Irwin A., 1995, CITIZEN SCI STUDY PE
   Kays R., 2010, INT J RES REV WIREL, V1, P19
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   SZEWCZYK R., 2004, P 2 INT C EMB NETW S, DOI DOI 10.1145/1031495.1031521
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Vitousek PM, 1997, SCIENCE, V277, P494, DOI 10.1126/science.277.5325.494
   Wang J., 2010, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2010.5540018
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 37
TC 45
Z9 47
U1 5
U2 22
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2472-1573
BN 978-1-5090-5004-8
J9 PR INT CONF DATA SC
PY 2017
BP 40
EP 49
DI 10.1109/DSAA.2017.31
PG 10
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BL6PK
UT WOS:000454622300005
DA 2022-02-10
ER

PT J
AU Tekeli, U
   Bastanlar, Y
AF Tekeli, Ulas
   Bastanlar, Yalin
TI Elimination of useless images from raw camera-trap data
SO TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES
LA English
DT Article
DE Camera-trap; image processing; computer vision; object detection;
   background subtraction; convolutional neural networks; deep learning
ID MANAGEMENT; SOFTWARE
AB Camera-traps are motion triggered cameras that are used to observe animals in nature. The number of images collected from camera-traps has increased significantly with the widening use of camera-traps thanks to advances in digital technology. A great workload is required for wild-life researchers to group and label these images. We propose a system to decrease the amount of time spent by the researchers by eliminating useless images from raw camera-trap data. These images are too bright, too dark, blurred, or they contain no animals To eliminate bright, dark, and blurred images we employ techniques based on image histograms and fast Fourier transform. To eliminate the images without animals, we propose a system combining convolutional neural networks and background subtraction. We experimentally show that the proposed approach keeps 99% of photos with animals while eliminating more than 50% of photos without animals. We also present a software prototype that employs developed algorithms to eliminate useless images.
C1 [Tekeli, Ulas; Bastanlar, Yalin] Izmir Inst Technol, Comp Engn Dept, Izmir, Turkey.
RP Bastanlar, Y (corresponding author), Izmir Inst Technol, Comp Engn Dept, Izmir, Turkey.
EM yalinbastanlar@iyte.edu.tr
RI Bastanlar, Yalin/AAA-7114-2022
OI Tekeli, Ulas/0000-0003-0492-3059
FU Scientific and Technological Research Council of Turkey (TUBITAK)Turkiye
   Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK) [115E918]; NVIDIA
   Corporation
FX This work was supported by the Scientific and Technological Research
   Council of Turkey (TUBITAK) (Grant no. 115E918). We are grateful to
   Republic of Turkey, Ministry of Forest and Water Affairs for sharing the
   camera-trap dataset. We also acknowledge the support of NVIDIA
   Corporation with the donation of the GPU used for this research.
CR Boom BJ, 2014, ECOL INFORM, V23, P83, DOI 10.1016/j.ecoinf.2013.10.006
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Dosselmann RW, 2012, TECHNICAL REPORT
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hernandez-Serna A, 2014, PEERJ, V2, DOI 10.7717/peerj.563
   Islam J, 2017, MACH LEARN HLTH WORK
   Ju C, 2018, J APPL STAT, V45, P2800, DOI 10.1080/02664763.2018.1441383
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Orhan S, 2018, ELECTRON LETT, V54, P424, DOI 10.1049/el.2017.4725
   Pavlovic G, 1992, IEEE T IMAGE PROCESS, V1, P496, DOI 10.1109/83.199919
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Song DZ, 2010, IEEE T IMAGE PROCESS, V19, P2321, DOI 10.1109/TIP.2010.2048151
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tong HH, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P17, DOI 10.1109/ICME.2004.1394114
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 30
TC 1
Z9 1
U1 3
U2 9
PU TUBITAK SCIENTIFIC & TECHNICAL RESEARCH COUNCIL TURKEY
PI ANKARA
PA ATATURK BULVARI NO 221, KAVAKLIDERE, ANKARA, 00000, TURKEY
SN 1300-0632
EI 1303-6203
J9 TURK J ELECTR ENG CO
JI Turk. J. Electr. Eng. Comput. Sci.
PY 2019
VL 27
IS 4
BP 2395
EP 2411
DI 10.3906/elk-1808-130
PG 17
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA IT3HG
UT WOS:000482742800002
OA Bronze, Green Submitted
DA 2022-02-10
ER

PT J
AU Christin, S
   Hervet, E
   Lecomte, N
AF Christin, Sylvain
   Hervet, Eric
   Lecomte, Nicolas
TI Applications for deep learning in ecology
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE artificial intelligence; automatic monitoring; deep learning; ecology;
   neural network; pattern recognition
ID NEURAL-NETWORKS; OPPORTUNITIES; EVOLUTION; DYNAMICS; TRACKING; TOOLS
AB A lot of hype has recently been generated around deep learning, a novel group of artificial intelligence approaches able to break accuracy records in pattern recognition. Over the course of just a few years, deep learning has revolutionized several research fields such as bioinformatics and medicine with its flexibility and ability to process large and complex datasets. As ecological datasets are becoming larger and more complex, we believe these methods can be useful to ecologists as well. In this paper, we review existing implementations and show that deep learning has been used successfully to identify species, classify animal behaviour and estimate biodiversity in large datasets like camera-trap images, audio recordings and videos. We demonstrate that deep learning can be beneficial to most ecological disciplines, including applied contexts, such as management and conservation. We also identify common questions about how and when to use deep learning, such as what are the steps required to create a deep learning network, which tools are available to help, and what are the requirements in terms of data and computer power. We provide guidelines, recommendations and useful resources, including a reference flowchart to help ecologists get started with deep learning. We argue that at a time when automatic monitoring of populations and ecosystems generates a vast amount of data that cannot be effectively processed by humans anymore, deep learning could become a powerful reference tool for ecologists.
C1 [Christin, Sylvain; Lecomte, Nicolas] Univ Moncton, Dept Biol, Canada Res Chair Polar & Boreal Ecol, Moncton, NB, Canada.
   [Hervet, Eric] Univ Moncton, Dept Comp Sci, Moncton, NB, Canada.
RP Lecomte, N (corresponding author), Univ Moncton, Dept Biol, Canada Res Chair Polar & Boreal Ecol, Moncton, NB, Canada.
EM nicolas.lecomte@umoncton.ca
RI Christin, Sylvain/AAS-8910-2020
OI Lecomte, Nicolas/0000-0002-8473-5375
FU Canada Research Chair in Polar and Boreal Ecology; New Brunswick
   Innovation Fund; Polar Knowledge Canada
FX Canada Research Chair in Polar and Boreal Ecology; New Brunswick
   Innovation Fund; Polar Knowledge Canada
CR Abrams J.F., 2018, 483222 BIORXIV, DOI [10.1101/483222, DOI 10.1101/483222]
   Asner GP, 2018, BIOL CONSERV, V217, P289, DOI 10.1016/j.biocon.2017.10.020
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
   Beijbom O., 2015, ARXIV151004811CS
   Browning E, 2018, METHODS ECOL EVOL, V9, P681, DOI 10.1111/2041-210X.12926
   Candela L, 2015, J ASSOC INF SCI TECH, V66, P1747, DOI 10.1002/asi.23358
   Cantrell B, 2017, TRENDS ECOL EVOL, V32, P156, DOI 10.1016/j.tree.2016.12.004
   Carey CC, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2753
   Chen D., 2016, ARXIV160909353CSQBIO
   Chollet F., 2016, ARXIV161002357CS
   Chon TS, 2001, ECOL MODEL, V146, P181, DOI 10.1016/S0304-3800(01)00305-2
   Christin S., 2019, APPL DEEP LEARNING E
   Cutler DR, 2007, ECOLOGY, V88, P2783, DOI 10.1890/07-0539.1
   Desjardins-Proulx P., 2017, 089771 BIORXIV, DOI [10.1101/089771, DOI 10.1101/089771]
   Di Minin E, 2018, NAT ECOL EVOL, V2, P406, DOI 10.1038/s41559-018-0466-x
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Dobrescu A., 2017, LEVERAGING MULTIPLE
   Douarre C., 2016, DEEP LEARNING BASED
   Drake JM, 2006, J APPL ECOL, V43, P424, DOI 10.1111/j.1365-2664.2006.01141.x
   Dugan P. J., 2016, ARXIV160500972CS
   Ellis EC, 2015, ECOL MONOGR, V85, P287, DOI 10.1890/14-2274.1
   Fairbrass AJ, 2019, METHODS ECOL EVOL, V10, P186, DOI 10.1111/2041-210X.13114
   Fernandez S, 2007, LECT NOTES COMPUT SC, V4669, P220
   Giuffrida M. V., 2017, ARIGAN SYNTHETIC ARA
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
   Guegan, 2012, ARTIFICIAL NEURONAL
   Guirado E., 2018, 443671 BIORXIV, DOI [10.1101/443671, DOI 10.1101/443671]
   Hart AG, 2018, METHODS ECOL EVOL, V9, P2194, DOI 10.1111/2041-210X.13063
   Heaton JB, 2017, APPL STOCH MODEL BUS, V33, P3, DOI 10.1002/asmb.2209
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jeong KS, 2001, ECOL MODEL, V146, P115, DOI 10.1016/S0304-3800(01)00300-3
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7_24
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kalin U., 2018, 441733 BIORXIV, DOI [10.1101/441733, DOI 10.1101/441733]
   Kiskin I, 2020, NEURAL COMPUT APPL, V32, P915, DOI 10.1007/s00521-018-3626-7
   Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kroodsma DA, 2018, SCIENCE, V359, P904, DOI 10.1126/science.aao5646
   Lample Guillaume, 2017, 31 AAAI C ART INT
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee H, 2019, ECOL INDIC, V96, P505, DOI 10.1016/j.ecolind.2018.08.035
   Lek S, 1996, ECOL MODEL, V90, P39, DOI 10.1016/0304-3800(95)00142-5
   Li K., 2017, RECURRENT NEURAL NET
   Lowndes JSS, 2017, NAT ECOL EVOL, V1, DOI 10.1038/s41559-017-0160
   Mac Aodha O, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005995
   Marcus G., 2018, ARXIV180100631CSSTAT
   Min S, 2017, BRIEF BIOINFORM, V18, P851, DOI 10.1093/bib/bbw068
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Namin ST, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0333-4
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Olden JD, 2008, Q REV BIOL, V83, P171, DOI 10.1086/587826
   Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5
   Potamitis I, 2015, ECOL INFORM, V26, P6, DOI 10.1016/j.ecoinf.2015.01.002
   Qiao M., 2018, 467878 BIORXIV, DOI [10.1101/467878, DOI 10.1101/467878]
   Recknagel F, 2001, ECOL MODEL, V146, P303, DOI 10.1016/S0304-3800(01)00316-7
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Ryan MJ, 2000, BRAIN BEHAV EVOLUT, V56, P45, DOI 10.1159/000006677
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8
   Salamon J, 2017, INT CONF ACOUST SPEE, P141, DOI 10.1109/ICASSP.2017.7952134
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schneider S., 2018, ARXIV180310842CS
   Sevilla A., 2017, WORKING NOTES CLEF 2
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516-044442, 10.1146/annurev-bioeng-071516044442]
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Song Q, 2017, NEUROCOMPUTING, V226, P16, DOI 10.1016/j.neucom.2016.11.018
   STOCKWELL DRB, 1992, MATH COMPUT SIMULAT, V33, P385, DOI 10.1016/0378-4754(92)90126-2
   Sutskever I., 2014, ARXIV14093215CS
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Turesson H. K., 2016, 079566 BIORXIV
   Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
   Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
   Wachtmeister CA, 2000, BEHAV ECOL, V11, P405, DOI 10.1093/beheco/11.4.405
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
   Wild B., 2018, ARXIV180204557CS
   Wilson G, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005510
   Xu R, 2018, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.02235
   Younis S, 2018, BOT LETT, V165, P377, DOI 10.1080/23818107.2018.1446357
NR 81
TC 89
Z9 91
U1 56
U2 147
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD OCT
PY 2019
VL 10
IS 10
BP 1632
EP 1644
DI 10.1111/2041-210X.13256
EA JUL 2019
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA JB0UB
UT WOS:000488345800001
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Shepley, A
   Falzon, G
   Lawson, C
   Meek, P
   Kwan, P
AF Shepley, Andrew
   Falzon, Greg
   Lawson, Christopher
   Meek, Paul
   Kwan, Paul
TI U-Infuse: Democratization of Customizable Deep Learning for Object
   Detection
SO SENSORS
LA English
DT Article
DE animal identification; artificial intelligence; camera-trap images;
   camera trapping; deep convolutional neural networks; deep learning;
   environmental software; wildlife ecology; wildlife monitoring;
   ecological object detection
ID CAMERA TRAP IMAGES
AB Image data is one of the primary sources of ecological data used in biodiversity conservation and management worldwide. However, classifying and interpreting large numbers of images is time and resource expensive, particularly in the context of camera trapping. Deep learning models have been used to achieve this task but are often not suited to specific applications due to their inability to generalise to new environments and inconsistent performance. Models need to be developed for specific species cohorts and environments, but the technical skills required to achieve this are a key barrier to the accessibility of this technology to ecologists. Thus, there is a strong need to democratize access to deep learning technologies by providing an easy-to-use software application allowing non-technical users to train custom object detectors. U-Infuse addresses this issue by providing ecologists with the ability to train customised models using publicly available images and/or their own images without specific technical expertise. Auto-annotation and annotation editing functionalities minimize the constraints of manually annotating and pre-processing large numbers of images. U-Infuse is a free and open-source software solution that supports both multiclass and single class training and object detection, allowing ecologists to access deep learning technologies usually only available to computer scientists, on their own device, customised for their application, without sharing intellectual property or sensitive data. It provides ecological practitioners with the ability to (i) easily achieve object detection within a user-friendly GUI, generating a species distribution report, and other useful statistics, (ii) custom train deep learning models using publicly available and custom training data, (iii) achieve supervised auto-annotation of images for further training, with the benefit of editing annotations to ensure quality datasets. Broad adoption of U-Infuse by ecological practitioners will improve ecological image analysis and processing by allowing significantly more image data to be processed with minimal expenditure of time and resources, particularly for camera trap images. Ease of training and use of transfer learning means domain-specific models can be trained rapidly, and frequently updated without the need for computer science expertise, or data sharing, protecting intellectual property and privacy.
C1 [Shepley, Andrew; Falzon, Greg; Lawson, Christopher] Univ New England, Sch Sci & Technol, Armidale, NSW 2350, Australia.
   [Falzon, Greg] Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA 5001, Australia.
   [Meek, Paul] NSW Dept Primary Ind, Vertebrate Pest Res Unit, POB 530, Coffs Harbour, NSW 2450, Australia.
   [Meek, Paul] Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2350, Australia.
   [Kwan, Paul] Melbourne Inst Technol, Sch IT & Engn, Melbourne, Vic 3000, Australia.
RP Shepley, A (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW 2350, Australia.
EM asheple2@une.edu.au; greg.falzon@flinders.edu.au;
   christopher.lawson@uon.edu.au; paul.meek@dpi.nsw.gov.au;
   pkwan@mit.edu.au
OI Shepley, Andrew/0000-0001-7511-4967; Falzon,
   Gregory/0000-0002-1989-9357; Kwan, Paul Wing Hing/0000-0002-4959-5274
FU NSW Environmental Trust "Developing Strategies for Effective Feral Cat
   Management" project; Australian Government Research Training Program
   (RTP) ScholarshipAustralian GovernmentDepartment of Industry, Innovation
   and Science; University of New England
FX This research was funded by the NSW Environmental Trust "Developing
   Strategies for Effective Feral Cat Management" project. Andrew Shepley
   acknowledges the support provided through the Australian Government
   Research Training Program (RTP) Scholarship. The APC was funded by the
   University of New England.
CR Abadi M., 2016, 12 USENIX S OPERATIN, P265
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   [Anonymous], 2017, DRIVEN DATA PROJECT
   [Anonymous], 2015, TZUTALIN LABELIMG GI
   Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
   Beery S, 2018, RECOGNITION TERRA IN
   Bengsen A, 2014, ECOL MANAG RESTOR, V15, P97, DOI 10.1111/emr.12086
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Fegraus E.H., 2016, CAMERA TRAPPING WILD, P33
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Greenberg S, 2019, ECOL EVOL, V9, P13706, DOI 10.1002/ece3.5767
   Hendry H, 2018, ORYX, V52, P15, DOI 10.1017/S0030605317001818
   Lashley MA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22638-6
   Legge S, 2020, WILDLIFE RES, V47, P731, DOI 10.1071/WR20089
   Legge S, 2020, WILDLIFE RES, V47, P523, DOI [10.1071/WR19174, 10.1071/WRv47n8_ED]
   Li XY, 2018, DIVERS DISTRIB, V24, P1560, DOI 10.1111/ddi.12792
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Meek P., 2014, CAMERA TRAPPING WILD
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   R Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
   Rahman DA, 2017, ORYX, V51, P665, DOI 10.1017/S0030605316000429
   Redmon J, 2016, ARXIV 161208242
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Shepley A, 2021, ECOL EVOL, V11, P4494, DOI [10.1002/ece3.7344, 10.5281/ZENODO.4544073, 10.5281/ZENODO.4544074]
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tiwary U.S, 2018, INT HUM COMP INT 10
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zaumyslova O.Y., 2015, ACHIEV LIFE SCI, V9, P15, DOI [10.1016/j.als.2015.05.003, DOI 10.1016/J.ALS.2015.05.003]
   Zhang E., 2009, ENCY DATABASE SYSTEM, DOI [DOI 10.1007/978-0-387-39940-9_482, 10.1007/978-0-387-39940-9_482]
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
NR 42
TC 0
Z9 0
U1 1
U2 2
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD APR
PY 2021
VL 21
IS 8
AR 2611
DI 10.3390/s21082611
PG 17
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA RU0CQ
UT WOS:000644820300001
PM 33917792
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Yousif, H
   Yuan, JH
   Kays, R
   He, ZH
AF Yousif, Hayder
   Yuan, Jianhe
   Kays, Roland
   He, Zhihai
TI Object detection from dynamic scene using joint background modeling and
   fast deep learning classification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Human-animal detection; Camera-trap images; Background subtraction; Deep
   convolutional neural networks; Wildlife monitoring
ID SEGMENTATION
AB In this paper, we couple effective dynamic background modeling with fast deep learning classification to develop an accurate scheme for human-animal detection from camera-trap images with cluttered moving objects. We introduce a new block-wise background model, named as Minimum Feature Difference (MFD), to model the variation of the background of the camera-trap sequences and generate the foreground object proposals. We then develop a region proposals verification to reduce the number of false alarms. Finally, we perform complexity-accuracy analysis of DCNN to construct a fast deep learning classification scheme to classify these region proposals into three categories: human, animals, and background patches. The optimized DCNN is able to maintain high level of accuracy while reducing the computational complexity by 14 times, which allows near real-time implementation of the proposed method on CPU machines. Our experimental results demonstrate that the proposed method outperforms existing methods on our and Alexander von Humboldt Institute camera-trap datasets in both foreground segmentation and object detection. (C) 2018 Elsevier Inc. All rights reserved.
C1 [Yousif, Hayder; Yuan, Jianhe; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27601 USA.
RP Yousif, H (corresponding author), Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM hyypp5@mail.missouri.edu; yuanjia@missouri.edu; rwkay@ncsu.edu;
   hezhi@missouri.edu
RI He, Zhihai/A-5885-2019; Yousif, Hayder/AAG-2259-2020; Yuan,
   Jianhe/ABG-1712-2020
OI Yousif, Hayder/0000-0002-7638-9505; Yuan, Jianhe/0000-0002-4004-6236;
   Kays, Roland/0000-0002-2947-6665
FU National Science FoundationNational Science Foundation (NSF)
   [CyberSEES-1539389]
FX This work has been supported in part by National Science Foundation
   under grant CyberSEES-1539389.
CR Azzam R, 2016, J VIS COMMUN IMAGE R, V36, P90, DOI 10.1016/j.jvcir.2015.11.009
   Balzano L., 2010, 2010 48 ANN ALL C CO, P704
   BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P293, DOI 10.1109/36.377929
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009
   Bubnicki J. W., 2016, METHODS ECOL EVOL
   Bunyak Filiz, 2007, Journal of Multimedia, V2, P20, DOI 10.4304/jmm.2.4.20-33
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheung SCS, 2005, EURASIP J APPL SIG P, V2005, P2330, DOI 10.1155/ASP.2005.2330
   Choudhury S. K., 2016, EVALUATION BACKGROUN
   Dalal N., 2021, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gillis N, 2015, SIAM J MATRIX ANAL A, V36, P1404, DOI 10.1137/140993272
   Giraldo-Zuluaga J.-H., ARXIV170108180
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Hu WC, 2015, J VIS COMMUN IMAGE R, V30, P164, DOI 10.1016/j.jvcir.2015.03.003
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lee K., RESIDUAL FEATURES UN
   Ling Q, 2014, NEUROCOMPUTING, V133, P32, DOI 10.1016/j.neucom.2013.11.034
   Liu LH, 2016, LECT NOTES COMPUT SC, V9914, P676, DOI 10.1007/978-3-319-48881-3_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu CY, 2013, J VIS COMMUN IMAGE R, V24, P111, DOI 10.1016/j.jvcir.2012.05.003
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   Miller AB, 2017, J OUTDOOR REC TOUR, V17, P44, DOI 10.1016/j.jort.2016.09.007
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
   Paragios, 2004, P 2004 IEEE COMP SOC, V2, pII
   Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
   Portmann J, 2014, IEEE INT CONF ROBOT, P1794, DOI 10.1109/ICRA.2014.6907094
   Reddy V, 2013, IEEE T CIRC SYST VID, V23, P83, DOI 10.1109/TCSVT.2012.2203199
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Schick A., 2012, P IEEE COMP SOC C CO, V10, P27, DOI DOI 10.1109/CVPRW.2012.6238923
   Shaoqing Ren R. G. J. S., ARXIV150601497
   Shihao Zhang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P447, DOI 10.1109/ICMEW.2017.8026235
   Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495
   Sobral A, 2016, HANDBOOK OF ROBUST LOW-RANK AND SPARSE MATRIX DECOMPOSITION: APPLICATIONS IN IMAGE AND VIDEO PROCESSING
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Vedaldi A., 2008, VLFEAT OPEN PORTABLE
   Wang SH, 2014, J VIS COMMUN IMAGE R, V25, P263, DOI 10.1016/j.jvcir.2013.11.005
   Wright J., 2009, P NIPS, P2080
   Yeh CH, 2014, J VIS COMMUN IMAGE R, V25, P891, DOI 10.1016/j.jvcir.2014.02.012
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 54
TC 4
Z9 6
U1 1
U2 23
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2018
VL 55
BP 802
EP 815
DI 10.1016/j.jvcir.2018.08.013
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GU5IB
UT WOS:000445318100071
DA 2022-02-10
ER

PT J
AU Yousif, H
   Yuan, JH
   Kays, R
   He, ZH
AF Yousif, Hayder
   Yuan, Jianhe
   Kays, Roland
   He, Zhihai
TI Animal Scanner: Software for classifying humans, animals, and empty
   frames in camera trap images
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE background subtraction; camera trap images; deep convolutional neural
   networks; human-animal detection; wildlife monitoring
ID OCCUPANCY
AB Camera traps are a popular tool to sample animal populations because they are noninvasive, detect a variety of species, and can record many thousands of animal detections per deployment. Cameras are typically set to take bursts of multiple photographs for each detection and are deployed in arrays of dozens or hundreds of sites, often resulting in millions of photographs per study. The task of converting photographs to animal detection records from such large image collections is daunting, and made worse by situations that generate copious empty pictures from false triggers (e.g., camera malfunction or moving vegetation) or pictures of humans. We developed computer vision algorithms to detect and classify moving objects to aid the first step of camera trap image filteringseparating the animal detections from the empty frames and pictures of humans. Our new work couples foreground object segmentation through background subtraction with deep learning classification to provide a fast and accurate scheme for human-animal detection. We provide these programs as both Matlab GUI and command prompt developed with C++. The software reads folders of camera trap images and outputs images annotated with bounding boxes around moving objects and a text file summary of results. This software maintains high accuracy while reducing the execution time by 14 times. It takes about 6 seconds to process a sequence of ten frames (on a 2.6 GHZ CPU computer). For those cameras with excessive empty frames due to camera malfunction or blowing vegetation automatically removes 54% of the false-triggers sequences without influencing the human/animal sequences. We achieve 99.58% on image-level empty versus object classification of Serengeti dataset. We offer the first computer vision tool for processing camera trap images providing substantial time savings for processing large image datasets, thus improving our ability to monitor wildlife across large scales with camera traps.
C1 [Yousif, Hayder; Yuan, Jianhe; He, Zhihai] Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27695 USA.
   [Kays, Roland] North Carolina Museum Nat Sci, Raleigh, NC USA.
RP Yousif, H (corresponding author), Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM hyypp5@mail.missouri.edu
RI Yousif, Hayder/AAG-2259-2020
OI Yousif, Hayder/0000-0002-7638-9505; Kays, Roland/0000-0002-2947-6665
FU National Science FoundationNational Science Foundation (NSF)
   [CyberSEES-1539389]
FX National Science Foundation, Grant/Award Number: CyberSEES-1539389
CR BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P293, DOI 10.1109/36.377929
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bowler MT, 2017, REMOTE SENS ECOL CON, V3, P146, DOI 10.1002/rse2.35
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong P, 2016, IEEE T IMAGE PROCESS, V25, P5035, DOI 10.1109/TIP.2016.2598680
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Huang HC, 2015, SENSORS-BASEL, V15, P27116, DOI 10.3390/s151027116
   Kays R., 2016, CANDID CREATURES CAM
   Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee C.-Y., 2014, ARXIV14095185
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Ren S., 2015, ARXIV150601497
   Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495
   Steenweg R, 2016, BIOL CONSERV, V201, P192, DOI 10.1016/j.biocon.2016.06.020
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Yousif H., 2017, IEEE INT C IM PROC
   Yousif H., 2017, CIRC SYST 2017 ISCAS, P1894
NR 30
TC 16
Z9 16
U1 1
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD FEB
PY 2019
VL 9
IS 4
BP 1578
EP 1589
DI 10.1002/ece3.4747
PG 12
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA HO7HZ
UT WOS:000461114900005
PM 30847057
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Hsing, PY
   Bradley, S
   Kent, VT
   Hill, RA
   Smith, GC
   Whittingham, MJ
   Cokill, J
   Crawley, D
   Volunteers, M
   Stephens, PA
AF Hsing, Pen-Yuan
   Bradley, Steven
   Kent, Vivien T.
   Hill, Russell A.
   Smith, Graham C.
   Whittingham, Mark J.
   Cokill, Jim
   Crawley, Derek
   Volunteers, MammalWeb
   Stephens, Philip A.
TI Economical crowdsourcing for camera trap image classification
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article
DE Camera traps; citizen science; crowdsourcing; data classification; data
   science; MammalWeb
ID CITIZEN SCIENCE; ECOLOGICAL RESEARCH; DATA QUALITY; WILDLIFE;
   CONSERVATION; TECHNOLOGIES; TERRESTRIAL; SENSORS; FUTURE; TOOL
AB Camera trapping is widely used to monitor mammalian wildlife but creates large image datasets that must be classified. In response, there is a trend towards crowdsourcing image classification. For high-profile studies of charismatic faunas, many classifications can be obtained per image, enabling consensus assessments of the image contents. For more local-scale or less charismatic communities, however, demand may outstrip the supply of crowdsourced classifications. Here, we consider MammalWeb, a local-scale project in North East England, which involves citizen scientists in both the capture and classification of sequences of camera trap images. We show that, for our global pool of image sequences, the probability of correct classification exceeds 99% with about nine concordant crowdsourced classifications per sequence. However, there is high variation among species. For highly recognizable species, species-specific consensus algorithms could be even more efficient; for difficult to spot or easily confused taxa, expert classifications might be preferable. We show that two types of incorrect classifications - misidentification of species and overlooking the presence of animals - have different impacts on the confidence of consensus classifications, depending on the true species pictured. Our results have implications for data capture and classification in increasingly numerous, local-scale citizen science projects. The species-specific nature of our findings suggests that the performance of crowdsourcing projects is likely to be highly sensitive to the local fauna and context. The generality of consensus algorithms will, thus, be an important consideration for ecologists interested in harnessing the power of the crowd to assist with camera trapping studies.
C1 [Hsing, Pen-Yuan; Stephens, Philip A.] Univ Durham, Dept Biosci, Conservat Ecol Grp, South Rd, Durham DH1 3LE, England.
   [Bradley, Steven] Univ Durham, Dept Comp Sci, South Rd, Durham DH1 3LE, England.
   [Kent, Vivien T.; Cokill, Jim] Durham Wildlife Trust, Houghton Le Spring DH4 6PU, Tyne & Wear, England.
   [Kent, Vivien T.; Hill, Russell A.] Univ Durham, Dept Anthropol, South Rd, Durham DH1 3LE, England.
   [Smith, Graham C.] Anim & Plant Hlth Agcy, Natl Wildlife Management Ctr, Sand Hutton Campus, York YO41 1LZ, N Yorkshire, England.
   [Whittingham, Mark J.] Newcastle Univ, Sch Nat & Environm Sci, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
   [Crawley, Derek] Mammal Soc, 18 St Johns Church Rd, London E9 6E1, England.
RP Hsing, PY (corresponding author), Univ Durham, Dept Biosci, Conservat Ecol Grp, South Rd, Durham DH1 3LE, England.
EM penyuanhsing@posteo.is
RI Stephens, Philip/B-8397-2008; Hill, Russell/D-9113-2013; Smith, Graham
   C/J-2593-2013
OI Stephens, Philip/0000-0001-5849-788X; Hill, Russell/0000-0002-7601-5802;
   Smith, Graham C/0000-0002-9897-6794; Hsing, Pen-Yuan/0000-0002-5394-879X
FU Heritage Lottery Fund [OH-14-06474]; Durham University; British
   Ecological Society
FX This work was supported by the Heritage Lottery Fund (OH-14-06474),
   Durham University, British Ecological Society.
CR AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Battersby JE, 2004, MAMMAL REV, V34, P3, DOI 10.1046/j.0305-1838.2003.00023.x
   Bonney R, 2014, SCIENCE, V343, P1436, DOI 10.1126/science.1251554
   Bower A., 2015, 2015 C HUM COMP CROW
   Cohn JP, 2008, BIOSCIENCE, V58, P192, DOI 10.1641/B580303
   Conrad CC, 2011, ENVIRON MONIT ASSESS, V176, P273, DOI 10.1007/s10661-010-1582-5
   Croft S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176339
   Danielsen F, 2014, BIOSCIENCE, V64, P236, DOI 10.1093/biosci/biu001
   Dickinson JL, 2012, FRONT ECOL ENVIRON, V10, P291, DOI 10.1890/110236
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Eveleigh A., 2014, DESIGNING DABBLERS D, P2985, DOI 10.1145/2556288.2557262
   Everett G, 2016, BMC ECOL, V16, DOI 10.1186/s12898-016-0062-3
   Forrester TD, 2017, BIOL CONSERV, V208, P98, DOI 10.1016/j.biocon.2016.06.025
   Gaston KJ, 2008, TRENDS ECOL EVOL, V23, P14, DOI 10.1016/j.tree.2007.11.001
   Geider RJ, 2001, GLOBAL CHANGE BIOL, V7, P849, DOI 10.1046/j.1365-2486.2001.00448.x
   Goodchild MF, 2007, GEOJOURNAL, V69, P211, DOI 10.1007/s10708-007-9111-y
   Greenwood JJD, 2007, J ORNITHOL, V148, pS77, DOI 10.1007/s10336-007-0239-9
   Grolemond G, 2011, J STAT SOFTW, V40, P1
   Haklay M, 2013, CROWDSOURCING GEOGRA, P105, DOI DOI 10.1007/978-94-007-4587-2_7
   Harris S. J., 2016, BREEDING BIRD SURVEY
   Hennon C. C., 2014, B AM METEOROL SOC, V96, P591
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Jennett C, 2016, JCOM-J SCI COMMUN, V15, DOI 10.22323/2.15030205
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Lorimer J, 2007, ENVIRON PLANN D, V25, P911, DOI 10.1068/d71j
   Lukyanenko R, 2016, CONSERV BIOL, V30, P447, DOI 10.1111/cobi.12706
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Millard S.P., 2013, ENVSTATS R PACKAGE E
   Newman G, 2012, FRONT ECOL ENVIRON, V10, P298, DOI 10.1890/110294
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   R Core Team, 2017, R LANG ENV STAT COMP
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   Ratcliff Jessica, 2008, TRANSIT VENUS ENTERP
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Sauermann H, 2015, P NATL ACAD SCI USA, V112, P679, DOI 10.1073/pnas.1408907112
   Siddharthan A, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2776896
   Silvertown J, 2015, ZOOKEYS, P125, DOI 10.3897/zookeys.480.8803
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stephens PA, 2015, J APPL ECOL, V52, P1, DOI 10.1111/1365-2664.12383
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Thom H., 2017, THESIS
   van der Wal R, 2016, CONSERV BIOL, V30, P550, DOI 10.1111/cobi.12705
   Verma A, 2016, GEOFORUM, V75, P75, DOI 10.1016/j.geoforum.2016.07.002
   Wald DM, 2016, CONSERV BIOL, V30, P562, DOI 10.1111/cobi.12627
   Wickham H., 2017, dplyr: A grammar of data manipulation (R package Version 0.7.8) [Computer software]
   Wickham H., 2016, GGPLOT2 ELEGANT GRAP, DOI [10.1007/978-3-319-24277-4, DOI 10.1007/978-3-319-24277-4_9]
   Willett KW, 2013, MON NOT R ASTRON SOC, V435, P2835, DOI 10.1093/mnras/stt1458
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 53
TC 16
Z9 17
U1 3
U2 13
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
PD DEC
PY 2018
VL 4
IS 4
BP 361
EP 374
DI 10.1002/rse2.84
PG 14
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA HD7XT
UT WOS:000452768000006
OA gold, Green Published, Green Accepted
DA 2022-02-10
ER

PT J
AU Falzon, G
   Lawson, C
   Cheung, KW
   Vernes, K
   Ballard, GA
   Fleming, PJS
   Glen, AS
   Milne, H
   Mather-Zardain, A
   Meek, PD
AF Falzon, Greg
   Lawson, Christopher
   Cheung, Ka-Wai
   Vernes, Karl
   Ballard, Guy A.
   Fleming, Peter J. S.
   Glen, Alistair S.
   Milne, Heath
   Mather-Zardain, Atalya
   Meek, Paul D.
TI ClassifyMe: A Field-Scouting Software for the Identification of Wildlife
   in Camera Trap Images
SO ANIMALS
LA English
DT Article
DE camera traps; camera trap data management; deep learning; ecological
   software; species recognition; wildlife monitoring
AB Simple Summary Camera trap wildlife surveys can generate vast amounts of imagery. A key problem in the wildlife ecology field is that vast amounts of time is spent reviewing this imagery to identify the species detected. Valuable resources are wasted, and the scale of studies is limited by this review process. The use of computer software capable of extracting false positives, automatically identifying animals detected and sorting imagery could greatly increase efficiency. Artificial intelligence has been demonstrated as an effective option for automatically identifying species from camera trap imagery. Currently available code bases are inaccessible to the majority of users; requiring high-performance computers, advanced software engineering skills and, often, high-bandwidth internet connections to access cloud services. The ClassifyMe software tool is designed to address this gap and provides users the opportunity to utilise state-of-the-art image recognition algorithms without the need for specialised computer programming skills. ClassifyMe is especially designed for field researchers, allowing users to sweep through camera trap imagery using field computers instead of office-based workstations.
   Abstract We present ClassifyMe a software tool for the automated identification of animal species from camera trap images. ClassifyMe is intended to be used by ecologists both in the field and in the office. Users can download a pre-trained model specific to their location of interest and then upload the images from a camera trap to a laptop or workstation. ClassifyMe will identify animals and other objects (e.g., vehicles) in images, provide a report file with the most likely species detections, and automatically sort the images into sub-folders corresponding to these species categories. False Triggers (no visible object present) will also be filtered and sorted. Importantly, the ClassifyMe software operates on the user's local machine (own laptop or workstation)-not via internet connection. This allows users access to state-of-the-art camera trap computer vision software in situ, rather than only in the office. The software also incurs minimal cost on the end-user as there is no need for expensive data uploads to cloud services. Furthermore, processing the images locally on the users' end-device allows them data control and resolves privacy issues surrounding transfer and third-party access to users' datasets.
C1 [Falzon, Greg; Lawson, Christopher; Cheung, Ka-Wai] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
   [Vernes, Karl; Ballard, Guy A.; Fleming, Peter J. S.; Meek, Paul D.] Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2351, Australia.
   [Ballard, Guy A.; Milne, Heath] NSW Dept Primary Ind, Vertebrate Pest Res Unit, Allingham St, Armidale, NSW 2351, Australia.
   [Fleming, Peter J. S.] NSW Dept Primary Ind, Vertebrate Pest Res Unit, 1447 Forest Rd, Orange, NSW 2800, Australia.
   [Glen, Alistair S.] Manaaki Whenua Landcare Res, Private Bag 92170, Auckland 1142, New Zealand.
   [Mather-Zardain, Atalya] IO Design Australia, Armidale, NSW 2350, Australia.
   [Meek, Paul D.] NSW Dept Primary Ind, Vertebrate Pest Res Unit, POB 530, Coffs Harbour, NSW 2450, Australia.
RP Falzon, G (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
EM gfalzon2@une.edu.au; clawso21@une.edu.au; kcheun22@une.edu.au;
   kvernes@une.edu.au; guy.ballard@dpi.nsw.gov.au;
   peter.fleming@dpi.nsw.gov.au; GlenA@landcareresearch.co.nz;
   heath.milne@dpi.nsw.gov.au; io.atalya@gmail.com;
   paul.meek@dpi.nsw.gov.au
RI Falzon, Greg A/A-2657-2012; Vernes, Karl/A-2925-2011
OI Falzon, Greg A/0000-0002-1989-9357; Vernes, Karl/0000-0003-1635-9950;
   Meek, Paul/0000-0002-3792-5723; Ballard, Guy/0000-0002-0287-9720;
   Fleming, Peter/0000-0002-3490-6148
FU Centre for Invasive Species Solutions; Department of Agriculture and
   Water ResourcesAustralian GovernmentDepartment of Agriculture and Water
   Resources; NSW Department of Primary Industries, University of New
   England; Meat and Livestock AustraliaMeat and Livestock Australia;
   Australian Wool InnovationAustralian Wool Innovation
FX This research was funded by the 'Wild Dog Alert' research initiative
   delivered through the Invasive Animals Cooperative Research Centre (now
   Centre for Invasive Species Solutions), with major financial and in kind
   resources provided by the Department of Agriculture and Water Resources
   and NSW Department of Primary Industries, University of New England,
   Meat and Livestock Australia and Australian Wool Innovation.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Adamko P, 2017, GLOBALIZATION AND ITS SOCIO-ECONOMIC CONSEQUENCES, PTS I - VI, P1
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Beery S., 2018, P EUR C COMP VIS ECC, P456
   Bennett EL, 2015, CONSERV BIOL, V29, P54, DOI 10.1111/cobi.12377
   Butler DA, 2013, TORTS LAW J, V20, P235
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Claridge AW, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P205
   Csillik O, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2040039
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Falzon G., 2018, P 31 AUSTR WILDL MAN, P102
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009
   Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010
   Forrester T., 2013, P 98 ANN M EC SOC AM
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Gormley AM, 2011, J APPL ECOL, V48, P25, DOI 10.1111/j.1365-2664.2010.01911.x
   Gowen C, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P61
   Harmsen BJ, 2009, J MAMMAL, V90, P612, DOI 10.1644/08-MAMM-A-140R.1
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Jackson RM, 2006, WILDLIFE SOC B, V34, P772, DOI 10.2193/0091-7648(2006)34[772:ESLPAU]2.0.CO;2
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Khorozyan IG, 2008, INTEGR ZOOL, V3, P322, DOI 10.1111/j.1749-4877.2008.00111.x
   Lindenmayer D, 2017, SCIENCE, V356, P800, DOI 10.1126/science.aan1362
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Liu W, 2016, EUR C COMP VIS, V21, P37, DOI DOI 10.1007/978-3-319-46448-0_2
   Meek P., 2014, CAMERA TRAPPING WILD
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Meek P.D., 2015, CAMERA TRAPPING WILD
   Meek P.D., 2016, CAMERA TRAPPING WILD, P219
   Meek Paul D., 2020, Australian Zoologist, V40, P392, DOI 10.7882/AZ.2019.035
   Meek PD, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P349
   Meek PD, 2012, AUST MAMMAL, V34, P223, DOI 10.1071/AM11032
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Ramachandran P, 2018, METHODS ECOL EVOL, V9, P785, DOI 10.1111/2041-210X.12892
   Ramsey David S.L., 2015, Journal of Wildlife Management, V79, P491
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28
   Rovero F., 2016, CAMERA TRAPPING WILD
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Trolle M, 2003, J MAMMAL, V84, P607, DOI 10.1644/1545-1542(2003)084<0607:EOODIT>2.0.CO;2
   Valan M, 2019, SYST BIOL, V68, P876, DOI 10.1093/sysbio/syz014
   Vernes K, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P215
   Vernes Karl, 2015, Cat News, V62, P18
   Vernes K, 2014, AUST MAMMAL, V36, P128, DOI 10.1071/AM13037
   Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang J, 2016, ARTIF INTELL REV, V46, P543, DOI 10.1007/s10462-016-9491-9
   Zhang X, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124308
   Zolanvari, 2018, J OPEN SOURCE SOFTW, V3, P729, DOI [10.21105/joss.00729, DOI 10.21105/JOSS.00729]
NR 65
TC 16
Z9 17
U1 6
U2 20
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD JAN
PY 2020
VL 10
IS 1
AR 58
DI 10.3390/ani10010058
PG 16
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Veterinary Sciences; Zoology
GA KO2FZ
UT WOS:000515364400058
PM 31892236
OA Green Published, gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Whytock, RC
   Swiezewski, J
   Zwerts, JA
   Pambo, AFK
   Rogala, M
   Bahaa-el-din, L
   Boekee, K
   Brittain, S
   Cardoso, AW
   Henschel, P
   Lehmann, D
   Momboua, B
   Opepa, CK
   Orbell, C
   Pitman, RT
   Robinson, HS
   Abernethy, KA
AF Whytock, Robin C.
   Swiezewski, Jedrzej
   Zwerts, Joeri A.
   Pambo, Aurelie Flore Koumba
   Rogala, Marek
   Bahaa-el-din, Laila
   Boekee, Kelly
   Brittain, Stephanie
   Cardoso, Anabelle W.
   Henschel, Philipp
   Lehmann, David
   Momboua, Brice
   Opepa, Cisquet Kiebou
   Orbell, Christopher
   Pitman, Ross T.
   Robinson, Hugh S.
   Abernethy, Katharine A.
TI Robust ecological analysis of camera trap data labelled by a machine
   learning model
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE artificial intelligence; biodiversity; birds; Central Africa; mammals
ID ESTIMATING SITE OCCUPANCY
AB Ecological data are collected over vast geographic areas using digital sensors such as camera traps and bioacoustic recorders. Camera traps have become the standard method for surveying many terrestrial mammals and birds, but camera trap arrays often generate millions of images that are time-consuming to label. This causes significant latency between data collection and subsequent inference, which impedes conservation at a time of ecological crisis. Machine learning algorithms have been developed to improve the speed of labelling camera trap data, but it is uncertain how the outputs of these models can be used in ecological analyses without secondary validation by a human.
   Here, we present our approach to developing, testing and applying a machine learning model to camera trap data for the purpose of achieving fully automated ecological analyses. As a case-study, we built a model to classify 26 Central African forest mammal and bird species (or groups). The model generalizes to new spatially and temporally independent data (n = 227 camera stations, n = 23,868 images), and outperforms humans in several respects (e.g. detecting 'invisible' animals). We demonstrate how ecologists can evaluate a machine learning model's precision and accuracy in an ecological context by comparing species richness, activity patterns (n = 4 species tested) and occupancy (n = 4 species tested) derived from machine learning labels with the same estimates derived from expert labels.
   Results show that fully automated species labels can be equivalent to expert labels when calculating species richness, activity patterns (n = 4 species tested) and estimating occupancy (n = 3 of 4 species tested) in a large, completely out-of-sample test dataset. Simple thresholding using the Softmax values (i.e. excluding 'uncertain' labels) improved the model's performance when calculating activity patterns and estimating occupancy but did not improve estimates of species richness.
   We conclude that, with adequate testing and evaluation in an ecological context, a machine learning model can generate labels for direct use in ecological analyses without the need for manual validation. We provide the user-community with a multi-platform, multi-language graphical user interface that can be used to run our model offline.
C1 [Whytock, Robin C.; Lehmann, David; Orbell, Christopher; Abernethy, Katharine A.] Univ Stirling, Fac Nat Sci, Stirling, Scotland.
   [Whytock, Robin C.; Pambo, Aurelie Flore Koumba; Lehmann, David; Momboua, Brice] Agence Natl Pares Nationaux, Libreville, Gabon.
   [Swiezewski, Jedrzej; Rogala, Marek] Appsilon AI Good, Warsaw, Poland.
   [Zwerts, Joeri A.] Univ Utrecht, Utrecht, Netherlands.
   [Bahaa-el-din, Laila] Univ KwaZulu Natal, Sch Life Sci, Pietermaritzburg, South Africa.
   [Boekee, Kelly] Program Sustainable Management Nat Resources, Buea, Cameroon.
   [Boekee, Kelly] Smithsonian Trop Res Inst, Ctr Trop Forest Sci, Balboa, Ancon, Panama.
   [Brittain, Stephanie] Univ Oxford, Interdisciplinary Ctr Conservat Sci, Dept Zool, Oxford, England.
   [Brittain, Stephanie] Zool Soc London, Inst Zool, London, England.
   [Cardoso, Anabelle W.] Yale Univ, Dept Ecol & Evolutionary Biol, New Haven, CT USA.
   [Henschel, Philipp; Orbell, Christopher; Pitman, Ross T.; Robinson, Hugh S.] Panthera, New York, NY USA.
   [Henschel, Philipp; Abernethy, Katharine A.] CENAREST, Inst Rech Ecol Trop, Libreville, Gabon.
   [Opepa, Cisquet Kiebou] Wildlife Conservat Soc, Kinshasa, DEM REP CONGO.
   [Robinson, Hugh S.] Univ Montana, Wildlife Biol Program, WA Franke Coll Forestry & Conservat, Missoula, MT 59812 USA.
RP Whytock, RC (corresponding author), Univ Stirling, Fac Nat Sci, Stirling, Scotland.; Whytock, RC (corresponding author), Agence Natl Pares Nationaux, Libreville, Gabon.
EM robbie.whytock1@stir.ac.uk
OI Rogala, Marek/0000-0002-9949-4551; Zwerts, Joeri/0000-0003-3841-6389;
   Swiezewski, Jedrzej/0000-0001-7005-8003; Cardoso, Anabelle
   Williamson/0000-0002-4327-7259; Brittain, Stephanie/0000-0002-7865-0391;
   Whytock, Robin/0000-0002-0127-6071
FU Google Cloud for EducationGoogle Incorporated; EU 11th FED ECOFAC6;
   University of Oxford Hertford College Mortimer May Fund
FX Google Cloud for Education; EU 11th FED ECOFAC6; University of Oxford
   Hertford College Mortimer May Fund
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
   Arje J., 2019, ARXIV170806899CSQBIO
   Baha-El-Din Laila, 2013, Small Carnivore Conservation, V48, P19
   Bahaa-el-din L, 2018, AFR J ECOL, V56, P690, DOI 10.1111/aje.12581
   Beery S., 2019, BIODIVERSITY INFORM, V3, pe37222, DOI [10.3897/biss.3.37222, DOI 10.3897/BISS.3.37222]
   Beery S., 2018, P EUR C COMP VIS ECC, P456
   Bessone M, 2020, J APPL ECOL, V57, P963, DOI 10.1111/1365-2664.13602
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Cardoso AW, 2020, ECOSYSTEMS, V23, P602, DOI 10.1007/s10021-019-00424-3
   Dietze MC, 2018, P NATL ACAD SCI USA, V115, P1424, DOI 10.1073/pnas.1710231115
   Farley SS, 2018, BIOSCIENCE, V68, P563, DOI 10.1093/biosci/biy068
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Howard J, 2020, INFORMATION, V11, DOI 10.3390/info11020108
   Kurakin A, 2017, ARXIV160702533CSSTAT ARXIV160702533CSSTAT
   Lin H, 2020, RESNEST SPLIT ATTENT RESNEST SPLIT ATTENT
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Norouzzadeh M. S., 2019, ARXIV191009716CSEESS
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2020, REMOTE SENS ECOL CON, V6, P168, DOI 10.1002/rse2.132
   Orbell C., 2021, DATASTORRE STIRLING
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
   Schneider S., 2018, ARXIV180310842CS
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Smith L. N., 2018, RXIV180309820CSSTAT
   Sun Y., 2017, ARXIV170604599CS
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tan M., 2020, ARXIV190511946CSSTAT
   Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
NR 38
TC 5
Z9 5
U1 6
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JUN
PY 2021
VL 12
IS 6
BP 1080
EP 1092
DI 10.1111/2041-210X.13576
EA MAR 2021
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SL1YX
UT WOS:000627162500001
OA Green Published, hybrid
DA 2022-02-10
ER

PT C
AU Giraldo-Zuluaga, JH
   Salazar, A
   Gomez, A
   Diaz-Pulido, A
AF Giraldo-Zuluaga, Jhony-Heriberto
   Salazar, Augusto
   Gomez, Alexander
   Diaz-Pulido, Angelica
GP IEEE
TI Recognition of Mammal Genera on Camera-Trap Images using Multi-Layer
   Robust Principal Component Analysis and Mixture Neural Networks
SO 2017 IEEE 29TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE (ICTAI 2017)
SE Proceedings-International Conference on Tools With Artificial
   Intelligence
LA English
DT Proceedings Paper
CT 29th Annual IEEE International Conference on Tools with Artificial
   Intelligence (ICTAI)
CY NOV 06-08, 2017
CL Boston, MA
SP IEEE, IEEE Comp Soc
DE Camera-trap; mammal recognition; Convolutional Neural Networks;
   Multi-Layer Robust Principal Component Analysis; Least Absolute
   Shrinkage and Selection Operator
ID CLASSIFICATION
AB The segmentation and classification of animals from camera-trap images is a difficult task due to the conditions under which the images are taken. This work presents a method for recognizing mammal genera from camera-trap images. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA) for segmenting, Convolutional Neural Networks (CNNs) for extracting features, Least Absolute Shrinkage and Selection Operator (LASSO) for selecting features, and Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for classifying mammal genera present in the Colombian forest. Our classification method mixes the features of several CNNs. We evaluated our method with the camera-trap images from the Instituto de Investigacion de Recursos Biologicos Alexander von Humboldt. We obtained an accuracy of 92.65% classifying 8 mammal genera and a False Positive (FP) class, using automatic-segmented images. On the other hand, we reached 90.32% of accuracy classifying 10 mammal genera, using ground-truth images only. Unlike all previous works, we confront the animal segmentation and genera classification on the camera-trap framework. This method shows a new approach toward a fully-automatic detection of animals from camera-trap images.
C1 [Giraldo-Zuluaga, Jhony-Heriberto; Salazar, Augusto; Gomez, Alexander] Univ Antioquia, Grp Invest SISTEMIC, Medellin, Colombia.
   [Diaz-Pulido, Angelica] Inst Invest Recursos Biol Alexander von Humboldt, Bogota, Colombia.
RP Giraldo-Zuluaga, JH (corresponding author), Univ Antioquia, Grp Invest SISTEMIC, Medellin, Colombia.
EM robilas@montclair.edu; augusto.salazar@udea.edu.co; alviurlex@gmail.com;
   adiaz@humboldt.org.co
RI Zuluaga, Jhony Heriberto Giraldo/O-7502-2019
OI Zuluaga, Jhony Heriberto Giraldo/0000-0002-0039-1270
FU Colombian National Fund for Science, Technology and Innovation,
   Francisco Jose de Caldas - COLCIENCIAS (Colombia) [111571451061]
FX This work was supported by the Colombian National Fund for Science,
   Technology and Innovation, Francisco Jose de Caldas - COLCIENCIAS
   (Colombia). Project No. 111571451061.
CR Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Giraldo-Zuluaga J.-H., 2017, ARXIV170108180
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kumar YHS, 2015, PROCEDIA COMPUT SCI, V45, P336, DOI 10.1016/j.procs.2015.03.156
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Santoro A., 2016, ARXIV160506065
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zahzah, 2015, ROBUST LOW RANK SPAR
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 22
TC 7
Z9 7
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1082-3409
BN 978-1-5386-3876-7
J9 PROC INT C TOOLS ART
PY 2017
BP 53
EP 60
DI 10.1109/ICTAI.2017.00020
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BK3OG
UT WOS:000435294700009
DA 2022-02-10
ER

PT J
AU Hoye, TT
   Arje, J
   Bjerge, K
   Hansen, OLP
   Iosifidis, A
   Leese, F
   Mann, HMR
   Meissner, K
   Melvad, C
   Raitoharju, J
AF Hoye, Toke T.
   Arje, Johanna
   Bjerge, Kim
   Hansen, Oskar L. P.
   Iosifidis, Alexandros
   Leese, Florian
   Mann, Hjalte M. R.
   Meissner, Kristian
   Melvad, Claus
   Raitoharju, Jenni
TI Deep learning and computer vision will transform entomology
SO PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA
LA English
DT Article
DE automated monitoring; ecology; insects; image-based identification;
   machine learning
ID CAMERA-TRAPS; INSECT; RADAR; TERRESTRIAL; CLASSIFICATION;
   IDENTIFICATION; DIGITIZATION; BIODIVERSITY; POLLINATORS; NETWORKS
AB Most animal species on Earth are insects, and recent reports suggest that their abundance is in drastic decline. Although these reports come from a wide range of insect taxa and regions, the evidence to assess the extent of the phenomenon is sparse. Insect populations are challenging to study, and most monitoring methods are labor intensive and inefficient. Advances in computer vision and deep learning provide potential new solutions to this global challenge. Cameras and other sensors can effectively, continuously, and noninvasively perform entomological observations throughout diurnal and seasonal cycles. The physical appearance of specimens can also be captured by automated imaging in the laboratory. When trained on these data, deep learning models can provide estimates of insect abundance, biomass, and diversity. Further, deep learning models can quantify variation in phenotypic traits, behavior, and interactions. Here, we connect recent developments in deep learning and computer vision to the urgent demand for more cost-efficient monitoring of insects and other invertebrates. We present examples of sensor-based monitoring of insects. We show how deep learning tools can be applied to exceptionally large datasets to derive ecological information and discuss the challenges that lie ahead for the implementation of such solutions in entomology. We identify four focal areas, which will facilitate this transformation: 1) validation of image-based taxonomic identification; 2) generation of sufficient training data; 3) development of public, curated reference databases; and 4) solutions to integrate deep learning and molecular tools.
C1 [Hoye, Toke T.; Arje, Johanna; Hansen, Oskar L. P.; Mann, Hjalte M. R.] Aarhus Univ, Dept Biosci, DK-8410 Ronde, Denmark.
   [Hoye, Toke T.; Arje, Johanna; Hansen, Oskar L. P.; Mann, Hjalte M. R.; Melvad, Claus] Aarhus Univ, Arctic Res Ctr, DK-8410 Ronde, Denmark.
   [Arje, Johanna] Tampere Univ, Unit Comp Sci, FI-33720 Tampere, Finland.
   [Bjerge, Kim; Melvad, Claus] Aarhus Univ, Sch Engn, DK-8200 Aarhus N, Denmark.
   [Hansen, Oskar L. P.] Nat Hist Museum Aarhus, DK-8000 Aarhus C, Denmark.
   [Hansen, Oskar L. P.] Aarhus Univ, Dept Biol, Ctr Biodivers Dynam Changing World, DK-8000 Aarhus C, Denmark.
   [Hansen, Oskar L. P.] Aarhus Univ, Dept Biol Ecoinformat & Biodivers, DK-8000 Aarhus C, Denmark.
   [Iosifidis, Alexandros] Aarhus Univ, Dept Engn, DK-8200 Aarhus, Denmark.
   [Leese, Florian] Univ Duisburg Essen, Aquat Ecosyst Res, D-45141 Essen, Germany.
   [Meissner, Kristian; Raitoharju, Jenni] Finnish Environm Inst, Programme Environm Informat, FI-40500 Jyvaskyla, Finland.
RP Hoye, TT (corresponding author), Aarhus Univ, Dept Biosci, DK-8410 Ronde, Denmark.; Hoye, TT (corresponding author), Aarhus Univ, Arctic Res Ctr, DK-8410 Ronde, Denmark.
EM tth@bios.au.dk
RI Høye, Toke Thomas/A-7701-2008; Iosifidis, Alexandros/G-2433-2013; Leese,
   Florian/D-4277-2012; Meissner, Kristian/E-8390-2014
OI Høye, Toke Thomas/0000-0001-5387-3284; Iosifidis,
   Alexandros/0000-0003-4807-1345; Bjerge, Kim/0000-0001-6742-9504; Mann,
   Hjalte/0000-0002-4768-4767; Leese, Florian/0000-0002-5465-913X; Hansen,
   Oskar/0000-0002-1598-5733; Raitoharju, Jenni/0000-0003-4631-9298;
   Meissner, Kristian/0000-0001-6316-8554; Arje,
   Johanna/0000-0003-0710-9044; Melvad, Claus/0000-0002-5720-6523
FU Villum FoundationVillum Foundation [17523]; Independent Research Fund
   Denmark Grant [8021-00423B]; Nordic Council of Ministers Project
   [18103]; Academy of FinlandAcademy of FinlandEuropean Commission
   [324475]
FX David Wagner is thanked for convening the session "Insect declines in
   the Anthropocene" at the Entomological Society of America Annual Meeting
   2019 in St. Louis, MO, which brought the group of contributors to the
   special feature together. Valuable inputs to the manuscript from David
   Wagner, Matthew L. Forister, and four anonymous reviewers are greatly
   appreciated. T.T.H. acknowledges funding from Villum Foundation Grant
   17523 and Independent Research Fund Denmark Grant 8021-00423B. K.M.
   acknowledges funding from Nordic Council of Ministers Project 18103
   (SCANDNAnet). J.R. acknowledges funding from Academy of Finland Project
   324475.
CR Arje J, 2020, METHODS ECOL EVOL, V11, P922, DOI 10.1111/2041-210X.13428
   Arje J, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115917
   Balla E, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20040982
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Barlow SE, 2020, CURR OPIN INSECT SCI, V38, P15, DOI 10.1016/j.cois.2020.01.008
   Blagoderov V, 2012, ZOOKEYS, P133, DOI 10.3897/zookeys.209.3178
   Blowes SA, 2019, SCIENCE, V366, P339, DOI 10.1126/science.aaw1620
   Bonawitz K., 2019, ARXIV190201046
   Braukmann TWA, 2019, MOL ECOL RESOUR, V19, P711, DOI 10.1111/1755-0998.13008
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Bzdok D, 2018, NAT METHODS, V15, P232, DOI 10.1038/nmeth.4642
   Cardoso P, 2011, BIOL CONSERV, V144, P2647, DOI 10.1016/j.biocon.2011.07.024
   Ceballos G, 2017, P NATL ACAD SCI USA, V114, pE6089, DOI 10.1073/pnas.1704949114
   Chapman JW, 2011, ANNU REV ENTOMOL, V56, P337, DOI 10.1146/annurev-ento-120709-144820
   Chapman JW, 2004, INT J PEST MANAGE, V50, P225, DOI 10.1080/09670870410001731961
   Chapman JW, 2002, COMPUT ELECTRON AGR, V35, P95, DOI 10.1016/S0168-1699(02)00013-3
   Chapman JW, 2002, ECOL ENTOMOL, V27, P641, DOI 10.1046/j.1365-2311.2002.00472.x
   Chen YP, 2014, JOVE-J VIS EXP, DOI 10.3791/52111
   Chesmore ED, 2004, B ENTOMOL RES, V94, P319, DOI 10.1079/BER2004306
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Collett RA, 2017, ECOL EVOL, V7, P7527, DOI 10.1002/ece3.3275
   Cordier T, 2018, MOL ECOL RESOUR, V18, P1381, DOI 10.1111/1755-0998.12926
   Tran DT, 2020, IEEE T NEUR NET LEAR, V31, P710, DOI 10.1109/TNNLS.2019.2914082
   Tran DT, 2018, EUR SIGNAL PR CONF, P405, DOI 10.23919/EUSIPCO.2018.8553494
   Tran DT, 2018, NEURAL NETWORKS, V105, P328, DOI 10.1016/j.neunet.2018.05.017
   de la Hidalga A. Nieva, DATA QUALITY MANAGEM, DOI [10.5281/zenodo.3469520, DOI 10.5281/ZENODO.3469520]
   Dean J., 2012, P 25 INT C NEURAL IN, P1223
   Ding WG, 2016, COMPUT ELECTRON AGR, V123, P17, DOI 10.1016/j.compag.2016.02.003
   Dombos M, 2017, METHODS ECOL EVOL, V8, P313, DOI 10.1111/2041-210X.12662
   Dornelas M, 2018, GLOBAL ECOL BIOGEOGR, V27, P760, DOI 10.1111/geb.12729
   Elbrecht V, 2019, PEERJ, V7, DOI 10.7717/peerj.7745
   Elbrecht V, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130324
   Estes L, 2018, NAT ECOL EVOL, V2, P819, DOI 10.1038/s41559-018-0524-4
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Geng CX, 2021, IEEE T PATTERN ANAL, V43, P3614, DOI 10.1109/TPAMI.2020.2981604
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hajibabaei M, 2006, P NATL ACAD SCI USA, V103, P968, DOI 10.1073/pnas.0510466103
   Hallmann CA, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185809
   Hamel S, 2013, METHODS ECOL EVOL, V4, P105, DOI 10.1111/j.2041-210x.2012.00262.x
   Hansen OLP, 2020, ECOL EVOL, V10, P737, DOI 10.1002/ece3.5921
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hebert PDN, 2004, P NATL ACAD SCI USA, V101, P14812, DOI 10.1073/pnas.0406166101
   Hedrick BP, 2020, BIOSCIENCE, V70, P243, DOI 10.1093/biosci/biz163
   Helsing-Nielsen F., 2020, LIGHT TRAP COMPUTER, DOI [10.1101/2020.03.18.996447., DOI 10.1101/2020.03.18.996447]
   Horn G. V., 2017, ARXIV170706642
   Hortal J, 2015, ANNU REV ECOL EVOL S, V46, P523, DOI 10.1146/annurev-ecolsys-112414-054400
   Hoye T. T., 2020, 371 AARH U DCE NAT C
   Hu G, 2016, SCIENCE, V354, P1584, DOI 10.1126/science.aah4379
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Hueppop O, 2019, ECOGRAPHY, V42, P912, DOI 10.1111/ecog.04063
   Inoue H., 2018, ARXIV180102929
   Jeliazkov A, 2016, GLOB ECOL CONSERV, V6, P208, DOI 10.1016/j.gecco.2016.02.008
   Jing XY, 2021, IEEE T PATTERN ANAL, V43, P139, DOI 10.1109/TPAMI.2019.2929166
   Joly A., 2019, OVERVIEW LIFECLEF 20, P387
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7_24
   Kalamatianos R, 2018, J IMAGING, V4, DOI 10.3390/jimaging4110129
   Kawakita S, 2019, APIDOLOGIE, V50, P71, DOI 10.1007/s13592-018-0619-6
   Kelly MG, 2015, WIRES WATER, V2, P159, DOI 10.1002/wat2.1068
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kiskin I, 2020, NEURAL COMPUT APPL, V32, P915, DOI 10.1007/s00521-018-3626-7
   Kissling WD, 2014, BIOL REV, V89, P511, DOI 10.1111/brv.12065
   Krehenwinkel H, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-17333-x
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leese F, 2018, ADV ECOL RES, V58, P63, DOI 10.1016/bs.aecr.2018.01.001
   Lu CY, 2019, IFAC PAPERSONLINE, V52, P1, DOI 10.1016/j.ifacol.2019.12.406
   Luo Wenhan, 2014, ARXIV14097618
   Macher JN, 2016, ECOL INDIC, V61, P159, DOI 10.1016/j.ecolind.2015.08.024
   MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a
   Maggiora R, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-48511-8
   Martineau M, 2017, PATTERN RECOGN, V65, P273, DOI 10.1016/j.patcog.2016.12.020
   Mayo M, 2007, KNOWL-BASED SYST, V20, P195, DOI 10.1016/j.knosys.2006.11.012
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Meineke EK, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11369
   Meineke EK, 2019, PHILOS T R SOC B, V374, DOI 10.1098/rstb.2017.0393
   Milosevic D, 2020, SCI TOTAL ENVIRON, V711, DOI 10.1016/j.scitotenv.2019.135160
   Montgomery GA, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108327
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Park J, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57875-1
   Pawar S, 2003, BIOSCIENCE, V53, P861, DOI 10.1641/0006-3568(2003)053[0861:TCATMC]2.0.CO;2
   Pegoraro L, 2020, EMERG TOP LIFE SCI, V4, P87, DOI 10.1042/ETLS20190074
   Perera P, 2019, PROC CVPR IEEE, P11536, DOI 10.1109/CVPR.2019.01181
   Piechaud N, 2019, MAR ECOL PROG SER, V615, P15, DOI 10.3354/meps12925
   Poland TM, 2019, J PEST SCI, V92, P37, DOI 10.1007/s10340-018-1004-y
   Potamitis I, 2018, J SENSORS, V2018, DOI 10.1155/2018/3949415
   Potamitis I, 2017, ROBOTICS, V6, DOI 10.3390/robotics6030019
   Raitoharju J, 2019, 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2019), P1338, DOI 10.1109/SSCI44817.2019.9002975
   Raitoharju J, 2018, IMAGE VISION COMPUT, V78, P73, DOI 10.1016/j.imavis.2018.06.005
   Ratnasingham S, 2007, MOL ECOL NOTES, V7, P355, DOI 10.1111/j.1471-8286.2007.01678.x
   Ratnasingham S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066213
   Ruczynski I, 2020, METHODS ECOL EVOL, V11, P294, DOI 10.1111/2041-210X.13339
   Rustia D. J. A., 2019, P 2019 ASABE ANN INT, DOI [DOI 10.13031/AIM.201900477, 10.13031/aim.201900477]
   Santos DAA, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SMART AND SUSTAINABLE TECHNOLOGIES (SPLITECH), P23
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Seibold S, 2019, NATURE, V574, P671, DOI 10.1038/s41586-019-1684-3
   Short AEZ, 2018, ANNU REV ENTOMOL, V63, P513, DOI 10.1146/annurev-ento-031616-035536
   Sohrab F., 2020, ARXIV200210420
   Steen R, 2017, METHODS ECOL EVOL, V8, P203, DOI 10.1111/2041-210X.12654
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stepanian PM, 2020, P NATL ACAD SCI USA, V117, P2987, DOI 10.1073/pnas.1913598117
   Strobel B, 2018, ZOOKEYS, P1, DOI 10.3897/zookeys.759.24584
   Sun Y, 2018, BIOSYST ENG, V176, P140, DOI 10.1016/j.biosystemseng.2018.10.012
   Thomsen PF, 2019, ECOL EVOL, V9, P1665, DOI 10.1002/ece3.4809
   Turkoz M, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107119
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Valan M, 2019, SYST BIOL, V68, P876, DOI 10.1093/sysbio/syz014
   Valiente-Banuet A, 2015, FUNCT ECOL, V29, P299, DOI 10.1111/1365-2435.12356
   van Klink R, 2020, SCIENCE, V368, P417, DOI 10.1126/science.aax9931
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wagner DL, 2020, ANNU REV ENTOMOL, V65, P457, DOI 10.1146/annurev-ento-011019-025151
   Wang JN, 2012, KNOWL-BASED SYST, V33, P102, DOI 10.1016/j.knosys.2012.03.014
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wotton KR, 2019, CURR BIOL, V29, P2167, DOI 10.1016/j.cub.2019.05.036
   Wu XP, 2019, PROC CVPR IEEE, P8779, DOI 10.1109/CVPR.2019.00899
   Xia D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124169
   Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhong YH, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051489
NR 121
TC 30
Z9 30
U1 41
U2 71
PU NATL ACAD SCIENCES
PI WASHINGTON
PA 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA
SN 0027-8424
EI 1091-6490
J9 P NATL ACAD SCI USA
JI Proc. Natl. Acad. Sci. U. S. A.
PD JAN 12
PY 2021
VL 118
IS 2
AR e2002545117
DI 10.1073/pnas.2002545117
PG 10
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA PR5LD
UT WOS:000607277100005
PM 33431561
OA Green Published, Green Submitted, hybrid
DA 2022-02-10
ER

PT C
AU Tekeli, U
   Bastanlar, Y
AF Tekeli, Ulas
   Bastanlar, Yalin
GP IEEE
TI Detection of Images with Animals in Raw Camera-Trap Data
SO 2018 26TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)
SE Signal Processing and Communications Applications Conference
LA Turkish
DT Proceedings Paper
CT 26th IEEE Signal Processing and Communications Applications Conference
   (SIU)
CY MAY 02-05, 2018
CL Izmir, TURKEY
SP IEEE, Huawei, Aselsan, NETAS, IEEE Turkey Sect, IEEE Signal Proc Soc, IEEE Commun Soc, ViSRATEK, Adresgezgini, Rohde & Schwarz, Integrated Syst & Syst Design, Atilim Univ, Havelsan, Izmir Katip Celebi Univ
DE camera-trap; image processing; object detection; background subtraction;
   convolutional neural networks; deep learning
AB Camera-traps are motion sensored cameras that are used to observe animals in nature. The number of images collected from camera-traps has increased significantly with developing technologies. A great workload is required for researchers to group and label these images. By employing convolutional neural networks and background subtraction, we propose a system to eliminate the images without animals from raw data sets of camera traps in order to decrease the amount of time spent by researchers.
C1 [Tekeli, Ulas; Bastanlar, Yalin] Izmir Yuksek Teknol Univ, Bilgisayar Muhendisligi Bolumu, Izmir, Turkey.
RP Tekeli, U (corresponding author), Izmir Yuksek Teknol Univ, Bilgisayar Muhendisligi Bolumu, Izmir, Turkey.
EM tekeliulas@gmail.com; yalinbastanlar@iyte.edu.tr
RI Bastanlar, Yalin/AAA-7114-2022
CR Fegraus E.H., 2011, ECOLOGIC INFORM, V6
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Redmon J, 2016, IEEE C COMP VIS PATT
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet Pierre, 2013, ARXIV13126229
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 9
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2165-0608
BN 978-1-5386-1501-0
J9 SIG PROCESS COMMUN
PY 2018
PG 4
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BO3SQ
UT WOS:000511448500067
DA 2022-02-10
ER

PT C
AU Schneider, S
   Taylor, GW
   Kremer, SC
AF Schneider, Stefan
   Taylor, Graham W.
   Kremer, Stefan C.
GP IEEE
TI Similarity Learning Networks for Animal Individual Re-Identification -
   Beyond the Capabilities of a Human Observer
SO 2020 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION WORKSHOPS
   (WACVW)
SE IEEE Winter Conference on Applications of Computer Vision Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CY MAR 01-05, 2020
CL Snowmass, CO
SP IEEE, IEEE Comp Soc, CVF
AB Deep learning has become the standard methodology to approach computer vision tasks when large amounts of labeled data are available. One area where traditional deep learning approaches fail to perform is one-shot learning tasks where a model must correctly classify a new category after seeing only one example. One such domain is animal re-identification, an application of computer vision which can be used globally as a method to automate species population estimates from camera trap images. Our work demonstrates both the application of similarity comparison networks to animal re-identification, as well as the capabilities of deep convolutional neural networks to generalize across domains. Few studies have considered animal reidentification methods across species. Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean average precision (mAP)@1 and mAP@5. We consider five data sets corresponding to five different species: humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart for all species. Without any species-specific modifications, our results demonstrate that similarity comparison networks can reach a performance level beyond that of humans for the task of animal re-identification. The ability for researchers to re-identify an animal individual upon re-encounter is fundamental for addressing a broad range of questions in the study of population dynamics and community/behavioural ecology. Our expectation is that similarity comparison networks are the beginning of a major trend that could stand to revolutionize animal reidentification from camera trap data.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
   [Taylor, Graham W.] Univ Guelph, Sch Engn, Vector Inst Artificial Intelligence, Guelph, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca; gwtaylor@uoguelph.ca; skremer@uoguelph.ca
OI Kremer, Stefan C./0000-0002-3667-4379
CR Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   BOUMA S, 2018, INT CONF IMAG VIS, pNI317
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Carter SJB, 2014, J EXP MAR BIOL ECOL, V452, P105, DOI 10.1016/j.jembe.2013.12.010
   Deb D., 2018, ARXIV180408790
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hermans A., 2017, ARXIV ARXIV170307737
   Hiby L., 2009, BIOL LETT, prsbl
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Krebs C. J., 1989, TECH REP
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2006, 2006 IEEE COMPUTER S, V2, P1735
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li S., 2019, ARXIV190605586
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Martinel N, 2015, IEEE T PATTERN ANAL, V37, P1656, DOI 10.1109/TPAMI.2014.2377748
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Norouzzadeh M. S., 2017, ARXIV170305830V5
   Parham J, 2018, IEEE WINT CONF APPL, P1075, DOI 10.1109/WACV.2018.00123
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schneider J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0205043
   Schneider S., C COMP ROB VIS
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
NR 36
TC 9
Z9 9
U1 1
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2572-4398
BN 978-1-7281-7162-3
J9 IEEE WINT CONF APPL
PY 2020
BP 44
EP 52
PG 9
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ3RU
UT WOS:000587895300007
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Ahmed, A
   Yousif, H
   Kays, R
   He, ZH
AF Ahmed, Ahmed
   Yousif, Hayder
   Kays, Roland
   He, Zhihai
TI Semantic region of interest and species classification in the deep
   neural network feature domain
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Animal species classification; Deep neural networks; Semantic regions;
   Graph cut
ID ENERGY MINIMIZATION; CAMERA-TRAP
AB In this paper, we focus on animal object detection and species classification in camera-trap images collected in highly cluttered natural scenes. Using a deep neural network (DNN) model training for animal- background image classification, we analyze the input camera-trap images to generate a multi-level visual representation of the input image. We detect semantic regions of interest for animals from this representation using k-mean clustering and graph cut in the DNN feature domain. These animal regions are then classified into animal species using multi-class deep neural network model. According the experimental results, our method achieves 99.75% accuracy for classifying animals and background and 90.89% accuracy for classifying 26 animal species on the Snapshot Serengeti dataset, outperforming existing image classification methods.
C1 [Ahmed, Ahmed; Yousif, Hayder; He, Zhihai] Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
   [Kays, Roland] North Carolina Museum Nat Sci, Raleigh, NC 27601 USA.
RP He, ZH (corresponding author), Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
EM hezhi@missouri.edu
RI Ahmed, Ahmed Q./ABH-6918-2020; Yousif, Hayder/AAG-2259-2020
OI Yousif, Hayder/0000-0002-7638-9505; Kays, Roland/0000-0002-2947-6665
FU NSFNational Science Foundation (NSF) [CyberSEES-1539389]
FX This work was supported in part by NSF grant CyberSEES-1539389.
CR Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everitt BS., 2011, INTRO CLASSIFICATION
   Ge ZY, 2015, IEEE IMAGE PROC, P561, DOI 10.1109/ICIP.2015.7350861
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Kays Roland, 2011, International Journal of Research and Reviews in Wireless Sensor Networks, V1, P19
   Kays R., 2014, P N AM CONSERVATION, P80
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kumar YHS, 2015, PROCEDIA COMPUT SCI, V45, P336, DOI 10.1016/j.procs.2015.03.156
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matuska S, 2014, AASRI PROC, V9, P25, DOI 10.1016/j.aasri.2014.09.006
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rovero F, 2008, ORYX, V42, P16
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C., 2013, ADV NEURAL INF PROCE, V26, P2553, DOI DOI 10.5555/2999792.2999897
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Xiaodong Cui, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5582, DOI 10.1109/ICASSP.2014.6854671
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 35
TC 2
Z9 2
U1 0
U2 9
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD JUL
PY 2019
VL 52
BP 57
EP 68
DI 10.1016/j.ecoinf.2019.05.006
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA IF3ML
UT WOS:000472984800007
DA 2022-02-10
ER

PT J
AU Miao, ZQ
   Liu, ZW
   Gaynor, KM
   Palmer, MS
   Yu, SX
   Getz, WM
AF Miao, Zhongqi
   Liu, Ziwei
   Gaynor, Kaitlyn M.
   Palmer, Meredith S.
   Yu, Stella X.
   Getz, Wayne M.
TI Iterative human and automated identification of wildlife images
SO NATURE MACHINE INTELLIGENCE
LA English
DT Article
ID BIODIVERSITY
AB Camera trapping is increasingly being used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has substantially advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static datasets, whereas wildlife data are intrinsically dynamic and involve long-tailed distributions. These drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning, which facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve an similar to 90% accuracy employing only similar to 20% of the human annotations of existing approaches. Our synergistic collaboration of humans and machines transforms deep learning from a relatively inefficient post-annotation tool to a collaborative ongoing annotation tool that vastly reduces the burden of human annotation and enables efficient and constant model updates.
   Camera trapping is a widely adopted method for monitoring terrestrial mammals. However, a drawback is the amount of human annotation needed to keep pace with continuous data collection. The authors developed a hybrid system of machine learning and humans in the loop, which minimizes annotation load and improves efficiency.
C1 [Miao, Zhongqi; Yu, Stella X.; Getz, Wayne M.] Univ Calif Berkeley, Dept Environm Sci Policy & Management, Berkeley, CA 94720 USA.
   [Miao, Zhongqi; Yu, Stella X.] Univ Calif Berkeley, Int Comp Sci Inst, Berkeley, CA 94720 USA.
   [Liu, Ziwei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Gaynor, Kaitlyn M.] Univ Calif Santa Barbara, Natl Ctr Ecol Anal & Synth, Santa Barbara, CA 93106 USA.
   [Palmer, Meredith S.] Princeton Univ, Dept Ecol & Evolutionary Biol, Princeton, NJ 08544 USA.
   [Getz, Wayne M.] Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
RP Miao, ZQ; Getz, WM (corresponding author), Univ Calif Berkeley, Dept Environm Sci Policy & Management, Berkeley, CA 94720 USA.; Miao, ZQ (corresponding author), Univ Calif Berkeley, Int Comp Sci Inst, Berkeley, CA 94720 USA.; Getz, WM (corresponding author), Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
EM zhongqi.miao@berkeley.edu; wgetz@berkeley.edu
RI ; Getz, Wayne/I-4521-2013
OI Gaynor, Kaitlyn/0000-0002-5747-0543; Miao, Zhongqi/0000-0002-0439-8592;
   Getz, Wayne/0000-0001-8784-9354
FU HHMI BioInteractive; Rufford Foundation; Idea Wild; Explorers Club; UC
   Berkeley Center for African Studies; NTU NAP; Rhodes Trust; National
   Center for Ecological Analysis and Synthesis Director's Postdoctoral
   Fellowship; National Science FoundationNational Science Foundation (NSF)
   [1810586]; Schmidt Science Fellows
FX We thank T. Gu, A. Ke, H. Rosen, A. Wu, C. Jurgensen, E. Lai, M. Levy
   and E. Silverberg for annotating the images used in this study, as well
   as everyone else involved in this project. Data collection was supported
   by J. Brashares and through grants to K.M.G. from HHMI BioInteractive,
   the Rufford Foundation, Idea Wild, the Explorers Club and the UC
   Berkeley Center for African Studies. We are grateful for the support of
   Gorongosa National Park, especially M. Stalmans, in permitting and
   facilitating this research. Z.L. is supported by NTU NAP. K.M.G. is
   supported by Schmidt Science Fellows in partnership with the Rhodes
   Trust, and the National Center for Ecological Analysis and Synthesis
   Director's Postdoctoral Fellowship. M.S.P. is funded by National Science
   Foundation grant no. PRFB #1810586.
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Anderson TM, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0314
   Arjovsky Martin, 2019, ARXIV190702893
   Barlow J, 2016, NATURE, V535, P144, DOI 10.1038/nature18326
   Barnosky AD, 2011, NATURE, V471, P51, DOI 10.1038/nature09678
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   CHEN T, 2020, PREPRINT
   Clavero M, 2005, TRENDS ECOL EVOL, V20, P110, DOI 10.1016/j.tree.2005.01.003
   Darrell T., 2020, ARXIV200805659
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Gaynor KM, 2021, ANIM CONSERV, V24, P510, DOI 10.1111/acv.12661
   Girshick, 2020, P IEEE CVF C COMP VI, P9729, DOI DOI 10.1109/CVPR42600.2020.00975
   Hautier Y, 2015, SCIENCE, V348, P336, DOI 10.1126/science.aaa1788
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, STAT-US, V1050, P9
   Kays R, 2020, DIVERS DISTRIB, V26, P644, DOI 10.1111/ddi.12993
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Lee Dong-Hyun, 2013, PSEUDO LABEL SIMPLE
   Liu Weitang, 2020, ADV NEURAL INFORM PR
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Liu Ziwei, 2020, P IEEE CVF C COMP VI
   Mech L. David, 2019, Canadian Field-Naturalist, V133, P60, DOI 10.22621/cfn.v133i1.2078
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Palmer MS, 2017, ECOL LETT, V20, P1364, DOI 10.1111/ele.12832
   Pardo LE, 2021, S AFR J SCI, V117, DOI 10.17159/sajs.2021/8134
   Pimm SL, 2014, SCIENCE, V344, P987, DOI 10.1126/science.1246752
   Prach K, 2011, TRENDS ECOL EVOL, V26, P119, DOI 10.1016/j.tree.2010.12.007
   Rich LN, 2017, GLOBAL ECOL BIOGEOGR, V26, P918, DOI 10.1111/geb.12600
   Ripple WJ, 2017, BIOSCIENCE, V67, P197
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2020, ECOL EVOL, V10, P10374, DOI 10.1002/ece3.6692
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Taylor G, 2017, TRENDS ECOL EVOL, V32, P873, DOI 10.1016/j.tree.2017.08.002
   Wallach H., 2019, P ADV NEUR INF PROC, P8024, DOI DOI 10.1038/S41591-021-01287-9
   Whytock RC, 2021, METHODS ECOL EVOL, V12, P1080, DOI 10.1111/2041-210X.13576
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yosinski J, 2014, ADV NEUR IN, V27
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NATURE PORTFOLIO
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
EI 2522-5839
J9 NAT MACH INTELL
JI Nat. Mach. Intell.
PD OCT
PY 2021
VL 3
IS 10
BP 885
EP +
DI 10.1038/s42256-021-00393-0
PG 14
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WI6LY
UT WOS:000708470700004
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Green, SE
   Rees, JP
   Stephens, PA
   Hill, RA
   Giordano, AJ
AF Green, Sian E.
   Rees, Jonathan P.
   Stephens, Philip A.
   Hill, Russell A.
   Giordano, Anthony J.
TI Innovations in Camera Trapping Technology and Approaches: The
   Integration of Citizen Science and Artificial Intelligence
SO ANIMALS
LA English
DT Review
DE camera trapping; citizen science; artificial intelligence; engagement;
   camera traps; public awareness; data processing; conservation technology
ID CONSERVATION; BIODIVERSITY; MOTIVATIONS; SCIENTISTS; BEHAVIOR; NETWORK;
   TRAPS; EXTINCTION; EXPERIENCE; ATTITUDES
AB Simple Summary Camera traps, also known as "game cameras" or "trail cameras", have increasingly been used in wildlife research over the last 20 years. Although early units were bulky and the set-up was complicated, modern camera traps are compact, integrated units able to collect vast digital datasets. Some of the challenges now facing researchers include the time required to view, classify, and sort all of the footage collected, as well as the logistics of establishing and maintaining camera trap sampling arrays across wide geographic areas. One solution to this problem is to enlist or recruit the public for help as 'citizen scientists' collecting and processing data. Artificial Intelligence (AI) is also being used to identify animals in digital photos and video; however, this process is relatively new, and machine-based classifications are not yet fully reliable. By combining citizen science with AI, it should be possible to improve efficiency and increase classification accuracy, while simultaneously maintaining and promoting the benefits associated with public engagement with, and awareness of, wildlife.
   Abstract Camera trapping has become an increasingly reliable and mainstream tool for surveying a diversity of wildlife species. Concurrent with this has been an increasing effort to involve the wider public in the research process, in an approach known as 'citizen science'. To date, millions of people have contributed to research across a wide variety of disciplines as a result. Although their value for public engagement was recognised early on, camera traps were initially ill-suited for citizen science. As camera trap technology has evolved, cameras have become more user-friendly and the enormous quantities of data they now collect has led researchers to seek assistance in classifying footage. This has now made camera trap research a prime candidate for citizen science, as reflected by the large number of camera trap projects now integrating public participation. Researchers are also turning to Artificial Intelligence (AI) to assist with classification of footage. Although this rapidly-advancing field is already proving a useful tool, accuracy is variable and AI does not provide the social and engagement benefits associated with citizen science approaches. We propose, as a solution, more efforts to combine citizen science with AI to improve classification accuracy and efficiency while maintaining public involvement.
C1 [Green, Sian E.; Hill, Russell A.] Univ Durham, Dept Anthropol, Durham DH1 3LE, England.
   [Green, Sian E.; Rees, Jonathan P.; Stephens, Philip A.] Univ Durham, Conservat Ecol Grp, Dept Biosci, Durham DH1 3LE, England.
   [Green, Sian E.; Giordano, Anthony J.] SPECIES, Ventura, CA 93006 USA.
RP Green, SE (corresponding author), Univ Durham, Dept Anthropol, Durham DH1 3LE, England.; Green, SE (corresponding author), Univ Durham, Conservat Ecol Grp, Dept Biosci, Durham DH1 3LE, England.; Green, SE (corresponding author), SPECIES, Ventura, CA 93006 USA.
EM sian.e.green@durham.ac.uk; jonathan.p.rees@durham.ac.uk;
   philip.stephens@durham.ac.uk; r.a.hill@durham.ac.uk;
   speciesl@hotmail.com
RI Stephens, Philip/B-8397-2008; Hill, Russell/D-9113-2013
OI Stephens, Philip/0000-0001-5849-788X; Hill, Russell/0000-0002-7601-5802;
   Green, Sian/0000-0003-0513-8490
FU NERC Iapetus Doctoral Training Partnership; NERCUK Research & Innovation
   (UKRI)Natural Environment Research Council (NERC) [NE/R008485/1]
FX Funding for this research was provided through a NERC Iapetus Doctoral
   Training Partnership and NERC Training Grant number NE/R008485/1.
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ballard HL, 2017, BIOL CONSERV, V208, P65, DOI 10.1016/j.biocon.2016.05.024
   Barrueto M, 2014, ECOSPHERE, V5, DOI 10.1890/ES13-00382.1
   Beery Sara, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11220), P472, DOI 10.1007/978-3-030-01270-0_28
   Bonney R., 2009, PUBLIC PARTICIPATION
   Bowley C, 2019, J COMPUT SCI-NETH, V34, P102, DOI 10.1016/j.jocs.2019.04.010
   Bowser A., 2013, P 1 INT C GAM DES RE, P18, DOI DOI 10.1145/2583008.2583011
   Bratman GN, 2015, LANDSCAPE URBAN PLAN, V138, P41, DOI 10.1016/j.landurbplan.2015.02.005
   Brower M, 2008, HIST PHOTOGR, V32, P169, DOI 10.1080/03087290801895761
   Burgess HK, 2017, BIOL CONSERV, V208, P113, DOI 10.1016/j.biocon.2016.05.014
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Catlin-Groves C.L., 2012, INT J ZOOL, V2012, P1, DOI [10.1155/2012/349630, DOI 10.1155/2012/349630]
   Chandler M, 2017, BIOL CONSERV, V213, P280, DOI 10.1016/j.biocon.2016.09.004
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Cox DTC, 2017, INT J ENV RES PUB HE, V14, DOI 10.3390/ijerph14020172
   Cox DTC, 2017, LANDSCAPE URBAN PLAN, V160, P79, DOI 10.1016/j.landurbplan.2016.12.006
   Curtis V, 2015, SCI COMMUN, V37, P723, DOI 10.1177/1075547015609322
   Dayer AA, 2019, PEOPLE NAT, V1, P138, DOI 10.1002/pan3.17
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Domroese MC, 2017, BIOL CONSERV, V208, P40, DOI 10.1016/j.biocon.2016.08.020
   Dorji S, 2019, BIODIVERS CONSERV, V28, P3277, DOI 10.1007/s10531-019-01821-9
   Eaton DP, 2017, BIOL CONSERV, V208, P29, DOI 10.1016/j.biocon.2016.09.010
   Eitzel M. V., 2017, CITIZ SCI THEORY PRA, V2, P1, DOI [10.5334/cstp.96, DOI 10.5334/CSTP.96]
   Ellwood ER, 2017, BIOL CONSERV, V208, P1, DOI 10.1016/j.biocon.2016.10.014
   Engemann K, 2019, P NATL ACAD SCI USA, V116, P5188, DOI 10.1073/pnas.1807504116
   Evans C, 2005, CONSERV BIOL, V19, P589, DOI 10.1111/j.1523-1739.2005.00s01.x
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Forrester TD, 2017, BIOL CONSERV, V208, P98, DOI 10.1016/j.biocon.2016.06.025
   Ganzevoort W, 2017, BIODIVERS CONSERV, V26, P2821, DOI 10.1007/s10531-017-1391-z
   Genovart M, 2013, BIOL CONSERV, V159, P484, DOI 10.1016/j.biocon.2012.10.028
   Geoghegan H., 2016, UNDERSTANDING MOTIVA
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Griffiths Mike, 1993, Tropical Biodiversity, V1, P131
   Haklay M, 2013, CROWDSOURCING GEOGRA, P105, DOI DOI 10.1007/978-94-007-4587-2_7
   Haywood BK, 2016, CONSERV BIOL, V30, P476, DOI 10.1111/cobi.12702
   Hobbs SJ, 2012, J NAT CONSERV, V20, P364, DOI 10.1016/j.jnc.2012.08.002
   Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
   Jones M., 2013, IMPACT CITIZEN SCI A
   Kammerle JL, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0207545
   Kalan AK, 2019, CURR BIOL, V29, P1211, DOI 10.1016/j.cub.2019.02.024
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Koivuniemi M, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214269
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Lewandowski EJ, 2017, BIOL CONSERV, V208, P106, DOI 10.1016/j.biocon.2015.07.029
   Lintott CJ, 2008, MON NOT R ASTRON SOC, V389, P1179, DOI 10.1111/j.1365-2966.2008.13689.x
   MacKenzie CM, 2017, BIOL CONSERV, V208, P121, DOI 10.1016/j.biocon.2016.07.027
   Masters Karen, 2016, J SCI COMMUNICATION, P1, DOI DOI 10.22323/2.15030207
   McKie R., GUARDIAN
   Meek PD, 2012, WILDLIFE RES, V39, P649, DOI 10.1071/WR12138
   Meek PD, 2015, AUST MAMMAL, V37, P1, DOI 10.1071/AM14021
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien L, 2010, VOLUNTAS, V21, P525, DOI 10.1007/s11266-010-9149-1
   OED, 2019, CIT N ADJ
   Orta-Martinez M, 2018, ENVIRON RES, V160, P514, DOI 10.1016/j.envres.2017.10.009
   Parsons AW, 2018, ELIFE, V7, DOI 10.7554/eLife.38012
   Paxton AB, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2687
   Pierce J., 2013, P 2013 C COMP SUPP C, P1463, DOI [10.1145/2441776.2441941, DOI 10.1145/2441776.2441941]
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Rotman D., 2014, ICONFERENCE 2014 P, P110, DOI DOI 10.9776/14054
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2017, SCI TOTAL ENVIRON, V574, P914, DOI 10.1016/j.scitotenv.2016.09.146
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rovero Francesco, 2010, Abc Taxa, V8, P100
   Roy H. E., 2012, UNDERSTANDING CITIZE
   Schuttler SG, 2019, BIOSCIENCE, V69, P69, DOI 10.1093/biosci/biy141
   Schuttler SG, 2018, FRONT ECOL ENVIRON, V16, P405, DOI 10.1002/fee.1826
   Scotson L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185336
   SEYDACK AHW, 1984, S AFR J WILDL RES, V14, P10
   Shirk JL, 2012, ECOL SOC, V17, DOI 10.5751/ES-04705-170229
   Soga M, 2016, FRONT ECOL ENVIRON, V14, P94, DOI 10.1002/fee.1225
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tanner D., 2010, Human Dimensions of Wildlife, V15, P418, DOI 10.1080/10871209.2010.503236
   Thapa K, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216504
   Tinati R, 2017, COMPUT HUM BEHAV, V73, P527, DOI 10.1016/j.chb.2016.12.074
   Toomey AH, 2013, HUM ECOL REV, V20, P50
   Trnovszky T, 2017, ADV ELECTR ELECTRON, V15, P517, DOI 10.15598/aeee.v15i3.2202
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Vernes K, 2014, AUST MAMMAL, V36, P128, DOI 10.1071/AM13037
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wearn OR, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181748
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Welbourne DJ, 2016, REMOTE SENS ECOL CON, V2, P77, DOI 10.1002/rse2.20
   Welbourne DJ, 2015, WILDLIFE RES, V42, P414, DOI 10.1071/WR15054
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Williams ST, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.161090
NR 92
TC 12
Z9 13
U1 10
U2 19
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD JAN
PY 2020
VL 10
IS 1
AR 132
DI 10.3390/ani10010132
PG 16
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Agriculture; Veterinary Sciences; Zoology
GA KO2FZ
UT WOS:000515364400132
PM 31947586
OA Green Accepted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Clapham, M
   Miller, E
   Nguyen, M
   Darimont, CT
AF Clapham, Melanie
   Miller, Ed
   Nguyen, Mary
   Darimont, Chris T.
TI Automated facial recognition for wildlife that lack unique markings: A
   deep learning approach for brown bears
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE deep learning; face recognition; grizzly bear; individual ID; machine
   learning; wildlife monitoring
ID CAMERA TRAPS; REIDENTIFICATION; INDIVIDUALS; NETWORKS
AB Emerging technologies support a new era of applied wildlife research, generating data on scales from individuals to populations. Computer vision methods can process large datasets generated through image-based techniques by automating the detection and identification of species and individuals. With the exception of primates, however, there are no objective visual methods of individual identification for species that lack unique and consistent body markings. We apply deep learning approaches of facial recognition using object detection, landmark detection, a similarity comparison network, and an support vector machine-based classifier to identify individuals in a representative species, the brown bear Ursus arctos. Our open-source application, BearID, detects a bear's face in an image, rotates and extracts the face, creates an "embedding" for the face, and uses the embedding to classify the individual. We trained and tested the application using labeled images of 132 known individuals collected from British Columbia, Canada, and Alaska, USA. Based on 4,674 images, with an 80/20% split for training and testing, respectively, we achieved a facial detection (ability to find a face) average precision of 0.98 and an individual classification (ability to identify the individual) accuracy of 83.9%. BearID and its annotated source code provide a replicable methodology for applying deep learning methods of facial recognition applicable to many other species that lack distinguishing markings. Further analyses of performance should focus on the influence of certain parameters on recognition accuracy, such as age and body size. Combining BearID with camera trapping could facilitate fine-scale behavioral research such as individual spatiotemporal activity patterns, and a cost-effective method of population monitoring through mark-recapture studies, with implications for species and landscape conservation and management. Applications to practical conservation include identifying problem individuals in human-wildlife conflicts, and evaluating the intrapopulation variation in efficacy of conservation strategies, such as wildlife crossings.
C1 [Clapham, Melanie; Miller, Ed; Nguyen, Mary] BearID Project, Sooke, BC, Canada.
   [Clapham, Melanie; Darimont, Chris T.] Univ Victoria, Dept Geog, 3800 Finnerty Rd, Victoria, BC V8P 5C2, Canada.
   [Darimont, Chris T.] Raincoast Conservat Fdn, Bella Bella, BC, Canada.
RP Clapham, M (corresponding author), Univ Victoria, Dept Geog, 3800 Finnerty Rd, Victoria, BC V8P 5C2, Canada.
EM melanie@understandingbears.com
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)CGIAR [CRDPJ
   523329-18]
FX Natural Sciences and Engineering Research Council of Canada, Grant/Award
   Number: CRDPJ 523329-18
CR Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Chen P, 2020, ECOL EVOL, V10, P3561, DOI 10.1002/ece3.6152
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Clapham M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0035404
   Clutton-Brock T, 2010, TRENDS ECOL EVOL, V25, P562, DOI 10.1016/j.tree.2010.08.002
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Dalal N., 2021, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Deb D., 2018, 2018 IEEE 9 INT C BI, P1, DOI DOI 10.1109/BTAS.2018.8698538
   Dexter CE, 2018, AUST MAMMAL, V40, P67, DOI 10.1071/AM16043
   Ernst A., 2011, 8 IEEE INT C ADV VID, P279, DOI DOI 10.1109/AVSS.2011.6027337
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   HAN B, 2018, ADV NEUR IN, V31
   Hertel AG, 2017, BEHAV ECOL, V28, P1524, DOI 10.1093/beheco/arx122
   Hilderbrand GV, 1999, CAN J ZOOL, V77, P132, DOI 10.1139/cjz-77-1-132
   Huang G.B., 2007, LABELED FACES WILD D
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   King D. E., 2015, 150200046 ARXIV
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Loos A, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P116, DOI 10.1109/ISM.2012.30
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Moreira TP, 2017, MULTIMED TOOLS APPL, V76, P15325, DOI 10.1007/s11042-016-3824-1
   Rashmi P., 2017, INT J RECENT TRENDS, V3, P207, DOI [10.23883/IJRTER.2017.3215.TXUQG, DOI 10.23883/IJRTER.2017.3215.TXUQG]
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Russell R., 1983, BEARS THEIR BIOL MAN, P174, DOI DOI 10.2307/3872535
   Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swan GJF, 2017, TRENDS ECOL EVOL, V32, P518, DOI 10.1016/j.tree.2017.03.011
   Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Witham CL, 2018, J NEUROSCI METH, V300, P157, DOI 10.1016/j.jneumeth.2017.07.020
NR 37
TC 4
Z9 4
U1 2
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD DEC
PY 2020
VL 10
IS 23
BP 12883
EP 12892
DI 10.1002/ece3.6840
EA NOV 2020
PG 10
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA OZ9EC
UT WOS:000585885000001
PM 33304501
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Villa, AG
   Salazar, A
   Vargas, F
AF Gomez Villa, Alexander
   Salazar, Augusto
   Vargas, Francisco
TI Towards automatic wild animal monitoring: Identification of animal
   species in camera-trap images using very deep convolutional neural
   networks
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Animal species recognition; Deep convolutional neural networks;
   Camera-trap; Snapshot Serengeti
AB Non-intrusive monitoring of animals in the wild is possible using camera trapping networks. The cameras are triggered by sensors in order to disturb the animals as little as possible. This approach produces a high volume of data (in the order of thousands or millions of images) that demands laborious work to analyze both useless (incorrect detections, which are the most) and useful (images with presence of animals). In this work, we show that as soon as some obstacles are overcome, deep neural networks can cope with the problem of the automated species classification appropriately. As case of study, the most common 26 of 48 species from the Snapshot Serengeti (SSe) dataset were selected and the potential of the Very Deep Convolutional neural networks framework for the species identification task was analyzed. In the worst-case scenario (unbalanced training dataset containing empty images) the method reached 35.4% Top-1 and 60.4% Top-5 accuracy. For the best scenario (balanced dataset, images containing foreground animals only, and manually segmented) the accuracy reached a 88.9% Top-1 and 98.1% Top-5, respectively. To the best of our knowledge, this is the first published attempt on solving the automatic species recognition on the SSe dataset. In addition, a comparison with other approaches on a different dataset was carried out, showing that the architectures used in this work outperformed previous approaches. The limitations of the method, drawbacks, as well as new challenges in automatic camera-trap species classification are widely discussed.
C1 [Gomez Villa, Alexander; Salazar, Augusto; Vargas, Francisco] Univ Antioquia UdeA, Fac Ingn, Grp Invest SISTEMIC, Calle 70 52-21, Medellin, Colombia.
RP Salazar, A (corresponding author), SUPSI, App SUPSI DTI, Via Sorengo 22,Lugano 6900, Ticino, Switzerland.
EM alexander.gomezvilla@supsi.ch; augusto.salazar@udea.edu.co;
   jesus.vargas@udea.edu.co
RI Villa, Alexander Gomez/AAB-4534-2019
OI Villa, Alexander Gomez/0000-0003-0469-3425
CR Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Glorot X., 2010, P 13 INT C ARTIFICIA, P249
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 18
TC 71
Z9 76
U1 5
U2 51
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD SEP
PY 2017
VL 41
BP 24
EP 32
DI 10.1016/j.ecoinf.2017.07.004
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA FI8OD
UT WOS:000412261000003
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Swanson, A
   Kosmala, M
   Lintott, C
   Simpson, R
   Smith, A
   Packer, C
AF Swanson, Alexandra
   Kosmala, Margaret
   Lintott, Chris
   Simpson, Robert
   Smith, Arfon
   Packer, Craig
TI Snapshot Serengeti, high-frequency annotated camera trap images of 40
   mammalian species in an African savanna
SO SCIENTIFIC DATA
LA English
DT Article
ID ESTIMATING SITE OCCUPANCY; CITIZEN SCIENCE; DENSITY; TOOL; POPULATIONS;
   MANAGEMENT; ABUNDANCE; SOFTWARE; PATTERNS; MODELS
AB Camera traps can be used to address large-scale questions in community ecology by providing systematic data on an array of wide-ranging species. We deployed 225 camera traps across 1,125 km(2) in Serengeti National Park, Tanzania, to evaluate spatial and temporal inter-species dynamics. The cameras have operated continuously since 2010 and had accumulated 99,241 camera-trap days and produced 1.2 million sets of pictures by 2013. Members of the general public classified the images via the citizen-science website www.snapshotserengeti.org. Multiple users viewed each image and recorded the species, number of individuals, associated behaviours, and presence of young. Over 28,000 registered users contributed 10.8 million classifications. We applied a simple algorithm to aggregate these individual classifications into a final 'consensus' dataset, yielding a final classification for each image and a measure of agreement among individual answers. The consensus classifications and raw imagery provide an unparalleled opportunity to investigate multi-species dynamics in an intact ecosystem and a valuable resource for machine-learning and computer-vision research.
C1 [Swanson, Alexandra; Kosmala, Margaret; Packer, Craig] Univ Minnesota, Dept Ecol Evolut & Behav, St Paul, MN 55108 USA.
   [Swanson, Alexandra; Lintott, Chris; Simpson, Robert] Univ Oxford, Dept Phys, Oxford OX1 3RH, England.
   [Smith, Arfon] Adler Planetarium, Dept Citizen Sci, Chicago, IL 60605 USA.
RP Swanson, A (corresponding author), Univ Minnesota, Dept Ecol Evolut & Behav, St Paul, MN 55108 USA.
EM ali@zooniverse.org
OI Smith, Arfon/0000-0002-3957-2474
FU NSFNational Science Foundation (NSF) [DEB-1020479]; University of
   Minnesota Supercomputing Institute; Explorer's Club; UMN Thesis Research
   Grants; UMN Office of International Programs; American Society of
   Mammalogists; Minnesota Zoo Ulysses S. Seal Conservation fund; Alfred P.
   Sloan FoundationAlfred P. Sloan Foundation; Division Of Environmental
   BiologyNational Science Foundation (NSF)NSF - Directorate for Biological
   Sciences (BIO) [1405385] Funding Source: National Science Foundation
FX Research clearance was provided by the Tanzania Wildlife Research
   Institute and Tanzania National Parks. We thank members of the Serengeti
   Lion Project, particularly Daniel Rosengren, George Gwaltu Lohay, and
   Stanslaus Mwampeta, the Zooniverse staff, and the 28,040 volunteers who
   contributed to Snapshot Serengeti classifications (complete list at
   www.snapshotserengeti.org/#/authors). This work was supported by NSF
   grant DEB-1020479 to CP for maintenance of the long-term Lion Project,
   the University of Minnesota Supercomputing Institute, private donations
   raised during the Serengeti Live and Save Snapshot Serengeti
   crowd-funding campaigns, and by grants to AS from Explorer's Club, UMN
   Thesis Research Grants, UMN Office of International Programs, American
   Society of Mammalogists, and Minnesota Zoo Ulysses S. Seal Conservation
   fund. Snapshot Serengeti website development was funded by awards to the
   Zooniverse from the Alfred P. Sloan Foundation.
CR Bailey LL, 2007, ECOL APPL, V17, P281, DOI 10.1890/1051-0761(2007)017[0281:SDTIOS]2.0.CO;2
   Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Hines G., 2015, P 27 ANN C INN APPL
   Holden J, 2003, ORYX, V37, P34, DOI 10.1017/S0030605303000097
   Holdo RM, 2009, AM NAT, V173, P431, DOI 10.1086/597229
   K~ery M., 2010, INTRO WINBUGS ECOLOG, VFirst
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kery M, 2005, ECOL APPL, V15, P1450, DOI 10.1890/04-1120
   Kery M, 2010, J ANIM ECOL, V79, P453, DOI 10.1111/j.1365-2656.2009.01632.x
   Kosmala M., 2015, SNAPSHOTSERENGETISCR
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Lintott C., 2015, SNAPSHOT SERENGETI
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2004, J ANIM ECOL, V73, P546, DOI 10.1111/j.0021-8790.2004.00828.x
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Nichols JD, 2007, ECOLOGY, V88, P1395, DOI 10.1890/06-1474
   O'Brien TG, 2008, ANIM CONSERV, V11, P179, DOI 10.1111/j.1469-1795.2008.00178.x
   O'Brien TG, 2010, ANIM CONSERV, V13, P335, DOI 10.1111/j.1469-1795.2010.00357.x
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien TG, 2011, ECOL APPL, V21, P2908, DOI 10.1890/10-2284.1
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Packer C., 2012, PLOS ONE, V6
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Royle J.A., 2008, HIERARCHICAL MODELIN
   Royle JA, 2007, ECOLOGY, V88, P1813, DOI 10.1890/06-0669.1
   Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sauermann H, 2015, P NATL ACAD SCI USA, V112, P679, DOI 10.1073/pnas.1408907112
   Sinclair A.R.E, 2008, SERENGETI
   Sinclair A.R.E., 1979, SERENGETI DYNAMICS E
   Sollmann R, 2013, ECOLOGY, V94, P553, DOI 10.1890/12-1256.1
   Strauss MKL, 2013, J ZOOL, V289, P134, DOI 10.1111/j.1469-7998.2012.00972.x
   Surridge A., 1999, NATURE, V400
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   TAWIRI, 2010, AER CENS SER EC
   TAWIRI, 2008, AER CENS SER EC
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Tulloch AIT, 2013, BIOL CONSERV, V165, P128, DOI 10.1016/j.biocon.2013.05.025
NR 46
TC 158
Z9 161
U1 16
U2 66
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
EI 2052-4463
J9 SCI DATA
JI Sci. Data
PY 2015
VL 2
AR 150026
DI 10.1038/sdata.2015.26
PG 14
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA V45VM
UT WOS:000209844100022
PM 26097743
OA gold, Green Submitted, Green Published
DA 2022-02-10
ER

PT J
AU de Silva, EMK
   Kumarasinghe, P
   Indrajith, KKDAK
   Pushpakumara, TV
   Vimukthi, RDY
   de Zoysa, K
   Gunawardana, K
   de Silva, S
AF de Silva, Elgiriyage M. K.
   Kumarasinghe, Prabhash
   Indrajith, Kottahachchi K. D. A. K.
   Pushpakumara, Tennekoon, V
   Vimukthi, Ranapura D. Y.
   de Zoysa, Kasun
   Gunawardana, Kasun
   de Silva, Shermin
TI Feasibility of using convolutional neural networks for
   individual-identification of wild Asian elephants
SO MAMMALIAN BIOLOGY
LA English
DT Article; Early Access
DE Artificial Intelligence; Computer Vision; Image Classification;
   Individual-Identification; Machine Learning; Pattern Recognition
ID CAMERA TRAPS; POPULATION; PHOTOIDENTIFICATION; MAXIMUS
AB Individual identification is a basic requirement for research in behavior, ecology and conservation. Photographic records are commonly used in situations where individuals are visually distinct. However, keeping track of identities becomes challenging with increasing population sizes and corresponding datasets. There is growing interest in the potential of deep-learning methods for computer vision to assist with automating this task. Here we apply Convolutional Neural Networks, a popular architecture for Artificial Neural Networks used in image classification, to the problem of identifying individual Asian elephants through photographs. We evaluate the performance of five different types of CNN models used in facial recognition (VGG16, ResNet50, InceptionV3, Xception, and Alexnet), on datasets representing three different feature regions (the full body, face, and ears), trained with two techniques (transfer learning vs. training from scratch) for n = 56 elephants. We tested accuracy in matching the top candidate as well as top five candidates. We found that VGG16 trained with the transfer-learning technique outperformed other models on the body and face datasets with accuracies of 21.34% and 42.35%, respectively, in matching the top candidate. Nevertheless, the best performance was achieved by an Xception model trained from the scratch on the ear dataset, with an accuracy of 89.02% for matching the top candidate and 99.27% for including the correct individual among the top five. However, this impressive level of accuracy was obtained with a dataset of 3816 labeled training images of 56 elephants. There are more than 1000 wild elephants in the population under observation, requiring extensive human effort and skill to initially annotate the images used as training data. Therefore, we consider this approach impractical for monitoring large wild populations. Nevertheless this it could be very useful in record keeping and fraud prevention for large captive elephant populations, as well as monitoring animals that have been rehabilitated and released or moved for management purposes.
C1 [de Silva, Elgiriyage M. K.; Kumarasinghe, Prabhash; Indrajith, Kottahachchi K. D. A. K.; Vimukthi, Ranapura D. Y.; de Zoysa, Kasun; Gunawardana, Kasun] Univ Colombo, Sch Comp, Colombo, Sri Lanka.
   [Pushpakumara, Tennekoon, V; de Silva, Shermin] EFECT, Colombo, Sri Lanka.
   [de Silva, Shermin] Trunks & Leaves Inc, Newtonville, MA 02460 USA.
RP de Silva, S (corresponding author), EFECT, Colombo, Sri Lanka.; de Silva, S (corresponding author), Trunks & Leaves Inc, Newtonville, MA 02460 USA.
EM shermin@trunksnleaves.org
FU U.S. Fish and Wildlife Asian Elephant Conservation Grant program
FX The photographic data were collected as part of the long-term research
   of the Udawalawe Elephant Research project with permission of the
   Department of Wildlife Conservation, Sri Lanka and supported by funding
   from the U.S. Fish and Wildlife Asian Elephant Conservation Grant
   program.
CR Adams Jeffrey D., 2006, Aquatic Mammals, V32, P374, DOI 10.1578/AM.32.3.2006.374
   Alexander JS, 2016, BIOL CONSERV, V197, P27, DOI 10.1016/j.biocon.2016.02.023
   Alexander JS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134815
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Barron UG, 2009, IRISH VET J, V62, P204
   Bedetti A., 2020, PACHYDERM, V61, P15
   Bush JM, 2016, ANIM BEHAV, V118, P65, DOI 10.1016/j.anbehav.2016.04.026
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   de Silva EMK., 2017, THESIS U COLOMBO
   de Silva Shermin, 2014, Gajah, V40, P46
   de Silva S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082788
   de Silva S, 2011, BIOL CONSERV, V144, P1742, DOI 10.1016/j.biocon.2011.03.011
   Fernando P, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0050917
   Gabriele CM, 2017, ECOSPHERE, V8, DOI 10.1002/ecs2.1641
   Ge HW, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/5987906
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Jackson RM, 2006, WILDLIFE SOC B, V34, P772, DOI 10.2193/0091-7648(2006)34[772:ESLPAU]2.0.CO;2
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   Karahan S., 2016, IEEE BIOSIG, P1, DOI [10.1109/BIOSIG.2016.7736924, DOI 10.1109/BIOSIG.2016.7736924]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krogh A, 2008, NAT BIOTECHNOL, V26, P195, DOI 10.1038/nbt1386
   Kumar S, 2017, J REAL-TIME IMAGE PR, V13, P505, DOI 10.1007/s11554-016-0645-4
   Kwasnicka H, 2010, STUD COMPUT INTELL, V263, P387
   Lahiri M, 2011, P 1 ACM INT C MULT R, P1, DOI DOI 10.1145/1991996.1992002
   Langtimm CA, 2004, MAR MAMMAL SCI, V20, P438, DOI 10.1111/j.1748-7692.2004.tb01171.x
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lonsdorf EV, 2017, J NEUROSCI RES, V95, P213, DOI 10.1002/jnr.23862
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McCoy E, 2018, FRONT MAR SCI, V5, DOI 10.3389/fmars.2018.00271
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Menon V, 2019, International Zoo Yearbook, V53, P17, DOI 10.1111/izy.12247
   Mikolajczyk Agnieszka, 2018, 2018 International Interdisciplinary PhD Workshop (IIPhDW), P117, DOI 10.1109/IIPHDW.2018.8388338
   Ostner J, 2018, INTERDISC EVOL RES, V5, P97, DOI 10.1007/978-3-319-93776-2_7
   Prakash TGSL, 2020, NAT CONSERV-BULGARIA, P51, DOI 10.3897/natureconservation.42.57283
   Raj A., 2015, OPEN INT J TECHNOL I, V15, P1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Rood E, 2010, DIVERS DISTRIB, V16, P975, DOI 10.1111/j.1472-4642.2010.00704.x
   Royle JA, 2018, ECOGRAPHY, V41, P444, DOI 10.1111/ecog.03170
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi CM, 2020, INTEGR ZOOL, V15, P461, DOI 10.1111/1749-4877.12453
   Shinde PP, 2018, 2018 FOURTH INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION (ICCUBEA)
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Thitaram Chatchote, 2020, Gajah, V52, P56
   Towner AV, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066035
   Vidya T.N.C., 2014, Gajah, V40, P3
   Wang M, 2019, ARXIV180406655CS
   Wardrope DD, 1995, VET REC, V137, P675
   Weideman HJ, 2020, IEEE WINT CONF APPL, P1265, DOI 10.1109/WACV45572.2020.9093266
   Wursig B., 1990, Reports of the International Whaling Commission Special Issue, P43
NR 54
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
DI 10.1007/s42991-021-00206-2
EA JAN 2022
PG 11
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA YK8VD
UT WOS:000745482700003
DA 2022-02-10
ER

PT J
AU Greenberg, S
   Godin, T
   Whittington, J
AF Greenberg, Saul
   Godin, Theresa
   Whittington, Jesse
TI Design patterns for wildlife-related camera trap image analysis
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera traps; data encoding and acquisition; design patterns; experience
   design; human-computer interaction; image inspection; tagging; wildlife
   monitoring
ID DATA-MANAGEMENT; SOFTWARE; POPULATION; MAMMALS; TOOL
AB This paper describes and explains design patterns for software that supports how analysts can efficiently inspect and classify camera trap images for wildlife-related ecological attributes. Broadly speaking, a design pattern identifies a commonly occurring problem and a general reusable design approach to solve that problem. A developer can then use that design approach to create a specific software solution appropriate to the particular situation under consideration. In particular, design patterns for camera trap image analysis by wildlife biologists address solutions to commonly occurring problems they face while inspecting a large number of images and entering ecological data describing image attributes. We developed design patterns for image classification based on our understanding of biologists' needs that we acquired over 8 years during development and application of the freely available Timelapse image analysis system. For each design pattern presented, we describe the problem, a design approach that solves that problem, and a concrete example of how Timelapse addresses the design pattern. Our design patterns offer both general and specific solutions related to: maintaining data consistency, efficiencies in image inspection, methods for navigating between images, efficiencies in data entry including highly repetitious data entry, and sorting and filtering image into sequences, episodes, and subsets. These design patterns can inform the design of other camera trap systems and can help biologists assess how competing software products address their project-specific needs along with determining an efficient workflow.
C1 [Greenberg, Saul] Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
   [Godin, Theresa] Univ British Columbia, BC Res Evaluat & Dev Sect, Freshwater Fisheries Soc, Vancouver, BC, Canada.
   [Whittington, Jesse] Parks Canada, Banff Natl Pk, Banff, AB, Canada.
RP Greenberg, S (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM saul@ucalgary.ca
OI Greenberg, Saul/0000-0003-0174-9665
FU National Science and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)
FX National Science and Engineering Research Council of Canada
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Alexander C., 1977, PATTERN LANGUAGE TOW
   Blake John G., 2017, Neotropical Biodiversity, V3, P57, DOI 10.1080/23766808.2017.1292756
   Borchers J. O., 2001, AI & Society, V15, P359, DOI 10.1007/BF01206115
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campbell J. M., 2010, ECOSYSTEM BASED MANA
   Carpendale S., 2004, P ACM S US INT SOFTW, P71, DOI DOI 10.1145/1029632.1029645
   Cheema GS, 2017, LECT NOTES ARTIF INT, V10536, P27, DOI 10.1007/978-3-319-71273-4_3
   Clevenger AP, 2010, ECOL SOC, V15
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Diakopoulos N, 2016, DESIGNING USER INTER
   Fairfax RJ, 2014, CURR ISSUES TOUR, V17, P72, DOI 10.1080/13683500.2012.714749
   Fisher JT, 2018, FRONT ECOL ENVIRON, V16, P323, DOI 10.1002/fee.1807
   Gamma E., 1994, DESIGN PATTERNS ELEM
   Garcia-Salgado G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0127585
   Glover-Kapfer, 2017, WWF CONSERVATION TEC, V1
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Goswami VR, 2007, ANIM CONSERV, V10, P391, DOI 10.1111/j.1469-1795.2007.00124.x
   Greenberg S., 2019, TIMELAPSE USER GUIDE
   Greenberg S, 2015, FISHERIES, V40, P276, DOI 10.1080/03632415.2015.1038380
   Heilbrun RD, 2006, WILDLIFE SOC B, V34, P69, DOI 10.2193/0091-7648(2006)34[69:EBAUAT]2.0.CO;2
   Hossain AM, 2016, BIOL CONSERV, V201, P314, DOI 10.1016/j.biocon.2016.07.023
   Ivan JS, 2016, METHODS ECOL EVOL, V7, P499, DOI 10.1111/2041-210X.12503
   Jumeau J, 2017, ECOL EVOL, V7, P7399, DOI 10.1002/ece3.3149
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Laskin DN, 2019, NAT CLIM CHANGE, V9, P419, DOI 10.1038/s41558-019-0454-4
   Microsoft, 2019, AI EARTH CAM TRAP IM
   Mills LS, 2018, SCIENCE, V359, P1033, DOI 10.1126/science.aan8097
   Nielsen J., 1994, USABILITY ENG
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P233, DOI 10.1007/978-4-431-99495-4_13
   Oberosler V, 2017, MAMM BIOL, V87, P50, DOI 10.1016/j.mambio.2017.05.005
   Pollock SZ, 2017, ECOSPHERE, V8, DOI 10.1002/ecs2.1985
   Reconyx Inc, 2016, REC MAPVIEW PROF SOF
   Rollack CE, 2013, J RAPTOR RES, V47, P153, DOI 10.3356/JRR-12-40.1
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Royle JA, 2018, ECOGRAPHY, V41, P444, DOI 10.1111/ecog.03170
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Spence R., 2014, INFORM VISUALIZATION
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swann DE, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P27, DOI 10.1007/978-4-431-99495-4_3
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Ware C., 1995, ACM C HUM FACT COMP
   Whittington J, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40581-y
   Whittington J, 2018, J APPL ECOL, V55, P157, DOI 10.1111/1365-2664.12954
   WildTrax, 2019, WEBS ALB BIOD MON I
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
NR 55
TC 5
Z9 5
U1 2
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD DEC
PY 2019
VL 9
IS 24
BP 13706
EP 13730
DI 10.1002/ece3.5767
EA DEC 2019
PG 25
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA KF8XQ
UT WOS:000508011300001
PM 31938476
OA Green Published, gold
DA 2022-02-10
ER

PT C
AU Imaduddin, H
   Anwar, MK
   Perdana, MI
   Sulistijono, IA
   Risnumawan, A
AF Imaduddin, Hasan
   Anwar, Muhamad Khoirul
   Perdana, Muhammad Ilham
   Sulistijono, Indra Adji
   Risnumawan, Anhar
BE Ardiansyah, MF
   Permatasari, DI
   Muarifin
   Muliawati, TH
   Sari, DM
TI Indonesian Vehicle License Plate Number Detection using Deep
   Convolutional Neural Network
SO 2018 INTERNATIONAL ELECTRONICS SYMPOSIUM ON KNOWLEDGE CREATION AND
   INTELLIGENT COMPUTING (IES-KCIC)
LA English
DT Proceedings Paper
CT 20th International Electronics Symposium on Knowledge Creation and
   Intelligent Computing (IES-KCIC)
CY OCT 29-30, 2018
CL Politeknik Elektronika Negeri Surabaya, Bali, INDONESIA
SP Inst Elect & Elect Engineers, Indonesia Sect
HO Politeknik Elektronika Negeri Surabaya
DE Deep Learning; Convolutional Neural Network; Indonesian License Plate
AB In Indonesia, the license plate can be the identity of each vehicle, absolutely in every traffic, often occur traffic violations by the driver, the easiest way to identify them is by using their license plate number. Conventional plate detection is proved not solving the problem and not effective. Some problem that usually occurs when using conventional detection is unable to crack down the violator simultaneously, fraud, etc. Therefore to overcome those problems, automatic camera detection becomes an important thing. license plate detection only using camera visualization is an interesting challenge, by applying the latest artificial intelligence that is deep learning, one of them is Convolutional Neural Network (CNN) which is very good on image classification recently. The system will automatically detect the license plate number each vehicle, when the riders entering the zebra cross area, then his license plate will be detected and stored. Besides the system can also detect the plate on the vehicle that has excessive speed, by counting on the counter timer, and when there are vehicles that exceeding the speed limit it will detect the license plate.
C1 [Imaduddin, Hasan] PENS, Comp Engn Div, Kampus PENS,Jalan Raya ITS Sukolilo, Surabaya 60111, Indonesia.
   [Anwar, Muhamad Khoirul; Perdana, Muhammad Ilham; Sulistijono, Indra Adji; Risnumawan, Anhar] PENS, Mechatron Engn Div, Kampus PENS,Jalan Raya ITS Sukolilo, Surabaya 60111, Indonesia.
RP Imaduddin, H (corresponding author), PENS, Comp Engn Div, Kampus PENS,Jalan Raya ITS Sukolilo, Surabaya 60111, Indonesia.
EM hasanimaduddin@ce.student.pens.ac.id; muhkhoi@me.student.pens.ac.id;
   ilhamperdana17@me.student.pens.ac.id; indra@pens.ac.id; anhar@pens.ac.id
FU Directorate General of Research Strengthening and Development, Ministry
   of Research, Technology, and Higher Education, Republic of Indonesia by
   INSINAS 2nd PhaseMinistry of Research and Technology of the Republic of
   Indonesia (RISTEK) [13/E/KPT/2018]
FX The authors would like to thanks to Directorate General of Research
   Strengthening and Development, Ministry of Research, Technology, and
   Higher Education, Republic of Indonesia for supporting this research by
   INSINAS 2nd Phase, No. 13/E/KPT/2018, Date May 7, 2018.
CR Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
NR 11
TC 2
Z9 2
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-8079-7
PY 2018
BP 158
EP 163
PG 6
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BM1JG
UT WOS:000459878100026
DA 2022-02-10
ER

PT J
AU Palencia, P
   Fernandez-Lopez, J
   Vicente, J
   Acevedo, P
AF Palencia, Pablo
   Fernandez-Lopez, Javier
   Vicente, Joaquin
   Acevedo, Pelayo
TI Innovations in movement and behavioural ecology from camera traps: Day
   range as model parameter
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE activity; machine learning; mammals; REM; simulation; speed
ID HETEROGENEOUS LANDSCAPES; ACTIVITY PATTERNS; ANIMAL MOVEMENTS; WOLVES
AB Camera-trapping methods have been used to monitor movement and behavioural ecology parameters of wildlife. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise DR results will be biased. For instance, some wildlife populations present movement patterns characteristic of each behaviour (e.g. foraging or displacement between habitat patches), and further research is needed to integrate the behaviours in the estimation of movement parameters. In this respect, the day range (average daily distance travelled by an individual, DR) is a model parameter that relies on movement and behaviour. This study aims to provide a step forward concerning the use of camera-trapping in movement and behavioural ecology.
   We describe a machine learning procedure to differentiate movement behaviours from camera-trap data, and revisit the approach to consider different behaviours in the estimation of DR. Second, working within a simulated framework we tested the performance of three approaches to estimate DR: DROB (i.e. estimating DR without behavioural identification), DRTB (i.e. estimating DR by identifying behaviours manually and weighting each behaviour on the basis of the encounter rate obtained) and DRRB (i.e. estimating DR based on the classification of movement behaviours by a machine learning procedure and the ratio between speeds). Finally, we evaluated these approaches for 24 wild mammal species with different behavioural and ecological traits.
   The machine learning procedure to differentiate behaviours showed high accuracy (mean = 0.97). The DROB approach generated accurate results in scenarios with a speed-ratio (fast relative to slow behaviours) lower than 10, and for scenarios in which the animals spend most of the activity period on the slow behaviour. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise the DR results will be biased. The new approach, DRRB, generated accurate results in all the scenarios. The results obtained from real populations were consistent with the simulations.
   In conclusion, the integration of behaviours and speed-ratio in camera-trap studies makes it possible to obtain unbiased DR. Speed-ratio should be considered so that fast behaviour is not overrepresented. The procedures described in this work extend the applicability of camera-trap-based approaches in both movement and behavioural ecology.
C1 [Palencia, Pablo; Fernandez-Lopez, Javier; Vicente, Joaquin; Acevedo, Pelayo] Inst Invest Recursos Cineget IREC CSIC UCLM JCCM, Ciudad Real, Spain.
RP Palencia, P (corresponding author), Inst Invest Recursos Cineget IREC CSIC UCLM JCCM, Ciudad Real, Spain.
EM palencia.pablo.m@gmail.com
RI Fernández-López, Javier/AAU-7282-2020; Acevedo, Pelayo/L-6737-2013;
   Vicente, Joaquin/K-7822-2013
OI Fernández-López, Javier/0000-0003-4352-0252; Palencia,
   Pablo/0000-0002-2928-4241; Acevedo, Pelayo/0000-0002-3509-7696; Vicente,
   Joaquin/0000-0001-8416-3672
FU MINECO-FEDERSpanish Government [PID2019-111699RB-I00]; MINECO-UCLM
   [FPU16/00039]
FX MINECO-UCLM, Grant/Award Number: FPU16/00039; MINECO-FEDER, Grant/ Award
   Number: PID2019-111699RB-I00
CR AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Barraquand F, 2008, ECOLOGY, V89, P3336, DOI 10.1890/08-0162.1
   Buderman FE, 2018, ECOGRAPHY, V41, P126, DOI 10.1111/ecog.03030
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2020, CONSERV SCI PRACT, V2, DOI 10.1111/csp2.239
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Charrad M, 2014, J STAT SOFTW, V61, P1
   Dalloz MF, 2012, MAMM BIOL, V77, P307, DOI 10.1016/j.mambio.2012.03.001
   Duffy KJ, 2011, S AFR J WILDL RES, V41, P21, DOI 10.3957/056.041.0107
   ENETwild Consortium, 2018, EFSA SUPPORTING PUBL, DOI [DOI 10.2903/SP.EFSA.2018.EN-1449, 10.2903/sp.efsa.2018.EN-1449]
   Erdtmann D, 2020, PEERJ, V8, DOI 10.7717/peerj.10409
   Fortin D, 2005, ECOLOGY, V86, P1320, DOI 10.1890/04-0953
   Fowler J., 2013, PRACTICAL STAT FIELD
   Fryxell JM, 2008, P NATL ACAD SCI USA, V105, P19114, DOI 10.1073/pnas.0801737105
   GOODMAN LA, 1960, J AM STAT ASSOC, V55, P708, DOI 10.2307/2281592
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Holyoak M, 2008, P NATL ACAD SCI USA, V105, P19060, DOI 10.1073/pnas.0800483105
   Iijima H, 2020, MAMM STUDY, V45, P177, DOI 10.3106/ms2019-0082
   Joo R, 2020, J ANIM ECOL, V89, P248, DOI 10.1111/1365-2656.13116
   Kery M, 2011, CONSERV BIOL, V25, P356, DOI 10.1111/j.1523-1739.2010.01616.x
   Kindberg J, 2009, BIOL CONSERV, V142, P159, DOI 10.1016/j.biocon.2008.10.009
   Kovacs V, 2017, APPL ANIM BEHAV SCI, V195, P112, DOI 10.1016/j.applanim.2017.05.019
   Larrucea ES, 2007, J WILDLIFE MANAGE, V71, P1682, DOI 10.2193/2006-407
   Leuchtenberger C, 2014, ETHOL ECOL EVOL, V26, P19, DOI 10.1080/03949370.2013.821673
   Likas A, 2003, PATTERN RECOGN, V36, P451
   Lopez-Bao JV, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-20675-9
   Macdonald Peter, 2018, CRAN
   Martin J., 2009, THESIS CLAUDE BERNAR, P89
   MARTIN R, 1995, OECOLOGIA, V101, P45, DOI 10.1007/BF00328898
   Michelot T, 2016, METHODS ECOL EVOL, V7, P1308, DOI 10.1111/2041-210X.12578
   Morales JM, 2002, ECOLOGY, V83, P2240
   Neilson EW, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2092
   Neumann W, 2015, MOV ECOL, V3, DOI 10.1186/s40462-015-0036-7
   Niedballa J, 2019, REMOTE SENS ECOL CON, V5, P272, DOI 10.1002/rse2.107
   Ogurtsov SS, 2018, NAT CONSERV RES, V3, P68, DOI 10.24189/ncr.2018.031
   Owen-Smith N, 2020, MAMMAL REV, V50, P252, DOI 10.1111/mam.12193
   Palencia P, 2019, J ZOOL, V309, P182, DOI 10.1111/jzo.12710
   Palencia P., 2020, TRAPPINGMOTION INTEG
   Palencia P., 2021, ZENODO, DOI [10.5281/zenodo.4623758, DOI 10.5281/ZENODO.4623758]
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2012, METHODS ECOL EVOL, V3, P653, DOI 10.1111/j.2041-210X.2012.00197.x
   Rowcliffe JM, 2019, Activity: animal activity statistics. R package version 1.3
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Schaus J, 2020, REMOTE SENS ECOL CON, V6, P514, DOI 10.1002/rse2.153
   Scherer C, 2020, OIKOS, V129, P651, DOI 10.1111/oik.07002
   Sennhenn-Reulen Holger, 2017, Primate Biol, V4, P143, DOI 10.5194/pb-4-143-2017
   Smith JA, 2020, J ANIM ECOL, V89, P1997, DOI 10.1111/1365-2656.13264
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Tobler MW, 2009, J TROP ECOL, V25, P261, DOI 10.1017/S0266467409005896
   Vazquez C, 2019, METHODS ECOL EVOL, V10, P2057, DOI 10.1111/2041-210X.13290
   Wearn O.R., 2017, WWF CONSERVATION TEC, V1, P70
   Zuur AF, 2010, METHODS ECOL EVOL, V1, P3, DOI 10.1111/j.2041-210X.2009.00001.x
NR 56
TC 4
Z9 4
U1 4
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JUL
PY 2021
VL 12
IS 7
BP 1201
EP 1212
DI 10.1111/2041-210X.13609
EA MAY 2021
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA TC0ZC
UT WOS:000647824700001
DA 2022-02-10
ER

PT J
AU Shahinfar, S
   Meek, P
   Falzon, G
AF Shahinfar, Saleh
   Meek, Paul
   Falzon, Greg
TI "How many images do I need?" Understanding how sample size per class
   affects deep learning model performance metrics for balanced designs in
   autonomous wildlife monitoring
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Camera traps; Deep learning; Ecological informatics; Generalised
   additive models; Learning curves; Predictive modelling; Wildlife
AB Deep learning (DL) algorithms are the state of the art in automated classification of wildlife camera trap images. The challenge is that the ecologist cannot know in advance how many images per species they need to collect for model training in order to achieve their desired classification accuracy. In fact there is limited empirical evidence in the context of camera trapping to demonstrate that increasing sample size will lead to improved accuracy.
   In this study we explore in depth the issues of deep learning model performance for progressively increasing per class (species) sample sizes. We also provide ecologists with an approximation formula to estimate how many images per animal species they need for certain accuracy level a priori. This will help ecologists for optimal allocation of resources, work and efficient study design.
   In order to investigate the effect of number of training images; seven training sets with 10, 20, 50, 150, 500, 1000 images per class were designed. Six deep learning architectures namely ResNet-18, ResNet-50, ResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a common exclusive testing set of 250 images per class. The whole experiment was repeated on three similar datasets from Australia, Africa and North America and the results were compared. Simple regression equations for use by practitioners to approximate model performance metrics are provided. Generalizes additive models (GAM) are shown to be effective in modelling DL performance metrics based on the number of training images per class, tuning scheme and dataset.
   Overall, our trained models classified images with 0.94 accuracy (ACC), 0.73 precision (PRC), 0.72 true positive rate (TPR), and 0.03 false positive rate (FPR). Variation in model performance metrics among datasets, species and deep learning architectures exist and are shown distinctively in the discussion section. The ordinary least squares regression models explained 57%, 54%, 52%, and 34% of expected variation of ACC, PRC, TPR, and FPR according to number of images available for training. Generalised additive models explained 77%, 69%, 70%, and 53% of deviance for ACC, PRC, TPR, and FPR respectively.
   Predictive models were developed linking number of training images per class, model, dataset to performance metrics. The ordinary least squares regression and Generalised additive models developed provides a practical toolbox to estimate model performance with respect to different numbers of training images.
C1 [Shahinfar, Saleh; Falzon, Greg] Univ New England, Sch Sci & Technol, Armidale, NSW, Australia.
   [Shahinfar, Saleh] Agr Victoria, AgriBio Ctr, Dept Jobs Precincts & Reg, Bundoora, Vic, Australia.
   [Meek, Paul] NSW Dept Primary Ind, POB 530, Coffs Harbour, NSW, Australia.
   [Meek, Paul] Univ New England, Sch Environm & Rural Sci, Armidale, NSW, Australia.
RP Shahinfar, S (corresponding author), Agr Victoria, AgriBio Ctr, Dept Jobs Precincts & Reg, Bundoora, Vic, Australia.
EM shahinfar@uwalumni.com
OI Falzon, Gregory/0000-0002-1989-9357; Shahinfar,
   Saleh/0000-0003-0730-7577; Meek, Paul/0000-0002-3792-5723
FU Australian Government Department of Agriculture and Water
   ResourcesAustralian Government
FX Funding for this project was provided by the Australian Government
   Department of Agriculture and Water Resources through the e-Technology
   Hub - Utilising Technology to Improve Pest Management Effectiveness and
   Enhance Welfare Outcomes project.
CR Adamko P, 2017, GLOBALIZATION AND ITS SOCIO-ECONOMIC CONSEQUENCES, PTS I - VI, P1
   Baktashmotlagh M, 2019, ARXIV190210847
   Barz B, 2020, IEEE WINT CONF APPL, P1360, DOI 10.1109/WACV45572.2020.9093286
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Beery S, 2020, IEEE WINT CONF APPL, P852, DOI 10.1109/WACV45572.2020.9093570
   Cho J., 2016, MUCH DATA IS NEEDED
   Clare JDJ, 2019, ECOL APPL, V29, DOI 10.1002/eap.1849
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hastie T., 1986, GEN ADDITIVE MODELS, V1, P297
   Hastie T.J., 1990, GEN ADDITIVE MODELS, V43
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G ., 2014, DISTILLING KNOWLEDGE
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Meek Paul D., 2020, Australian Zoologist, V40, P392, DOI 10.7882/AZ.2019.035
   Meek PD, 2015, AUST MAMMAL, V37, P1, DOI 10.1071/AM14021
   NAZIR S, 2017, PLOS, P69758
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Patterson J, 2017, DEEP LEARNING, V1st
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Smith L.N, 2018, ARXIV 2018 180309820
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wisconsin Department of Natural Resources W, 2019, SNAPSH WISC VOL BAS
   Wood S., 2017, GEN ADDITIVE MODELS
   Wood SN, 2016, J AM STAT ASSOC, V111, P1548, DOI 10.1080/01621459.2016.1180986
   Wood SN, 2011, J R STAT SOC B, V73, P3, DOI 10.1111/j.1467-9868.2010.00749.x
   Xu W, 2018, DESTECH TRANS COMP, P186
   Yosinski J, 2014, ADV NEUR IN, V27
   Zoph B., 2019, ARXIV PREPRINT ARXIV, P1
NR 42
TC 13
Z9 13
U1 1
U2 13
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAY
PY 2020
VL 57
AR 101085
DI 10.1016/j.ecoinf.2020.101085
PG 16
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA LG6NY
UT WOS:000528216500014
OA Bronze, Green Submitted
DA 2022-02-10
ER

PT C
AU Dhillon, A
   Verma, GK
AF Dhillon, Anamika
   Verma, Gyanendra K.
BE Tiwary, US
TI Wild Animal Detection from Highly Cluttered Forest Images Using Deep
   Residual Networks
SO INTELLIGENT HUMAN COMPUTER INTERACTION
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 10th International Conference on Intelligent Human Computer Interaction
   (IHCI)
CY DEC 07-09, 2018
CL Allahabad, INDIA
SP Indian Inst Informat Technol Allahabad
DE Wild animal detection; DCNN feature extractor; Ensemble tree; KNN;
   Natural scenes; SVM
AB Wild animal detection is a dynamic research field since last decades. The videos acquired from camera-trap comprises of scenes that are cluttered that poses a challenge for detection of the wild animal. In this paper, we proposed a deep learning based system to detect wild animal from highly cluttered natural forest images. We have utilized Deep Residual Network (ResNet) for features extraction from cluttered forest images. These features are feed to classification through some of the best in class machine learning techniques, to be specific Support Vector Machine, K-Nearest Neighbor and Ensemble Tree. Our outcomes demonstrate that our detection system through ResNet outperforms compare to existing systems reported in the literature.
C1 [Dhillon, Anamika; Verma, Gyanendra K.] Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
RP Verma, GK (corresponding author), Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
EM dhillon.anamika2390@gmail.com; gyanendra@nitkkr.ac.in
RI Verma, Gyanendra/A-1013-2016
OI Verma, Gyanendra/0000-0003-2567-3730
CR Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chauhan A. K., 2013, INT J ADV RES COMPUT, V3
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Joshi Ritesh, 2012, International Journal of Ecosystem, V2, P44
   Kays R., 2014, P N AM CONSERVATION, P80
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Rakibe Rupali S., 2013, INT J SCI RES PUBLIC, V3, P2250
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176
   Szegedy C., 2013, ADV NEURAL INF PROCE, V26, P2553, DOI DOI 10.5555/2999792.2999897
   Szegedy C., 2014, ARXIV14121441
   Tilak S., 2011, INT J RES REV WIRELE, V1, P1929
   Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 16
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-04021-5; 978-3-030-04020-8
J9 LECT NOTES COMPUT SC
PY 2018
VL 11278
BP 230
EP 238
DI 10.1007/978-3-030-04021-5_21
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ4GA
UT WOS:000589562600021
DA 2022-02-10
ER

PT J
AU Delisle, ZJ
   Flaherty, EA
   Nobbe, MR
   Wzientek, CM
   Swihart, RK
AF Delisle, Zackary J.
   Flaherty, Elizabeth A.
   Nobbe, Mackenzie R.
   Wzientek, Cole M.
   Swihart, Robert K.
TI Next-Generation Camera Trapping: Systematic Review of Historic Trends
   Suggests Keys to Expanded Research Applications in Ecology and
   Conservation
SO FRONTIERS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE camera trap; diversity; ecoregions; image classification; occupancy;
   population attributes; technological diffusion; wildlife
ID ESTIMATING ANIMAL DENSITY; ACTIVITY PATTERNS; TRAPS; DISTANCE; PREY;
   BIODIVERSITY; OCCUPANCY; LANDSCAPE; RICHNESS; CANOPY
AB Camera trapping is an effective non-invasive method for collecting data on wildlife species to address questions of ecological and conservation interest. We reviewed 2,167 camera trap (CT) articles from 1994 to 2020. Through the lens of technological diffusion, we assessed trends in: (1) CT adoption measured by published research output, (2) topic, taxonomic, and geographic diversification and composition of CT applications, and (3) sampling effort, spatial extent, and temporal duration of CT studies. Annual publications of CT articles have grown 81-fold since 1994, increasing at a rate of 1.26 (SE = 0.068) per year since 2005, but with decelerating growth since 2017. Topic, taxonomic, and geographic richness of CT studies increased to encompass 100% of topics, 59.4% of ecoregions, and 6.4% of terrestrial vertebrates. However, declines in per article rates of accretion and plateaus in Shannon's H for topics and major taxa studied suggest upper limits to further diversification of CT research as currently practiced. Notable compositional changes of topics included a decrease in capture-recapture, recent decrease in spatial-capture-recapture, and increases in occupancy, interspecific interactions, and automated image classification. Mammals were the dominant taxon studied; within mammalian orders carnivores exhibited a unimodal peak whereas primates, rodents and lagomorphs steadily increased. Among biogeographic realms we observed decreases in Oceania and Nearctic, increases in Afrotropic and Palearctic, and unimodal peaks for Indomalayan and Neotropic. Camera days, temporal extent, and area sampled increased, with much greater rates for the 0.90 quantile of CT studies compared to the median. Next-generation CT studies are poised to expand knowledge valuable to wildlife ecology and conservation by posing previously infeasible questions at unprecedented spatiotemporal scales, on a greater array of species, and in a wider variety of environments. Converting potential into broad-based application will require transferable models of automated image classification, and data sharing among users across multiple platforms in a coordinated manner. Further taxonomic diversification likely will require technological modifications that permit more efficient sampling of smaller species and adoption of recent improvements in modeling of unmarked populations. Environmental diversification can benefit from engineering solutions that expand ease of CT sampling in traditionally challenging sites.
C1 [Delisle, Zackary J.; Flaherty, Elizabeth A.; Nobbe, Mackenzie R.; Wzientek, Cole M.; Swihart, Robert K.] Purdue Univ, Dept Forestry & Nat Resources, W Lafayette, IN 47907 USA.
RP Delisle, ZJ (corresponding author), Purdue Univ, Dept Forestry & Nat Resources, W Lafayette, IN 47907 USA.
EM zdelisle@purdue.edu
FU Indiana DNR grant [W-48-R-02]; Purdue University; USDA National
   Institute of Food and Agriculture, Hatch Project [1019737]
FX Funding was provided by Indiana DNR grant W-48-R-02 and by Purdue
   University. Support for EF provided by the USDA National Institute of
   Food and Agriculture, Hatch Project #1019737.
CR Ahmed A, 2019, ECOL INFORM, V52, P57, DOI 10.1016/j.ecoinf.2019.05.006
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Anderson D.R., 2002, MODEL SELECTION MULT
   Apps P, 2018, AFR J ECOL, V56, P710, DOI 10.1111/aje.12573
   Atkin DJ, 2015, MASS COMMUN SOC, V18, P623, DOI 10.1080/15205436.2015.1066014
   Augustine B, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2627
   Bischof R, 2014, J ZOOL, V293, P40, DOI 10.1111/jzo.12100
   Blackburn Tim M., 1998, Diversity and Distributions, V4, P121, DOI 10.1046/j.1365-2699.1998.00015.x
   BLACKBURN TM, 1994, OIKOS, V70, P127, DOI 10.2307/3545707
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Chan BPL, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-72641-z
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chapman F.M., 1927, NATL, V52, P330
   Chler R., 2020, R PACKAGE UNMARKED M
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Cusack JJ, 2017, OIKOS, V126, P812, DOI 10.1111/oik.03403
   Cutler TL, 1999, WILDLIFE SOC B, V27, P571
   Delibes-Mateos M, 2014, MAMM BIOL, V79, P393, DOI 10.1016/j.mambio.2014.04.006
   Dinerstein E, 2017, BIOSCIENCE, V67, P534, DOI 10.1093/biosci/bix014
   Dunn P. K., 1996, J COMPUTATIONAL GRAP, V5, P236, DOI [DOI 10.2307/1390802, 10.1080/10618600.1996.10474708]
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Frey S, 2017, REMOTE SENS ECOL CON, V3, P123, DOI 10.1002/rse2.60
   Gilbert NA, 2021, CONSERV BIOL, V35, P88, DOI 10.1111/cobi.13517
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Godoy-Guinao J, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2424
   Green AM, 2020, FRONT ECOL EVOL, V8, DOI 10.3389/fevo.2020.563477
   Green SE, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010132
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   Heiniger J, 2018, WILDLIFE RES, V45, P578, DOI 10.1071/WR18078
   Hepler SA, 2021, ENVIRONMETRICS, V32, DOI 10.1002/env.2657
   Hofmeester TR, 2019, ECOL EVOL, V9, P2320, DOI 10.1002/ece3.4878
   Hofmeester TR, 2017, REMOTE SENS ECOL CON, V3, P81, DOI 10.1002/rse2.25
   HOPE ACA, 1968, J ROY STAT SOC B, V30, P582
   Howe EJ, 2019, METHODS ECOL EVOL, V10, P38, DOI 10.1111/2041-210X.13082
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Kays R, 2020, DIVERS DISTRIB, V26, P644, DOI 10.1111/ddi.12993
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Keim JL, 2019, J ANIM ECOL, V88, P690, DOI 10.1111/1365-2656.12960
   Kellner KF, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111436
   Kery M., 2015, APPL HIERARCHICAL MO
   Kery M., 2017, R PACKAGE AHMBOOK FU
   Kitzes J, 2019, ENVIRON CONSERV, V46, P247, DOI 10.1017/S0376892919000146
   Macaulay LT, 2020, J WILDLIFE MANAGE, V84, P301, DOI 10.1002/jwmg.21803
   Magsamen-Conrad K, 2020, COMPUT HUM BEHAV, V112, DOI 10.1016/j.chb.2020.106456
   MANTEL N, 1982, Biometrical Journal, V24, P579, DOI 10.1002/bimj.4710240607
   Maronde L, 2020, ECOL EVOL, V10, P13968, DOI 10.1002/ece3.6990
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McCleery RA, 2014, WILDLIFE SOC B, V38, P887, DOI 10.1002/wsb.447
   McIntyre T, 2020, WILDLIFE RES, V47, P177, DOI 10.1071/WR19040
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   MILLER RG, 1974, BIOMETRIKA, V61, P1
   Mills CA, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0146142
   Moeller AK, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2331
   Moll RJ, 2020, WILDLIFE RES, V47, P158, DOI 10.1071/WR19004
   Mos J, 2020, MAMMAL RES, V65, P843, DOI 10.1007/s13364-020-00513-y
   Nakashima Y, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108381
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   Nekaris KAI, 2020, DIVERSITY-BASEL, V12, DOI 10.3390/d12100399
   Ngoprasert D, 2019, GLOB ECOL CONSERV, V20, DOI 10.1016/j.gecco.2019.e00792
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Ortmann CR, 2021, J ZOOL, V313, P202, DOI 10.1111/jzo.12849
   Palmer MS, 2017, ECOL LETT, V20, P1364, DOI 10.1111/ele.12832
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Pollock KH, 2002, ENVIRONMETRICS, V13, P105, DOI 10.1002/env.514
   Rice RE, 2017, HUM COMMUN RES, V43, P531, DOI 10.1111/hcre.12119
   Rich LN, 2017, GLOBAL ECOL BIOGEOGR, V26, P918, DOI 10.1111/geb.12600
   Rivas-Romero JA, 2015, SOUTHWEST NAT, V60, P366, DOI 10.1894/0038-4909-60.4.366
   Rogers EM., 1962, DIFFUSION INNOVATION
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe M., 2019, R PACKAGE ACTIVITY A
   Royle J.A., 2020, APPL HIERARCHICAL MO
   Salman A, 2020, ICES J MAR SCI, V77, P1295, DOI 10.1093/icesjms/fsz025
   Sarkar J., 1998, J EC SURVEYS, V12, P131, DOI 10.1111/1467-6419.00051
   Schneider F.D., 2020, R PACKAGE TRAITDATAF
   Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Smith JA, 2020, J ANIM ECOL, V89, P1997, DOI 10.1111/1365-2656.13264
   Soininen EM, 2015, REMOTE SENS ECOL CON, V1, P29, DOI 10.1002/rse2.2
   Sollmann R, 2018, AFR J ECOL, V56, P740, DOI 10.1111/aje.12557
   Solomon S, 2007, AR4 CLIMATE CHANGE 2007: THE PHYSICAL SCIENCE BASIS, P1
   Spellerberg IF, 2003, GLOBAL ECOL BIOGEOGR, V12, P177, DOI 10.1046/j.1466-822X.2003.00015.x
   Steenweg R, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2639
   Steenweg R, 2018, ECOLOGY, V99, P172, DOI 10.1002/ecy.2054
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Struthers DP, 2015, FISHERIES, V40, P502, DOI 10.1080/03632415.2015.1082472
   Sundaram DM, 2020, NEURAL PROCESS LETT, V52, P727, DOI 10.1007/s11063-020-10246-3
   Suzuki KK, 2019, MAMMALIA, V83, P372, DOI 10.1515/mammalia-2018-0055
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swihart RK, 2020, WILDLIFE SOC B, V44, P77, DOI 10.1002/wsb.1037
   Tabak MA, 2020, ECOL EVOL, V10, P10374, DOI 10.1002/ece3.6692
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Team RC, 2014, R LANG ENV STAT COMP
   Tennant EN, 2020, WILDLIFE SOC B, V44, P610, DOI 10.1002/wsb.1103
   Thau D., 2019, BIODIVERS INF SCI ST, V3, DOI [10.3897/biss.3.38233, DOI 10.3897/BISS.3.38233]
   Wang Y, 2012, METHODS ECOL EVOL, V3, P471, DOI 10.1111/j.2041-210X.2012.00190.x
   Warton DI, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181790
   Wearn OR, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181748
   Wilson DE., 2005, MAMMAL SPECIES WORLD, V3rd
   Wood S.N., 2017, GEN ADDITIVE MODELS, V2nd Edn, DOI DOI 10.1201/9781315370279
   Wood Simon, 2021, CRAN
   Wood SN, 2003, J ROY STAT SOC B, V65, P95, DOI 10.1111/1467-9868.00374
   Yoshioka A, 2020, PEERJ, V8, DOI 10.7717/peerj.9681
   Zaffran M., 2020, R PACKAGE QGAM SMOOT
   Zak E, 2019, Distance Estimation Using Multi-Camera Device, Patent No. [10192312 B2, 10192312]
NR 115
TC 2
Z9 2
U1 12
U2 20
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 2296-701X
J9 FRONT ECOL EVOL
JI Front. Ecol. Evol.
PD FEB 26
PY 2021
VL 9
AR 617996
DI 10.3389/fevo.2021.617996
PG 18
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QZ9XS
UT WOS:000631073400001
OA gold
DA 2022-02-10
ER

PT J
AU Verma, GK
   Gupta, P
AF Verma, Gyanendra K.
   Gupta, Pragya
TI Wild Animal Detection from Highly Cluttered Images Using Deep
   Convolutional Neural Network
SO INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS
LA English
DT Article
DE Wild animal detection; convolutional neural network; VGGNet; ResNet;
   SVM; ensemble tree; KNN; natural scenes
AB Monitoring wild animals became easy due to camera trap network, a technique to explore wildlife using automatically triggered camera on the presence of wild animal and yields a large volume of multimedia data. Wild animal detection is a dynamic research field since the last several decades. In this paper, we propose a wild animal detection system to monitor wildlife and detect wild animals from highly cluttered natural images. The data acquired from the camera-trap network comprises of scenes that are highly cluttered that poses a challenge for detection of wild animals bringing about low recognition rates and high false discovery rates. To deal with the issue, we have utilized a camera trap database that provides candidate regions utilizing multilevel graph cut in the spatiotemporal area. The regions are utilized to make a validation stage that recognizes whether animals are present or not in a scene. These features from cluttered images are extracted using Deep Convolutional Neural Network (CNN). We have implemented the system using two prominent CNN models namely VGGNet and ResNet, on standard camera trap database. Finally, the CNN features fed to some of the best in class machine learning techniques for classification. Our outcomes demonstrate that our proposed system is superior compared to existing systems reported in the literature.
C1 [Verma, Gyanendra K.; Gupta, Pragya] Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
RP Verma, GK (corresponding author), Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
EM gyanendra@nitkkr.ac.in; pragyagupta10@gmail.com
CR Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Chatfield K., 2014, ARXIV201414053531
   Chauhan A. K., 2013, INT J ADV RES COMPUT, V3
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P104, DOI 10.1109/CCAA.2017.8229781
   Hongfei Yu W. L., 2014, IET COMPUT VIS, V9, P13
   Joshi KA., 2012, INT J SOFT COMPUT EN, V2, P44
   Kays R., 2014, P N AM CONSERVATION, P80
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lee WT, 2009, PROC CVPR IEEE, P1590, DOI 10.1109/CVPRW.2009.5206521
   Mahadevan V., 2008, P 2008 IEEE C COMP V, P1
   Mammeri A, 2014, IEEE ICC, P1854, DOI 10.1109/ICC.2014.6883593
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Rakibe Rupali S., 2013, INT J SCI RES PUBLIC, V3, P2250
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sermanet P., 2013, P INT C LEARN REPR
   Szegedy C., 2014, ARXIV14121441
   Szegedy C., 2013, ADV NEURAL INF PROCE, V26, P2553, DOI DOI 10.5555/2999792.2999897
   Tilak S., 2011, INT J RES REV WIRELE, V1, P19
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 27
TC 0
Z9 0
U1 2
U2 7
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1469-0268
EI 1757-5885
J9 INT J COMPUT INTELL
JI Int. J. Comput. Intell. Appl.
PD DEC
PY 2018
VL 17
IS 4
AR 1850021
DI 10.1142/S1469026818500219
PG 17
WC Computer Science, Artificial Intelligence
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA HI2FS
UT WOS:000456261200004
DA 2022-02-10
ER

PT J
AU Kosmala, M
   Hufkens, K
   Richardson, AD
AF Kosmala, Margaret
   Hufkens, Koen
   Richardson, Andrew D.
TI Integrating camera imagery, crowdsourcing, and deep learning to improve
   high-frequency automated monitoring of snow at continental-to-global
   scales
SO PLOS ONE
LA English
DT Article
ID LAND-COVER; SPATIAL-RESOLUTION; MODIS; PHOTOGRAPHY; CLASSIFICATION;
   RETRIEVAL; QUALITY; PHOTOS; AREA
AB Snow is important for local to global climate and surface hydrology, but spatial and temporal heterogeneity in the extent of snow cover make accurate, fine-scale mapping and monitoring of snow an enormous challenge. We took 184,453 daily near-surface images acquired by 133 automated cameras and processed them using crowdsourcing and deep learning to determine whether snow was present or absent in each image. We found that the crowdsourced data had an accuracy of 99.1% when compared with expert evaluation of the same imagery. We then used the image classification to train a deep convolutional neural network via transfer learning, with accuracies of 92% to 98%, depending on the image set and training method. The majority of neural network errors were due to snow that was present not being detected. We used the results of the neural networks to validate the presence or absence of snow inferred from the MODIS satellite sensor and obtained similar results to those from other validation studies. This method of using automated sensors, crowdsourcing, and deep learning in combination produced an accurate high temporal dataset of snow presence across a continent. It holds broad potential for real-time large-scale acquisition and processing of ecological and environmental data in support of monitoring, management, and research objectives.
C1 [Kosmala, Margaret; Hufkens, Koen; Richardson, Andrew D.] Harvard Univ, Dept Organism & Evolutionary Biol, Cambridge, MA 02138 USA.
   [Hufkens, Koen] INRA, Unite Mixte Rech Interact Sol Plante Atmosphere, Villenave Dornon, France.
   [Richardson, Andrew D.] No Arizona Univ, Sch Informat Comp & Cyber Syst, Flagstaff, AZ USA.
   [Richardson, Andrew D.] No Arizona Univ, Ctr Ecosyst Sci & Soc, Flagstaff, AZ USA.
RP Kosmala, M (corresponding author), Harvard Univ, Dept Organism & Evolutionary Biol, Cambridge, MA 02138 USA.
EM mkosmala@gmail.com
RI Richardson, Andrew D./F-5691-2011
OI Richardson, Andrew D./0000-0002-0148-6714; Hufkens,
   Koen/0000-0002-5070-8109
FU U.S. National Science Foundation's Macrosystems Biology program
   [EF-1065029, EF-1702697]
FX The development of PhenoCam has been supported by the U.S. National
   Science Foundation's Macrosystems Biology program (awards EF-1065029 and
   EF-1702697 to AR). http://www.nsf.gov.The funders had no role in study
   design, data collection and analysis, decision to publish, or
   preparation of the manuscript.
CR Amato A, 2013, DIVIDE CONQUER ATOMI, P21
   Amato A, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P211
   [Anonymous], 2002, POLAR GEOGRAPHY, DOI DOI 10.1080/789610195
   Arsenault KR, 2014, HYDROL PROCESS, V28, P980, DOI 10.1002/hyp.9636
   Barnett TP, 2005, NATURE, V438, P303, DOI 10.1038/nature04141
   Barrett A. P., 2003, NATL OPERATIONAL HYD
   Bengio Y., 2012, P ICML WORKSH UNS TR, P17, DOI DOI 10.5555/3045796.3045800
   Bernard E, 2013, ISPRS J PHOTOGRAMM, V75, P92, DOI 10.1016/j.isprsjprs.2012.11.001
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Buus-Hinkler J, 2006, REMOTE SENS ENVIRON, V105, P237, DOI 10.1016/j.rse.2006.06.016
   Campbell JL, 2005, FRONT ECOL ENVIRON, V3, P314, DOI 10.2307/3868565
   Chen XW, 2014, IEEE ACCESS, V2, P514, DOI 10.1109/ACCESS.2014.2325029
   Cohen J., 1994, Weather, V49, P150
   De Freitas N., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.14288/1.0165555
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dyer J, 2008, J GEOPHYS RES-ATMOS, V113, DOI 10.1029/2008JD010031
   Eamer J., 2007, GLOBAL OUTLOOK ICE S
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Frei A, 2012, ADV SPACE RES, V50, P1007, DOI 10.1016/j.asr.2011.12.021
   Gascoin S, 2015, HYDROL EARTH SYST SC, V19, P2337, DOI 10.5194/hess-19-2337-2015
   Graybeal DY, 2006, J APPL METEOROL CLIM, V45, P178, DOI 10.1175/JAM2330.1
   Hall D. K., 2014, MODIS TERRA SNOW COV, DOI [10.5067/MODIS/MOD10A1.006, DOI 10.5067/MODIS/MOD10A1.006]
   Hall DK, 2001, IEEE T GEOSCI REMOTE, V39, P432, DOI 10.1109/36.905251
   Hall DK, 2007, HYDROL PROCESS, V21, P1534, DOI 10.1002/hyp.6715
   Hinkler J, 2002, INT J REMOTE SENS, V23, P4669, DOI 10.1080/01431160110113881
   Jones H.G., 2001, SNOW ECOLOGY INTERDI
   Keenan TF, 2014, ECOL APPL, V24, P1478, DOI 10.1890/13-0652.1
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Kosmala M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090726
   KUDO G, 1991, ARCTIC ALPINE RES, V23, P436, DOI 10.2307/1551685
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Kuter S, 2018, REMOTE SENS ENVIRON, V205, P236, DOI 10.1016/j.rse.2017.11.021
   Lawrence DM, 2005, GEOPHYS RES LETT, V32, DOI 10.1029/2005GL025080
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LISTON GE, 1995, J APPL METEOROL, V34, P1705, DOI 10.1175/1520-0450-34.7.1705
   Mote TL, 2008, J APPL METEOROL CLIM, V47, P2008, DOI 10.1175/2007JAMC1823.1
   Painter TH, 2009, REMOTE SENS ENVIRON, V113, P868, DOI 10.1016/j.rse.2009.01.001
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Parajka J, 2012, HYDROL PROCESS, V26, P3327, DOI 10.1002/hyp.8389
   Pedersen SH, 2016, ARCT ANTARCT ALP RES, V48, P653, DOI 10.1657/AAAR0016-028
   Raleigh MS, 2013, REMOTE SENS ENVIRON, V128, P44, DOI 10.1016/j.rse.2012.09.016
   Richardson AD, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.28
   Rittger K, 2013, ADV WATER RESOUR, V51, P367, DOI 10.1016/j.advwatres.2012.03.002
   Romanovsky VE, 2010, PERMAFROST PERIGLAC, V21, P106, DOI 10.1002/ppp.689
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379
   Salomonson VV, 2004, REMOTE SENS ENVIRON, V89, P351, DOI 10.1016/j.rse.2003.10.016
   Salvatori R, 2011, ITAL J REMOTE SENS, V43, P137, DOI 10.5721/ItJRS201143211
   Scott GJ, 2017, IEEE GEOSCI REMOTE S, V14, P549, DOI 10.1109/LGRS.2017.2657778
   Serreze MC, 1999, WATER RESOUR RES, V35, P2145, DOI 10.1029/1999WR900090
   Simonyan K., 2015, INT C LEARN REPR
   Sirguey P, 2009, REMOTE SENS ENVIRON, V113, P160, DOI 10.1016/j.rse.2008.09.008
   Todhunter PE, 2001, J AM WATER RESOUR AS, V37, P1263, DOI 10.1111/j.1752-1688.2001.tb03637.x
   Wang JY, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P452, DOI 10.1109/ICCVW.2013.66
   Wang JL, 2016, PROCEEDINGS OF 2016 IEEE ADVANCED INFORMATION MANAGEMENT, COMMUNICATES, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IMCEC 2016), P1097, DOI 10.1109/IMCEC.2016.7867381
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xing HF, 2018, ISPRS J PHOTOGRAMM, V141, P237, DOI 10.1016/j.isprsjprs.2018.04.025
   Xu G, 2017, ENVIRON MODELL SOFTW, V91, P127, DOI 10.1016/j.envsoft.2017.02.004
   Yan T, 2010, P MOBISYS JUN 15 18, DOI DOI 10.1145/1814433.1814443
   Zhang TJ, 2005, REV GEOPHYS, V43, DOI 10.1029/2004RG000157
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 62
TC 7
Z9 7
U1 0
U2 7
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD DEC 27
PY 2018
VL 13
IS 12
AR e0209649
DI 10.1371/journal.pone.0209649
PG 19
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA HF7KM
UT WOS:000454418200037
PM 30589858
OA gold, Green Published, Green Submitted
DA 2022-02-10
ER

PT C
AU Singh, P
   Lindshield, SM
   Zhu, FQ
   Reitman, AR
AF Singh, Praneet
   Lindshield, Stacy M.
   Zhu, Fengqing
   Reitman, Amy R.
GP IEEE
TI Animal Localization in Camera-Trap Images with Complex Backgrounds
SO 2020 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION
   (SSIAI 2020)
SE IEEE Southwest Symposium on Image Analysis and Interpretation
LA English
DT Proceedings Paper
CT IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)
CY MAR 29-31, 2020
CL ELECTR NETWORK
SP Inst Elect & Elect Engineers, IEEE Comp Soc
DE Camera traps; Deep convolution networks; Robust Principal Component
   Analysis; Spatial Attention; Animal localization
AB Motion-sensor camera traps help collect images of animals in the wild without intruding upon their native habitat. To obtain key insights about animal health and population densities, accurate counting, detection and classification of animals is important. Deep convolution neural networks perform well on these tasks when the background is free from dense vegetation, shadows, occlusions and rapid illumination changes. However, when the camera traps arc located in regions with extremely complex backgrounds, performance of these models degrades significantly. This is due to the fact that the models learn to focus on aspects of the image that arc unrelated to the animals. In this paper, we propose a system based on Robust Principal Component Analysis (Robust PCA) that spatially localizes the animals in the image. This localization can then he integrated into existing models to improve classification and detection accuracy. We demonstrate that our system creates better localizations than those of a pre-trained R-3 Net.
C1 [Singh, Praneet; Zhu, Fengqing; Reitman, Amy R.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Lindshield, Stacy M.] Purdue Univ, Sch Liberal Arts, Dept Anthropol, W Lafayette, IN 47907 USA.
RP Singh, P (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
FU Purdue University; Universite Cheikh Anta Diop; National Science
   FoundationNational Science Foundation (NSF); Leakey Foundation; Rufford
   Foundation; Primate Conservation, Inc.
FX We thank the Direction des Pares Nationaux de la Rcipublique du S(.'n
   for authorizing data collection in Niokolo-Koba National Park, and
   especially the support of Souleye Ndiaye, Ablaye Diop, the late Ousmane
   Kane, and MalR Gueye, Papa Ibnou Ndiaye provided critical logistical and
   technical expertise in Senegal, We also thank Ousmane Diedhiou, Kaly
   Bindia, Landing Badji, Natalia Roberts Buceta, Grace Marotta, lbrahima
   Ndao, Manic Abdou Faye, Bouchoura Keita, Ablaye Senghor, Tafsir Diop,
   Falilou Diouf, Mbacke Diouf, Amadou Diouf, Pape Mor Faye, Laraine Sane,
   Bouhacar Diallo, Philippe Dieme, and Mathieu Kabatou for technical
   support. The camera trapping surveys were supported by Purdue
   University, Universite Cheikh Anta Diop, National Science Foundation,
   Leakey Foundation, Rufford Foundation, and Primate Conservation, Inc.
CR Beery S., 2018, P EUR C COMP VIS ECC, P456
   Bouwmans T, 2018, P IEEE, V106, P1427, DOI 10.1109/JPROC.2018.2853589
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Giraldo-Zuluaga J., 2017, CORR
   Giraldo-Zuluaga JH, 2019, VISUAL COMPUT, V35, P335, DOI 10.1007/s00371-017-1463-9
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Guyon C, 2012, PRINCIPAL COMPONENT, DOI DOI 10.5772/38267
   He, 2017, 2017 IEEE INT S CIRC, P1, DOI DOI 10.1109/ISCAS.2017.8050762
   He K., 2015, CORR
   Jaderberg Max, 2015, CORR
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Li Q., 2018, ARXIV180805560
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Selvaraju R. R., 2016, ABS161002391 CORR
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Zhou ZH, 2010, IEEE INT SYMP INFO, P1518, DOI 10.1109/ISIT.2010.5513535
NR 24
TC 1
Z9 1
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1550-5782
BN 978-1-7281-5745-0
J9 IEEE SW SYMP IMAG
PY 2020
BP 66
EP 69
PG 4
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BQ3RL
UT WOS:000587759600018
DA 2022-02-10
ER

PT C
AU Chen, GB
   Han, TX
   He, ZH
   Kays, R
   Forrester, T
AF Chen, Guobin
   Han, Tony X.
   He, Zhihai
   Kays, Roland
   Forrester, Tavis
GP IEEE
TI DEEP CONVOLUTIONAL NEURAL NETWORK BASED SPECIES RECOGNITION FOR WILD
   ANIMAL MONITORING
SO 2014 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
SE IEEE International Conference on Image Processing ICIP
LA English
DT Proceedings Paper
CT IEEE International Conference on Image Processing (ICIP)
CY OCT 27-30, 2014
CL Paris, FRANCE
SP IEEE
DE Species recognition; wild animal monitor; image classification; deep
   convolutional neural networks; large scale learning
ID POPULATIONS
AB We proposed a novel deep convolutional neural network based species recognition algorithm for wild animal classification on very challenging camera-trap imagery data. The imagery data were captured with motion triggered camera trap and were segmented automatically using the state of the art graph-cut algorithm. The moving foreground is selected as the region of interests and is fed to the proposed species recognition algorithm. For the comparison purpose, we use the traditional bag of visual words model as the baseline species recognition algorithm. It is clear that the proposed deep convolutional neural network based species recognition achieves superior performance. To our best knowledge, this is the first attempt to the fully automatic computer vision based species recognition on the real camera-trap images. We also collected and annotated a standard camera-trap dataset of 20 species common in North America, which contains 14, 346 training images and 9, 530 testing images, and is available to public for evaluation and benchmark purpose.
C1 [Chen, Guobin; Han, Tony X.; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65203 USA.
   [Kays, Roland; Forrester, Tavis] N Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
RP Chen, GB (corresponding author), Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65203 USA.
EM gcn38@missouri.edu; hantx@missouri.edu; hezhi@missouri.edu;
   rokays@gmail.com; ForresterT@si.edu
RI Chen, Guobin/AAN-1575-2021; He, Zhihai/A-5885-2019
OI Kays, Roland/0000-0002-2947-6665
CR Akyildiz IF, 2002, COMPUT NETW, V38, P393, DOI 10.1016/S1389-1286(01)00302-4
   Bengio, 2012, AISTATS, P127
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Freeman SN, 2007, BIRD STUDY, V54, P61, DOI 10.1080/00063650709461457
   GHARAVI H, 2002, WORLD WIR C 3G WIR 2
   Hinton GE, 1989, NEURAL COMPUT, V1, P143, DOI 10.1162/neco.1989.1.1.143
   Hulbert IAR, 2001, J APPL ECOL, V38, P869, DOI 10.1046/j.1365-2664.2001.00624.x
   Juang P., 2002, 10 INT C ARCH SUPP P
   Kavukcuoglu Koray, 2010, P ADV NEUR INF PROC, V23, P1090
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Link WA, 2008, J WILDLIFE MANAGE, V72, P44, DOI 10.2193/2007-299
   Markovchick-Nicholls L, 2008, CONSERV BIOL, V22, P99, DOI 10.1111/j.1523-1739.2007.00846.x
   Mech LD, 1983, HDB ANIMAL RADIO TRA
   Moil RJ, 2007, TRENDS ECOL EVOL, V22, P660, DOI 10.1016/j.tree.2007.09.007
   Ng, 2011, ADV NEURAL INFORM PR, P801
   Pidgeon AM, 2007, ECOL APPL, V17, P1989, DOI 10.1890/06-1489.1
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Riley SJ, 2003, ECOSCIENCE, V10, P455, DOI 10.1080/11956860.2003.11682793
   Sauer JR, 2008, N AM BREEDING BIRD S
   Szewczyk R., 2004, P 2 ACM C EMB NETW S
   Veech JA, 2006, CONSERV BIOL, V20, P1422, DOI 10.1111/j.1523-1739.2006.00487.x
   Wikle CK, 2003, ECOLOGY, V84, P1382, DOI 10.1890/0012-9658(2003)084[1382:HBMFPT]2.0.CO;2
   Williams M., 1998, AGROS NEWSLETTER, V53
   Young J., 2007, ANIMAL TRACKING BASI
   Yu D, 2013, IEEE T AUDIO SPEECH, V21, P388, DOI 10.1109/TASL.2012.2227738
NR 28
TC 59
Z9 59
U1 1
U2 24
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1522-4880
BN 978-1-4799-5751-4
J9 IEEE IMAGE PROC
PY 2014
BP 858
EP 862
PG 5
WC Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA BE2TU
UT WOS:000370063601007
DA 2022-02-10
ER

PT C
AU Cunha, F
   dos Santos, EM
   Barreto, R
   Colonna, JG
AF Cunha, Fagner
   dos Santos, Eulanda M.
   Barreto, Raimundo
   Colonna, Juan G.
GP IEEE Comp Soc
TI Filtering Empty Camera Trap Images in Embedded Systems
SO 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGITION
   WORKSHOPS (CVPRW 2021)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 19-25, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, CVF
AB Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency.1
C1 [Cunha, Fagner; dos Santos, Eulanda M.; Barreto, Raimundo; Colonna, Juan G.] Univ Fed Amazonas, Manaus, Amazonas, Brazil.
RP Cunha, F (corresponding author), Univ Fed Amazonas, Manaus, Amazonas, Brazil.
EM fagner.cunha@icomp.ufam.edu.br; emsantos@icomp.ufam.edu.br;
   rbarreto@icomp.ufam.edu.br; juancolonna@icomp.ufam.edu.br
FU Samsung Electronics of Amazonia LtdaSamsung [003/2019, 6.008/2006,
   8.387/1991, 48]; ICOMP/UFAM; Foundation for Research Support of the
   State of Amazonas (FAPEAM) - POSGRAD Project; Coordination for the
   Improvement of Higher Education Personnel Brazil (CAPES)Coordenacao de
   Aperfeicoamento de Pessoal de Nivel Superior (CAPES) [001]
FX This research, according to Article 48 of Decree no 6.008/2006, was
   partially funded by Samsung Electronics of Amazonia Ltda, under the
   terms of Federal Law no 8.387/1991, through agreement no 003/2019,
   signed with ICOMP/UFAM. This study was supported by the Foundation for
   Research Support of the State of Amazonas (FAPEAM) - POSGRAD Project,
   and the Coordination for the Improvement of Higher Education Personnel
   Brazil (CAPES) - Finance Code 001. The funders had no role in study
   design, data collection and analysis, decision to publish, or
   preparation of the manuscript.
CR Beery S., 2019, BIODIVERSITY INFORM, V3, pe37222, DOI [10.3897/biss.3.37222, DOI 10.3897/BISS.3.37222]
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Cubuk Ekin D., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Kolesnikov A., 2020, COMPUTER VISION ECCV, DOI DOI 10.1007/978-3-030-58558-7_29
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Olsson Sara, 2020, THESIS
   Ren SQ, 2015, ADV NEUR IN, V28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schnaubelt S, 2020, EUR HEART J-CASE REP, V4, DOI 10.1093/ehjcr/ytaa166
   Swanson AB, 2015, DATA SNAPSHOT SERENG
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tan MX, 2019, PR MACH LEARN RES, V97
   TAN MX, 2019, P IEEE CVF C COMP VI, V97
   TEAM Network, 2011, TERR VER PROT IMPL M
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xiong Y., 2020, ARXIV PREPRINT ARXIV
   Zualkernan IA, 2020, 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT), P111, DOI 10.1109/GCAIOT51063.2020.9345858
NR 25
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2160-7508
BN 978-1-6654-4899-4
J9 IEEE COMPUT SOC CONF
PY 2021
BP 2438
EP 2446
DI 10.1109/CVPRW53098.2021.00276
PG 9
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS2PO
UT WOS:000705890202061
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Vargas-Felipe, M
   Pellegrin, L
   Guevara-Carrizales, AA
   Lopez-Monroy, AP
   Escalante, HJ
   Gonzalez-Fraga, JA
AF Vargas-Felipe, Manuel
   Pellegrin, Luis
   Guevara-Carrizales, Aldo A.
   Lopez-Monroy, A. Pastor
   Escalante, Hugo Jair
   Gonzalez-Fraga, Jose A.
TI Desert bighorn sheep (Ovis canadensis) recognition from camera traps
   based on learned features
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Desert bighorn sheep; Deep learning; Convolutional neural networks;
   Image classification; Species recognition; Monitoring; Camera traps
ID ANIMALS
AB Monitoring wildlife in geographical areas is essential for the conservation of the biological heritage. At present, the strenuous task of animal observation on-site has been mitigated with the use of camera traps. The information gathered by these devices comprises a sequence of images, which are triggered by motion or heat, and enables the monitoring of several geographical areas at the same time without perturbing the fauna. Notwithstanding the advances of these kind of monitoring approaches, the captured images still must be manually classified, becoming into an expensive process. This paper describes an automatic methodology for labeling images captured with camera traps as a support tool for animal behaviour analysis. Specifically, we focus on the analysis of the Ovis canadensis better known as desert bighorn sheep, a species that inhabits in northwestern Mexico, USA and Canada. The importance of this species lies in that it is an emblematic one, of great historical, cultural and social value. We adopted a methodology based on a residual neural network (ResNet) as feature extractor and standard models for the classification of images depicting species of interest. The method is built (trained) and evaluated on realistic images captured by camera traps in the field. We achieve classification performances ranging from 89% for a multiclass classification setting (7 classes associated to the animal of interest) to 99% in a binary classification scenario (presence vs. absence of the species). The collected data set, model and extracted features are publicly available under request. We foresee the released data set and the proposed solution will boost research on the analysis of this species.
C1 [Vargas-Felipe, Manuel; Pellegrin, Luis; Guevara-Carrizales, Aldo A.; Gonzalez-Fraga, Jose A.] Univ Autonoma Baja California UABC, Fac Sci, Ensenada, Baja California, Mexico.
   [Lopez-Monroy, A. Pastor] Ctr Invest Matemat CIMAT AC, Comp Sci Dept, Guanajuato, Mexico.
   [Escalante, Hugo Jair] Inst Nacl Astrofis Opt & Elect INAOE, Comp Sci Dept, Cholula, Mexico.
RP Pellegrin, L (corresponding author), Univ Autonoma Baja California UABC, Fac Sci, Ensenada, Baja California, Mexico.
EM luis.pellegrin@uabc.edu.mx
RI GONZALEZ-FRAGA, JOSE ANGEL/B-3487-2008
OI GONZALEZ-FRAGA, JOSE ANGEL/0000-0003-2144-8835; Luis Pellegrin,
   Luis/0000-0002-4898-1632
FU PRODEP [UABC-PTC-792]
FX The authors thank to Director of APFF'Valle de los Cirios'Biol. Victor
   Gelasio Sanchez Sotomayor for his kind attention and offers facilities
   to this project. Also, we thank to Dr. Guillermo Romero for his help
   during the data set labeling. This work was supported by PRODEP, grant
   UABC-PTC-792.
CR AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1007/BF00153759
   Beery S., 2019, ABS190707617 CORR
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Buehler P, 2019, ECOL INFORM, V50, P191, DOI 10.1016/j.ecoinf.2019.02.003
   Carl C, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-01404-y
   Cheema GS, 2017, LECT NOTES ARTIF INT, V10536, P27, DOI 10.1007/978-3-319-71273-4_3
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Escobar-Flores Jonathan G., 2015, Therya, V6, P519, DOI 10.12933/therya-15-284
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gobierno del Estado de Baja California, 2012, ESTRATEGIA ESTATAL C
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Guevara-Carrizales A.A., 2016, MAMIFEROS BAJA CALIF, P63
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   John G. H., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P338
   Korschens M., 2018, ICEI 2018 10 INT C E, DOI [10.22032/dbt.37903, DOI 10.22032/DBT.37903]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee R., 2013, DESERT BIGHORN COUNC, V52, P40
   Lee R., 2011, DESERT BIGHORN COUNC, V51, P46
   Long J.L., 2014, ADV NEURAL INFORM PR, P1601
   Manterola-y-Pina C., 2000, TECHNICAL REPORT UNI
   Martinez-Gallardo R., 2017, ESTUDIOS BORREGO CIM
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Mitchell T.M., 1997, MACH LEARN
   Monson G., 1980, DESERT BIGHORN ITS L
   Montoya M., 2017, ESTUDIOS BORREGO CIM
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Ross Quinlan J., 1993, C4 5 PROGRAMS MACHIN
   Ruiz E., 2014, THESIS U NACL AUTONO
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Timm M, 2018, IEEE COMPUT SOC CONF, P1977, DOI 10.1109/CVPRW.2018.00252
   Valdez R., 2018, THERYA, V9, P219
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Witten IH., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI 10.1145/1656274.1656278
   Yosinski J, 2014, ADV NEUR IN, V27
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 46
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD SEP
PY 2021
VL 64
AR 101328
DI 10.1016/j.ecoinf.2021.101328
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA UK1VH
UT WOS:000691763100004
DA 2022-02-10
ER

PT J
AU Bjerge, K
   Mann, HMR
   Hoye, TT
AF Bjerge, Kim
   Mann, Hjalte M. R.
   Hoye, Toke Thomas
TI Real-time insect tracking and monitoring with computer vision and deep
   learning
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article; Early Access
DE Computer vision; deep learning; insects; pollinators; real-time;
   tracking
ID POLLINATORS
AB Insects are declining in abundance and diversity, but their population trends remain uncertain as insects are difficult to monitor. Manual methods require substantial time investment in trapping and subsequent species identification. Camera trapping can alleviate some of the manual fieldwork, but the large quantities of image data are challenging to analyse. By embedding the image analyses into the recording process using computer vision techniques, it is possible to focus efforts on the most ecologically relevant image data. Here, we present an intelligent camera system, capable of detecting, tracking, and identifying individual insects in situ. We constructed the system from commercial off-the-shelf components and used deep learning open source software to perform species detection and classification. We present the Insect Classification and Tracking algorithm (ICT) that performs real-time classification and tracking at 0.33 frames per second. The system can upload summary data on the identity and movement track of insects to a server via the internet on a daily basis. We tested our system during the summer 2020 and detected 2994 insect tracks across 98 days. We achieved an average precision of 89% for correctly classified insect tracks of eight different species. This result was based on 504 manually verified tracks observed in videos during 10 days with varying insect activities. Using the track data, we could estimate the mean residence time for individual flower visiting insects within the field of view of the camera, and we were able to show a substantial variation in residence time among insect taxa. For honeybees, which were most abundant, residence time also varied through the season in relation to the plant species in bloom. Our proposed automated system showed promising results in non-destructive and real-time monitoring of insects and provides novel information about phenology, abundance, foraging behaviour, and movement ecology of flower visiting insects.
C1 [Bjerge, Kim] Aarhus Univ, Dept Elect & Comp Engn, Finlandsgade 22, DK-8200 Aarhus N, Denmark.
   [Mann, Hjalte M. R.; Hoye, Toke Thomas] Aarhus Univ, Dept Ecosci, Grenavej 14, DK-8410 Ronde, Denmark.
   [Mann, Hjalte M. R.; Hoye, Toke Thomas] Aarhus Univ, Arctic Res Ctr, Grenavej 14, DK-8410 Ronde, Denmark.
RP Bjerge, K (corresponding author), Aarhus Univ, Dept Elect & Comp Engn, Finlandsgade 22, DK-8200 Aarhus N, Denmark.
EM kbe@ece.au.dk
OI Bjerge, Kim/0000-0001-6742-9504
FU European Union's Horizon 2020 Research and Innovation programme [773554]
FX The work was supported by the European Union's Horizon 2020 Research and
   Innovation programme, under Grant Agreement no. 773554 (EcoStack).
CR Arje J, 2020, METHODS ECOL EVOL, V11, P922, DOI 10.1111/2041-210X.13428
   Barlow SE, 2017, CURR BIOL, V27, P2552, DOI 10.1016/j.cub.2017.07.012
   Bjerge K, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020343
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Collett RA, 2017, ECOL EVOL, V7, P7527, DOI 10.1002/ece3.3275
   Didham RK, 2020, INSECT CONSERV DIVER, V13, P103, DOI 10.1111/icad.12408
   Eliopoulos P, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7090161
   Epsky ND., 2008, ENCY ENTOMOLOGY, P3887
   Gilpin AM, 2017, ECOL ENTOMOL, V42, P383, DOI 10.1111/een.12394
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hansen OLP, 2020, ECOL EVOL, V10, P737, DOI 10.1002/ece3.5921
   Hoye T.T., 2020, DCE NATL CTR MILJO O, V371, P18
   Hoye TT, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2002545117
   Huang Y, 2017, P 2017 C EMP METH NA, P1803, DOI [10.18653/v1/d17-1191, DOI 10.18653/V1/D17-1191]
   Ju MR, 2019, IEEE ACCESS, V7, P85771, DOI 10.1109/ACCESS.2019.2924960
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Logitech, 2020, C920 HD PRO WEBC
   Logitech, 2021, BRIO ULTR HD PRO WEB
   Lortie Christopher J., 2011, Journal of Pollination Ecology, V6, P125
   MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a
   Montgomery GA, 2021, FRONT ECOL EVOL, V8, DOI 10.3389/fevo.2020.579193
   Motion, 2021, MOT OP SOURC PROGR M
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Naz H, 2014, J ENV AGR SCI, V1, P5
   NVIDIA, 2021, JETS NAN DEV KIT US
   Pegoraro L, 2020, EMERG TOP LIFE SCI, V4, P87, DOI 10.1042/ETLS20190074
   Redmon J., 2018, ARXIV PREPRINT ARXIV
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Steen R, 2017, METHODS ECOL EVOL, V8, P203, DOI 10.1111/2041-210X.12654
   Wagner DL, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2023989118
   Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011
   Wignall VR, 2020, OECOLOGIA, V192, P351, DOI 10.1007/s00442-019-04576-w
   Wojcik VA, 2018, ENVIRON ENTOMOL, V47, P822, DOI [10.1093/ee/nvy077, 10.1093/e]
   Xia D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124169
   Zoller L, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78165-w
NR 36
TC 0
Z9 0
U1 9
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
DI 10.1002/rse2.245
EA NOV 2021
PG 14
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA XF2QC
UT WOS:000723919300001
OA gold
DA 2022-02-10
ER

PT J
AU Hansen, OLP
   Svenning, JC
   Olsen, K
   Dupont, S
   Garner, BH
   Iosifidis, A
   Price, BW
   Hoye, TT
AF Hansen, Oskar L. P.
   Svenning, Jens-Christian
   Olsen, Kent
   Dupont, Steen
   Garner, Beulah H.
   Iosifidis, Alexandros
   Price, Benjamin W.
   Hoye, Toke T.
TI Species-level image classification with convolutional neural network
   enables insect identification from habitus images
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE arthropod sampling; automatic species identification; camera trap;
   entomological collection; image classification; image database
ID PITFALL TRAPS; GRASSLANDS; EFFICIENCY
AB Changes in insect biomass, abundance, and diversity are challenging to track at sufficient spatial, temporal, and taxonomic resolution. Camera traps can capture habitus images of ground-dwelling insects. However, currently sampling involves manually detecting and identifying specimens. Here, we test whether a convolutional neural network (CNN) can classify habitus images of ground beetles to species level, and estimate how correct classification relates to body size, number of species inside genera, and species identity. We created an image database of 65,841 museum specimens comprising 361 carabid beetle species from the British Isles and fine-tuned the parameters of a pretrained CNN from a training dataset. By summing up class confidence values within genus, tribe, and subfamily and setting a confidence threshold, we trade-off between classification accuracy, precision, and recall and taxonomic resolution. The CNN classified 51.9% of 19,164 test images correctly to species level and 74.9% to genus level. Average classification recall on species level was 50.7%. Applying a threshold of 0.5 increased the average classification recall to 74.6% at the expense of taxonomic resolution. Higher top value from the output layer and larger sized species were more often classified correctly, as were images of species in genera with few species. Fine-tuning enabled us to classify images with a high mean recall for the whole test dataset to species or higher taxonomic levels, however, with high variability. This indicates that some species are more difficult to identify because of properties such as their body size or the number of related species. Together, species-level image classification of arthropods from museum collections and ecological monitoring can substantially increase the amount of occurrence data that can feasibly be collected. These tools thus provide new opportunities in understanding and predicting ecological responses to environmental change.
C1 [Hansen, Oskar L. P.; Svenning, Jens-Christian] Aarhus Univ, Dept Biosci, Ctr Biodivers Dynam Changing World BIOCHANGE, Ny Munkegade 114, DK-8000 Aarhus C, Denmark.
   [Hansen, Oskar L. P.; Svenning, Jens-Christian] Aarhus Univ, Sect Ecoinformat & Biodivers, Dept Biosci, Aarhus C, Denmark.
   [Hansen, Oskar L. P.; Olsen, Kent] Nat Hist Museum Aarhus, Aarhus C, Denmark.
   [Dupont, Steen; Garner, Beulah H.; Price, Benjamin W.] Nat Hist Museum, Life Sci, London, England.
   [Iosifidis, Alexandros] Aarhus Univ, Dept Engn Signal Proc, Aarhus N, Denmark.
   [Hoye, Toke T.] Aarhus Univ, Dept Biosci, Ronde, Denmark.
   [Hoye, Toke T.] Aarhus Univ, Arctic Res Ctr, Ronde, Denmark.
RP Hansen, OLP (corresponding author), Aarhus Univ, Dept Biosci, Ctr Biodivers Dynam Changing World BIOCHANGE, Ny Munkegade 114, DK-8000 Aarhus C, Denmark.
EM oli@bios.au.dk
RI Svenning, Jens-Christian/C-8977-2012; Hoye, Toke T./ABF-7808-2020;
   Olsen, Kent/AAI-1228-2019; Høye, Toke Thomas/A-7701-2008; Iosifidis,
   Alexandros/G-2433-2013
OI Svenning, Jens-Christian/0000-0002-3415-0862; Hoye, Toke
   T./0000-0001-5387-3284; Olsen, Kent/0000-0002-5624-128X; Høye, Toke
   Thomas/0000-0001-5387-3284; Price, Benjamin/0000-0001-5497-4087; garner,
   beulah/0000-0002-5229-2450; Hansen, Oskar/0000-0002-1598-5733;
   Iosifidis, Alexandros/0000-0003-4807-1345; Dupont,
   Steen/0000-0001-6766-2840
FU 15. Juni Fonden [2017-N-10]; Danish Agency for Culture and Palaces under
   the Danish Ministry of Culture [FORM. 2016-0025]; Villum FondenVillum
   Foundation [16549, 17523]; Innovation Fund Denmark [6171-00034B];
   Carlsberg Foundation Semper Ardens project MegaPast2Future [CF16-0005]
FX 15. Juni Fonden, Grant/Award Number: 2017-N-10; Danish Agency for
   Culture and Palaces under the Danish Ministry of Culture, Grant/Award
   Number: FORM. 2016-0025; Villum Fonden, Grant/Award Number: 16549 and
   17523; Innovation Fund Denmark, Grant/Award Number: 6171-00034B;
   Carlsberg Foundation Semper Ardens project MegaPast2Future, Grant/Award
   Number: CF16-0005
CR Abadi, 2015, TENSORFLOW LARGE SCA
   Agrawal AA, 2018, SCIENCE, V360, P1294, DOI 10.1126/science.aat5066
   Arje J., 2019, HUMAN EXPERTS VS MAC
   Asmus AL, 2018, ECOL ENTOMOL, V43, P647, DOI 10.1111/een.12644
   Blagoderov V, 2012, ZOOKEYS, P133, DOI 10.3897/zookeys.209.3178
   Brown GR, 2016, ECOL EVOL, V6, P3953, DOI 10.1002/ece3.2176
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Chamberlain Scott A, 2013, F1000Res, V2, P191, DOI 10.12688/f1000research.2-191.v1
   Collett RA, 2017, ECOL EVOL, V7, P7527, DOI 10.1002/ece3.3275
   Deng J, 2009, 2009 IEEE C COMP VIS
   Digweed SC, 1995, PEDOBIOLOGIA, V39, P561
   Dolek M, 2017, J INSECT CONSERV, V21, P573, DOI 10.1007/s10841-017-9996-9
   Duff A., 2012, CHECKLIST BEETLES BR
   Engel J, 2017, ECOSPHERE, V8, DOI 10.1002/ecs2.1790
   Hallmann CA, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185809
   HALSALL NB, 1988, ECOL ENTOMOL, V13, P293, DOI 10.1111/j.1365-2311.1988.tb00359.x
   Hoye Toke T., 2008, BMC Ecology, V8, P8, DOI 10.1186/1472-6785-8-8
   Hudson LN, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0143402
   Joutsijoki H, 2014, ECOL INFORM, V20, P1, DOI 10.1016/j.ecoinf.2014.01.004
   Lister BC, 2018, P NATL ACAD SCI USA, V115, pE10397, DOI 10.1073/pnas.1722477115
   Loboda S, 2018, ECOGRAPHY, V41, P265, DOI 10.1111/ecog.02747
   Mantle BL, 2012, ZOOKEYS, P147, DOI 10.3897/zookeys.209.3169
   Marques ACR, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0192011
   Martineau M, 2017, PATTERN RECOGN, V65, P273, DOI 10.1016/j.patcog.2016.12.020
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Schirmel J, 2010, ENTOMOL EXP APPL, V136, P206, DOI 10.1111/j.1570-7458.2010.01020.x
   Schneider S., 2018, DEEP LEARNING OBJECT
   Seibold S, 2019, NATURE, V574, P671, DOI 10.1038/s41586-019-1684-3
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Skvarla M. J., 2014, Journal of the Entomological Society of Ontario, V145, P15
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tensorflow, 2019, RETRAIN IMAGE CLASSI
   van Horn G, 2017, INATURALIST CHALLENG
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wagner D. L., 2019, ANNU REV ENTOMOL, V7, P24, DOI [10.1146/annurev-ento-011019, DOI 10.1146/ANNUREV-ENTO-011019]
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zaller JG, 2015, WEB ECOL, V15, P15, DOI 10.5194/we-15-15-2015
   Zhang ZQ, 2011, ZOOTAXA, P7, DOI 10.11646/zootaxa.3148.1.3
NR 39
TC 17
Z9 17
U1 5
U2 17
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD JAN
PY 2020
VL 10
IS 2
BP 737
EP 747
DI 10.1002/ece3.5921
EA DEC 2019
PG 11
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA KG3BK
UT WOS:000504112000001
PM 32015839
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Sundaram, DM
   Loganathan, A
AF Sundaram, Divya Meena
   Loganathan, Agilandeeswari
TI A New Supervised Clustering Framework Using Multi Discriminative Parts
   and Expectation-Maximization Approach for a Fine-Grained Animal Breed
   Classification (SC-MPEM)
SO NEURAL PROCESSING LETTERS
LA English
DT Article
DE Expectation-maximization; Fine-grained classification; Multi-part CNN;
   Snapshot serengeti; Supervised clustering
AB Fine-grained image classification is active research in the field of computer vision. Specifically, animal breed classification is an arduous task due to the challenges in camera traps images like occlusion, camouflage, poor illumination, pose variation, etc. In this paper, we propose a fine-grained animal breed classification model using supervised clustering based on Multi Part-Convolutional Neural Network (MP-CNN) and Expectation-Maximization (EM) clustering. The proposed model follows a straightforward pipeline that combines the deep feature extraction using the CNN pre-trained on ImageNet and classifies unsupervised data using EM clustering. Further, we also propose a multi discriminative part selection and detection for the precise classification of animal breeds without using bounding box and annotations on both training and testing phases. The model is tested on several benchmark datasets for animals, including the largest camera trap Snapshot Serengeti dataset and has achieved a cumulative accuracy of 98.4%. The results from the proposed model strengthen the belief that supervised training of deep CNN on a large and versatile dataset, extracts better features than most of the traditional approaches, even for the unsupervised tasks.
C1 [Sundaram, Divya Meena; Loganathan, Agilandeeswari] Vellore Inst Technol, Sch Informat Technol & Engn, Vellore, Tamil Nadu, India.
RP Loganathan, A (corresponding author), Vellore Inst Technol, Sch Informat Technol & Engn, Vellore, Tamil Nadu, India.
EM agila.l@vit.ac.in
CR Antonio WHS, 2019, APPL ARTIF INTELL, V33, P1093, DOI 10.1080/08839514.2019.1673993
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Boots B., 2018, ARXIV180707760
   Branson S., 2014, ARXIV14062952
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divya Meena S., 2019, INT J ENG ADV TECHNO, V9, P495, DOI [10.35940/ijeat.A1089.1291S319, DOI 10.35940/IJEAT.A1089.1291S319, 10.35940/ijeat.a1089.1291s319]
   Dubey A., 2018, P EUR C COMP VIS ECC, P70
   Feng H, 2018, ARXIVABS180803935
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Guerin J, 2017, ARXIVABS170701700
   Guo, 2019, ARXIV190406252
   Gupta P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P104, DOI 10.1109/CCAA.2017.8229781
   Hu T., 2019, ABS190109891 CORR
   Hu T, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P8441
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jasko G, 2017, INT C INTELL COMP CO, P363, DOI 10.1109/ICCP.2017.8117031
   Khosla A., 2011, P CVPR WORKSH FIN GR, V2, P1
   Kolesnikov Alexander, 2019, ARXIV191211370
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Lee J., 2020, ARXIV200106268
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu DYH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4146
   Liu JE, 2020, SCI PROGRAMMING-NETH, V2020, DOI 10.1155/2020/7607612
   Liu X., 2016, ARXIV160306765
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Meena SD, 2020, ADV INTELL SYST, V1057, P513, DOI 10.1007/978-981-15-0184-5_44
   Meena SD, 2019, IEEE ACCESS, V7, P151783, DOI 10.1109/ACCESS.2019.2947717
   Mulligan K, 2019, 21 INT C ART INT ICA
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Qiu WT, 2012, 2012 INTERNATIONAL CONFERENCE ON CONTROL ENGINEERING AND COMMUNICATION TECHNOLOGY (ICCECT 2012), P177, DOI 10.1109/ICCECT.2012.78
   Sharma SU, 2017, IEEE ACCESS, V5, P347, DOI 10.1109/ACCESS.2016.2642981
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Sun G, 2019, ARXIV191206842
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Touvron H., 2019, ADV NEURAL INFORM PR, P8250
   Xie LX, 2013, IEEE I CONF COMP VIS, P1641, DOI 10.1109/ICCV.2013.206
   Xie LX, 2014, IEEE T IMAGE PROCESS, V23, P1994, DOI 10.1109/TIP.2014.2310117
   Xu Z., 2016, IEEE T PATTERN ANAL
   Xu Z, 2017, IEEE T IMAGE PROCESS, V26, P135, DOI 10.1109/TIP.2016.2621661
   Yan XF, 2019, ISPRS J PHOTOGRAMM, V150, P259, DOI 10.1016/j.isprsjprs.2019.02.010
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yue YL, 2018, INT WIREL COMMUN, P805, DOI 10.1109/IWCMC.2018.8450320
   Zhang L, 2019, ARXIV191106866
   Zhang LM, 2016, IEEE T IMAGE PROCESS, V25, P553, DOI 10.1109/TIP.2015.2502147
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, IEEE T IMAGE PROCESS, V25, P878, DOI 10.1109/TIP.2015.2509425
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhuang P., 2020, ARXIV200210191
NR 53
TC 5
Z9 5
U1 1
U2 10
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1370-4621
EI 1573-773X
J9 NEURAL PROCESS LETT
JI Neural Process. Lett.
PD AUG
PY 2020
VL 52
IS 1
SI SI
BP 727
EP 766
DI 10.1007/s11063-020-10246-3
EA JUN 2020
PG 40
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ8GU
UT WOS:000541206000001
DA 2022-02-10
ER

PT J
AU Howe, EJ
   Buckland, ST
   Despres-Einspenner, ML
   Kuhl, HS
AF Howe, Eric J.
   Buckland, Stephen T.
   Despres-Einspenner, Marie-Lyne
   Kuehl, Hjalmar S.
TI Distance sampling with camera traps
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal abundance; camera trapping; density; distance sampling; Maxwell's
   duiker
ID LINE TRANSECT TECHNIQUES; UDZUNGWA MOUNTAINS; ACTIVITY PATTERNS;
   DENSITY; DUNG; POPULATIONS; ABUNDANCE; UNGULATE; RANGE; NEED
AB 1. Reliable estimates of animal density and abundance are essential for effective wildlife conservation and management. Camera trapping has proven efficient for sampling multiple species, but statistical estimators of density from camera trapping data for species that cannot be individually identified are still in development.
   2. We extend point-transect methods for estimating animal density to accommodate data from camera traps, allowing researchers to exploit existing distance sampling theory and software for designing studies and analysing data. We tested it by simulation, and used it to estimate densities of Maxwell's duikers (Philantomba maxwellii) in Tai National Park, Cote d'Ivoire.
   3. Densities estimated from simulated data were unbiased when we assumed animals were not available for detection during long periods of rest. Estimated duiker densities were higher than recent estimates from line transect surveys, which are believed to underestimate densities of forest ungulates.
   4. We expect these methods to provide an effective means to estimate animal density from camera trapping data and to be applicable in a variety of settings.
C1 [Howe, Eric J.; Buckland, Stephen T.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews KY16 9LZ, Fife, Scotland.
   [Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Deutsch Pl 5e, D-04103 Leipzig, Germany.
RP Howe, EJ (corresponding author), Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews KY16 9LZ, Fife, Scotland.
EM ejh20@st-andrews.ac.uk
RI Buckland, Stephen T/A-1998-2012
OI Buckland, Stephen/0000-0002-9939-709X
FU Robert Bosch Foundation; Max Planck SocietyMax Planck SocietyFoundation
   CELLEX; University of St Andrews
FX We thank the Robert Bosch Foundation, the Max Planck Society and the
   University of St Andrews for funding, the Ministere de l'Enseignement
   Superieur et de la Recherche Scientifique and the Ministere de
   l'Environnement et des Eaux et Forets in Cote d'Ivoire for permission to
   conduct field research in Taii National Park, and Dr. Roman Wittig for
   permitting data collection in the area of the Taii Chimpanzee Project.
CR Balestrieri A, 2016, MAMM BIOL, V81, P439, DOI 10.1016/j.mambio.2016.05.005
   Bowkett AE, 2009, CONSERV GENET, V10, P251, DOI 10.1007/s10592-008-9564-7
   Buckland S.T., 2001, pi
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   BUCKLAND ST, 1984, BIOMETRICS, V40, P811, DOI 10.2307/2530926
   Buckland ST., 2004, ADV DISTANCE SAMPLIN
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Cruz P, 2014, MAMM BIOL, V79, P376, DOI 10.1016/j.mambio.2014.06.003
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   Denes FV, 2015, METHODS ECOL EVOL, V6, P543, DOI 10.1111/2041-210X.12333
   Despres-Einspenner M. -L., 2017, AM J PRIMATOLOGY
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Howe E. J., 2017, DRYAD DIGITAL REPOSI
   Jathanna D, 2003, J ZOOL, V261, P285, DOI 10.1017/S0952836903004278
   KOSTER SH, 1988, AFR J ECOL, V26, P117, DOI 10.1111/j.1365-2028.1988.tb00962.x
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Laake JL, 2011, J AGR BIOL ENVIR ST, V16, P389, DOI 10.1007/s13253-011-0059-5
   Le Saout S, 2014, WILDLIFE BIOL, V20, P122, DOI 10.2981/wlb.13048
   Lynam AJ, 2013, RAFFLES B ZOOL, V61, P407
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   Marini F, 2009, EUR J WILDLIFE RES, V55, P107, DOI 10.1007/s10344-008-0222-7
   Marques TA, 2010, BIOMETRICS, V66, P1247, DOI 10.1111/j.1541-0420.2009.01381.x
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Marshall AR, 2008, AM J PRIMATOL, V70, P452, DOI 10.1002/ajp.20516
   N'Goran P. K., 2006, QUELQUES RESULTATS P
   Newing H, 2001, BIODIVERS CONSERV, V10, P99, DOI 10.1023/A:1016671524034
   Newing HS, 1994, THESIS
   Plumptre AJ, 2000, J APPL ECOL, V37, P356, DOI 10.1046/j.1365-2664.2000.00499.x
   Rovero F, 2004, TROP ZOOL, V17, P267, DOI 10.1080/03946975.2004.10531208
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   Sollmann R, 2013, BIOL CONSERV, V159, P405, DOI 10.1016/j.biocon.2012.12.025
   Sunarto, 2013, RAFFLES B ZOOL, P21
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Todd AF, 2008, INT J PRIMATOL, V29, P549, DOI 10.1007/s10764-008-9247-8
   Wearn OR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077598
   Zero VH, 2013, ORYX, V47, P410, DOI 10.1017/S0030605312000324
NR 47
TC 72
Z9 73
U1 12
U2 82
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD NOV
PY 2017
VL 8
IS 11
BP 1558
EP 1565
DI 10.1111/2041-210X.12790
PG 8
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA FM1AK
UT WOS:000414701900019
OA Green Accepted, Bronze
DA 2022-02-10
ER

PT C
AU Castelblanco, LP
   Narvaez, CI
   Pulido, AD
AF Pulido Castelblanco, Luis
   Isaza Narvaez, Claudia
   Diaz Pulido, Angelica
BE Verikas, A
   Radeva, P
   Nikolaev, DP
   Zhang, W
   Zhou, J
TI Methodology for Mammal Classification in Camera Trap Images
SO NINTH INTERNATIONAL CONFERENCE ON MACHINE VISION (ICMV 2016)
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT 9th International Conference on Machine Vision (ICMV)
CY NOV 18-20, 2016
CL Nice, FRANCE
SP SPIE
DE Artificial neural network; camera trap; fuzzy classifiers; image
   processing; matched filter
ID MANAGEMENT; DENSITIES; SOFTWARE; DESIGN
AB Using camera traps in animal ecology studies has increased because it facilitates the work of biologists and allows them to obtain information that otherwise would be impossible. A large number of photographs are capturing with this wildlife photography technique making difficult their posterior analysis. This paper presents a method to automatically identify the images with at least one animal and to classify them between birds and mammals. In this work a fuzzy classifier and a matched filter were used to identify the image with animals and to segment the images. An artificial neural network was employed to classify the segments between birds and mammals. We obtained a classification accuracy of 73.1% validating the model over real camera trap sessions. The database includes several difficulties, as the constant changes in the scene by climatic factors or animals partially occluded by the environment. This method was implemented in a software that is currently using in the Alexander von Humboldt Biological Resources Research Institute for studies of biodiversity in Colombia.
C1 [Pulido Castelblanco, Luis; Isaza Narvaez, Claudia] Univ Antioquia UDEA, Engn Fac, SISTEMIC, Cl 70 52-21, Medellin, Colombia.
   [Diaz Pulido, Angelica] Alexander von Humboldt Biol Resources Res Inst, Bogota, DC, Colombia.
RP Castelblanco, LP (corresponding author), Univ Antioquia UDEA, Engn Fac, SISTEMIC, Cl 70 52-21, Medellin, Colombia.
OI Isaza, Claudia/0000-0003-1044-9429
FU Universidad de Antioquia; Alexander von Humboldt Institute for Research
   on Biological Resources; Colombian National Fund for Science, Technology
   and Innovation, Francisco Jose de Caldas - COLCIENCIAS (Colombia)
   [111571451061]
FX This work was supported by the Universidad de Antioquia and the
   Alexander von Humboldt Institute for Research on Biological Resources.
   Authors also thank the Colombian National Fund for Science, Technology
   and Innovation, Francisco Jose de Caldas - COLCIENCIAS (Colombia).
   Project No. 111571451061.
CR Basheer IA, 2000, J MICROBIOL METH, V43, P3, DOI 10.1016/S0167-7012(00)00201-3
   Brunet D, 2011, LECT NOTES COMPUT SC, V6753, P100, DOI 10.1007/978-3-642-21593-3_11
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   CHEN PC, 1979, COMPUT VISION GRAPH, V10, P172, DOI 10.1016/0146-664X(79)90049-2
   Cozzuol MA, 2013, J MAMMAL, V94, P1331, DOI 10.1644/12-MAMM-A-169.1
   Davies S. J. J. F., 2002, RATITES TINAMOUS TIN
   DIAZ PULIDO Angelica, 2012, MANUAL FOTOTRAMPEO H
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Helgen KM, 2013, ZOOKEYS, P1, DOI 10.3897/zookeys.324.5827
   Jang J.-S.R., 1997, NEURO FUZZY SOFT COM
   Karanth KU, 2004, P NATL ACAD SCI USA, V101, P4854, DOI 10.1073/pnas.0306210101
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Mosquera-Muñoz Diana Marcela, 2014, Bol. Cient. Mus. Hist. Nat. Univ. Caldas, V18, P144
   Medellin R. A., 2002, JAGUAR NUEVO MILENIO
   PULIDO L, 2012, THESIS
   Rojas R., 1996, NEURAL NETWORKS, P149
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 21
TC 1
Z9 1
U1 0
U2 10
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-5106-1132-0
J9 PROC SPIE
PY 2017
VL 10341
AR UNSP 103410I
DI 10.1117/12.2268732
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Optics
GA BI3AY
UT WOS:000410664800017
DA 2022-02-10
ER

PT J
AU Szenicer, A
   Reinwald, M
   Moseley, B
   Nissen-Meyer, T
   Muteti, ZM
   Oduor, S
   McDermott-Roberts, A
   Baydin, AG
   Mortimer, B
AF Szenicer, Alexandre
   Reinwald, Michael
   Moseley, Ben
   Nissen-Meyer, Tarje
   Muteti, Zachary Mutinda
   Oduor, Sandy
   McDermott-Roberts, Alex
   Baydin, Atilim G.
   Mortimer, Beth
TI Seismic savanna: machine learning for classifying wildlife and
   behaviours using ground-based vibration field recordings
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article; Early Access
DE African elephant; machine learning; seismic waves; wildlife monitoring
ID ELEPHANTS; AUGMENTATION; DISCRIMINATION
AB We develop a machine learning approach to detect and discriminate elephants from other species, and to recognise important behaviours such as running and rumbling, based only on seismic data generated by the animals. We demonstrate our approach using data acquired in the Kenyan savanna, consisting of 8000 h seismic recordings and 250 k camera trap pictures. Our classifiers, different convolutional neural networks trained on seismograms and spectrograms, achieved 80%-90% balanced accuracy in detecting elephants up to 100 m away, and over 90% balanced accuracy in recognising running and rumbling behaviours from the seismic data. We release the dataset used in this study: SeisSavanna represents a unique collection of seismic signals with the associated wildlife species and behaviour. Our results suggest that seismic data offer substantial benefits for monitoring wildlife, and we propose to further develop our methods using dense arrays that could result in a seismic shift for wildlife monitoring.
C1 [Szenicer, Alexandre; Nissen-Meyer, Tarje] Univ Oxford, Dept Earth Sci, South Parks Rd, Oxford OX1 3AN, England.
   [Reinwald, Michael; McDermott-Roberts, Alex; Mortimer, Beth] Univ Oxford, Dept Zool, Oxford, England.
   [Moseley, Ben; Baydin, Atilim G.] Univ Oxford, Dept Comp Sci, Oxford, England.
   [Muteti, Zachary Mutinda; Oduor, Sandy] Mpala Res Ctr, Nanyuki, Kenya.
   [Baydin, Atilim G.] Univ Oxford, Dept Engn Sci, Oxford, England.
RP Nissen-Meyer, T (corresponding author), Univ Oxford, Dept Earth Sci, South Parks Rd, Oxford OX1 3AN, England.
EM meyer@earth.ox.ac.uk
RI Szenicer, Alexandre/N-9921-2016
OI Szenicer, Alexandre/0000-0002-4829-5739
FU National GeographicNational Geographic Society [NGS50019R-18]; Royal
   SocietyRoyal Society of LondonEuropean Commission [URF R1 191033]; John
   Fell Oxford University Press Research Fund; Royal Commission for the
   Exhibition of 1851; Centre for Doctoral Training in Autonomous
   Intelligent Machines and Systems at the University of Oxford, Oxford,
   UK; UK Engineering and Physical Sciences Research CouncilUK Research &
   Innovation (UKRI)Engineering & Physical Sciences Research Council
   (EPSRC); EPSRC/MURI grantUK Research & Innovation (UKRI)Engineering &
   Physical Sciences Research Council (EPSRC) [EP/N019474/1]; Lawrence
   Berkeley National Lab
FX We would like to express our gratitude to all the staff at the Mpala
   Research Centre for assisting with fieldwork operations and for creating
   a warm and welcoming environment, in particular Dino Martins and Cosmas
   Nzomo. We thank Frank Pope and staff at Save the Elephants for help
   getting research permits and Kenyan Wildlife Service affiliations, and
   Paula Koelemeijer for help with sensor deployment. A. Szenicer thanks
   the anonymous donor of his PhD grant. We thank National Geographic
   (NGS50019R-18), Royal Society (URF R1 191033), John Fell Oxford
   University Press Research Fund, Royal Commission for the Exhibition of
   1851 for funding. This research has been supported by the Centre for
   Doctoral Training in Autonomous Intelligent Machines and Systems at the
   University of Oxford, Oxford, UK, and the UK Engineering and Physical
   Sciences Research Council. A. G. Baydin is supported by EPSRC/MURI grant
   EP/N019474/1 and by Lawrence Berkeley National Lab.
CR Barnosky AD, 2004, SCIENCE, V306, P70, DOI 10.1126/science.1101476
   Bellwood DR, 2004, NATURE, V429, P827, DOI 10.1038/nature02691
   Bianco MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50381-z
   Blake S, 2001, AFR J ECOL, V39, P178, DOI 10.1046/j.1365-2028.2001.00296.x
   Brown T., 2020, ARXIV 200514165, V33, P1877
   Ceballos G, 2020, P NATL ACAD SCI USA, V117, P13596, DOI 10.1073/pnas.1922686117
   Chase MJ, 2016, PEERJ, V4, DOI 10.7717/peerj.2354
   Clemente J, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP 2019), P417, DOI 10.1109/SMARTCOMP.2019.00081
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Douglas-Hamilton I., 1987, Oryx, V6, P11
   Duporge I, 2021, REMOTE SENS ECOL CON, V7, P369, DOI 10.1002/rse2.195
   DZIEWONSKI AM, 1977, J GEOPHYS RES, V82, P239, DOI 10.1029/JB082i002p00239
   Falcin A, 2021, J VOLCANOL GEOTH RES, V411, DOI 10.1016/j.jvolgeores.2020.107151
   Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013
   Galanti Valeria, 2000, Hystrix, V11, P27
   Gobush K., 2021, LOXODONTA AFRICANA I
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hosseini K, 2020, GEOPHYS J INT, V220, P96, DOI 10.1093/gji/ggz394
   Iandola F. N., 2017, ARXIV PREPRINT ARXIV
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Lamb O.D., 2021, FRONTIERS CONSERVATI, V1, DOI [10.3389/fcosc.2020.630967, DOI 10.3389/FCOSC.2020.630967]
   Leonid TT, 2021, J AMB INTEL HUM COMP, V12, P5269, DOI 10.1007/s12652-020-02005-y
   Li ZF, 2018, GEOPHYS RES LETT, V45, P4773, DOI 10.1029/2018GL077870
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mangewa LJ, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11216116
   Meier MA, 2019, J GEOPHYS RES-SOL EA, V124, P788, DOI 10.1029/2018JB016661
   Mortimer B, 2018, CURR BIOL, V28, pR547, DOI 10.1016/j.cub.2018.03.062
   Moseley B, 2020, SOLID EARTH, V11, P1527, DOI 10.5194/se-11-1527-2020
   Mousavi SM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17591-w
   Mousavi SM, 2020, GEOPHYS RES LETT, V47, DOI 10.1029/2019GL085976
   Ngene SM, 2010, AFR J ECOL, V48, P386, DOI 10.1111/j.1365-2028.2009.01125.x
   Palanisamy K., 2020, ARXIV200711154
   Poole Joyce H., 2011, P125
   Randler C, 2018, ECOL EVOL, V8, P7151, DOI 10.1002/ece3.4240
   Reinwald M, 2021, J R SOC INTERFACE, V18, DOI 10.1098/rsif.2021.0264
   Ren L, 2008, J R SOC INTERFACE, V5, P195, DOI 10.1098/rsif.2007.1095
   REW R, 1990, IEEE COMPUT GRAPH, V10, P76, DOI 10.1109/38.56302
   Rost S, 2002, REV GEOPHYS, V40, DOI 10.1029/2000RG000100
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Scheele B, 2019, SCIENCE, V363, P1459, DOI 10.1126/science.aav0379
   Senior AW, 2020, NATURE, V577, P706, DOI 10.1038/s41586-019-1923-7
   Shamout FE, 2020, IEEE J BIOMED HEALTH, V24, P437, DOI 10.1109/JBHI.2019.2937803
   Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Smit J, 2019, ORYX, V53, P368, DOI 10.1017/S0030605317000345
   Springenberg J. T., 2014, ARXIV14126806
   Stephens M, 1988, ALV VIS C, P147, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]
   Sugumar SJ, 2013, CURR SCI INDIA, V104, P1515
   Sukumar R., 2003, LIVING ELEPHANTS EVO
   Szenicer A, 2020, GEOPHYS J INT, V223, P1247, DOI 10.1093/gji/ggaa349
   Szenicer A, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw6548
   Thouless C., 2016, OCCASIONAL PAPER SER
   Wasser SK, 2008, CONSERV BIOL, V22, P1065, DOI 10.1111/j.1523-1739.2008.01012.x
   Wood JD, 2005, J APPL ECOL, V42, P587, DOI 10.1111/j.1365-2664.2005.01044.x
   Zhu WQ, 2020, ADV GEOPHYS, V61, P151, DOI 10.1016/bs.agph.2020.07.003
NR 55
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
DI 10.1002/rse2.242
EA NOV 2021
PG 15
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA WU0OM
UT WOS:000716254000001
OA gold
DA 2022-02-10
ER

PT J
AU Scotson, L
   Johnston, LR
   Iannarilli, F
   Wearn, OR
   Mohd-Azlan, J
   Wong, WM
   Gray, TNE
   Dinata, Y
   Suzuki, A
   Willard, CE
   Frechette, J
   Loken, B
   Steinmetz, R
   Mossbrucker, AM
   Clements, GR
   Fieberg, J
AF Scotson, Lorraine
   Johnston, Lisa R.
   Iannarilli, Fabiola
   Wearn, Oliver R.
   Mohd-Azlan, Jayasilan
   Wong, Wai Ming
   Gray, Thomas N. E.
   Dinata, Yoan
   Suzuki, Ai
   Willard, Clarie E.
   Frechette, Jackson
   Loken, Brent
   Steinmetz, Robert
   Mossbrucker, Alexander M.
   Clements, Gopalasamy Reuben
   Fieberg, John
TI Best practices and software for the management and sharing of camera
   trap data for small and large scales studies
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Review
DE Bycatch data; data management; macrosystem ecology; metadata; population
   trends; species identification
ID WILDLIFE PICTURE INDEX; HABITAT PREFERENCES; FOREST MAMMALS; ECOLOGY;
   CONSERVATION; FUTURE; ISSUES; CARNIVORES; DIVERSITY; COMMUNITY
AB Camera traps typically generate large amounts of bycatch data of non-target species that are secondary to the study's objectives. Bycatch data pooled from multiple studies can answer secondary research questions; however, variation in field and data management techniques creates problems when pooling data from multiple sources. Multi-collaborator projects that use standardized methods to answer broad-scale research questions are rare and limited in geographical scope. Many small, fixed-term independent camera trap studies operate in poorly represented regions, often using field and data management methods tailored to their own objectives. Inconsistent data management practices lead to loss of bycatch data, or an inability to share it easily. As a case study to illustrate common problems that limit use of bycatch data, we discuss our experiences processing bycatch data obtained by multiple research groups during a range-wide assessment of sun bears Helarctos malayanus in Southeast Asia. We found that the most significant barrier to using bycatch data for secondary research was the time required, by the owners of the data and by the secondary researchers (us), to retrieve, interpret and process data into a form suitable for secondary analyses. Furthermore, large quantities of data were lost due to incompleteness and ambiguities in data entry. From our experiences, and from a review of the published literature and online resources, we generated nine recommendations on data management best practices for field site metadata, camera trap deployment metadata, image classification data and derived data products. We cover simple techniques that can be employed without training, special software and Internet access, as well as options for more advanced users, including a review of data management software and platforms. From the range of solutions provided here, researchers can employ those that best suit their needs and capacity. Doing so will enhance the usefulness of their camera trap bycatch data by improving the ease of data sharing, enabling collaborations and expanding the scope of research.
C1 [Scotson, Lorraine; Iannarilli, Fabiola; Fieberg, John] Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, 2003 Upper Buford Circle, St Paul, MN 55108 USA.
   [Johnston, Lisa R.] Univ Minnesota Twin Cities Lib, Minneapolis, MN 55455 USA.
   [Wearn, Oliver R.] Zool Soc London, Inst Zool, Regents Pk, London NW1 4RY, England.
   [Mohd-Azlan, Jayasilan] Univ Malaysia Sarawak, Fac Resource Sci & Technol, Dept Zool, Kota Samarahan 94300, Sarawak, Malaysia.
   [Wong, Wai Ming] Panthera, 8 West 40th St,Floor 18, New York, NY 10018 USA.
   [Gray, Thomas N. E.] Wildlife Alliance, 86,St 123,Toultompong 1, Chamcamon, Phnom Penh, Cambodia.
   [Dinata, Yoan] ZSL, Indonesia Programme, Jalan Papandayan 18, Bogor, West Java, Indonesia.
   [Suzuki, Ai] Kyoto Univ, Grad Sch Asian & African Area Studies, Div Southeast Asian Studies, Ecol & Environm, Kyoto, Japan.
   [Willard, Clarie E.] WCS Cambodia Programme, 21 St 21, Khan Chamkarmorn 12000, Phnom Penh, Cambodia.
   [Frechette, Jackson] Fauna & Flora Int, 19 St 360, Phnom Penh, Cambodia.
   [Loken, Brent] EAT Initiat, POB 1232 Vika, N-0110 Oslo, Norway.
   [Loken, Brent] Stockholm Univ, Stockholm Resilience Ctr, Kraftriket 2B, SE-10691 Stockholm, Sweden.
   [Steinmetz, Robert] WWF Thailand, 92-2 Soi Phaholyothin 5,Phaholyothin Rd, Bangkok 10400, Thailand.
   [Mossbrucker, Alexander M.] FZS, Jl A Chatib 60, Jambi 36124, Indonesia.
   [Clements, Gopalasamy Reuben] Sunway Univ, Dept Biol Sci, 5 Jalan Univ, Bandar Sunway 47500, Selangor, Malaysia.
RP Scotson, L (corresponding author), Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, 2003 Upper Buford Circle, St Paul, MN 55108 USA.
EM scotsonuk@gmail.com
RI Gray, Thomas NE/AAK-6800-2020; Mohd-Azlan, Jayasilan/AAJ-7304-2020;
   Iannarilli, Fabiola/AAG-7774-2021; Fieberg, John/Y-3988-2019
OI Gray, Thomas NE/0000-0002-3642-4724; Mohd-Azlan,
   Jayasilan/0000-0001-5513-6237; Iannarilli, Fabiola/0000-0002-7018-3557;
   Fieberg, John/0000-0002-3180-7021; Dinata, Yoan/0000-0001-5507-3379;
   Wearn, Oliver/0000-0001-8258-3534; Clements, Gopalasamy
   Reuben/0000-0002-9715-4385
FU Ministry of Higher Education (MOHE), Niche Research Grant Scheme
   [NRGS/1087/2013(01)]; Minnesota Department of Natural Resources;
   University of Minnesota Doctoral Dissertation FellowshipUniversity of
   Minnesota System; University of Minnesota Conservation Biology Summer
   Grant
FX MAJ was supported by a Ministry of Higher Education (MOHE), Niche
   Research Grant Scheme: NRGS/1087/2013(01), Iannarilli was funded by the
   Minnesota Department of Natural Resources, and Scotson was funded by a
   University of Minnesota Doctoral Dissertation Fellowship and University
   of Minnesota Conservation Biology Summer Grant while this manuscript was
   prepared.
CR Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Balmford A, 2003, TRENDS ECOL EVOL, V18, P326, DOI 10.1016/S0169-5347(03)00067-3
   Balmford A., 2005, HIMALAYAN J SCI, V3, P43
   Beaudrot L, 2016, PLOS BIOL, V14, DOI 10.1371/journal.pbio.1002357
   Bengsen AJ, 2011, J WILDLIFE MANAGE, V75, P1222, DOI 10.1002/jwmg.132
   Borer E.T., 2009, B ECOL SOC AM, V90, P205, DOI DOI 10.1890/0012-9623-90.2.205
   Briney K., 2015, DATA MANAGEMENT RES
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chutipong W, 2014, RAFFLES B ZOOL, V62, P521
   Clements G. R., 2013, THESIS
   Collen B, 2008, TROP CONSERV SCI, V1, P75, DOI 10.1177/194008290800100202
   Council of Science Editors, 2014, SCI STYL FORM CSE MA, V2, P44
   Cutler TL, 1999, WILDLIFE SOC B, V27, P571
   Dinata Y., 2008, THESIS
   Dobson A, 2010, ANIM CONSERV, V13, P347, DOI 10.1111/j.1469-1795.2010.00381.x
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Goring SJ, 2014, FRONT ECOL ENVIRON, V12, P39, DOI 10.1890/120370
   Gray TNE, 2012, BIOTROPICA, V44, P531, DOI 10.1111/j.1744-7429.2011.00846.x
   Gray TNE, 2011, RAFFLES B ZOOL, V59, P311
   Hampton SE, 2013, FRONT ECOL ENVIRON, V11, P156, DOI 10.1890/120103
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Heffernan JB, 2014, FRONT ECOL ENVIRON, V12, P5, DOI 10.1890/130017
   Herold P., 2015, J LIBRARIANSHIP SCHO, V3
   Ivan JS, 2016, METHODS ECOL EVOL, V7, P499, DOI 10.1111/2041-210X.12503
   Jansen PA, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P263
   Kratz JE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0117619
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Linkie M, 2013, BIOL CONSERV, V162, P107, DOI 10.1016/j.biocon.2013.03.028
   Loken B, 2013, AM J PRIMATOL, V75, P1129, DOI 10.1002/ajp.22174
   Lynam AJ, 2012, INTEGR ZOOL, V7, P389, DOI 10.1111/1749-4877.12002
   Mathai J, 2016, RAFFLES B ZOOL, P186
   Maxwell S, 2016, NATURE, V536, P143, DOI 10.1038/536143a
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McGill B., 2016, DYNAMIC ECOLOGY WEB
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek P., 2014, CAMERA TRAPPING WILD
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Michener WK, 2012, TRENDS ECOL EVOL, V27, P85, DOI 10.1016/j.tree.2011.11.016
   Mohd-Azlan J, 2013, RAFFLES B ZOOL, V61, P397
   Nichols JD, 2010, ANIM CONSERV, V13, P344, DOI 10.1111/j.1469-1795.2010.00382.x
   Niedballa J., 2016, METHODS ECOL EVOL EA
   O'Brien TG, 2010, ANIM CONSERV, V13, P350, DOI 10.1111/j.1469-1795.2010.00384.x
   Olsen AR, 1999, ENVIRON MONIT ASSESS, V54, P1, DOI 10.1023/A:1005823911258
   R Core Team, 2017, R LANG ENV STAT COMP
   Reichman OJ, 2011, SCIENCE, V331, P703, DOI 10.1126/science.1197962
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Sanderson JG, 2005, AM SCI, V93, P148, DOI 10.1511/2005.52.958
   Sandve GK, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003285
   Schipper J, 2008, SCIENCE, V322, P225, DOI 10.1126/science.1165115
   Spehar SN, 2015, BIOL CONSERV, V191, P185, DOI 10.1016/j.biocon.2015.06.013
   Strasser CA, 2012, ECOSPHERE, V3, DOI 10.1890/ES12-00139.1
   Sunarto, 2013, RAFFLES B ZOOL, P21
   Sundaresan S.R., 2011, B ECOL SOC AM, V92, P188, DOI DOI 10.1890/0012-9623-92.2.188
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Thorn M, 2009, S AFR J WILDL RES, V39, P1, DOI 10.3957/056.039.0101
   Tobler M., 2007, CAMERA BASE
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Whitlock MC, 2011, TRENDS ECOL EVOL, V26, P61, DOI 10.1016/j.tree.2010.11.006
   Wong WM, 2013, ANIM CONSERV, V16, P216, DOI 10.1111/j.1469-1795.2012.00587.x
   Wong WM, 2013, DIVERS DISTRIB, V19, P700, DOI 10.1111/ddi.12020
NR 65
TC 17
Z9 20
U1 3
U2 37
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
PD SEP
PY 2017
VL 3
IS 3
SI SI
BP 158
EP 172
DI 10.1002/rse2.54
PG 15
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA FG8YO
UT WOS:000410724800006
OA Green Accepted, gold
DA 2022-02-10
ER

PT C
AU Timm, M
   Maji, S
   Fuller, T
AF Timm, Mikayla
   Maji, Subhransu
   Fuller, Todd
GP IEEE
TI Large-Scale Ecological Analyses of Animals in the Wild using Computer
   Vision
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
SP IEEE Comp Soc
AB Camera traps are increasingly being deployed by ecologists and citizen-scientists as a cost-effective way of obtaining large amounts of animal images in the wild. In order to analyze this data, the images are labeled manually by ecologists, where they identify species of animals and more fine-grained details, such as animal sex or age, or even individual animal identities. However, with the number of camera trap images quickly outgrowing the capacity of the labelers, ecologists are unable to keep up with the wealth of data they are obtaining. Using computer vision, we can automatically generate labels for new camera trap images at the rate that they are being obtained, allowing ecologists to uncover ecological and biological information at a scale previously not possible. In this paper, we explore computer vision approaches for species identification in camera trap images and for individual jaguar identification, both of which show promising results. We make this novel dataset publicly available for future research directions and further exploration.
C1 [Timm, Mikayla; Maji, Subhransu; Fuller, Todd] Univ Massachusetts, Amherst, MA 01003 USA.
RP Timm, M (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.
EM mtimm@cs.umass.edu; smaji@cs.umass.edu; tkfuller@eco.umass.edu
CR Deng J., 2009, IEEE CVPR
   Girshick R., 2014, IEEE CVPR
   Krizhevsky A., 2012, IMAGENET CLASSIFICAT, V25
   Long J., 2015, IEEE CVPR
   RoyChowdhury A., 2015, ICCV
   Szegedy C., 2016, IEEE CVPR
   Van Horn G., 2018, IEEE CVPR
NR 7
TC 1
Z9 1
U1 1
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 1977
EP 1979
DI 10.1109/CVPRW.2018.00252
PG 3
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800245
DA 2022-02-10
ER

PT J
AU Despres-Einspenner, ML
   Howe, EJ
   Drapeau, P
   Kuhl, HS
AF Despres-Einspenner, Marie-Lyne
   Howe, Eric J.
   Drapeau, Pierre
   Kuehl, Hjalmar S.
TI An empirical evaluation of camera trapping and spatially explicit
   capture-recapture models for estimating chimpanzee density
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE camera trapping; chimpanzee; density; monitoring; spatially explicit
   capture-recapture; survey design
ID TAI-NATIONAL-PARK; POPULATION-SIZE; FOREST; RATES; ABUNDANCE; SELECTION;
   PATTERNS; WILDLIFE; DECLINE
AB Empirical validations of survey methods for estimating animal densities are rare, despite the fact that only an application to a population of known density can demonstrate their reliability under field conditions and constraints. Here, we present a field validation of camera trapping in combination with spatially explicit capture-recapture (SECR) methods for enumerating chimpanzee populations. We used 83 camera traps to sample a habituated community of western chimpanzees (Pan troglodytes verus) of known community and territory size in Tai National Park, Ivory Coast, and estimated community size and density using spatially explicit capture-recapture models. We aimed to: (1) validate camera trapping as a means to collect capture-recapture data for chimpanzees; (2) validate SECR methods to estimate chimpanzee density from camera trap data; (3) compare the efficacy of targeting locations frequently visited by chimpanzees versus deploying cameras according to a systematic design; (4) evaluate the performance of SECR estimators with reduced sampling effort; and (5) identify sources of heterogeneity in detection probabilities. Ten months of camera trapping provided abundant capture-recapture data. All weaned individuals were detected, most of them multiple times, at both an array of targeted locations, and a systematic grid of cameras positioned randomly within the study area, though detection probabilities were higher at targeted locations. SECR abundance estimates were accurate and precise, and analyses of subsets of the data indicated that the majority of individuals in a community could be detected with as few as five traps deployed within their territory. Our results highlight the potential of camera trapping for cost-effective monitoring of chimpanzee populations.
C1 [Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Despres-Einspenner, Marie-Lyne; Drapeau, Pierre] Univ Quebec Montreal, Dept Sci Biol, Ctr Forest Res, Montreal, PQ, Canada.
   [Howe, Eric J.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP Despres-Einspenner, ML (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM marie_despres@eva.mpg.de
FU Centre for Forest Research-Fonds de Recherche Quebec Nature et
   Technologies International internship program
FX The authors thank the Max Planck Society in Germany, Robert Bosch
   Foundation, Centre Suisse de Recherches Scientifiques, Ministere de
   l'Enseignement Superieur et de la Recherche Scientifique and Ministere
   de l'Environnement et des Eaux et Forets in Cote d'Ivoire, as well as
   the TaiChimpanzee Project for the possibility of conducting field
   research in Cote d'Ivoire. This study was conducted with the financial
   support of the Centre for Forest Research-Fonds de Recherche Quebec
   Nature et Technologies International internship program (M.Sc.
   scholarship to Despres-Einspenner). We thank Appollinaire Gnahe Djirian,
   Oulai Landry, Frederic Yehanon Oulai, Anna Preis, and Liran Samuni for
   providing the tracking data, and Frederic Yehanon Oulai and Liran Samuni
   for the invaluable help they provided with identifying the chimpanzees
   in the videos. At last, we thank Stephen Buckland, David Borchers,
   Marina Cords, and three anonymous reviewers for providing insightful
   comments on earlier versions of the manuscript. Field protocols, data
   collection, and data analysis complied with animal care regulations and
   applicable national laws in Germany and Ivory Coast.
CR Anderson DP, 2005, BIOTROPICA, V37, P631, DOI 10.1111/j.1744-7429.2005.00080.x
   ANDERSON DR, 1994, ECOLOGY, V75, P1780, DOI 10.2307/1939637
   Balcomb SR, 2000, AM J PRIMATOL, V51, P197, DOI 10.1002/1098-2345(200007)51:3&lt;197::AID-AJP4&gt;3.0.CO;2-C
   Baldwin P.J., 1982, International Journal of Primatology, V3, P367, DOI 10.1007/BF02693739
   Basabose AK, 2005, INT J PRIMATOL, V26, P33, DOI 10.1007/s10764-005-0722-1
   Beyer HL, 2004, HAWTHS ANAL TOOLS AR
   Boesch C., 2000, CHIMPANZEES TAI FORE, P328
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers D, 2012, J ORNITHOL, V152, pS435, DOI 10.1007/s10336-010-0583-z
   Buckland S.T., 2001, INTRO DISTANCE SAMPL, P448
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campbell G, 2008, CURR BIOL, V18, pR903, DOI 10.1016/j.cub.2008.08.015
   CHAPMAN CA, 1993, AM J PRIMATOL, V31, P263, DOI 10.1002/ajp.1350310403
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Creel S, 2003, MOL ECOL, V12, P2003, DOI 10.1046/j.1365-294X.2003.01868.x
   Doran D, 1997, INT J PRIMATOL, V18, P183, DOI 10.1023/A:1026368518431
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford MG, 2013, METHODS ECOL EVOL, V4, P629, DOI 10.1111/2041-210X.12049
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Efford MG, 2011, ECOLOGY, V92, P2202, DOI 10.1890/11-0332.1
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Efford MG, 2009, ECOLOGY, V90, P2676, DOI 10.1890/08-1735.1
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Goodall J., 1988, SHADOW MAN, P297
   Granjon AC, 2017, J WILDLIFE MANAGE, V81, P279, DOI 10.1002/jwmg.21190
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Herbinger I, 2001, INT J PRIMATOL, V22, P143, DOI 10.1023/A:1005663212997
   Hill K, 2001, J HUM EVOL, V40, P437, DOI 10.1006/jhev.2001.0469
   HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kuhl H, 2008, BEST PRACTICE GUIDEL, P32
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lehmann J, 2004, BEHAV ECOL SOCIOBIOL, V56, P207, DOI 10.1007/s00265-004-0781-x
   Link WA, 2010, BIOMETRICS, V66, P178, DOI 10.1111/j.1541-0420.2009.01244.x
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Mathewson PD, 2008, ECOL APPL, V18, P208, DOI 10.1890/07-0385.1
   McCarthy MS, 2015, BMC ECOL, V15, DOI 10.1186/s12898-015-0052-x
   McLennan MR, 2010, AM J PRIMATOL, V72, P907, DOI 10.1002/ajp.20839
   Mills LS, 2000, ECOL APPL, V10, P283, DOI 10.2307/2641002
   Moore DL, 2014, AM J PRIMATOL, V76, P335, DOI 10.1002/ajp.22237
   Mugerwa B, 2013, AFR J ECOL, V51, P21, DOI 10.1111/aje.12004
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   O'Connell A. F., 2010, CAMERA TRAPS ANIMAL, P271
   Obbard ME, 2010, J APPL ECOL, V47, P76, DOI 10.1111/j.1365-2664.2009.01758.x
   Otis D. L., 1978, WILDLIFE MONOGR, V62, P3, DOI DOI 10.2307/3830650
   Pledger S, 2000, BIOMETRICS, V56, P434, DOI 10.1111/j.0006-341X.2000.00434.x
   R Core Team, 2015, R LANG ENV STAT COMP
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Royle JA, 2008, ECOLOGY, V89, P2281, DOI 10.1890/07-0601.1
   Spehar SN, 2015, BIOL CONSERV, V191, P185, DOI 10.1016/j.biocon.2015.06.013
   Treves A, 2010, BIOL CONSERV, V143, P521, DOI 10.1016/j.biocon.2009.11.025
   Walsh PD, 2003, NATURE, V422, P611, DOI 10.1038/nature01566
   Whittington J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134446
   Wich SA, 2008, ORYX, V42, P329, DOI 10.1017/S003060530800197X
   WORTON BJ, 1989, ECOLOGY, V70, P164, DOI 10.2307/1938423
   Wrangham R. W., 1979, J REPROD FERTILITY S, V28, P13
NR 60
TC 30
Z9 30
U1 2
U2 46
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD JUL
PY 2017
VL 79
IS 7
SI SI
AR e22647
DI 10.1002/ajp.22647
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA EY4CO
UT WOS:000403924400007
PM 28267880
DA 2022-02-10
ER

PT J
AU Janzen, M
   Visser, K
   Visscher, D
   MacLeod, I
   Vujnovic, D
   Vujnovic, K
AF Janzen, Michael
   Visser, Kaitlyn
   Visscher, Darcy
   MacLeod, Ian
   Vujnovic, Dragomir
   Vujnovic, Ksenija
TI Semi-automated camera trap image processing for the detection of
   ungulate fence crossing events
SO ENVIRONMENTAL MONITORING AND ASSESSMENT
LA English
DT Article
DE Camera trap; Computer vision; Monitoring; Remote camera; Image
   processing
ID DENSITY-ESTIMATION; IDENTIFICATION
AB Remote cameras are an increasingly important tool for ecological research. While remote camera traps collect field data with minimal human attention, the images they collect require post-processing and characterization before it can be ecologically and statistically analyzed, requiring the input of substantial time and money from researchers. The need for post-processing is due, in part, to a high incidence of non-target images. We developed a stand-alone semi-automated computer program to aid in image processing, categorization, and data reduction by employing background subtraction and histogram rules. Unlike previous work that uses video as input, our program uses still camera trap images. The program was developed for an ungulate fence crossing project and tested against an image dataset which had been previously processed by a human operator. Our program placed images into categories representing the confidence of a particular sequence of images containing a fence crossing event. This resulted in a reduction of 54.8% of images that required further human operator characterization while retaining 72.6% of the known fence crossing events. This program can provide researchers using remote camera data the ability to reduce the time and cost required for image post-processing and characterization. Further, we discuss how this procedure might be generalized to situations not specifically related to animal use of linear features.
C1 [Janzen, Michael; Visser, Kaitlyn; Visscher, Darcy; MacLeod, Ian] Kings Univ, Edmonton, AB, Canada.
   [Vujnovic, Dragomir; Vujnovic, Ksenija] Alberta Pk, Edmonton, AB, Canada.
RP Janzen, M (corresponding author), Kings Univ, Edmonton, AB, Canada.
EM Michael.Janzen@kingsu.ca; Darcy.Visscher@kingsu.ca
FU King's University; King's Centre for Visualization in Science; Alberta
   Parks
FX We thank Alberta Parks for funding for this project and for helping with
   field setup and data collection and the King's University and the King's
   Centre for Visualization in Science for funding and support. This work,
   in part, comes from the undergraduate research projects of K. Visser and
   I. MacLeod. We also thank Dr. Jose Alexander Elvir and an anonymous
   reviewer for their comments.
CR Anderson CJR, 2010, J MAMMAL, V91, P1350, DOI 10.1644/09-MAMM-A-425.1
   Ardekani R, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-61
   Bubnicki J. W., 2016, METHODS ECOLOGY EVOL
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Forrester Tavis, 2013, P 98 ESA ANN CONV 20
   Goehner K, 2015, P IEEE INT C E-SCI, P187, DOI 10.1109/eScience.2015.10
   Gonzalez R.C., 2007, DIGITAL IMAGE PROCES
   Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
   Jhala YV, 2009, BIODIVERS CONSERV, V18, P3383, DOI 10.1007/s10531-009-9648-9
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Osterrieder SK, 2015, J MAMMAL, V96, P988, DOI 10.1093/jmammal/gyv102
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Siren APK, 2016, DIVERSITY-BASEL, V8, DOI 10.3390/d8010003
   Spampinato C, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2015-1
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Visscher DR, 2017, WILDLIFE SOC B, V41, P162, DOI 10.1002/wsb.741
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 21
TC 3
Z9 3
U1 0
U2 11
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0167-6369
EI 1573-2959
J9 ENVIRON MONIT ASSESS
JI Environ. Monit. Assess.
PD OCT
PY 2017
VL 189
IS 10
AR 527
DI 10.1007/s10661-017-6206-x
PG 13
WC Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA FK2ZW
UT WOS:000413354400015
PM 28956203
DA 2022-02-10
ER

PT J
AU Thomas, ML
   Baker, L
   Beattie, JR
   Baker, AM
AF Thomas, Morgan L.
   Baker, Lynn
   Beattie, James R.
   Baker, Andrew M.
TI Determining the efficacy of camera traps, live capture traps, and
   detection dogs for locating cryptic small mammal species
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE Antechinus arktos; black-tailed dusky antechinus; camera trapping;
   effectiveness; live trapping
ID ESTIMATING SITE OCCUPANCY; ANTECHINUS-ARKTOS; DENSITY; POPULATIONS;
   PERFORMANCE; COMPETITION; EXTINCTION; MARSUPIALS; ABUNDANCE; REPTILES
AB Metal box (e.g., Elliott, Sherman) traps and remote cameras are two of the most commonly employed methods presently used to survey terrestrial mammals. However, their relative efficacy at accurately detecting cryptic small mammals has not been adequately assessed. The present study therefore compared the effectiveness of metal box (Elliott) traps and vertically oriented, close range, white flash camera traps in detecting small mammals occurring in the Scenic Rim of eastern Australia. We also conducted a preliminary survey to determine effectiveness of a conservation detection dog (CDD) for identifying presence of a threatened carnivorous marsupial, Antechinus arktos, in present-day and historical locations, using camera traps to corroborate detections. 200 Elliott traps and 20 white flash camera traps were set for four deployments per method, across a site where the target small mammals, including A. arktos, are known to occur. Camera traps produced higher detection probabilities than Elliott traps for all four species. Thus, vertically mounted white flash cameras were preferable for detecting the presence of cryptic small mammals in our survey. The CDD, which had been trained to detect A. arktos scat, indicated in total 31 times when deployed in the field survey area, with subsequent camera trap deployments specifically corroborating A. arktos presence at 100% (3) indication locations. Importantly, the dog indicated twice within Border Ranges National Park, where historical (1980s-1990s) specimen-based records indicate the species was present, but extensive Elliott and camera trapping over the last 5-10 years have resulted in zero A. arktos captures. Camera traps subsequently corroborated A. arktos presence at these sites. This demonstrates that detection dogs can be a highly effective means of locating threatened, cryptic species, especially when traditional methods are unable to detect low-density mammal populations.
C1 [Thomas, Morgan L.; Baker, Andrew M.] Queensland Univ Technol, Sci & Engn Fac, Sch Earth Environm & Biol Sci, Brisbane, Qld, Australia.
   [Baker, Lynn] Canines Wildlife, Brierfield, NSW, Australia.
   [Beattie, James R.] Australian Natl Univ, Res Sch Astron & Astrophys, Canberra, ACT, Australia.
   [Baker, Andrew M.] Queensland Museum, Biodivers Program, South Brisbane, Qld, Australia.
RP Thomas, ML (corresponding author), Queensland Univ Technol, Sci & Engn Fac, Sch Earth Environm & Biol Sci, Brisbane, Qld, Australia.
EM m44.thomas@connect.qut.edu.au
RI Beattie, James/AAU-6049-2020
OI Beattie, James/0000-0001-9199-7771; Baker, Andrew/0000-0001-8825-1522;
   Baker, Lynn/0000-0002-2803-761X
FU Saving Our Species (SOS; NSW Environment and Heritage); Fitzroy Basin
   Association; Burnett Mary Regional Group
FX The current project was generously supported by funding from Saving Our
   Species (SOS; NSW Environment and Heritage). Funding for trapping
   equipment used in the study was furnished by the Fitzroy Basin
   Association and the Burnett Mary Regional Group. Thanks to Aila Keto
   (Australian Rainforest Conservation Society) for her support and
   accommodation throughout the study period and the School of Earth,
   Environmental and Biological Sciences (EEBS) at the Queensland
   University of Technology (QUT), who provided access to field equipment
   and vehicles. An enormous thank you to Steve Austin (professional
   detection dog trainer), Canines for Wildlife, Brad Nesbitt (backup
   handler), and Bunya the dog for their time, assistance, and support
   before, during and after detection dog field surveys. Finally, we would
   like to thank Kate Moffatt, Emma Gray, Dimity Ball, Matthew Neill, Jack
   Nesbitt, Isaac Towers, Ellie Frederiksen, Oscar Lehman, and Artie Ziff,
   who all graciously volunteered their time to help with certain
   components of field work, quantitative methods, or provided other
   contributions to the study.
CR Akenson JJ, 2001, URSUS-SERIES, V12, P203
   Arnett EB, 2006, WILDLIFE SOC B, V34, P1440, DOI 10.2193/0091-7648(2006)34[1440:APEOTU]2.0.CO;2
   Baker AM, 2014, ZOOTAXA, V 3765, P101, DOI 10.11646/zootaxa.3765.2.1
   Barnosky AD, 2011, NATURE, V471, P51, DOI 10.1038/nature09678
   Bilney RJ, 2014, AUSTRAL ECOL, V39, P875, DOI 10.1111/aec.12145
   Browne CM, 2015, J VET BEHAV, V10, P496, DOI 10.1016/j.jveb.2015.08.002
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Clare J, 2017, ECOL APPL, V27, P2031, DOI 10.1002/eap.1587
   Cristescu RH, 2015, SCI REP-UK, V5, DOI 10.1038/srep08349
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   DICKMAN CR, 1991, OECOLOGIA, V85, P464, DOI 10.1007/BF00323757
   DICKMAN CR, 1986, OECOLOGIA, V70, P536, DOI 10.1007/BF00379900
   Diggins CA, 2016, WILDLIFE SOC B, V40, P654, DOI 10.1002/wsb.715
   Duggan JM, 2011, J WILDLIFE MANAGE, V75, P1209, DOI 10.1002/jwmg.150
   Fisher JT, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161055
   Fisher JT, 2014, J WILDLIFE MANAGE, V78, P1087, DOI 10.1002/jwmg.750
   Garden JG, 2007, WILDLIFE RES, V34, P218, DOI 10.1071/WR06111
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Gonzalez-Esteban J, 2004, EUR J WILDLIFE RES, V50, P33, DOI 10.1007/s10344-003-0031-y
   Gray EL, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181592
   Gray EL, 2017, MAMMAL RES, V62, P47, DOI 10.1007/s13364-016-0281-1
   Gray EL, 2016, AUST J ZOOL, V64, P249, DOI 10.1071/ZO16044
   Griffiths M., 1994, POPULATION DENSITY S, P93
   Harrison RL, 2006, WILDLIFE SOC B, V34, P548, DOI 10.2193/0091-7648(2006)34[548:ACOSMF]2.0.CO;2
   Hunter RJ, 2003, WORLD HERITAGE ASS N
   Hurt A., 2009, CANINE ERGONOMICS, P175
   Jones L, 1996, 1996 ICHEME RESEARCH EVENT - SECOND EUROPEAN CONFERENCE FOR YOUNG RESEARCHERS IN CHEMICAL ENGINEERING, VOLS 1 AND 2, P115
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Leigh KA, 2015, METHODS ECOL EVOL, V6, P745, DOI 10.1111/2041-210X.12374
   Leung LKP, 1999, WILDLIFE RES, V26, P287, DOI 10.1071/WR96042
   Long RA, 2007, J WILDLIFE MANAGE, V71, P2018, DOI 10.2193/2006-292
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McCallum ML, 2015, BIODIVERS CONSERV, V24, P2497, DOI 10.1007/s10531-015-0940-6
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Meek PD, 2012, WILDLIFE RES, V39, P649, DOI 10.1071/WR12138
   Meek PD, 2016, AUST MAMMAL, V38, P44, DOI 10.1071/AM15016
   Nichols JD, 2008, J APPL ECOL, V45, P1321, DOI 10.1111/j.1365-2664.2008.01509.x
   Paull DJ, 2012, WILDLIFE RES, V39, P546, DOI 10.1071/WR12034
   Peterson LM, 1998, WILDLIFE SOC B, V26, P592
   READ DG, 1988, AUST WILDLIFE RES, V15, P139
   Reed SE, 2011, J WILDLIFE MANAGE, V75, P243, DOI 10.1002/jwmg.8
   Reindl-Thompson SA, 2006, WILDLIFE SOC B, V34, P1435, DOI 10.2193/0091-7648(2006)34[1435:EOSDID]2.0.CO;2
   Rendall AR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086592
   Rovero F, 2017, BIODIVERS CONSERV, V26, P1103, DOI 10.1007/s10531-016-1288-2
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Stanley TR, 2005, J WILDLIFE MANAGE, V69, P874, DOI 10.2193/0022-541X(2005)069[0874:ESOAAU]2.0.CO;2
   Sweitzer RA, 2000, J WILDLIFE MANAGE, V64, P531, DOI 10.2307/3803251
   Tasker Elizabeth M., 2002, Australian Mammalogy, V23, P77
   Thompson GG, 2007, WILDLIFE RES, V34, P491, DOI 10.1071/WR06081
   Trolle M, 2008, BIOTROPICA, V40, P211, DOI 10.1111/j.1744-7429.2007.00350.x
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Van Dyck S., 2008, MAMMALS AUSTR
   Vine SJ, 2009, WILDLIFE RES, V36, P436, DOI 10.1071/WR08069
   Weerakoon MK, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P307
   Wiewel AS, 2007, J MAMMAL, V88, P250, DOI 10.1644/06-MAMM-A-098R1.1
   Woinarski JCZ, 2015, P NATL ACAD SCI USA, V112, P4531, DOI 10.1073/pnas.1417301112
   WOOD D H, 1970, Australian Journal of Zoology, V18, P185, DOI 10.1071/ZO9700185
   WOOD D H, 1971, Australian Journal of Zoology, V19, P371, DOI 10.1071/ZO9710371
NR 61
TC 4
Z9 7
U1 4
U2 27
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD JAN
PY 2020
VL 10
IS 2
BP 1054
EP 1068
DI 10.1002/ece3.5972
EA JAN 2020
PG 15
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA KG3BK
UT WOS:000506025400001
PM 32015864
OA Green Published, gold
DA 2022-02-10
ER

PT C
AU Yang, XY
   Mirmehdi, M
   Burghardt, T
AF Yang, Xinyu
   Mirmehdi, Majid
   Burghardt, Tilo
GP IEEE
TI Great Ape Detection in Challenging Jungle Camera Trap Footage via
   Attention-Based Spatial and Temporal Feature Blending
SO 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS
   (ICCVW)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF International Conference on Computer Vision (ICCV)
CY OCT 27-NOV 02, 2019
CL Seoul, SOUTH KOREA
SP IEEE, IEEE Comp Soc, CVF
AB We propose the first multi-frame video object detection framework trained to detect great apes. It is applicable to challenging camera trap footage in complex jungle environments and extends a traditional feature pyramid architecture by adding self-attention driven feature blending in both the spatial as well as the temporal domain. We demonstrate that this extension can detect distinctive species appearance and motion signatures despite significant partial occlusion. We evaluate the framework using 500 camera trap videos of great apes from the Pan African Programme containing 180K frames, which we manually annotated with accurate per-frame animal bounding boxes. These clips contain significant partial occlusions, challenging lighting, dynamic backgrounds, and natural camouflage effects. We show that our approach performs highly robustly and significantly outperforms frame-based detectors. We also perform detailed ablation studies and a validation on the full ILSVRC 2015 VID data corpus to demonstrate wider applicability at adequate performance levels. We conclude that the framework is ready to assist human camera trap inspection efforts. We publish key parts of the code as well as network weights and ground truth annotations with this paper.
C1 [Yang, Xinyu; Mirmehdi, Majid; Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
RP Yang, XY (corresponding author), Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
EM xinyu.yang@bristol.ac.uk; m.mirmehdi@bristol.ac.uk; tilo@cs.bris.ac.uk
OI Mirmehdi, Majid/0000-0002-6478-1403
FU Max Planck SocietyMax Planck SocietyFoundation CELLEX; Max Planck
   Society Innovation Fund
FX We would like to thank the entire team of the Pan African Programme:
   `The Cultured Chimpanzee' and its collaborators for allowing the use of
   their data for this paper. Please contact the copyright holder Pan
   African Programme at http://panafrican.eva.mpg.de to obtain the dataset.
   Particularly, we thank: H Kuehl, C Boesch, M Arandjelovic, and P
   Dieguez. We would also like to thank: K Zuberbuehler, K Corogenes, E
   Normand, V Vergnes, A Meier, J Lapuente, D Dowd, S Jones, V Leinert,
   EWessling, H Eshuis, K Langergraber, S Angedakin, S Marrocoli, K Dierks,
   T C Hicks, J Hart, K Lee, and M Murai. Thanks also to the team at
   https://www.chimpandsee.org.The work that allowed for the collection of
   the dataset was funded by the Max Planck Society, Max Planck Society
   Innovation Fund, and Heinz L. Krekeler. In this respect we would also
   like to thank: Foundation Ministre de la Recherche Scientifique, and
   Ministre des Eaux et Forlts in Cote d'Ivoire; Institut Congolais pour la
   Conservation de la Nature and Ministre de la Recherche Scientifique in
   DR Congo; Forestry Development Authority in Liberia; Direction des Eaux,
   Forlts Chasses et de la Conservation des Sols, Senegal; and Uganda
   National Council for Science and Technology, UgandaWildlife Authority,
   National Forestry Authority in Uganda.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao Yue, 2019, ARXIV190411492CS
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen K, 2018, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR.2018.00815
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   DrivenData, COMP PRIM FACT
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Han Wei, 2016, TECHNICAL REPORT
   ImageNet, IMAGENET CALL 2015 O
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Li B., 2018, ARXIV181211703
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, P568
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 31
TC 2
Z9 2
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2473-9936
BN 978-1-7281-5023-9
J9 IEEE INT CONF COMP V
PY 2019
BP 255
EP 262
DI 10.1109/ICCVW.2019.00034
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BP4UH
UT WOS:000554591600028
OA Green Submitted
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Despres-Einspenner, ML
   Samuni, L
   Mundry, R
   Lemoine, S
   Preis, A
   Wittig, RM
   Boesch, C
   Kuhl, HS
AF McCarthy, Maureen S.
   Despres-Einspenner, Marie-Lyne
   Samuni, Liran
   Mundry, Roger
   Lemoine, Sylvain
   Preis, Anna
   Wittig, Roman M.
   Boesch, Christophe
   Kuehl, Hjalmar S.
TI An assessment of the efficacy of camera traps for studying demographic
   composition and variation in chimpanzees (Pan troglodytes)
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE biomonitoring; camera trap; demography; Pan troglodytes; party size;
   seasonal variation
ID MAMMAL COMMUNITIES; TOOL USE; FOREST; WILD; POPULATION; ABUNDANCE;
   BEHAVIOR; DENSITY; CONSEQUENCES; PREDATION
AB Demographic factors can strongly influence patterns of behavioral variation in animal societies. Traditionally, these factors are measured using longitudinal observation of habituated social groups, particularly in social animals like primates. Alternatively, noninvasive biomonitoring methods such as camera trapping can allow researchers to assess species occupancy, estimate population abundance, and study rare behaviors. However, measures of fine-scale demographic variation, such as those related to age and sex structure or subgrouping patterns, pose a greater challenge. Here, we compare demographic data collected from a community of habituated chimpanzees (Pan troglodytes verus) in the Tai Forest using two methods: camera trap videos and observational data from long-term records. By matching data on party size, seasonal variation in party size, measures of demographic composition, and changes over the study period from both sources, we compared the accuracy of camera trap records and long-term data to assess whether camera trap data could be used to assess such variables in populations of unhabituated chimpanzees. When compared to observational data, camera trap data tended to underestimate measures of party size, but revealed similar patterns of seasonal variation as well as similar community demographic composition (age/sex proportions) and dynamics (particularly emigration and deaths) during the study period. Our findings highlight the potential and limitations of camera trap surveys for estimating fine-scale demographic composition and variation in primates. Continuing development of field and statistical methods will further improve the usability of camera traps for demographic studies.
C1 [McCarthy, Maureen S.; Despres-Einspenner, Marie-Lyne; Samuni, Liran; Lemoine, Sylvain; Preis, Anna; Wittig, Roman M.; Boesch, Christophe; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Mundry, Roger] Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
   [Samuni, Liran; Wittig, Roman M.] CSRS, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP McCarthy, MS (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM maureen_mc@eva.mpg.de
OI Lemoine, Sylvain/0000-0001-9853-5246
FU Fonds de Recherche Quebec-Nature et Technologies; Robert Bosch Stiftung;
   Max-Planck-GesellschaftMax Planck Society
FX Fonds de Recherche Quebec-Nature et Technologies; Robert Bosch Stiftung;
   Max-Planck-Gesellschaft
CR Aars J, 2000, AM NAT, V155, P252, DOI 10.1086/303317
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   ALTMANN J, 1974, BEHAVIOUR, V49, P227, DOI 10.1163/156853974X00534
   Anderson DP, 2002, BEHAVIOURAL DIVERSITY IN CHIMPANZEES AND BONOBOS, P90, DOI 10.1017/CBO9780511606397.010
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D., 2014, J STAT SOFTW, V1406, P5823, DOI DOI 10.18637/jss.v067.i01
   Bluff LA, 2010, P ROY SOC B-BIOL SCI, V277, P1377, DOI 10.1098/rspb.2009.1953
   Boesch C, 2000, CHIMPANZEES TAI FORE
   Boyer-Ontl KM, 2014, INT J PRIMATOL, V35, P881, DOI 10.1007/s10764-014-9783-3
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Butchart SHM, 2010, SCIENCE, V328, P1164, DOI 10.1126/science.1187512
   Clutton-Brock T, 2010, TRENDS ECOL EVOL, V25, P562, DOI 10.1016/j.tree.2010.08.002
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Doran D, 1997, INT J PRIMATOL, V18, P183, DOI 10.1023/A:1026368518431
   Fedigan LM, 2010, AM J PRIMATOL, V72, P754, DOI 10.1002/ajp.20814
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   GOLDIZEN AW, 1988, TRENDS ECOL EVOL, V3, P36, DOI 10.1016/0169-5347(88)90045-6
   Goodall J., 1986, CHIMPANZEES GOMBE PA
   Gruen L, 2013, ILAR J, V54, P24, DOI 10.1093/ilar/ilt016
   Hanya G, 2006, PRIMATES, V47, P275, DOI 10.1007/s10329-005-0176-2
   Harmsen BJ, 2011, POPUL ECOL, V53, P253, DOI 10.1007/s10144-010-0211-z
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Kahlenberg SM, 2008, INT J PRIMATOL, V29, P931, DOI 10.1007/s10764-008-9276-3
   Kalan AK, 2015, ECOL INDIC, V54, P217, DOI 10.1016/j.ecolind.2015.02.023
   Kanamori T, 2017, PRIMATES, V58, P225, DOI 10.1007/s10329-016-0584-5
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
   Langergraber KE, 2017, P NATL ACAD SCI USA, V114, P7337, DOI 10.1073/pnas.1701582114
   Langergraber KE, 2014, AM J PRIMATOL, V76, P640, DOI 10.1002/ajp.22258
   LEIMGRUBER P, 1994, J WILDLIFE MANAGE, V58, P254, DOI 10.2307/3809388
   Markham AC, 2015, P NATL ACAD SCI USA, V112, P14882, DOI 10.1073/pnas.1517794112
   Matsumoto-Oda A, 1998, INT J PRIMATOL, V19, P999, DOI 10.1023/A:1020322203166
   McCarthy MS, 2015, BMC ECOL, V15, DOI 10.1186/s12898-015-0052-x
   Meyer NFV, 2015, J NAT CONSERV, V26, P28, DOI 10.1016/j.jnc.2015.04.003
   Mitani JC, 2006, PRIMATES, V47, P6, DOI 10.1007/s10329-005-0139-7
   Mitani JC, 2002, BEHAVIOURAL DIVERSITY IN CHIMPANZEES AND BONOBOS, P102, DOI 10.1017/CBO9780511606397.011
   Musgrave S, 2016, SCI REP-UK, V6, DOI 10.1038/srep34783
   Nishie H, 2018, AM J PHYS ANTHROPOL, V165, P194, DOI 10.1002/ajpa.23327
   Poulsen JR, 2004, INT J PRIMATOL, V25, P285, DOI 10.1023/B:IJOP.0000019153.50161.58
   R Core Team, 2017, R LANG ENV STAT COMP
   Rovero F, 2017, BIODIVERS CONSERV, V26, P1103, DOI 10.1007/s10531-016-1288-2
   Schielzeth H, 2009, BEHAV ECOL, V20, P416, DOI 10.1093/beheco/arn145
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   Siegel S., 1988, NONPARAMETRIC STAT B, V2nd ed
   Sirianni G, 2018, ANIM COGN, V21, P109, DOI 10.1007/s10071-017-1144-0
   STANFORD CB, 1994, BEHAVIOUR, V131, P1, DOI 10.1163/156853994X00181
   Stumpf Rebecca M., 2011, PRIMATES PERSPECTIVE, P340
   Tan CL, 2013, PRIMATES, V54, P1, DOI 10.1007/s10329-012-0318-2
   Torralvo K, 2017, PRIMATES, V58, P279, DOI 10.1007/s10329-017-0603-1
   Treves A, 2010, BIOL CONSERV, V143, P521, DOI 10.1016/j.biocon.2009.11.025
   VanderWaal KL, 2009, ANIM BEHAV, V77, P949, DOI 10.1016/j.anbehav.2008.12.028
   WATTS DP, 1989, ETHOLOGY, V81, P1
   Wegge P, 2004, ANIM CONSERV, V7, P251, DOI 10.1017/S1367943004001441
   Williamson Elizabeth A., 2003, P25
   Wittig R. M., 2017, ENCY ANIMAL COGNITIO, P1, DOI DOI 10.1007/978-3-319-47829-6_1564-1
NR 56
TC 9
Z9 10
U1 1
U2 28
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD SEP
PY 2018
VL 80
IS 9
AR e22904
DI 10.1002/ajp.22904
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Zoology
GA GW0KC
UT WOS:000446552900003
PM 30088683
DA 2022-02-10
ER

PT C
AU Chalmers, C
   Fergus, P
   Wich, S
   Longmore, SN
AF Chalmers, C.
   Fergus, P.
   Wich, S.
   Longmore, S. N.
GP IEEE
TI Modelling Animal Biodiversity Using Acoustic Monitoring and Deep
   Learning
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
LA English
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
SP Int Neural Network Soc, IEEE Computat Intelligence Soc
DE Conservation; Audio Classification; Acoustic Monitoring; Modelling
   Biodiversity; Deep Learning
ID ENVIRONMENT
AB For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists have identified species by ear; however, it is now common to deploy audio recording technology to monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation and territorial defence. Animal sounds provide valuable information and help conservationists to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running for long durations to provide insights on habitat and the sounds made by animals and illegal activity. While this technology brings enormous benefits, the amount of data that is generated makes processing a time-consuming process for conservationists. Consequently, there is interest among conservationists to automatically process acoustic data to help speed up biodiversity assessments. Processing these large data sources and extracting relevant sounds from background noise introduces significant challenges. In this paper we outline an approach for achieving this using state of the art in machine learning to automatically extract features from time-series audio signals and modelling deep learning models to classify different bird species based on the sounds they make. The acquired bird songs are processed using mel-frequency cepstrum (MFC) to extract features which are later classified using a multilayer perceptron (MLP). Our proposed method achieved promising results with 0.74 sensitivity, 0.92 specificity and an accuracy of 0.74.
OI Wich, Serge/0000-0003-3954-5174
CR Ba Jimmy, 2013, ADV NEURAL INFORM PR, P3084
   Bardeli R, 2010, PATTERN RECOGN LETT, V31, P1524, DOI 10.1016/j.patrec.2009.09.014
   Berto BP, 2020, J PARASITOL, V106, P707, DOI 10.1645/19-148
   Briggs F, 2012, J ACOUST SOC AM, V131, P4640, DOI 10.1121/1.4707424
   Eibl M, 2017, CLEF WORKING NOTES
   Farina A., 2017, ECOACOUSTICS ECOLOGI
   Hill AP, 2019, HARDWAREX, V6, DOI 10.1016/j.ohx.2019.e00073
   Juanes F, 2018, J NAT CONSERV, V42, P7, DOI 10.1016/j.jnc.2018.01.003
   Kanai S, 2017, ADV NEUR IN, V30
   Knight EC, 2019, BIOACOUSTICS, V28, P539, DOI 10.1080/09524622.2018.1503971
   Lasseck M., 2018, C LABS EV FOR AV FRA
   Muda L., 2010, J COMPUTING, V2, P138
   Nanni L, 2020, EURASIP J AUDIO SPEE, V2020, DOI 10.1186/s13636-020-00175-3
   Nanni L, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101084
   Piczak K. J., 2015, 2015 IEEE 25 INT WOR, DOI DOI 10.1109/MLSP.2015.7324337
   Poma Y., 2020, HYBRID INTELLIGENT S, P71
   Prince P, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030553
   Sasmaz E, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P625, DOI 10.1109/UBMK.2018.8566449
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Teixeira D, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.72
   Wrege PH, 2017, METHODS ECOL EVOL, V8, P1292, DOI 10.1111/2041-210X.12730
NR 21
TC 0
Z9 0
U1 1
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2161-4393
BN 978-0-7381-3366-9
J9 IEEE IJCNN
PY 2021
DI 10.1109/IJCNN52387.2021.9534195
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BS4TO
UT WOS:000722581707020
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Zhang, Z
   He, ZH
   Cao, GT
   Cao, WM
AF Zhang, Zhi
   He, Zhihai
   Cao, Guitao
   Cao, Wenming
TI Animal Detection From Highly Cluttered Natural Scenes Using
   Spatiotemporal Object Region Proposals and Patch Verification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background modeling; camera-trap images; graph cut; object proposal;
   object verification
AB In this paper, we consider the animal object detection and segmentation from wildlife monitoring videos captured by motion-triggered cameras, called camera-traps. For these types of videos, existing approaches often suffer from low detection rates due to low contrast between the foreground animals and the cluttered background, as well as high false positive rates due to the dynamic background. To address this issue, we first develop a new approach to generate animal object region proposals using multilevel graph cut in the spatiotemporal domain. We then develop a cross-frame temporal patch verification method to determine if these region proposals are true animals or background patches. We construct an efficient feature description for animal detection using joint deep learning and histogram of oriented gradient features encoded with Fisher vectors. Our extensive experimental results and performance comparisons over a diverse set of challenging camera-trap data demonstrate that the proposed spatiotemporal object proposal and patch verification framework outperforms the state-of-the-art methods, including the recent Faster-RCNN method, on animal object detection accuracy by up to 4.5%.
C1 [Zhang, Zhi; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Cao, Guitao] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
   [Cao, Wenming] Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
RP Zhang, Z (corresponding author), Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM zzbhf@mail.missouri.edu; hezhi@missouri.edu; gtcao@sei.ecnu.edu.cn;
   caom@shenzhenu.edu.cn
RI He, Zhihai/A-5885-2019
FU National Science FoundationNational Science Foundation (NSF)
   [CyberSEES-1539389, CPS-1544794]; National Science Foundation of
   ChinaNational Natural Science Foundation of China (NSFC) [61375015]
FX This work was supported in part by the National Science Foundation under
   Grant CyberSEES-1539389 and Grant CPS-1544794. The work of W. Cao was
   supported in part by the National Science Foundation of China under
   Grant 61375015. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Alessandro Piva.
CR Allebosch G, 2015, LECT NOTES COMPUT SC, V9386, P130, DOI 10.1007/978-3-319-25903-1_12
   Bianco S., 2015, FAR CAN YOU GET COMB
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Davis L., 2000, COMPUTER VISION ECCV, P751, DOI DOI 10.1007/3-540-45053-X_48
   De Gregorio M, 2015, LECT NOTES COMPUT SC, V9281, P493, DOI 10.1007/978-3-319-23222-5_60
   Doretto G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1236
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everingham M., 2010, PASCAL VISUAL OBJECT
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Gregorio M. D., 2004, P IEEE C COMP VIS PA, P409
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kahl F, 2004, LECT NOTES COMPUT SC, V3247, P117
   Kays R., 2014, P N AM CONSERVATION, P80
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Ko T, 2008, LECT NOTES COMPUT SC, V5304, P276, DOI 10.1007/978-3-540-88690-7_21
   Krahenbuhl P., 2014, P EUR C COMPUT VIS, P725
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Ma BP, 2012, LECT NOTES COMPUT SC, V7583, P413, DOI 10.1007/978-3-642-33863-2_41
   Mahadevan V., 2008, P 2008 IEEE C COMP V, P1
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Oneata D, 2014, LECT NOTES COMPUT SC, V8691, P737, DOI 10.1007/978-3-319-10578-9_48
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Oreifej O, 2010, PROC CVPR IEEE, P709, DOI 10.1109/CVPR.2010.5540147
   Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369
   PERRONNIN F, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383266
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Philips W., 2015, INT JOINT C COMP VIS, P433
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Redmon J., 2015, CORR
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Ren Y, 2003, MACH VISION APPL, V13, P332, DOI 10.1007/s00138-002-0091-0
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet P., 2013, P INT C LEARN REPR D
   Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   St-Charles PL, 2014, IEEE COMPUT SOC CONF, P414, DOI 10.1109/CVPRW.2014.67
   Sun J, 2006, LECT NOTES COMPUT SC, V3952, P628
   Szegedy C., 2013, ADV NEURAL INF PROCE, V26, P2553, DOI DOI 10.5555/2999792.2999897
   Szegedy C., 2014, SCALABLE HIGH QUALIT
   Tilak S., 2011, INT J RES REV WIRELE, V1, P19
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang B, 2014, IEEE COMPUT SOC CONF, P401, DOI 10.1109/CVPRW.2014.64
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 52
TC 35
Z9 36
U1 2
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2079
EP 2092
DI 10.1109/TMM.2016.2594138
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800014
OA hybrid
DA 2022-02-10
ER

PT J
AU Green, AM
   Chynoweth, MW
   Sekercioglu, CH
AF Green, Austin M.
   Chynoweth, Mark W.
   Sekercioglu, Cagan Hakki
TI Spatially Explicit Capture-Recapture Through Camera Trapping: A Review
   of Benchmark Analyses for Wildlife Density Estimation
SO FRONTIERS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE citizen science; conservation biology; biodiversity monitoring; mammals;
   Carnivora; wildlife ecology; density estimation
ID TIGER PANTHERA-TIGRIS; CITIZEN SCIENCE; POPULATION; TRAPS; FOREST;
   BIODIVERSITY; RECOMMENDATIONS; PARAMETERS; PRINCIPLES; OCCUPANCY
AB Camera traps have become an important research tool for both conservation biologists and wildlife managers. Recent advances in spatially explicit capture-recapture (SECR) methods have increasingly put camera traps at the forefront of population monitoring programs. These methods allow for benchmark analysis of species density without the need for invasive fieldwork techniques. We conducted a review of SECR studies using camera traps to summarize the current focus of these investigations, as well as provide recommendations for future studies and identify areas in need of future investigation. Our analysis shows a strong bias in species preference, with a large proportion of studies focusing on large felids, many of which provide the only baseline estimates of population density for these species. Furthermore, we found that a majority of studies produced density estimates that may not be precise enough for long-term population monitoring. We recommend simulation and power analysis be conducted before initiating any particular study design and provide examples using readily available software. Furthermore, we show that precision can be increased by including a larger study area that will subsequently increase the number of individuals photo-captured. As many current studies lack the resources or manpower to accomplish such an increase in effort, we recommend that researchers incorporate new technologies such as machine-learning, web-based data entry, and online deployment management into their study design. We also cautiously recommend the potential of citizen science to help address these study design concerns. In addition, modifications in SECR model development to include species that have only a subset of individuals available for individual identification (often called mark-resight models), can extend the process of explicit density estimation through camera trapping to species not individually identifiable.
C1 [Green, Austin M.; Sekercioglu, Cagan Hakki] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Chynoweth, Mark W.] Utah State Univ Uintah Basin, Dept Wildland Resources, Vernal, UT USA.
   [Sekercioglu, Cagan Hakki] Koc Univ, Coll Sci, Istanbul, Turkey.
RP Green, AM (corresponding author), Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
EM austin.m.green@utah.edu
OI Sekercioglu, Cagan H./0000-0003-3193-0377
FU Global Change and Sustainability Center at the University of Utah
FX AG would like to thank the Global Change and Sustainability Center at
   the University of Utah for supporting this work. CS thanks Hamit Batubay
   ozkan and Barbara J. Watkins for their generous support. The authors
   would like to thank Roland Kays, Adam Duarte, a reviewer and the
   handling editor for their helpful comments. Their revisions greatly
   improved the quality of the manuscript.
CR Abolafya M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068037
   Aceves-Bueno E., 2017, B ECOL SOC AM, V98, P278, DOI [10.1002/bes2.1336, DOI 10.1002/BES2.1336]
   Adler FR, 2020, ANN NY ACAD SCI, V1469, P52, DOI 10.1111/nyas.14340
   Alexandrino E.R., 2019, CITIZ SCI THEORY PRA, V4, P1, DOI DOI 10.5334/CSTP.198
   Beschta RL, 2009, BIOL CONSERV, V142, P2401, DOI 10.1016/j.biocon.2009.06.015
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Bowser A., 2018, CITIZEN SCI INNOVATI, DOI [10.2307/j.ctv550cf2, DOI 10.2307/J.CTV550CF2]
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cardinale BJ, 2006, NATURE, V443, P989, DOI 10.1038/nature05202
   Chandler M, 2017, BIOL CONSERV, V213, P280, DOI 10.1016/j.biocon.2016.09.004
   Conrad CC, 2011, ENVIRON MONIT ASSESS, V176, P273, DOI 10.1007/s10661-010-1582-5
   Dalerum F, 2009, BIOL LETTERS, V5, P35, DOI 10.1098/rsbl.2008.0520
   Dalerum F, 2008, BIODIVERS CONSERV, V17, P2939, DOI 10.1007/s10531-008-9406-4
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Devictor V, 2010, DIVERS DISTRIB, V16, P354, DOI 10.1111/j.1472-4642.2009.00615.x
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Dillon A, 2008, J ZOOL, V275, P391, DOI 10.1111/j.1469-7998.2008.00452.x
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford M., 2010, SECR 4 1 SPATIALLY E
   Efford M. G., 2019, SECRDESIGNAPP 1 3 IN
   Efford MG, 2019, METHODS ECOL EVOL, V10, P1529, DOI 10.1111/2041-210X.13239
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Efford MG, 2011, ECOLOGY, V92, P2202, DOI 10.1890/11-0332.1
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Ergon T, 2014, METHODS ECOL EVOL, V5, P1327, DOI 10.1111/2041-210X.12133
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Gallo T, 2011, BIOSCIENCE, V61, P459, DOI 10.1525/bio.2011.61.6.8
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Gardner B, 2010, J WILDLIFE MANAGE, V74, P318, DOI 10.2193/2009-101
   GERRODETTE T, 1993, WILDLIFE SOC B, V21, P515
   GERRODETTE T, 1987, ECOLOGY, V68, P1364, DOI 10.2307/1939220
   Gilbert NA, 2021, CONSERV BIOL, V35, P88, DOI 10.1111/cobi.13517
   Glen AS, 2003, WILDLIFE RES, V30, P29, DOI 10.1071/WR01059
   Gopal R, 2010, ORYX, V44, P383, DOI 10.1017/S0030605310000529
   Gopalaswamy AM, 2012, METHODS ECOL EVOL, V3, P1067, DOI 10.1111/j.2041-210X.2012.00241.x
   GRIFFITHS M, 1993, CONSERV BIOL, V7, P623, DOI 10.1046/j.1523-1739.1993.07030623.x
   Harihar A, 2009, EUR J WILDLIFE RES, V55, P97, DOI 10.1007/s10344-008-0219-2
   Hawthorne TL, 2015, APPL GEOGR, V56, P187, DOI 10.1016/j.apgeog.2014.10.005
   Hirakawa Hirofumi, 2005, Mammal Study, V30, P69, DOI 10.3106/1348-6160(2005)30[69:LBTTCA]2.0.CO;2
   Hooper DU, 2012, NATURE, V486, P105, DOI 10.1038/nature11118
   Horns JJ, 2018, BIOL CONSERV, V221, P151, DOI 10.1016/j.biocon.2018.02.027
   Jarvis RM, 2015, MAR POLICY, V57, P21, DOI 10.1016/j.marpol.2015.03.011
   Jimenez J, 2017, SCI REP-UK, V7, DOI 10.1038/srep41036
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Laundre J.W., 2010, OPEN ECOL J, V3, P1
   Linden DW, 2017, J APPL ECOL, V54, P2043, DOI 10.1111/1365-2664.12883
   Linkie M, 2006, J APPL ECOL, V43, P576, DOI 10.1111/j.1365-2664.2006.01153.x
   Loock DJE, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-34936-0
   Luskin MS, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01656-4
   Lyra-Jorge MC, 2008, EUR J WILDLIFE RES, V54, P739, DOI 10.1007/s10344-008-0205-8
   McClintock BT, 2012, ECOL MONOGR, V82, P335, DOI 10.1890/11-0326.1
   McClintock BT, 2009, BIOMETRICS, V65, P237, DOI 10.1111/j.1541-0420.2008.01047.x
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Neate-Clegg MHC, 2020, BIOL CONSERV, V248, DOI 10.1016/j.biocon.2020.108653
   Nichols JD, 2014, SPATIAL CAPTURE-RECAPTURE, P125, DOI 10.1016/B978-0-12-405939-9.00005-0
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   O'Connell AF, 2006, J WILDLIFE MANAGE, V70, P1625, DOI 10.2193/0022-541X(2006)70[1625:ESOADP]2.0.CO;2
   Obbard ME, 2010, J APPL ECOL, V47, P76, DOI 10.1111/j.1365-2664.2009.01758.x
   OTIS DL, 1978, WILDLIFE MONOGR, P1
   Paviolo A, 2008, ORYX, V42, P554, DOI 10.1017/S0030605308000641
   Pesenti E, 2013, J MAMMAL, V94, P73, DOI 10.1644/11-MAMM-A-322.1
   Petersen Wyatt Joseph, 2019, Journal of Threatened Taxa, V11, P13448, DOI 10.11609/jott.4553.11.4.13448-13458
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Roberts Nathan James, 2011, Bioscience Horizons, V4, P40, DOI 10.1093/biohorizons/hzr006
   Rotman D., 2012, P ACM 2012 C COMP SU, P217, DOI DOI 10.1145/2145204.2145238
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Royle J.A., 2008, HIERARCHICAL MODELIN
   Royle JA, 2008, ECOLOGY, V89, P2281, DOI 10.1890/07-0601.1
   Royle JA, 2018, ECOGRAPHY, V41, P444, DOI 10.1111/ecog.03170
   Royle JA, 2016, POPUL ECOL, V58, P53, DOI 10.1007/s10144-015-0524-z
   Royle JA, 2011, J WILDLIFE MANAGE, V75, P604, DOI 10.1002/jwmg.79
   Royle JA, 2009, ECOLOGY, V90, P3233, DOI 10.1890/08-1481.1
   Sauermann H, 2015, P NATL ACAD SCI USA, V112, P679, DOI 10.1073/pnas.1408907112
   Schaub M, 2014, METHODS ECOL EVOL, V5, P1316, DOI 10.1111/2041-210X.12134
   SEIDENSTICKER J, 1993, SYM ZOOL S, P105
   Seymour V., 2017, CITIZ, V2, P5, DOI [10.5334/cstp.66, DOI 10.5334/CSTP.66]
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Sullivan BL, 2017, BIOL CONSERV, V208, P5, DOI 10.1016/j.biocon.2016.04.031
   Tobler MW, 2013, BIOL CONSERV, V159, P109, DOI 10.1016/j.biocon.2012.12.009
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Turner A., 1997, BIG CATS THEIR FOSSI
   Vann-Sander S, 2016, MAR POLICY, V72, P82, DOI 10.1016/j.marpol.2016.06.026
   Venter O, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12558
   Wald DM, 2016, CONSERV BIOL, V30, P562, DOI 10.1111/cobi.12627
   Welbourne DJ, 2016, REMOTE SENS ECOL CON, V2, P77, DOI 10.1002/rse2.20
   White G., 1982, CAPTURE RECAPTURE RE
   Whittington J, 2018, J APPL ECOL, V55, P157, DOI 10.1111/1365-2664.12954
NR 92
TC 10
Z9 10
U1 8
U2 20
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 2296-701X
J9 FRONT ECOL EVOL
JI Front. Ecol. Evol.
PD DEC 18
PY 2020
VL 8
AR 563477
DI 10.3389/fevo.2020.563477
PG 11
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA PM8KV
UT WOS:000604041600001
OA gold, Green Published
DA 2022-02-10
ER

PT C
AU Loos, A
   Weigel, C
   Koehler, M
AF Loos, Alexander
   Weigel, Christian
   Koehler, Mona
GP IEEE
TI Towards Automatic Detection of Animals in Camera-Trap Images
SO 2018 26TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)
SE European Signal Processing Conference
LA English
DT Proceedings Paper
CT European Signal Processing Conference (EUSIPCO)
CY SEP 03-07, 2018
CL Rome, ITALY
SP European Assoc Signal Processing, IEEE Signal Processing Soc, ROMA TRE Univ Degli Studi, MathWorks, Amazon Devices
AB In recent years the world's biodiversity is declining on an unprecedented scale. Many species are endangered and remaining populations need to be protected. To overcome this agitating issue, biologist started to use remote camera devices for wildlife monitoring and estimation of remaining population sizes. Unfortunately, the huge amount of data makes the necessary manual analysis extremely tedious and highly cost intensive. In this paper we re-train and apply two state-of-the-art deep-learning based object detectors to localize and classify Serengeti animals in camera-trap images. Furthermore, we thoroughly evaluate both algorithms on a self-established dataset and show that the combination of the results of both detectors can enhance overall mean average precision. In contrast to previous work our approach is not only capable of classifying the main species in images but can also detect them and therefore count the number of individuals which is in fact an important information for biologists, ecologists, and wildlife epidemiologists.
C1 [Loos, Alexander; Weigel, Christian; Koehler, Mona] Fraunhofer IDMT, Metadata Audio Visual Syst, D-98693 Ilmenau, Germany.
RP Loos, A (corresponding author), Fraunhofer IDMT, Metadata Audio Visual Syst, D-98693 Ilmenau, Germany.
EM alexander.loos@idmt.fraunhofer.de; christian.weigel@idmt.fraunhofer.de;
   mona.koehler@idmt.fraunhofer.de
CR Balancap Paul, 2016, GITHUB REPOSITORY
   Bowyer Alex, 2015, HUMAN COMPUTATION CR
   Burghardt T., 2008, THESIS
   DEMPSTER AP, 1967, ANN MATH STAT, V38, P325, DOI 10.1214/aoms/1177698950
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Figueroa Karina, 2014, FAST AUTOMATIC DETEC, P940
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hoiem D., 2012, EUR C COMP VIS ECCV, P5
   King DB, 2015, ACS SYM SER, V1214, P1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Lee H, 2016, IEEE WINT CONF APPL
   Li Dong, 2016, C COMP VIS PATT REC
   Lin Tsung-Yi, 2014, 13 EUR C COMP VIS EC
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Norouzzadeh Mohammad Sadegh, 2017, CORR
   Redmon J., 2016, ARXIV161208242
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Teh E. W., 2016, BRIT MACH VIS C BMVC
   Tieleman T., 2012, COURSERA NEURAL NETW
   Trieu Trinh Hoang, 2016, DARKFL GITHUB REP
   Vie J., 2009, IUCN
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang Zhi, 2016, ANIMAL DETECTION HIG, V18, P1
   Zooniverse, 2017, COMP VIS SER
NR 27
TC 2
Z9 2
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2076-1465
BN 978-90-827970-1-5
J9 EUR SIGNAL PR CONF
PY 2018
BP 1805
EP 1809
PG 5
WC Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering
GA BL7TU
UT WOS:000455614900363
DA 2022-02-10
ER

PT J
AU Cove, MV
   Kays, R
   Bontrager, H
   Bresnan, C
   Lasky, M
   Frerichs, T
   Klann, R
   Lee, TE
   Crockett, SC
   Crupi, AP
   Weiss, KCB
   Rowe, H
   Sprague, T
   Schipper, J
   Tellez, C
   Lepczyk, CA
   Fantle-Lepczyk, JE
   Lapoint, S
   Williamson, J
   Fisher-Reid, MC
   King, SM
   Bebko, AJ
   Chrysafis, P
   Jensen, AJ
   Jachowski, DS
   Sands, J
   MacCombie, KA
   Herrera, DJ
   van der Merwe, M
   Knowles, TW
   Horan, RV
   Rentz, MS
   Brandt, LSE
   Nagy, C
   Barton, BT
   Thompson, WC
   Maher, SP
   Darracq, AK
   Hess, G
   Parsons, AW
   Wells, B
   Roemer, GW
   Hernandez, CJ
   Gompper, ME
   Webb, SL
   Vanek, JP
   Lafferty, DJR
   Bergquist, AM
   Hubbard, T
   Forrester, T
   Clark, D
   Cincotta, C
   Favreau, J
   Facka, AN
   Halbur, M
   Hammerich, S
   Gray, M
   Rega-Brodsky, CC
   Durbin, C
   Flaherty, EA
   Brooke, JM
   Coster, SS
   Lathrop, RG
   Russell, K
   Bogan, DA
   Cliche, R
   Shamon, H
   Hawkins, MTR
   Marks, SB
   Lonsinger, RC
   O'Mara, MT
   Compton, JA
   Fowler, M
   Barthelmess, EL
   Andy, KE
   Belant, JL
   Beyer, DE
   Kautz, TM
   Scognamillo, DG
   Schalk, CM
   Leslie, MS
   Nasrallah, SL
   Ellison, CN
   Ruthven, C
   Fritts, S
   Tleimat, J
   Gay, M
   Whittier, CA
   Neiswenter, SA
   Pelletier, R
   DeGregorio, BA
   Kuprewicz, EK
   Davis, ML
   Dykstra, A
   Mason, DS
   Baruzzi, C
   Lashley, MA
   Risch, DR
   Price, MR
   Allen, ML
   Whipple, LS
   Sperry, JH
   Hagen, RH
   Mortelliti, A
   Evans, BE
   Studds, CE
   Siren, APK
   Kilborn, J
   Sutherland, C
   Warren, P
   Fuller, T
   Harris, NC
   Carter, NH
   Trout, E
   Zimova, M
   Giery, ST
   Iannarilli, F
   Higdon, SD
   Revord, RS
   Hansen, CP
   Millspaugh, JJ
   Zorn, A
   Benson, JF
   Wehr, NH
   Solberg, JN
   Gerber, BD
   Burr, JC
   Sevin, J
   Green, AM
   Sekercioglu, CH
   Pendergast, M
   Barnick, KA
   Edelman, AJ
   Wasdin, JR
   Romero, A
   O'Neill, BJ
   Schmitz, N
   Alston, JM
   Kuhn, KM
   Lesmeister, DB
   Linnell, MA
   Appel, CL
   Rota, C
   Stenglein, JL
   Anhalt-Depies, C
   Nelson, C
   Long, RA
   Jaspers, KJ
   Remine, KR
   Jordan, MJ
   Davis, D
   Hernandez-Yanez, H
   Zhao, JY
   McShea, AJ
AF Cove, Michael V.
   Kays, Roland
   Bontrager, Helen
   Bresnan, Claire
   Lasky, Monica
   Frerichs, Taylor
   Klann, Renee
   Lee, Thomas E., Jr.
   Crockett, Seth C.
   Crupi, Anthony P.
   Weiss, Katherine C. B.
   Rowe, Helen
   Sprague, Tiffany
   Schipper, Jan
   Tellez, Chelsey
   Lepczyk, Christopher A.
   Fantle-Lepczyk, Jean E.
   Lapoint, Scott
   Williamson, Jacque
   Fisher-Reid, M. Caitlin
   King, Sean M.
   Bebko, Alexandra J.
   Chrysafis, Petros
   Jensen, Alex J.
   Jachowski, David S.
   Sands, Joshua
   MacCombie, Kelly Anne
   Herrera, Daniel J.
   van der Merwe, Marius
   Knowles, Travis W.
   Horan, Robert V., III
   Rentz, Michael S.
   Brandt, LaRoy S. E.
   Nagy, Christopher
   Barton, Brandon T.
   Thompson, Weston C.
   Maher, Sean P.
   Darracq, Andrea K.
   Hess, George
   Parsons, Arielle W.
   Wells, Brenna
   Roemer, Gary W.
   Hernandez, Cristian J.
   Gompper, Matthew E.
   Webb, Stephen L.
   Vanek, John P.
   Lafferty, Diana J. R.
   Bergquist, Amelia M.
   Hubbard, Tru
   Forrester, Tavis
   Clark, Darren
   Cincotta, Connor
   Favreau, Jorie
   Facka, Aaron N.
   Halbur, Michelle
   Hammerich, Steven
   Gray, Morgan
   Rega-Brodsky, Christine C.
   Durbin, Caleb
   Flaherty, Elizabeth A.
   Brooke, Jarred M.
   Coster, Stephanie S.
   Lathrop, Richard G.
   Russell, Katarina
   Bogan, Daniel A.
   Cliche, Rachel
   Shamon, Hila
   Hawkins, Melissa T. R.
   Marks, Sharyn B.
   Lonsinger, Robert C.
   O'Mara, M. Teague
   Compton, Justin A.
   Fowler, Melinda
   Barthelmess, Erika L.
   Andy, Katherine E.
   Belant, Jerrold L.
   Beyer, Dean E., Jr.
   Kautz, Todd M.
   Scognamillo, Daniel G.
   Schalk, Christopher M.
   Leslie, Matthew S.
   Nasrallah, Sophie L.
   Ellison, Caroline N.
   Ruthven, Chip
   Fritts, Sarah
   Tleimat, Jaquelyn
   Gay, Mandy
   Whittier, Christopher A.
   Neiswenter, Sean A.
   Pelletier, Robert
   DeGregorio, Brett A.
   Kuprewicz, Erin K.
   Davis, Miranda L.
   Dykstra, Adrienne
   Mason, David S.
   Baruzzi, Carolina
   Lashley, Marcus A.
   Risch, Derek R.
   Price, Melissa R.
   Allen, Maximilian L.
   Whipple, Laura S.
   Sperry, Jinelle H.
   Hagen, Robert H.
   Mortelliti, Alessio
   Evans, Bryn E.
   Studds, Colin E.
   Siren, Alexej P. K.
   Kilborn, Jillian
   Sutherland, Chris
   Warren, Paige
   Fuller, Todd
   Harris, Nyeema C.
   Carter, Neil H.
   Trout, Edward
   Zimova, Marketa
   Giery, Sean T.
   Iannarilli, Fabiola
   Higdon, Summer D.
   Revord, Ronald S.
   Hansen, Christopher P.
   Millspaugh, Joshua J.
   Zorn, Adam
   Benson, John F.
   Wehr, Nathaniel H.
   Solberg, Jaylin N.
   Gerber, Brian D.
   Burr, Jessica C.
   Sevin, Jennifer
   Green, Austin M.
   Sekercioglu, Cagan H.
   Pendergast, Mary
   Barnick, Kelsey A.
   Edelman, Andrew J.
   Wasdin, Joanne R.
   Romero, Andrea
   O'Neill, Brian J.
   Schmitz, Noel
   Alston, Jesse M.
   Kuhn, Kellie M.
   Lesmeister, Damon B.
   Linnell, Mark A.
   Appel, Cara L.
   Rota, Christopher
   Stenglein, Jennifer L.
   Anhalt-Depies, Christine
   Nelson, Carrie
   Long, Robert A.
   Jaspers, Kodi Jo
   Remine, Kathryn R.
   Jordan, Mark J.
   Davis, Daniel
   Hernandez-Yanez, Haydee
   Zhao, Jennifer Y.
   McShea, Andwilliam J.
TI SNAPSHOT USA 2019: a coordinated national camera trap survey of the
   United States
SO ECOLOGY
LA English
DT Article; Data Paper
DE biodiversity; biogeography; camera traps; carnivora; Cetartiodactyla;
   Cingulata; Didelphimorphia; Lagomorpha; mammals; occupancy modeling;
   Rodentia; species distribution modeling
AB With the accelerating pace of global change, it is imperative that we obtain rapid inventories of the status and distribution of wildlife for ecological inferences and conservation planning. To address this challenge, we launched the SNAPSHOT USA project, a collaborative survey of terrestrial wildlife populations using camera traps across the United States. For our first annual survey, we compiled data across all 50 states during a 14-week period (17 August-24 November of 2019). We sampled wildlife at 1,509 camera trap sites from 110 camera trap arrays covering 12 different ecoregions across four development zones. This effort resulted in 166,036 unique detections of 83 species of mammals and 17 species of birds. All images were processed through the Smithsonian's eMammal camera trap data repository and included an expert review phase to ensure taxonomic accuracy of data, resulting in each picture being reviewed at least twice. The results represent a timely and standardized camera trap survey of the United States. All of the 2019 survey data are made available herein. We are currently repeating surveys in fall 2020, opening up the opportunity to other institutions and cooperators to expand coverage of all the urban-wild gradients and ecophysiographic regions of the country. Future data will be available as the database is updated at eMammal.si.edu/snapshot-usa, as will future data paper submissions. These data will be useful for local and macroecological research including the examination of community assembly, effects of environmental and anthropogenic landscape variables, effects of fragmentation and extinction debt dynamics, as well as species-specific population dynamics and conservation action plans. There are no copyright restrictions; please cite this paper when using the data for publication.
C1 [Cove, Michael V.; Bontrager, Helen; Bresnan, Claire; Frerichs, Taylor; Klann, Renee] Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
   [Kays, Roland] North Carolina Museum Nat Sci, Raleigh, NC 27601 USA.
   [Kays, Roland; Lasky, Monica] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
   [Lee, Thomas E., Jr.; Crockett, Seth C.] Abilene Christian Univ, Dept Biol, Abilene, TX 79601 USA.
   [Crupi, Anthony P.] Alaska Dept Fish & Game, Div Wildlife Conservat, Douglas, AK 99824 USA.
   [Weiss, Katherine C. B.; Tellez, Chelsey] Arizona State Univ, Tempe, AZ 85281 USA.
   [Weiss, Katherine C. B.; Schipper, Jan; Tellez, Chelsey] Arizona Ctr Nat Conservat Phoenix Zoo, Field Conservat Res Dept, 455 N Galvin Pkwy, Phoenix, AZ 85008 USA.
   [Rowe, Helen; Sprague, Tiffany] McDowell Sonoran Conservancy, 7729 East Greenway Rd,Suite 100, Scottsdale, AZ 85260 USA.
   [Lepczyk, Christopher A.; Fantle-Lepczyk, Jean E.] Auburn Univ, Sch Forestry & Wildlife Sci, Auburn, AL 36849 USA.
   [Lapoint, Scott] Black Rock Forest, 65 Reservoir Rd, Carnwall, NY 12518 USA.
   [Williamson, Jacque] Brandywine Zoo Delaware State Parks, Dept Educ & Conservat, Wilmington, DC USA.
   [Fisher-Reid, M. Caitlin; King, Sean M.; Bebko, Alexandra J.] Bridgewater State Univ, Dept Biol Sci, Bridgewater, MA 02325 USA.
   [Chrysafis, Petros] Calif State Univ Fresno, Fresno, CA 93740 USA.
   [Jensen, Alex J.; Jachowski, David S.] Clemson Univ, Dept Forestry & Environm Conservat, Clemson, SC 29631 USA.
   [Sands, Joshua; MacCombie, Kelly Anne] Crocodile Lake Natl Wildlife Refuge, Key Largo, FL 33037 USA.
   [Herrera, Daniel J.] DC Cat Count Humane Rescue Alliance, Washington, DC 20011 USA.
   [van der Merwe, Marius] Dixie State Univ, Dept Biol Sci, St George, UT 84770 USA.
   [Knowles, Travis W.] Francis Marion Univ, Dept Biol, Florence, SC 29502 USA.
   [Horan, Robert V., III] Georgia Dept Nat Resources, Wildlife Resources Div, Brunswick, GA 31520 USA.
   [Rentz, Michael S.] Iowa State Univ, Nat Resource Ecol & Management, Ames, IA 50011 USA.
   [Brandt, LaRoy S. E.] Lincoln Mem Univ, Cumberland Mt Res Ctr, Harrogate, TN 37752 USA.
   [Nagy, Christopher] Mianus River Gorge, Bedford, NY 10506 USA.
   [Barton, Brandon T.; Thompson, Weston C.] Mississippi State Univ, Dept Biol Sci, Mississippi State, MS 39762 USA.
   [Maher, Sean P.] Missouri State Univ, Dept Biol, Springfield, MO 65897 USA.
   [Darracq, Andrea K.] Murray State Univ, Dept Biol, Murray, KY 42071 USA.
   [Hess, George; Parsons, Arielle W.; Wells, Brenna] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
   [Roemer, Gary W.; Hernandez, Cristian J.; Gompper, Matthew E.] New Mexico State Univ, Dept Fish Wildlife & Conservat Ecol, Las Cruces, NM 88003 USA.
   [Webb, Stephen L.] Noble Res Inst LLC, 2510 Sam Noble Pkwy, Ardmore, OK 73401 USA.
   [Vanek, John P.] Northern Illinois Univ, Dept Biol Sci, De Kalb, IL 60115 USA.
   [Lafferty, Diana J. R.; Bergquist, Amelia M.; Hubbard, Tru] Northern Michigan Univ, Dept Biol, Wildlife Ecol & Conservat Sci Lab, Marqeutte, MI 49855 USA.
   [Forrester, Tavis; Clark, Darren] Oregon Dept Fish & Wildlife, La Grande, OR 97850 USA.
   [Cincotta, Connor; Favreau, Jorie] Paul Smiths Coll, Paul Smiths, NY 12970 USA.
   [Facka, Aaron N.] Penn Game Commiss, Harrisburg, PA 17110 USA.
   [Halbur, Michelle; Hammerich, Steven; Gray, Morgan] Pepperwood Fdn, 2130 Pepperwood Preserve Rd, Santa Rosa, CA 95404 USA.
   [Rega-Brodsky, Christine C.; Durbin, Caleb] Pittsburg State Univ, Dept Biol, 1701 S Broadway, Pittsburg, KS 66762 USA.
   [Flaherty, Elizabeth A.; Brooke, Jarred M.] Purdue Univ, Dept Forestry & Nat Resources, W Lafayette, IN 47907 USA.
   [Coster, Stephanie S.] Randolph Macon Coll, Dept Biol, Ashland, VA 23005 USA.
   [Lathrop, Richard G.; Russell, Katarina] Rutgers State Univ, Dept Ecol Evolut & Nat Resources, New Brunswick, NJ 08901 USA.
   [Bogan, Daniel A.] Siena Coll, Dept Environm Studies & Sci, 515 Loudon Rd, New York, NY 12211 USA.
   [Cliche, Rachel] Silvio O Conte Natl Fish & Wildlife Refuge, Brunswick, VT 05905 USA.
   [Shamon, Hila] Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
   [Hawkins, Melissa T. R.] Natl Museum Nat Hist, Smithsonian Inst, Div Mammals, Dept Vertebrate Zool, Washington, DC 20560 USA.
   [Hawkins, Melissa T. R.; Marks, Sharyn B.] Humboldt State Univ, Dept Biol, 1 Harpst St, Arcata, CA 95521 USA.
   [Lonsinger, Robert C.] South Dakota State Univ, Dept Nat Resource Management, 1390 Coll Ave, Brookings, SD 57007 USA.
   [O'Mara, M. Teague] Southeastern Louisiana Univ, Dept Biol Sci, 808 N Pine St, Hammond, LA 70402 USA.
   [Compton, Justin A.; Fowler, Melinda] Springfield Coll, Dept Biol & Chem, Springfield, MA 01109 USA.
   [Barthelmess, Erika L.] St Lawrence Univ, Biol Dept, Canton, NY 13617 USA.
   [Barthelmess, Erika L.; Andy, Katherine E.] St Lawrence Univ, Nat North Program, Canton, NY 13617 USA.
   [Belant, Jerrold L.; Kautz, Todd M.] SUNY Coll Environm Sci & Forestry, Global Wildlife Conservat Ctr, Syracuse, NY 13210 USA.
   [Beyer, Dean E., Jr.] Michigan Dept Nat Resources, Wildlife Div, Lansing, MI 48909 USA.
   [Scognamillo, Daniel G.; Schalk, Christopher M.] Stephen F Austin State Univ, Arthur Temple Coll Forestry & Agr, Nacogdoches, TX 75962 USA.
   [Leslie, Matthew S.; Nasrallah, Sophie L.] Swarthmore Coll, Dept Biol, 500 Coll Ave, Swarthmore, PA 19081 USA.
   [Ellison, Caroline N.; Ruthven, Chip] Texas Parks & Wildlife Dept, Paducah, TX 79248 USA.
   [Fritts, Sarah; Tleimat, Jaquelyn; Gay, Mandy] Texas State Univ, Dept Biol, San Marcos, TX 78666 USA.
   [Whittier, Christopher A.] Tufts Univ, Cummings Sch Vet Med, Tufts Ctr Conservat Med, North Grafton, MA 01536 USA.
   [Neiswenter, Sean A.; Pelletier, Robert] Univ Nevada, Sch Life Sci, 4505 Maryland Pkwy, Las Vegas, NV 89154 USA.
   [DeGregorio, Brett A.] Univ Arkansas, US Geol Survey Fish & Wildlife Cooperat Res Unit, Fayetteville, AR 72701 USA.
   [Kuprewicz, Erin K.; Davis, Miranda L.] Univ Connecticut, Dept Ecol & Evolutionary Biol, Storrs, CT 06269 USA.
   [Dykstra, Adrienne; Mason, David S.] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.
   [Baruzzi, Carolina] Univ Florida, Sch Forest Resources & Conservat, Gainesville, FL 32611 USA.
   [Lashley, Marcus A.] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.
   [Risch, Derek R.; Price, Melissa R.] Univ Hawaii Manoa, Dept Nat Resources & Environm Management, Honolulu, HI 96822 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Allen, Maximilian L.; Whipple, Laura S.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin Ave, Urbana, IL 61801 USA.
   [Sperry, Jinelle H.] Engn Res & Dev Ctr, 2902 Newmark Dr, Champaign, IL 61826 USA.
   [Hagen, Robert H.] Univ Kansas, Environm Studies Program, Lawrence, KS 66045 USA.
   [Mortelliti, Alessio; Evans, Bryn E.] Univ Maine, Dept Wildlife Fisheries & Conservat Biol, 5755 Nutting Hall, Orono, ME 04469 USA.
   [Studds, Colin E.] Univ Maryland Baltimore Cty, Dept Geog & Environm Syst, Baltimore, MD 21250 USA.
   [Siren, Alexej P. K.] Univ Massachusetts, Dept Environm Conservat, Amherst, MA 01003 USA.
   [Kilborn, Jillian] New Hampshire Fish & Game Dept, Concord, NH 03301 USA.
   [Sutherland, Chris; Warren, Paige; Fuller, Todd] Univ Massachusetts, Dept Environm Conservat, Amherst, MA 01003 USA.
   [Harris, Nyeema C.] Univ Michigan, Appl Wildlife Ecol Lab, Ecol & Evolutionary Biol, Ann Arbor, MI 48109 USA.
   [Carter, Neil H.] Univ Michigan, Sch Environm & Sustainabil, Ann Arbor, MI 48109 USA.
   [Trout, Edward] Boise State Univ, Human Environm Syst, Boise, ID 83725 USA.
   [Zimova, Marketa] Univ Michigan, Sch Environm & Sustainabil, Ann Arbor, MI 48109 USA.
   [Giery, Sean T.] Penn State Univ, Eberly Coll Sci, Dept Biol, University Pk, PA 16802 USA.
   [Iannarilli, Fabiola] Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, St Paul, MN 55108 USA.
   [Higdon, Summer D.; Revord, Ronald S.] Univ Missouri, Ctr Agroforestry, 302 Anheuser Busch Nat Resources Bldg, Columbia, MO 65211 USA.
   [Hansen, Christopher P.; Millspaugh, Joshua J.] Univ Montana, WA Franke Coll Forestry & Conservat, Wildlife Biol Program, Missoula, MT 59812 USA.
   [Zorn, Adam] Univ Mt Union, Huston Brumbaugh Nat Ctr, Alliance, OH 44601 USA.
   [Benson, John F.; Wehr, Nathaniel H.] Univ Nebraska, Sch Nat Resources, Lincoln, NE 68583 USA.
   [Solberg, Jaylin N.] Univ North Dakota, Dept Biol, 10 Cornell St,Stop 9019, Grand Forks, ND 58202 USA.
   [Gerber, Brian D.; Burr, Jessica C.] Univ Rhode Isl, Dept Nat Resources Sci, Kingston, RI 02881 USA.
   [Sevin, Jennifer] Univ Richmond, Dept Biol, Richmond, VA 23173 USA.
   [Green, Austin M.] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Sekercioglu, Cagan H.] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Sekercioglu, Cagan H.] Koc Univ, Coll Sci, Istanbul, Sariyer, Turkey.
   [Pendergast, Mary] Wild Utah Project, Salt Lake City, UT 84101 USA.
   [Barnick, Kelsey A.] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Edelman, Andrew J.; Wasdin, Joanne R.] Univ West Georgia, Dept Biol, Carrollton, GA 30118 USA.
   [Romero, Andrea; O'Neill, Brian J.; Schmitz, Noel] Univ Wisconsin Whitewater, Dept Geog Geol & Environm Studies, Dept Biol Sci, Whitewater, WI 53190 USA.
   [Alston, Jesse M.] Univ Wyoming, Dept Zool & Physiol, Program Ecol, Laramie, WY 82071 USA.
   [Kuhn, Kellie M.] US Air Force Acad, Deparment Biol, USAFA, Colorado Springs, CO 80840 USA.
   [Lesmeister, Damon B.; Linnell, Mark A.] USDA Forest Serv, Pacific Northwest Res Stn, Portland, OR 97204 USA.
   [Appel, Cara L.] Oregon State Univ, Dept Fisheries & Wildlife, Corvallis, OR 97331 USA.
   [Rota, Christopher] West Virginia Univ, Div Forestry & Nat Resources, Morgantown, WV 26506 USA.
   [Stenglein, Jennifer L.; Anhalt-Depies, Christine] Wisconsin Dept Nat Resources, Off Appl Sci, Madison, WI 53707 USA.
   [Nelson, Carrie] US Forest Serv, Chequamegon Nicolet Natl Forest, Glidden, WI 54527 USA.
   [Long, Robert A.; Jaspers, Kodi Jo; Remine, Kathryn R.] Woodland Pk Zoo, Seattle, WA 98103 USA.
   [Jordan, Mark J.] Seattle Univ, Dept Biol, Seattle, WA 98122 USA.
   [Davis, Daniel] Smithsonian Inst, Off Chief Informat Officer, Washington, DC 20024 USA.
   [Hernandez-Yanez, Haydee; Zhao, Jennifer Y.; McShea, Andwilliam J.] Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
RP Cove, MV (corresponding author), Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
EM mvcove@ncsu.edu
RI Whittier, Christopher/AAX-2530-2021; Rega-Brodsky,
   Christine/AAN-7397-2021; Alston, Jesse/AAU-8218-2020; Edelman, Andrew
   J/C-2810-2012; O'Mara, M. Teague/J-6090-2012; Studds, Colin/C-3701-2012
OI Whittier, Christopher/0000-0001-9626-6513; Rega-Brodsky,
   Christine/0000-0002-3483-1465; Alston, Jesse/0000-0001-5309-7625; Kays,
   Roland/0000-0002-2947-6665; Mason, David/0000-0001-8456-5700;
   Sutherland, Chris/0000-0003-2073-1751; McShea,
   William/0000-0002-8102-0200; Hawkins, Melissa/0000-0001-8929-1593;
   Baruzzi, Carolina/0000-0003-1796-9355; King, Scott/0000-0001-6821-9234;
   O'Mara, M. Teague/0000-0002-6951-1648; Fisher-Reid, M.
   Caitlin/0000-0003-1587-7086; Crupi, Anthony/0000-0003-2788-6238;
   Lesmeister, Damon/0000-0003-1102-0122; Williamson,
   Jacque/0000-0002-5626-8706; Studds, Colin/0000-0001-5715-1692; Cove,
   Michael/0000-0001-5691-0634; Clark, Darren/0000-0002-8738-9826; Whipple,
   Laura/0000-0003-0736-9946; Sekercioglu, Cagan H./0000-0003-3193-0377;
   Coster, Stephanie/0000-0002-5170-4548; Lasky,
   Monica/0000-0002-9567-4643; Weiss, Katherine C. B./0000-0002-0034-2460;
   Bontrager, Helen/0000-0002-9457-2119; Iannarilli,
   Fabiola/0000-0002-7018-3557
FU Smithsonian InstitutionSmithsonian Institution Funding Source: Medline
NR 0
TC 1
Z9 1
U1 8
U2 11
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0012-9658
EI 1939-9170
J9 ECOLOGY
JI Ecology
PD JUN
PY 2021
VL 102
IS 6
AR e03353
DI 10.1002/ecy.3353
PG 2
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SN7SX
UT WOS:000658488800021
PM 33793977
OA Green Published, Bronze
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Despres-Einspenner, ML
   Farine, DR
   Samuni, L
   Angedakin, S
   Arandjelovic, M
   Boesch, C
   Dieguez, P
   Havercamp, K
   Knight, A
   Langergraber, KE
   Wittig, RM
   Kuhl, HS
AF McCarthy, Maureen S.
   Despres-Einspenner, Marie-Lyne
   Farine, Damien R.
   Samuni, Liran
   Angedakin, Samuel
   Arandjelovic, Mimi
   Boesch, Christophe
   Dieguez, Paula
   Havercamp, Kristin
   Knight, Alex
   Langergraber, Kevin E.
   Wittig, Roman M.
   Kuehl, Hjalmar S.
TI Camera traps provide a robust alternative to direct observations for
   constructing social networks of wild chimpanzees
SO ANIMAL BEHAVIOUR
LA English
DT Article
DE association patterns; biomonitoring; camera trap; chimpanzee;
   fission-fusion; Pan troglodytes; social network analysis
ID FISSION-FUSION DYNAMICS; COMMUNITY STRUCTURE; PAN-TROGLODYTES;
   TRANSMISSION; ORGANIZATION; POPULATION; BEHAVIOR; SOCIOECOLOGY;
   UNCERTAINTY; TOOLS
AB Social network analysis provides valuable opportunities to quantify the nature of social relationships in animal societies including aspects of group structure, dynamics and behaviour transmission. Remote monitoring approaches such as camera trapping offer rich data sets from groups and species that are difficult to observe, yet the robustness of these data for constructing social networks remains unexplored. Here we compared networks of party association based on camera traps with those based on direct observations over the same 9-month sampling period in a group of habituated western chimpanzees, Pan troglodytes verus. Networks based on camera traps and direct observations were both stable with sufficient sampling, and had very similar structures, patterns of sex assortment and individual network positions. However, camera trap data led to lower estimates of group density and dyadic association strengths, and slightly higher modularity, illustrating the limitations raised by differences in data collection methods for network comparisons. We then constructed a social network using camera trap data from unhabituated eastern chimpanzees, Rt. schweinfurthii, demonstrating the feasibility of this approach in the absence of extensive prior knowledge of the study subjects. Further, differences between the eastern and western chimpanzee social networks followed expected patterns based on recognized social differences, illustrating the promise of this approach for detecting within-species social variation. Although long-term behavioural observations will continue to provide rich data for many species, camera traps offer a powerful alternative to gain information on social group dynamics in elusive or unhabituated animals, as well as to conduct systematic multisite comparative studies. (C) 2019 The Association for the Study of Animal Behaviour. Published by Elsevier Ltd. All rights reserved.
C1 [McCarthy, Maureen S.; Despres-Einspenner, Marie-Lyne; Samuni, Liran; Angedakin, Samuel; Arandjelovic, Mimi; Boesch, Christophe; Dieguez, Paula; Wittig, Roman M.; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Farine, Damien R.] Max Planck Inst Anim Behav, Dept Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Konstanz, Ctr Adv Study Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Konstanz, Dept Biol, Constance, Germany.
   [Farine, Damien R.] Univ Oxford, Edward Grey Inst Field Ornithol, Dept Zool, Oxford, England.
   [Samuni, Liran; Wittig, Roman M.] CSRS, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Havercamp, Kristin] Kyoto Univ, Wildlife Res Ctr, Kyoto, Japan.
   [Knight, Alex] Univ Auckland, Sch Biol Sci, Auckland, New Zealand.
   [Langergraber, Kevin E.] Arizona State Univ, Sch Human Evolut & Social Change, Tempe, AZ USA.
   [Langergraber, Kevin E.] Arizona State Univ, Inst Human Origins, Tempe, AZ USA.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Halle, Germany.
RP McCarthy, MS (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM maureen_mc@eva.mpg.de
RI Farine, Damien R./Y-2454-2019
OI Farine, Damien R./0000-0003-2208-7613; , Kristin/0000-0002-4685-8366
FU Max Planck Society Innovation Fund; Heinz L. Krekeler Foundation; Robert
   Bosch Foundation; Centre Suisse de Recherches Scientifiques; Centre for
   Forest ResearcheFonds de Recherche Quebec Nature et Technologies
   International internship program; DFG Centre for Excellence 2117 'Centre
   for the Advanced Study of Collective Behaviour'German Research
   Foundation (DFG) [422037984]
FX For support, we thank the Max Planck Society Innovation Fund and the
   Heinz L. Krekeler Foundation, the Robert Bosch Foundation, Centre Suisse
   de Recherches Scientifiques, the Centre for Forest ResearcheFonds de
   Recherche Quebec Nature et Technologies International internship
   program, and the DFG Centre for Excellence 2117 'Centre for the Advanced
   Study of Collective Behaviour' (ID: 422037984). For research permission,
   we thank Ministere de l'Enseignement Superieur et de la Recherche
   Scientifique, Ministere de l'Environnement et des Eaux et Forets, the
   Office Ivoirien de Parcs et Reserves in Cote d'Ivoire, Uganda National
   Council for Science and Technology (UNCST) and Uganda Wildlife Authority
   (UWA). We thank the Tai Chimpanzee Project (TCP) and the Ngogo
   Chimpanzee Project for the opportunity to conduct research in Cote
   d'Ivoire and Uganda, respectively. We thank Appollinaire Gnahe Djirian,
   Oulai Landry, Frederic Yehanon Oulai and Anna Preis for providing party
   composition data extracted from the long-term database of the TCP, as
   well as Frederic Yehanon Oulai for identifying chimpanzees in the camera
   trap videos. We thank Mizuki Murai for logistical support in PanAf data
   collection and coordination.
CR Alarcon-Nieto G, 2018, METHODS ECOL EVOL, V9, P1536, DOI 10.1111/2041-210X.13005
   Allen J, 2013, SCIENCE, V340, P485, DOI 10.1126/science.1231976
   ALTMANN J, 1974, BEHAVIOUR, V49, P227, DOI 10.1163/156853974X00534
   Aplin LM, 2012, P ROY SOC B-BIOL SCI, V279, P4199, DOI 10.1098/rspb.2012.1591
   Berger-Wolf T. Y, 2017, BLOOMB DAT GOOD EXCH
   Brent LJN, 2011, AM J PRIMATOL, V73, P720, DOI 10.1002/ajp.20949
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Butts, 2008, J STAT SOFTW, V24, P1, DOI DOI 10.18637/JSS.V024.I06
   CAIRNS SJ, 1987, ANIM BEHAV, V35, P1454, DOI 10.1016/S0003-3472(87)80018-0
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Carter KD, 2013, ANIM BEHAV, V85, P385, DOI 10.1016/j.anbehav.2012.11.011
   CHAPMAN CA, 1993, AM J PRIMATOL, V31, P263, DOI 10.1002/ajp.1350310403
   Coles RC, 2012, INT J PRIMATOL, V33, P93, DOI 10.1007/s10764-011-9555-2
   Cross PC, 2005, ANIM BEHAV, V69, P499, DOI 10.1016/j.anbehav.2004.08.006
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Csardi G., 2006, INTERJOURNAL COMPLEX, V1695, P1, DOI DOI 10.3724/SP.J.1087.2009.02191
   Davis GH, 2018, ANIM BEHAV, V141, P29, DOI 10.1016/j.anbehav.2018.04.012
   Dekker D, 2007, PSYCHOMETRIKA, V72, P563, DOI 10.1007/s11336-007-9016-1
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Dorning J, 2016, THESIS
   Elbroch LM, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1701218
   Farine DR, 2015, J ANIM ECOL, V84, P1144, DOI 10.1111/1365-2656.12418
   Farine DR, 2015, ANIM BEHAV, V104, pE1, DOI 10.1016/j.anbehav.2014.11.019
   Farine DR, 2014, ANIM BEHAV, V89, P141, DOI 10.1016/j.anbehav.2014.01.001
   Farine DR, 2013, METHODS ECOL EVOL, V4, P1187, DOI 10.1111/2041-210X.12121
   Farine DR, 2012, ANIM BEHAV, V84, P1271, DOI 10.1016/j.anbehav.2012.08.008
   Foerster S, 2015, ANIM BEHAV, V105, P139, DOI 10.1016/j.anbehav.2015.04.012
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Goldenberg SZ, 2016, CURR BIOL, V26, P75, DOI 10.1016/j.cub.2015.11.005
   Gompper ME, 1996, BEHAV ECOL, V7, P254, DOI 10.1093/beheco/7.3.254
   Goodall J., 1986, CHIMPANZEES GOMBE PA
   Hamede RK, 2009, ECOL LETT, V12, P1147, DOI 10.1111/j.1461-0248.2009.01370.x
   HAWKINS RE, 1970, J WILDLIFE MANAGE, V34, P407, DOI 10.2307/3799027
   Hobaiter C, 2014, PLOS BIOL, V12, DOI 10.1371/journal.pbio.1001960
   Hohmann G, 2002, BEHAVIOURAL DIVERSITY IN CHIMPANZEES AND BONOBOS, P138, DOI 10.1017/CBO9780511606397.014
   Jacoby DMP, 2016, TRENDS ECOL EVOL, V31, P301, DOI 10.1016/j.tree.2016.01.011
   Johnson KVA, 2017, ANIM BEHAV, V128, P21, DOI 10.1016/j.anbehav.2017.04.001
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Kays RW, 2001, J ZOOL, V253, P491, DOI 10.1017/S0952836901000450
   KRACKHARDT D, 1988, SOC NETWORKS, V10, P359, DOI 10.1016/0378-8733(88)90004-4
   Kummer H., 1995, QUEST SACRED BABOON
   Lusseau D, 2008, ANIM BEHAV, V75, P1809, DOI 10.1016/j.anbehav.2007.10.029
   Maldonado-Chaparro AA, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2018.1577
   Mandal D, 2019, THESIS
   McCarthy MS, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22904
   McCreery EK, 2000, BEHAVIOUR, V137, P579, DOI 10.1163/156853900502222
   Metz MC, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017332
   Mielke A, 2017, ROY SOC OPEN SCI, V4, DOI [10.1098/rsos.171296, 10.1098/rsos.172143]
   Murray CM, 2007, ANIM BEHAV, V74, P1795, DOI 10.1016/j.anbehav.2007.03.024
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Nishida T., 1968, Primates, V9, P167, DOI 10.1007/BF01730971
   Ramos-Fernandez G, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2018.0532
   Ren BP, 2012, INT J PRIMATOL, V33, P1096, DOI 10.1007/s10764-012-9586-3
   Robley A., 2010, A RYLAH I ENV RES TE, V201, P1
   Rubenstein DI, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138645
   Ryder TB, 2012, BIOL LETTERS, V8, P917, DOI 10.1098/rsbl.2012.0536
   Sanz C, 2004, AM NAT, V164, P567, DOI 10.1086/424803
   Shizuka D, 2016, ANIM BEHAV, V112, P237, DOI 10.1016/j.anbehav.2015.12.007
   Siegel S., 1988, NONPARAMETRIC STAT B, V2nd ed
   Silk MJ, 2017, BIOSCIENCE, V67, P245, DOI 10.1093/biosci/biw175
   Smith JE, 2008, ANIM BEHAV, V76, P619, DOI 10.1016/j.anbehav.2008.05.001
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   SYMINGTON MM, 1990, INT J PRIMATOL, V11, P47, DOI 10.1007/BF02193695
   Vaidyanathan G, 2011, NATURE, V476, P266, DOI 10.1038/476266a
   van Schaik CP, 1999, PRIMATES, V40, P69, DOI 10.1007/BF02557703
   Weber N, 2013, CURR BIOL, V23, pR915, DOI 10.1016/j.cub.2013.09.011
   Whitehead H., 2008, ANAL ANIMAL SOC QUAN
   Wilson ML, 2014, NATURE, V513, P414, DOI 10.1038/nature13727
   Wittemyer G, 2005, ANIM BEHAV, V69, P1357, DOI 10.1016/j.anbehav.2004.08.018
   Wittiger L, 2013, BEHAV ECOL SOCIOBIOL, V67, P1097, DOI 10.1007/s00265-013-1534-5
   Wursig B., 1990, Reports of the International Whaling Commission Special Issue, P43
   [No title captured]
NR 73
TC 8
Z9 8
U1 1
U2 13
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0003-3472
EI 1095-8282
J9 ANIM BEHAV
JI Anim. Behav.
PD NOV
PY 2019
VL 157
BP 227
EP 238
DI 10.1016/j.anbehav.2019.08.008
PG 12
WC Behavioral Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Behavioral Sciences; Zoology
GA JI9KJ
UT WOS:000493780600024
DA 2022-02-10
ER

PT J
AU Schindler, F
   Steinhage, V
AF Schindler, Frank
   Steinhage, Volker
TI Saving costs for video data annotation in wildlife monitoring
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Automatic annotation; Neural networks; Wildlife monitoring; Tracking;
   Instance segmentation; Artificial intelligence
AB In wildlife monitoring, large amounts of video data are generated by recordings from camera traps. Training of deep learning methods demands for annotated video data, i.e. video data where each frame is annotated with the correct number and species designation of the observed animals. But manual annotation of video clips is extremely time-consuming and laborious. In this proof of concept we compare three different state-of-the-art approaches to the annotation of video data: Manual annotation using the VGG Image Annotator, interactive annotation using the MiVOS video annotator and automated annotation utilizing an adapted and customized Tracktor approach that propagates annotations from frame to frame through complete video clips. An experimental proof of concept on wildlife video clips captured by camera traps show extreme time savings from hours down to minutes (i.e. in order of a magnitude) thereby not only maintaining the detection scores of animals in each frame but also improving detection scores from 54.7% to 58.5% compared to the employment of perfect but costly manual annotations in training.
C1 [Schindler, Frank; Steinhage, Volker] Univ Bonn, Dept Comp Sci 4, Friedrich Hirzebruch Allee 8, D-53115 Bonn, Germany.
RP Schindler, F (corresponding author), Univ Bonn, Dept Comp Sci 4, Friedrich Hirzebruch Allee 8, D-53115 Bonn, Germany.
EM schindl@cs.uni-bonn.de
FU German Federal Ministry of Education and Research (Bundesministerium fur
   Bildung und Forschung (BMBF) , Bonn, Germany)Federal Ministry of
   Education & Research (BMBF) [FKZ 01LC1903B]
FX We want to thank Morris Klasen and Timm Haucke for fruitful discussions
   on aspects of this study. This work was partially done within the
   project "Automated Multisensor station for Monitoring Of species
   Diversity" (AMMOD) which is funded by the German Federal Ministry of
   Education and Research (Bundesministerium fur Bildung und Forschung
   (BMBF) , Bonn, Germany (FKZ 01LC1903B) .
CR Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096
   Athar Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P158, DOI 10.1007/978-3-030-58621-8_10
   Berg A, 2019, IEEE INT CONF COMP V, P2242, DOI 10.1109/ICCVW.2019.00277
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertasius G., 2020, CVPR
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Cheng H.K., ARXIV210307941
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Drozdov D., 2017, SUPERVISELY
   Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Hu YT, 2017, ADV NEUR IN, V30
   Jiale Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P1, DOI 10.1007/978-3-030-58568-6_1
   Lin Chung-Ching, 2020, P IEEE CVF C COMP VI, P13147
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Paszke A., ADV NEURAL INFORM PR, V32, P8024
   Porzi L, 2020, PROC CVPR IEEE, P6845, DOI 10.1109/CVPR42600.2020.00688
   Qi J., ARXIV210201558
   Reid I., 2016, ARXIV160300831
   Reza MA, 2019, IEEE INT C INT ROBOT, P4970, DOI 10.1109/IROS40897.2019.8968230
   Schindler F, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101215
   Wang, 2019, ARXIV190912605
   Wang Y., 2020, ARXIV201114503
   Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 30
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD NOV
PY 2021
VL 65
AR 101418
DI 10.1016/j.ecoinf.2021.101418
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA WB6TN
UT WOS:000703703200002
DA 2022-02-10
ER

PT J
AU Pal, R
   Bhattacharya, T
   Qureshi, Q
   Buckland, ST
   Sathyakumar, S
AF Pal, Ranjana
   Bhattacharya, Tapajit
   Qureshi, Qamar
   Buckland, Stephen T.
   Sathyakumar, Sambandam
TI Using distance sampling with camera traps to estimate the density of
   group-living and solitary mountain ungulates
SO ORYX
LA English
DT Article
DE Bharal; camera trapping; density estimates; musk deer; point transect
   method; subalpine; trans-Himalaya; Upper Bhagirathi basin
ID MOSCHUS-CHRYSOGASTER; DOMESTIC LIVESTOCK; NATIONAL-PARK; RESOURCE USE;
   MUSK DEER; SNOW; ABUNDANCE; UNCIA; DUNG; TIME
AB Throughout the Himalaya, mountain ungulates are threatened by hunting for meat and body parts, habitat loss, and competition with livestock. Accurate population estimates are important for conservation management but most of the available methods to estimate ungulate densities are difficult to implement in mountainous terrain. Here, we tested the efficacy of the recent extension of the point transect method, using camera traps for estimating density of two mountain ungulates: the group-living Himalayan blue sheep or bharal Pseudois nayaur and the solitary Himalayan musk deer Moschus leucogaster. We deployed camera traps in 2017-2018 for the bharal (summer: 21 locations; winter: 25) in the trans-Himalayan region (3,000-5,000 m) and in 2018-2019 for the musk deer (summer: 30 locations; winter: 28) in subalpine habitats (2,500-3,500 m) in the Upper Bhagirathi basin, Uttarakhand, India. Using distance sampling with camera traps, we estimated the bharal population to be 0.51 +/- SE 0.1 individuals/km(2) (CV = 0.31) in summer and 0.64 +/- SE 0.2 individuals/km(2) (CV = 0.37) in winter. For musk deer, the estimated density was 0.4 +/- SE 0.1 individuals/km(2) (CV = 0.34) in summer and 0.1 +/- SE 0.05 individuals/km(2) (CV = 0.48) in winter. The high variability in these estimates is probably a result of the topography of the landscape and the biology of the species. We discuss the potential application of distance sampling with camera traps to estimate the density of mountain ungulates in remote and rugged terrain, and the limitations of this method.
C1 [Pal, Ranjana; Qureshi, Qamar; Sathyakumar, Sambandam] Wildlife Inst India, Dehra Dun 248001, Uttarakhand, India.
   [Bhattacharya, Tapajit] Durgapur Govt Coll, Durgapur, India.
   [Buckland, Stephen T.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
RP Sathyakumar, S (corresponding author), Wildlife Inst India, Dehra Dun 248001, Uttarakhand, India.
EM ssk@wii.gov.in
OI SATHYAKUMAR, SAMBANDAM/0000-0003-2027-4706; Buckland,
   Stephen/0000-0002-9939-709X
FU Department of Science and Technology, Government of IndiaDepartment of
   Science & Technology (India) [DST/SPLICE/CCP/ NMSHE/TF-2/WII/2014[G]];
   Wildlife Institute of India; Miriam Rothschild Travel Bursary Programme
FX This work is part of a project initiated under the National Mission for
   Sustaining the Himalayan Ecosystem (NMSHE) Programme funded by the
   Department of Science and Technology, Government of India (grant no.:
   DST/SPLICE/CCP/ NMSHE/TF-2/WII/2014[G]). The Miriam Rothschild Travel
   Bursary Programme provided funding for a 4-week internship for R. Pal
   with S.T. Buckland at St Andrews University, UK. We thank the Director
   and Dean of the Wildlife Institute of India for their guidance and
   support; D.V.S. Khati, Principal Chief Conservator of Forests and Chief
   Wildlife Warden, Uttarakhand, for granting research permission; Sandeep
   Kumar, Divisional Forest Officer and former Deputy Director, Gangotri
   National Park, and Shrawan Kumar for their support and cooperation; and
   L. Corlatti for reviewing the manuscript.
CR Bagchi S, 2006, J ZOOL, V268, P217, DOI 10.1111/j.1469-7998.2005.00030.x
   Bagchi S, 2010, OECOLOGIA, V164, P1075, DOI 10.1007/s00442-010-1690-5
   Bhardwaj Manish, 2010, Galemys, V22, P545
   Bhattacharya T., 2012, Proceedings of the Zoological Society (Calcutta), V65, P11, DOI 10.1007/s12595-012-0025-4
   Bhattacharya T, 2012, MAMM STUDY, V37, P173, DOI 10.3106/041.037.0302
   Bhattacharya T, 2011, MT RES DEV, V31, P209, DOI 10.1659/MRD-JOURNAL-D-10-00069.1
   Buckland S.T., 2001, pi
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Cavallini Paolo, 1992, Journal of the Bombay Natural History Society, V89, P302
   Chandola S., 2009, THESIS HNB GARHWAL U
   Corlatti L, 2015, POPUL ECOL, V57, P409, DOI 10.1007/s10144-015-0481-6
   Dendup P, 2018, INT J CONSERV SCI, V9, P193
   Forsyth DM, 1997, NEW ZEAL J ECOL, V21, P97
   Green M.J.B., 1985, THESIS U CAMBRIDGE C
   Hofmeester TR, 2017, REMOTE SENS ECOL CON, V3, P81, DOI 10.1002/rse2.25
   Howe EJ, 2019, METHODS ECOL EVOL, V10, P38, DOI 10.1111/2041-210X.13082
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Jarvis A., 2008, CGIAR CSI SRTM 90M D
   Kittur S, 2010, EUR J WILDLIFE RES, V56, P195, DOI 10.1007/s10344-009-0302-3
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kumar A, 2017, CURR SCI INDIA, V113, P1032
   Laing SE, 2003, J APPL ECOL, V40, P1102, DOI 10.1111/j.1365-2664.2003.00861.x
   McCarthy KP, 2008, J WILDLIFE MANAGE, V72, P1826, DOI 10.2193/2008-040
   MCNAUGHTON SJ, 1979, AM NAT, V113, P691, DOI 10.1086/283426
   Mishra C, 2004, J APPL ECOL, V41, P344, DOI 10.1111/j.0021-8901.2004.00885.x
   Namgail T, 2007, ECOL RES, V22, P25, DOI 10.1007/s11284-006-0015-y
   O'Neill H., 2008, THESIS IMPERIAL COLL
   Pal R, 2021, ORYX, V55, P657, DOI 10.1017/S0030605319001352
   Prater S.H., 1980, BOOK INDIAN MAMMALS
   Qamar QUZ, 2008, PAK J ZOOL, V40, P159
   Rodgers WA., 2000, WILDLIFE PROTECTED A
   Rovero F., 2016, CAMERA TRAPPING WILD
   Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Sathyakumar S., 2006, RELEASE CAPTIVE HIMA
   Sathyakumar S., 2013, PROJECT REPORT
   Sathyakumar S., 2002, MOUNTAIN UNGULATES E
   Sathyakumar S., 1994, THESIS SAURASHTRA U
   Sathyakumar S., 2013, MAMMALS S ASIA, V2, P223
   SCHALLER GB, 1988, BIOL CONSERV, V45, P179, DOI 10.1016/0006-3207(88)90138-3
   Singh NJ, 2011, ORYX, V45, P38, DOI 10.1017/S0030605310000839
   Sulkava RT, 2007, ANN ZOOL FENN, V44, P179
   Suryawanshi K.R., 2020, ORYX, V55, P66
   Suryawanshi KR, 2019, POPUL ECOL, V61, P268, DOI 10.1002/1438-390X.1027
   Suryawanshi KR, 2012, OECOLOGIA, V169, P581, DOI 10.1007/s00442-011-2237-0
   Suryawanshi KR, 2010, OECOLOGIA, V162, P453, DOI 10.1007/s00442-009-1467-x
   Takeshita K, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0164345
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Timmins R.J., 2015, IUCN RED LIST THREAT
   Yoccoz NG, 2001, TRENDS ECOL EVOL, V16, P446, DOI 10.1016/S0169-5347(01)02205-4
NR 52
TC 1
Z9 1
U1 2
U2 2
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0030-6053
EI 1365-3008
J9 ORYX
JI Oryx
PD SEP
PY 2021
VL 55
IS 5
BP 668
EP 676
AR PII S003060532000071X
DI 10.1017/S003060532000071X
PG 9
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA UL6JI
UT WOS:000692754900017
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Miao, ZQ
   Gaynor, KM
   Wang, JY
   Liu, ZW
   Muellerklein, O
   Norouzzadeh, MS
   McInturff, A
   Bowie, RCK
   Nathan, R
   Yu, SX
   Getz, WM
AF Miao, Zhongqi
   Gaynor, Kaitlyn M.
   Wang, Jiayun
   Liu, Ziwei
   Muellerklein, Oliver
   Norouzzadeh, Mohammad Sadegh
   McInturff, Alex
   Bowie, Rauri C. K.
   Nathan, Ran
   Yu, Stella X.
   Getz, Wayne M.
TI Insights and approaches using deep learning to classify wildlife
SO SCIENTIFIC REPORTS
LA English
DT Article
AB The implementation of intelligent software to identify and classify objects and individuals in visual fields is a technology of growing importance to operatives in many fields, including wildlife conservation and management. To non-experts, the methods can be abstruse and the results mystifying. Here, in the context of applying cutting edge methods to classify wildlife species from camera-trap data, we shed light on the methods themselves and types of features these methods extract to make efficient identifications and reliable classifications. The current state of the art is to employ convolutional neural networks (CNN) encoded within deep-learning algorithms. We outline these methods and present results obtained in training a CNN to classify 20 African wildlife species with an overall accuracy of 87.5% from a dataset containing 111,467 images. We demonstrate the application of a gradient-weighted class-activation-mapping (Grad-CAM) procedure to extract the most salient pixels in the final convolution layer. We show that these pixels highlight features in particular images that in some cases are similar to those used to train humans to identify these species. Further, we used mutual information methods to identify the neurons in the final convolution layer that consistently respond most strongly across a set of images of one particular species. We then interpret the features in the image where the strongest responses occur, and present dataset biases that were revealed by these extracted features. We also used hierarchical clustering of feature vectors (i.e., the state of the final fully-connected layer in the CNN) associated with each image to produce a visual similarity dendrogram of identified species. Finally, we evaluated the relative unfamiliarity of images that were not part of the training set when these images were one of the 20 species "known" to our CNN in contrast to images of the species that were "unknown" to our CNN.
C1 [Miao, Zhongqi; Gaynor, Kaitlyn M.; Wang, Jiayun; McInturff, Alex; Getz, Wayne M.] Univ Calif Berkeley, Dept Env Sci Pol & Manag, Berkeley, CA 94704 USA.
   [Miao, Zhongqi; Liu, Ziwei; Muellerklein, Oliver; Yu, Stella X.] Univ Calif Berkeley, Int Comp Sci Inst, 1947 Ctr St, Berkeley, CA 94704 USA.
   [Muellerklein, Oliver; Yu, Stella X.] Univ Calif Berkeley, Vis Sci Grad Grp, Berkeley, CA 94704 USA.
   [Norouzzadeh, Mohammad Sadegh] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Bowie, Rauri C. K.] Univ Calif Berkeley, Dept Integr Biol, Berkeley, CA USA.
   [Bowie, Rauri C. K.] Univ Calif Berkeley, Museum Vertebrate Zool, Berkeley, CA USA.
   [Nathan, Ran] Hebrew Univ Jerusalem, Alexander Silberman Inst Life Sci, Dept EEB, Jerusalem, Israel.
   [Getz, Wayne M.] Univ KwaZulu Natal, Sch Math Sci, Durban, South Africa.
RP Miao, ZQ; Getz, WM (corresponding author), Univ Calif Berkeley, Dept Env Sci Pol & Manag, Berkeley, CA 94704 USA.; Miao, ZQ (corresponding author), Univ Calif Berkeley, Int Comp Sci Inst, 1947 Ctr St, Berkeley, CA 94704 USA.
EM zhongqi.miao@berkeley.edu; wgetz@berkeley.edu
RI Nathan, Ran/A-9380-2008; Liu, Ziwei/AAG-6939-2021; Bowie,
   Rauri/H-9165-2019
OI Bowie, Rauri/0000-0001-8328-6021; Gaynor, Kaitlyn/0000-0002-5747-0543
FU NSF-GRFPNational Science Foundation (NSF)NSF - Office of the Director
   (OD); Rufford Foundation; Idea Wild; Explorers Club; UC Berkeley Center
   for African Studies; NSF EEID Grant [1617982]; BSF GrantUS-Israel
   Binational Science Foundation [2015904]
FX Thanks to T. Gu, A. Ke, H. Rosen, A. Wu, C. Jurgensen, E. Lai, M. Levy,
   and E. Silverberg for annotating the images used in this study, and to
   everyone else involved in this project. Data collection was supported by
   J. Brashares and through grants to KMG from the NSF-GRFP, the Rufford
   Foundation, Idea Wild, the Explorers Club, and the UC Berkeley Center
   for African Studies. We are grateful for the support of Gorongosa
   National Park, especially M. Stalmans in permitting and facilitating
   this research. Z. M. was funded in part by NSF EEID Grant 1617982 to
   W.M.G., R.C.K.B. and R.N., and was also supported in part by BSF Grant
   2015904 to R. N. and W. M. G. Thanks to Z. Beba, T. Easter, P. Hammond,
   Z. Melvin, L. Reiswig, and N. Schramm for participating in the feature
   survey.
CR BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Bland LM, 2015, CONSERV BIOL, V29, P250, DOI 10.1111/cobi.12372
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Chattopadhyay P., 2017, ARXIV170805122
   Crisci C, 2012, ECOL MODEL, V240, P113, DOI 10.1016/j.ecolmodel.2012.03.001
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoque S, 2011, INT J BIOSCIENCE BIO, V3, P45
   Kampichler C, 2010, ECOL INFORM, V5, P441, DOI 10.1016/j.ecoinf.2010.06.003
   Kitchin R, 2014, BIG DATA SOC, V1, DOI 10.1177/2053951714528481
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   MacKay David J. C., 2002, INFORM THEORY INFERE
   Malisiewicz T., 2009, ADV NEURAL INFORM PR, P1222
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Rahman DA, 2017, ORYX, V51, P665, DOI 10.1017/S0030605316000429
   Rangel TF, 2012, NAT CONSERVACAO, V10, P119, DOI 10.4322/natcon.2012.030
   Rokach L, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P321, DOI 10.1007/0-387-25465-X_15
   Sejnowski TJ, 2016, COMPUT NEUROSCI-MIT, pIX
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Springenberg J. T., 2014, ARXIV14126806
   Tabak M. A., 2018, BIORXIV, DOI [10.1101/346809, DOI 10.1101/346809]
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Van Horn Grant, 2017, ARXIV170901450
   Vinyals O., 2016, ADV NEURAL INFORM PR
   Waldchen J., 2018, METHODS ECOL EVOL, P1
   Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808
   Zhou B., 2014, INT C LEARN REPR
NR 35
TC 17
Z9 17
U1 1
U2 17
PU NATURE RESEARCH
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD MAY 31
PY 2019
VL 9
AR 8137
DI 10.1038/s41598-019-44565-w
PG 9
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA IA7RF
UT WOS:000469752800015
PM 31148564
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Saito, H
   Otake, T
   Kato, H
   Tokutake, M
   Semba, S
   Tomioka, Y
   Kohira, Y
AF Saito, Hiroshi
   Otake, Tatsuki
   Kato, Hayato
   Tokutake, Masayuki
   Semba, Shogo
   Tomioka, Yoichi
   Kohira, Yukihide
TI Battery-Powered Wild Animal Detection Nodes with Deep Learning
SO IEICE TRANSACTIONS ON COMMUNICATIONS
LA English
DT Article
DE wild animal detection; deep learning; camera-trap; micro-computer
   boards; power saving
ID IMAGES
AB Since wild animals are causing more accidents and damages, it is important to safely detect them as early as possible. In this paper, we propose two battery-powered wild animal detection nodes based on deep learning that can automatically detect wild animals; the detection information is notified to the people concerned immediately. To use the proposed nodes outdoors where power is not available, we devise power saving techniques for the proposed nodes. For example, deep learning is used to save power by avoiding operations when wild animals are not detected. We evaluate the operation time and the power consumption of the proposed nodes. Then, we evaluate the energy consumption of the proposed nodes. Also, we evaluate the detection range of the proposed nodes, the accuracy of deep learning, and the success rate of communication through field tests to demonstrate that the proposed nodes can be used to detect wild animals outdoors.
C1 [Saito, Hiroshi; Otake, Tatsuki; Kato, Hayato; Tokutake, Masayuki; Semba, Shogo; Tomioka, Yoichi; Kohira, Yukihide] Univ Aizu, Aizu Wakamatsu, Fukushima 9658580, Japan.
RP Saito, H (corresponding author), Univ Aizu, Aizu Wakamatsu, Fukushima 9658580, Japan.
EM hiroshis@u-aizu.ac.jp
CR Ando Masaki, 2019, Honyurui Kagaku, V59, P49, DOI 10.11238/mammalianscience.59.49
   Campark, CAMP TRAIL CAM
   Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Google Inc, INC V3
   Hayashi H., 2017, PRMU201784 IEICE
   Indoor Corgi Elec, E32 SOLARCHARGER
   LAPIS Semiconductor, LAZ
   Miyasaka S., 2019, J REMOTE SENSING SOC, V36, P493
   Morita T., 2019, BIORXIV, P1, DOI [10.1101/775486, DOI 10.1101/775486]
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh M.S., 2017, P NATL ACAD SCI USA, V115, P1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Oishi Y., 2016, J REMOTE SENS SOC JP, V36, P152, DOI [10.11440/rssj.36.152, DOI 10.11440/RSSJ.36.152]
   Raspberry Pi Foundation, RASP PI
   Thangarasu R., 2019, INT J SCI TECHNOL RE, V8, P2613
NR 16
TC 0
Z9 0
U1 2
U2 6
PU IEICE-INST ELECTRONICS INFORMATION COMMUNICATIONS ENG
PI TOKYO
PA KIKAI-SHINKO-KAIKAN BLDG, 3-5-8, SHIBA-KOEN, MINATO-KU, TOKYO, 105-0011,
   JAPAN
SN 0916-8516
EI 1745-1345
J9 IEICE T COMMUN
JI IEICE Trans. Commun.
PD DEC
PY 2020
VL E103B
IS 12
BP 1394
EP 1402
DI 10.1587/transcom.2020SEP0004
PG 9
WC Engineering, Electrical & Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Telecommunications
GA OZ7VI
UT WOS:000595128600004
DA 2022-02-10
ER

PT J
AU Niwa, H
AF Niwa, Hideyuki
TI Assessing the activity of deer and their influence on vegetation in a
   wetland using automatic cameras and low altitude remote sensing (LARS)
SO EUROPEAN JOURNAL OF WILDLIFE RESEARCH
LA English
DT Article
DE Wildlife management; Wetland; Deer (Cervus nippon); Vegetation;
   Menyanthes trifoliata; Unmanned aerial vehicles (UAVs); Low altitude
   remote sensing (LARS); Camera traps
ID IMPACTS; IMAGES; UAV
AB It was thought by the author of the present study that if both automatic cameras and LARS were used, instead of one or the other, then it would be possible to efficiently gain knowledge that could be applied in the strategic management of deer in wetlands. In the Mizorogaike wetland, deer (Cervus nippon) graze plants throughout its entire area. Aerial images taken in the winter were used in the deer trail survey, as the vegetation would be withered, and the deer trails become easier to identify. The rare species selected to evaluate the effect of deer on vegetation was Menyanthes trifoliata, which was reported to be on the decline in the Mizorogaike wetland. The deer trails were identified visually. M. trifoliata was detected in the images using deep learning. I investigated the relationship between increases and decreases in areas covered by M. trifoliata during the period from 2016 to 2019 and the distance of these areas from deer trails. Three routes were selected where the deer trails indicated that the deer might be crossing the water. Four camera traps were set up on the land side facing the water. In the Mizorogaike wetland, during the period from 2015 to 2019, the area where deer trails could be observed spread to every corner of the wetland. Deer trails became conspicuous at the southwest edge of the Mizorogaike wetland. Although in 2015 the deer trails were limited to the northern half of the floating mat, by 2019, they had spread down into the southern half. Although camera traps were only used to investigate the wetland in 2019, it is thought that deer began to regularly swim across the water during the period from 2015 to 2019 and that this led to trails forming in the south of the floating mat. The area covered by M. trifoliata decreased by 20% from 2016 to 2019. The areas where M. trifoliata cover decreased tended to be nearer to deer trails than the areas where it increased.
C1 [Niwa, Hideyuki] Kyoto Univ Adv Sci, Fac Bioenvironm Sci, 1 Sogabe Cho, Kameoka, Kyoto 6218555, Japan.
RP Niwa, H (corresponding author), Kyoto Univ Adv Sci, Fac Bioenvironm Sci, 1 Sogabe Cho, Kameoka, Kyoto 6218555, Japan.
EM niwa.hideyuki@kuas.ac.jp
OI Niwa, Hideyuki/0000-0002-8199-3248
CR Beguin J, 2011, ECOL APPL, V21, P439, DOI 10.1890/09-2100.1
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   CHRETIEN L.-P., 2015, INT ARCH PHOTOGRAMM, P241, DOI DOI 10.5194/ISPRSARCHIVES-XL-1-W4-241-2015
   Cote SD, 2004, ANNU REV ECOL EVOL S, V35, P113, DOI 10.1146/annurev.ecolsys.35.021103.105725
   Filazzola A, 2014, FOREST ECOL MANAG, V313, P10, DOI 10.1016/j.foreco.2013.10.040
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Iijima H, 2015, ECOL INDIC, V48, P457, DOI 10.1016/j.ecolind.2014.09.009
   Iijima H, 2013, J WILDLIFE MANAGE, V77, P1038, DOI 10.1002/jwmg.556
   Inatomi Y, 2018, WETLAND RES, V8, P3
   Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   Niwa H, 2016, ECOL CIV ENG, V19, P47, DOI [10.3825/ece.19.47, DOI 10.3825/ECE.19.47]
   Pellerin S, 2006, BIOL CONSERV, V128, P316, DOI 10.1016/j.biocon.2005.09.039
   Russell FL, 2001, AM MIDL NAT, V146, P1, DOI 10.1674/0003-0031(2001)146[0001:EOWTDO]2.0.CO;2
   Saberioon MM, 2014, INT J APPL EARTH OBS, V32, P35, DOI 10.1016/j.jag.2014.03.018
   Scheffer M, 2001, NATURE, V413, P591, DOI 10.1038/35098000
   Takatsuki S, 2009, BIOL CONSERV, V142, P1922, DOI 10.1016/j.biocon.2009.02.011
   Tsujino R., 2007, Japanese Journal of Conservation Ecology, V12, P20
   Turner D, 2014, REMOTE SENS-BASEL, V6, P4003, DOI 10.3390/rs6054003
   van der Hoek Dirk-Jan, 2002, Great Plains Research, V12, P141
NR 20
TC 1
Z9 1
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1612-4642
EI 1439-0574
J9 EUR J WILDLIFE RES
JI Eur. J. Wildl. Res.
PD FEB
PY 2021
VL 67
IS 1
AR 3
DI 10.1007/s10344-020-01450-6
PG 7
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA PN5EX
UT WOS:000604503000001
DA 2022-02-10
ER

PT J
AU Cloyed, CS
   Cappelli, LR
   Tilson, DA
   Crawford, JA
   Dell, AI
AF Cloyed, Carl S.
   Cappelli, Laura R.
   Tilson, David A.
   Crawford, John A.
   Dell, Anthony, I
TI Using Camera Traps to Assess Mammal and Bird Assemblages in a Midwestern
   Forest
SO JOURNAL OF FISH AND WILDLIFE MANAGEMENT
LA English
DT Article
DE diversity; edge effects; species composition; biomass; riparian;
   agriculture
ID ARMADILLO DASYPUS-NOVEMCINCTUS; WHITE-TAILED DEER; SCIURUS-CAROLINENSIS;
   HABITAT USE; SQUIRRELS; COYOTE; FRAGMENTATION; POPULATION; CARNIVORES;
   PREDATORS
AB Ecologists are increasing the use of remote technologies in their research, as these methods are less labor intensive than traditional methods and oftentimes minimize the number of human errors. Camera traps can be used to remotely measure abundance and community composition and offer the potential to measure some phenotypic traits, such as body size. We designed a camera-trap setup that enabled us to capture images of both large and small animals and used our camera-trap design to investigate the community composition of mammals and birds and to estimate the biomass of mammals along two transects in a conservation reserve in Missouri. One transect ran from the edge of an agricultural field to an upland forest, and the other transect ran from the edge of a wetland to an upland forest. Over the 4.5-wk study, our cameras recorded 2,245 images that comprised 483 individuals of 16 species of mammals and birds. Coyotes Canis latrans and nine-banded armadillos Dasypus novemcinctus were unique to the riparian transect, as were several bird species. Fewer species used the forest immediately adjacent to the agricultural field, but more species used the forest immediately adjacent to the wetland. Biomass estimates from our camera-trap images were similar to those of published accounts. This is the first study to use camera traps to successfully estimate biomass. We showed that the value and utility of camera traps in wildlife studies and monitoring can be expanded by 1) using multiple cameras at different heights from the ground so as to capture different-sized animals and 2) obtaining phenotypic information of the captured animals.
C1 [Cloyed, Carl S.; Crawford, John A.; Dell, Anthony, I] Natl Great Rivers Res & Educ Ctr, East Alton, IL 62024 USA.
   [Cloyed, Carl S.; Dell, Anthony, I] Washington Univ, Dept Biol, Campus Box 1137, St Louis, MO 63130 USA.
   [Cloyed, Carl S.] Dauphin Isl Sea Lab, Dauphin Isl, AL 36528 USA.
   [Cappelli, Laura R.] SUNY Coll Environm Sci & Forestry, Syracuse, NY 13210 USA.
   [Tilson, David A.] Virginia Polytech Inst & State Univ, Dept Nat Resources & Environm, Blacksburg, VA 24061 USA.
RP Cloyed, CS (corresponding author), Natl Great Rivers Res & Educ Ctr, East Alton, IL 62024 USA.; Cloyed, CS (corresponding author), Washington Univ, Dept Biol, Campus Box 1137, St Louis, MO 63130 USA.; Cloyed, CS (corresponding author), Dauphin Isl Sea Lab, Dauphin Isl, AL 36528 USA.
EM ccloyed@disl.org
OI Cappelli, Laura/0000-0003-3159-2777
FU National Great Rivers Research and Education Center (NGRREC)
   [NGRREC-IP2015-05, NGRREC-IP2015-06]
FX We thank Lindenwood University for access to the field sites. We thank
   the National Great Rivers Research and Education Center (NGRREC) for
   providing undergraduate internships for L.R.C. (NGRREC-IP2015-05) and
   D.A.T. (NGRREC-IP2015-06).
CR Andren H, 1997, OIKOS, V80, P193, DOI 10.2307/3546534
   Barea-Azcon JM, 2007, BIODIVERS CONSERV, V16, P1213, DOI 10.1007/s10531-006-9114-x
   Cattet MRL, 2002, CAN J ZOOL, V80, P1156, DOI 10.1139/Z02-103
   CLARK WR, 1989, J WILDLIFE MANAGE, V53, P982, DOI 10.2307/3809599
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   DEMERS MN, 1995, CONSERV BIOL, V9, P1159, DOI 10.1046/j.1523-1739.1995.9051148.x-i1
   Elliott AG, 2006, WILDLIFE SOC B, V34, P485, DOI 10.2193/0091-7648(2006)34[485:SMRTSA]2.0.CO;2
   Gehring TM, 2003, BIOL CONSERV, V109, P283, DOI 10.1016/S0006-3207(02)00156-8
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Gonzales EK, 2005, CAN FIELD NAT, V119, P343, DOI 10.22621/cfn.v119i3.143
   Hargis CD, 1999, J APPL ECOL, V36, P157, DOI 10.1046/j.1365-2664.1999.00377.x
   Hilderbrand GV, 2000, J WILDLIFE MANAGE, V64, P178, DOI 10.2307/3802988
   Humber JM, 2011, BIOL INVASIONS, V13, P2361, DOI 10.1007/s10530-011-0048-1
   HUMPHREY SR, 1974, BIOSCIENCE, V24, P457, DOI 10.2307/1296853
   Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Keller LF, 2002, TRENDS ECOL EVOL, V17, P230, DOI 10.1016/S0169-5347(02)02489-8
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Lambin EF, 2001, GLOBAL ENVIRON CHANG, V11, P261, DOI 10.1016/S0959-3780(01)00007-3
   Lee JC, 2009, SOUTHEAST NAT, V8, P157, DOI 10.1656/058.008.0114
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Lingle S, 2002, ECOLOGY, V83, P2037
   Lingle S, 2001, ETHOLOGY, V107, P125, DOI 10.1046/j.1439-0310.2001.00647.x
   McCleery RA, 2007, J WILDLIFE MANAGE, V71, P1149, DOI 10.2193/2006-282
   McDonough CM, 2000, AM MIDL NAT, V144, P139, DOI 10.1674/0003-0031(2000)144[0139:SOONBA]2.0.CO;2
   MILLS LS, 1995, CONSERV BIOL, V9, P395, DOI 10.1046/j.1523-1739.1995.9020395.x
   Moorcroft PR, 2006, P ROY SOC B-BIOL SCI, V273, P1651, DOI 10.1098/rspb.2005.3439
   MURCIA C, 1995, TRENDS ECOL EVOL, V10, P58, DOI 10.1016/S0169-5347(00)88977-6
   Nixon Charles M., 1994, Transactions of the Illinois State Academy of Science, V87, P187
   Pierce RA, 2011, ECOLOGY MANAGEMENT W, P1
   PROTHERO J, 1992, AM J PHYSIOL, V262, pR492, DOI 10.1152/ajpregu.1992.262.3.R492
   R Core Development Team, 2016, R LANG ENV STAT COMP
   Reighard Steven L., 2004, Proceedings of the South Dakota Academy of Science, V83, P47
   Roberts CW, 2006, J WILDLIFE MANAGE, V70, P263, DOI 10.2193/0022-541X(2006)70[263:COCARS]2.0.CO;2
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Smith JK, 2012, AUST MAMMAL, V34, P196, DOI 10.1071/AM11034
   Soisalo MK, 2006, BIOL CONSERV, V129, P487, DOI 10.1016/j.biocon.2005.11.023
   Spritzer MD, 2002, AM MIDL NAT, V148, P271, DOI 10.1674/0003-0031(2002)148[0271:DMUASA]2.0.CO;2
   Srbek-Araujo AC, 2005, J TROP ECOL, V21, P121, DOI 10.1017/S0266467404001956
   Stein B, 2000, PRECIOUS HERITAGE
   Stephens HC, 2013, AUSTRAL ECOL, V38, P568, DOI 10.1111/aec.12001
   Stephens RB, 2014, AM MIDL NAT, V171, P139, DOI 10.1674/0003-0031-171.1.139
   Taulman JF, 1996, J BIOGEOGR, V23, P635, DOI 10.1111/j.1365-2699.1996.tb00024.x
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Vas E, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2014.0754
   Way JG, 2005, CAN FIELD NAT, V119, P139, DOI 10.22621/cfn.v119i1.98
   WIGGERS EP, 1986, J WILDLIFE MANAGE, V50, P129, DOI 10.2307/3801502
NR 51
TC 2
Z9 2
U1 4
U2 34
PU U S FISH & WILDLIFE SERVICE
PI SHEPHERDSTOWN
PA NATL CONSERVATION TRAINING CENTER, CONSERVATION LIBRARY, 698
   CONSERVATION WAY, SHEPHERDSTOWN, WV 25443 USA
SN 1944-687X
J9 J FISH WILDL MANAG
JI J. Fish Wildl. Manag.
PD DEC
PY 2018
VL 9
IS 2
BP 485
EP 495
DI 10.3996/122017-JFWM-103
PG 11
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA HE1PX
UT WOS:000453046200012
OA gold
DA 2022-02-10
ER

PT J
AU Peres, PHF
   Polverini, MS
   Oliveira, ML
   Duarte, JMB
AF Peres, Pedro Henrique F.
   Polverini, Maxihilian S.
   Oliveira, Marcio L.
   Duarte, Jose Mauricio B.
TI Accessing camera trap survey feasibility for estimating Blastocerus
   dichotomus (Cetartiodactyla, Cervidae) demographic parameters
SO IHERINGIA SERIE ZOOLOGIA
LA English
DT Article
DE Aerial survey; capture-recapture; distance sampling; population
   estimate; sex ratio
ID WHITE-TAILED DEER; INFRARED-TRIGGERED CAMERAS; MARSH DEER; PANTANAL
   WETLAND; HERD COMPOSITION; POPULATION-SIZE; AERIAL SURVEY; DENSITY;
   BRAZIL; ABUNDANCE
AB Demographic information is the basis for evaluating and planning conservation strategies for an endangered species. However, in numerous situations there are methodological or financial limitations to obtain such information for some species. The marsh deer, an endangered Neotropical cervid, is a challenging species to obtain biological information. To help achieve such aims, the study evaluated the applicability of camera traps to obtain demographic information on the marsh deer compared to the traditional aerial census method. Fourteen camera traps were installed for three months on the Capao da Cruz floodplain, in state of Sao Paulo, and ten helicopter flyovers were made along a 13-kilometer trajectory to detect resident marsh deer. In addition to counting deer, the study aimed to identify the sex, age group and individual identification of the antlered males recorded. Population estimates were performed using the capture-mark-recapture method with the camera trap data and by the distance sampling method for aerial observation data. The costs and field efforts expended for both methodologies were calculated and compared. Twenty independent photographic records and 42 sightings were obtained and generated estimates of 0.98 and 1.06 ind/km(2), respectively. In contrast to the aerial census, camera traps allowed us to individually identify branch-antlered males, determine the sex ratio and detect fawns in the population. The cost of camera traps was 78% lower but required 20 times more field effort. Our analysis indicates that camera traps present a superior cost-benefit ratio compared to aerial surveys, since they are more informative, cheaper and offer simpler logistics. Their application extends the possibilities of studying a greater number of populations in a long-term monitoring.
C1 [Peres, Pedro Henrique F.; Polverini, Maxihilian S.; Oliveira, Marcio L.; Duarte, Jose Mauricio B.] Univ Estadual Paulista, UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, FCAV, Via Acesso Paulo Donato Castellane S-N, BR-14884900 Jaboticabal, SP, Brazil.
RP Peres, PHF (corresponding author), Univ Estadual Paulista, UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, FCAV, Via Acesso Paulo Donato Castellane S-N, BR-14884900 Jaboticabal, SP, Brazil.
EM pedrof182@gmail.com
RI Duarte, José Maurício Barbanti/AAG-5149-2019; de Oliveira, Márcio
   L/F-3792-2012
OI Duarte, José Maurício Barbanti/0000-0002-7805-0265; de Oliveira, Márcio
   L/0000-0002-7705-0626; Peres, Pedro Henrique/0000-0002-3158-0963
FU Funbio; FAPESPFundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP)
FX Field work for this study was supported by Funbio in the context of
   Tropical Forest Conservation Act grant and the authors PHFP and MLO
   receives a scholarship from FAPESP. We thank FUNEP and NUPECCE for
   logistical support, the Instituto Florestal de Sao Paulo (COTEC SMA:
   260108-004.556/2013) and the Instituto Chico Mendes de Conservacao da
   Biodiversidade (SISBIO 38267-2) for permission to conduct research in
   the study site. Finally we sincerely thank Jonathan Hill for English
   review of the manuscript.
CR Akaike H., 1985, CELEBRATION STAT ISI, P387, DOI DOI 10.1007/978-1-4613-8560-8_1
   Andriolo A, 2005, BRAZ ARCH BIOL TECHN, V48, P807, DOI 10.1590/S1516-89132005000600017
   Andriolo A, 2013, ZOOLOGIA-CURITIBA, V30, P630, DOI 10.1590/S1984-46702013005000015
   Bender LC, 2006, WILDLIFE SOC B, V34, P1225, DOI 10.2193/0091-7648(2006)34[1225:UOHCAA]2.0.CO;2
   Cabrera A., 1961, CIENCIAS ZOOLOGICAS, V4, P309
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   CAUGHLEY G, 1977, J WILDLIFE MANAGE, V41, P605, DOI 10.2307/3799980
   CAUGHLEY G, 1981, AUST WILDLIFE RES, V8, P1
   Curtis Paul D., 2009, Human-Wildlife Conflicts, V3, P116
   DeYoung CA, 2011, BIOLOGY AND MANAGEMENT OF WHITE-TAILED DEER, P147
   Dougherty SQ, 2012, POPUL ECOL, V54, P357, DOI 10.1007/s10144-012-0311-z
   DUARTE J.M.B., 2012, BIODIVERS BRAS, P3
   Duarte J. M. B., 2016, IUCN RED LIST THREAT
   Duarte JMB, 1996, GUIA IDENTIFICACAO C
   Figueira C. J. M., 2005, Braz. J. Biol., V65, P263, DOI 10.1590/S1519-69842005000200009
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Hewison AJM, 1996, BEHAV ECOL, V7, P461, DOI 10.1093/beheco/7.4.461
   Hofmann R. K., 1976, REV FORESTAL PERU, V6, P1
   Ikeda T, 2013, MAMM STUDY, V38, P29, DOI 10.3106/041.038.0103
   Jacobson HA, 1997, WILDLIFE SOC B, V25, P547
   Junqueira J. F. D., 1940, CHACARAS QUINTAIS, V62, P330
   Karanth K. Ullas, 2004, P229
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   KENNEY AJ, 1998, PROGRAMS ECOLOGICAL
   Koerth BH, 1997, WILDLIFE SOC B, V25, P557
   Laake J.L., 1993, DISTANCE USERS GUIDE
   Mccoy JC, 2011, J WILDLIFE MANAGE, V75, P472, DOI 10.1002/jwmg.54
   Moore M. T., 2013, J SE ASS FISH WILDLI, V1, P127
   MOURAO G, 1995, BIOL CONSERV, V73, P27, DOI 10.1016/0006-3207(94)00097-A
   Mourao G, 2000, BIOL CONSERV, V92, P175, DOI 10.1016/S0006-3207(99)00051-8
   Noss A.J., 2003, Tapir Conservation, V12, P24
   Pease BS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0166689
   Pereira R. J. G., 2010, NEOTROPICAL CERVIDOL
   Pinder L, 1996, BIOL CONSERV, V75, P87, DOI 10.1016/0006-3207(95)00033-X
   PINDER L, 1991, Mammalian Species, P1, DOI 10.2307/3504311
   Piovezan Ubiratan, 2010, P66
   Pohler PS, 2014, WILDLIFE SOC B, V38, P466, DOI 10.1002/wsb.430
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Roberts CW, 2006, J WILDLIFE MANAGE, V70, P263, DOI 10.2193/0022-541X(2006)70[263:COCARS]2.0.CO;2
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   SAO PAULO, 2013, PLANO MANEJO ESTACAO
   Schaller G.B., 1978, Oryx, V14, P345
   Schumacher F. X., 1943, JOUR TENNESSEE ACAD SCI, V18, P228
   Shields AV, 2012, SCI WORLD J, DOI 10.1100/2012/846218
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Tiepolo LM, 2010, IHERINGIA SER ZOOL, V100, P111, DOI 10.1590/S0073-47212010000200004
   Tomas WM, 2001, STUD NEOTROP FAUNA E, V36, P9, DOI 10.1076/snfe.36.1.9.8877
   Tomas WM, 1997, BIOL CONSERVACAO CER, P24
   Trolle M, 2008, BIOTROPICA, V40, P211, DOI 10.1111/j.1744-7429.2007.00350.x
   Trolle M, 2007, BIODIVERS CONSERV, V16, P1197, DOI 10.1007/s10531-006-9105-y
   Watts DE, 2008, J WILDLIFE MANAGE, V72, P360, DOI 10.2193/2007-166
   Webb Stephen L., 2010, International Journal of Ecology, V2010, P1
   Weckel M, 2011, WILDLIFE SOC B, V35, P445, DOI 10.1002/wsb.64
   WILSON KR, 1985, J MAMMAL, V66, P13, DOI 10.2307/1380951
NR 57
TC 4
Z9 4
U1 2
U2 16
PU FUNDACAO ZOOBOTANICA  RIO GRANDE SUL, MUSEU CIENCIAS NATURAIS
PI PORTO ALEGRE
PA CAIXA POSTAL 1188, PORTO ALEGRE, RS 00000, BRAZIL
SN 0073-4721
EI 1678-4766
J9 IHERINGIA SER ZOOL
JI Iheringia Ser. Zool.
PY 2017
VL 107
AR e2017041
DI 10.1590/1678-4766e2017041
PG 8
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA FO1BQ
UT WOS:000416492100002
OA Green Submitted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Rheingantz, ML
   Leuchtenberger, C
   Zucco, CA
   Fernandez, FAS
AF Rheingantz, Marcelo Lopes
   Leuchtenberger, Caroline
   Zucco, Carlos Andre
   Fernandez, Fernando A. S.
TI Differences in activity patterns of the Neotropical otter Lontra
   longicaudis between rivers of two Brazilian ecoregions
SO JOURNAL OF TROPICAL ECOLOGY
LA English
DT Article
DE activity; camera trap; circular kernel; Neotropical otter; niche shift
ID ATLANTIC FOREST; CAMERA TRAPS; ECOLOGY
AB Circadian use of time is an important, but often neglected, part of an animal's niche. We compared the activity patterns of the Neotropical otter Lontra longicaudis in two different areas in Brazil using camera traps placed at the entrance of holts. We obtained 58 independent photos in the Atlantic Forest (273 camera trap-days) and 46 photos in Pantanal (300 camera trap-days). We observed different kernel density probabilities on these two areas (45.6% and 14.1% overlap between the 95% and 50% density isopleths respectively). We observed the plasticity in Neotropical otter activity behaviour with different activity patterns in the two areas. In the Pantanal, the Neotropical otter selected daylight (Ivlev = 0.23) and avoided night (Ivlev = -0.44), while in the Atlantic Forest it selected dawn (Ivlev = 0.24) and night (Ivlev = 0.14), avoiding daylight (Ivlev = -0.33). We believe that this pattern can be due to human activity or shifts in prey activity.
C1 [Rheingantz, Marcelo Lopes; Zucco, Carlos Andre; Fernandez, Fernando A. S.] Univ Fed Rio de Janeiro, Inst Biol, Dept Ecol, Lab Ecol & Conservacao Populacoes,Ilha Fundao,CCS, Ave Brigadeiro Trompowski S-N,Bloco A,Sala A2-102, BR-21941590 Rio De Janeiro, RJ, Brazil.
   [Rheingantz, Marcelo Lopes; Zucco, Carlos Andre; Fernandez, Fernando A. S.] Univ Fed Rio de Janeiro, PPGE, Inst Biol, Dept Ecol,CCS,Ilha Fundao, Ave Brigadeiro Trompowski S-N,Bloco A, BR-21941590 Rio de Janeiro, RJ, Brazil.
   [Leuchtenberger, Caroline] Embrapa Pantanal, Lab Vida Selvagem, 21 Setembro 1880, BR-79320900 Corumba, MS, Brazil.
RP Rheingantz, ML (corresponding author), Univ Fed Rio de Janeiro, Inst Biol, Dept Ecol, Lab Ecol & Conservacao Populacoes,Ilha Fundao,CCS, Ave Brigadeiro Trompowski S-N,Bloco A,Sala A2-102, BR-21941590 Rio De Janeiro, RJ, Brazil.; Rheingantz, ML (corresponding author), Univ Fed Rio de Janeiro, PPGE, Inst Biol, Dept Ecol,CCS,Ilha Fundao, Ave Brigadeiro Trompowski S-N,Bloco A, BR-21941590 Rio de Janeiro, RJ, Brazil.
EM mlrheingantz@gmail.com
RI Fernandez, Fernando/K-2412-2012; Rheingantz, Marcelo Lopes/F-3934-2014
OI Rheingantz, Marcelo Lopes/0000-0003-2521-3654
FU Brazilian Agricultural Research Corporation (Embrapa-Pantanal); Barranco
   Alto Farm; Reserva Botanica Aguas Claras; Universidade Federal do Rio de
   Janeiro; Fundacao Grupo Boticario; CNPqConselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPQ); CNPqscholarshipConselho
   Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)
FX We acknowledge the Brazilian Agricultural Research Corporation
   (Embrapa-Pantanal) and Barranco Alto Farm for financial and logistic
   support in the Pantanal biome. We also acknowledge Reserva Botanica
   Aguas Claras, Universidade Federal do Rio de Janeiro, Fundacao Grupo
   Boticario and CNPq for financial and logistical support in the Atlantic
   Forest biome. CL was recipient of CNPqscholarship. LucasLeuzinger helped
   us with camera trap survey at the Pantanal area. We also acknowledge
   Bernardo Araujo for the English review.
CR Britski HA, 1999, PEIXES PANTANAL MANU
   Di Bitetti MS, 2010, ACTA OECOL, V36, P403, DOI 10.1016/j.actao.2010.04.001
   Galliez M, 2009, J MAMMAL, V90, P93, DOI 10.1644/07-MAMM-A-397.1
   Gerber BD, 2012, J MAMMAL, V93, P667, DOI 10.1644/11-MAMM-A-265.1
   Gomez H, 2005, STUD NEOTROP FAUNA E, V40, P91, DOI 10.1080/01650520500129638
   Ivlev VS, 1961, EXPT ECOLOGY FEEDING
   Junk WJ, 2005, ECOL ENG, V24, P391, DOI 10.1016/j.ecoleng.2004.11.012
   Kruuk H., 2006, OTTERS ECOLOGY BEHAV
   Lerone L, 2015, WILDLIFE SOC B, V39, P193, DOI 10.1002/wsb.508
   Leuchtenberger C, 2014, ETHOL ECOL EVOL, V26, P19, DOI 10.1080/03949370.2013.821673
   LODE T, 1995, ETHOLOGY, V100, P295
   McClennen N, 2001, AM MIDL NAT, V146, P27, DOI 10.1674/0003-0031(2001)146[0027:TEOSAA]2.0.CO;2
   Metzger JP, 2009, BIOL CONSERV, V142, P1138, DOI 10.1016/j.biocon.2008.10.012
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Oliveira-Santos LGR, 2013, ANIM BEHAV, V85, P269, DOI 10.1016/j.anbehav.2012.09.033
   PICKLES R., 2011, IUCN OTTER SPECIALIS, V28, P39
   Rheingantz Marcelo Lopes, 2012, IUCN Otter Specialist Group Bulletin, V29, P80
   RILEY S. P., 2008, CONSERV BIOL, V17, P566
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Silva RE, 2014, J NAT HIST, V48, P465, DOI 10.1080/00222933.2013.800607
   Zhou Q, 2007, INT J PRIMATOL, V28, P657, DOI 10.1007/s10764-007-9144-6
NR 23
TC 14
Z9 15
U1 3
U2 44
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0266-4674
EI 1469-7831
J9 J TROP ECOL
JI J. Trop. Ecol.
PD MAR
PY 2016
VL 32
BP 170
EP 174
DI 10.1017/S0266467416000079
PN 2
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA DH1CR
UT WOS:000372522500010
DA 2022-02-10
ER

PT C
AU Zhu, CB
   Li, TH
   Li, G
AF Zhu, Chunbiao
   Li, Thomas H.
   Li, Ge
GP IEEE
TI Towards Automatic Wild Animal Detection in Low Quality Camera-trap
   Images Using Two-channeled Perceiving Residual Pyramid Networks
SO 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT 16th IEEE International Conference on Computer Vision (ICCV)
CY OCT 22-29, 2017
CL Venice, ITALY
SP IEEE, IEEE Comp Soc
AB Monitoring animals in the wild without disturbing them is possible using camera trapping framework, which is a technique to study wildlife using automatically triggered cameras and produces great volumes of data. However, camera trapping collects images often result in low image quality and includes a lot of false positives (images without animals), which must be detection before the post-processing step. This paper presents a two-channeled perceiving residual pyramid networks (TPRPN) for cameratrap images objection. Our TPRPN model attends to generating high-resolution and high-quality results. In order to provide enough local information, we extract depth cue from the original images and use two-channeled perceiving model as input to training our networks. Finally, the proposed three-layer residual blocks learn to merge all the information and generate full size detection results. Besides, we construct a new high-quality dataset with the help of Wildlife Thailand's Community and eMammal Organization. Experimental results on our dataset demonstrate that our method is superior to the existing object detection methods.
C1 [Zhu, Chunbiao; Li, Ge] Peking Univ, Shenzhen Grad Sch, SECE, Shenzhen, Peoples R China.
   [Li, Thomas H.] Gpower Semicond Inc, Suzhou, Peoples R China.
RP Zhu, CB (corresponding author), Peking Univ, Shenzhen Grad Sch, SECE, Shenzhen, Peoples R China.
EM zhuchunbiao@pku.edu.cn; thomas.li@gpower-semi.com; geli@ece.pku.edu.cn
RI Zhu, Chunbiao/AAK-6667-2020
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [U1611461]; Science and Technology Planning
   Project of Guangdong Province, China [2014B090910001]; Shenzhen Peacock
   Plan [20130408-183003656]
FX We would like to thank anonymous reviewers for their helpful comments on
   the paper. This work was supported by the grant of National Natural
   Science Foundation of China (No. U1611461), the grant of Science and
   Technology Planning Project of Guangdong Province, China (No.
   2014B090910001) and the grant of Shenzhen Peacock Plan (No.
   20130408-183003656).
CR Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Giraldozuluaga J. H., 2017, CAMERA TRAP IMAGES S
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Lucchi A, 2012, IEEE T MED IMAGING, V31, P474, DOI 10.1109/TMI.2011.2171705
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Ravela S., 2008, VISUAL RECAPTURE MOV
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Shukla A, 2016, IEEE IMAGE PROC, P3982, DOI 10.1109/ICIP.2016.7533107
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhu C., 2017, 2017 IEEE INT C COMP
   Zhu C., 2017, ACM MULTIMEDIA WORKS
   Zhu C., 2017, MULTILAYER BACKPROPA, P14
   Zhu CB, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P33, DOI 10.1109/BigMM.2017.22
NR 18
TC 37
Z9 38
U1 2
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2473-9936
BN 978-1-5386-1034-3
J9 IEEE INT CONF COMP V
PY 2017
BP 2860
EP 2864
DI 10.1109/ICCVW.2017.337
PG 5
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4OB
UT WOS:000425239602109
DA 2022-02-10
ER

PT C
AU Rosser, HK
   Wiggins, A
AF Rosser, Holly K.
   Wiggins, Andrea
BE Bui, TX
TI Crowds and Camera Traps: Genres in Online Citizen Science Projects
SO PROCEEDINGS OF THE 52ND ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM
   SCIENCES
LA English
DT Proceedings Paper
CT 52ndHawaii International Conference on System Sciences (HICSS)
CY JAN 08-11, 2019
CL HI
AB Despite the importance of instruction for effective task completion in crowdsourcing, particularly for scientific work, little attention has been given to the design of instructional materials in crowdsourcing and citizen science. Consequences of inattention to tutorial design are further magnified by the diversity of citizen science volunteers. We use digital genre theory to identify the norms of tutorial design for the most abundant citizen science project type on the Zooniverse platform, camera trap image classification, where a highly-standardized task structure makes it a strong candidate as a specific genre of citizen science.
   Comparative content analysis of 14 projects' features, tutorial design, and supporting materials identified a great deal of uniformity in some respects (indicating an emergent genre) but surprising variation in others. As further evidence of an emergent genre, the amount of mentoring the science team received and specific task features of the project appeared to impact tutorial design and supporting resources. Our findings suggest that genre theory provides a useful lens for understanding crowd science projects with otherwise disparate characteristics and identifying instances where the digital medium can be deployed more effectively for task instruction.
C1 [Rosser, Holly K.; Wiggins, Andrea] Univ Nebraska, Omaha, NE 68182 USA.
RP Rosser, HK (corresponding author), Univ Nebraska, Omaha, NE 68182 USA.
EM hrosser@unomaha.edu; wiggins@unomaha.edu
FU Global Impact Award from GoogleGoogle Incorporated; Alfred P. Sloan
   FoundationAlfred P. Sloan Foundation; Nebraska Research Initiative
FX This publication uses data generated via the Zooniverse.org platform,
   development of which is funded by generous support, including a Global
   Impact Award from Google, and by a grant from the Alfred P. Sloan
   Foundation.; This work was supported in part by the Nebraska Research
   Initiative.
CR Askehave Inger, 2005, P 38 ANN HAW INT C S, p98a, DOI DOI 10.1109/HICSS.2005.687
   Bowen GA, 2009, QUAL RES J, V9, P27, DOI 10.3316/QRJ0902027
   Bowyer Alex, 2015, ASS ADV ART INT AAAI, P1
   Causer T., 2012, DIGITAL HUMANITIES Q, V6
   Cox, 2014, DESIGNING DABBLERS D, P2985
   Crowston K, 2017, CSCW'17: COMPANION OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P163, DOI 10.1145/3022198.3026329
   Crowston K, 2010, TEXT SPEECH LANG TEC, V42, P69, DOI 10.1007/978-90-481-9178-9_4
   Finnerty A., 2013, P BIANN C IT CHAPT S, P14
   Hoffman C, 2017, ADV KNOWL ACQUISITIO, P50, DOI 10.4018/978-1-5225-0962-2.ch003
   Horton, 2013, P 2013 C COMP SUPP C, P1301, DOI DOI 10.1145/2441776.2441923
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Kwasnik Barbara H, GENRES DIGITAL DOCUM
   Law E, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1544, DOI 10.1145/2998181.2998197
   Miller Grant, 2015, WHOLE NEW ZOONIVERSE
   Neuendorf K.A., 2016, CONTENT ANAL GUIDEBO
   Rosso MA, 2008, J AM SOC INF SCI TEC, V59, P1053, DOI 10.1002/asi.20798
   Schulze T., 2011, ECIS, V11, P1
   Sprinks J, 2017, INT J HUM-COMPUT ST, V104, P50, DOI 10.1016/j.ijhcs.2017.03.003
   The Zooniverse, 2009, ZOONIVERSE IS GO
   Theobald EJ, 2015, BIOL CONSERV, V181, P236, DOI 10.1016/j.biocon.2014.10.021
   Tinati R, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P4069, DOI 10.1145/2702123.2702420
   Wiggins A., 2012, 2012 45th Hawaii International Conference on System Sciences (HICSS), P3426, DOI 10.1109/HICSS.2012.295
   Wiggins A., 2011, 2011 44 HAW INT C SY, V44, P1
   Yadav Poonam, 2016, ARXIV160500910
NR 24
TC 1
Z9 1
U1 0
U2 0
PU HICSS
PI Honolulu
PA Dept IT Mgmt, Shidler College of Business, Univ Hawaii at Manoa 2404
   Maile Way D307, Honolulu, Hawaii, UNITED STATES
BN 978-0-9981331-2-6
PY 2019
BP 5289
EP 5298
PG 10
WC Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ9KL
UT WOS:000625294905044
DA 2022-02-10
ER

PT J
AU Gadsden, GI
   Malhotra, R
   Schell, J
   Carey, T
   Harris, NC
AF Gadsden, Gabriel, I
   Malhotra, Rumaan
   Schell, Justin
   Carey, Tiffany
   Harris, Nyeema C.
TI Michigan ZoomIN: Validating Crowd-Sourcing to Identify Mammals from
   Camera Surveys
SO WILDLIFE SOCIETY BULLETIN
LA English
DT Article
DE accuracy; carnivores; citizen science; community; engagement; geographic
   variation; Michigan; mustelid; validation
ID CITIZEN SCIENCE DATA; CONSERVATION; TRAPS; BIODIVERSITY
AB Camera trap studies have become a popular medium to assess many ecological phenomena including population dynamics, patterns of biodiversity, and monitoring of endangered species. In conjunction with the benefit to scientists, camera traps present an unprecedented opportunity to involve the public in scientific research via image classifications. However, this engagement strategy comes with a myriad of complications. Volunteers vary in their familiarity with wildlife, thus, the accuracy of user-derived classifications may be biased by the commonness or popularity of species and user-experience. From an extensive multi-site camera trap study across Michigan, U.S.A, we compiled and classified images through a public science platform called Michigan ZoomIN. We aggregated responses from 15 independent users per image using multiple consensus methods to assess accuracy by comparing to species identification completed by wildlife experts. We also evaluated how different factors including consensus algorithms, study area, wildlife species, user support, and camera type influenced the accuracy of user-derived classifications. Overall accuracy of user-derived classification was 97%; although, several canid (e.g., Canis lupus, Vulpes vulpes) and mustelid (e.g., Neovison vison) species were repeatedly difficult to identify by users and had lower accuracy. When validating user-derived classification, we found that study area, consensus method, and user support best explained accuracy. To overcome hesitancy associated with data collected by untrained participants, we demonstrated their value by showing that the accuracy from volunteers was comparable to experts when classifying North American mammals. Our hierarchical workflow that integrated multiple consensus methods led to more image classifications without extensive training and even when the expertise of the volunteer was unknown. Ultimately, adopting such an approach can harness broader participation, expedite future camera trap data synthesis, and improve allocation of resources by scholars to enhance performance of public participants and increase accuracy of user-derived data. (c) 2021 The Wildlife Society.
C1 [Gadsden, Gabriel, I; Malhotra, Rumaan; Carey, Tiffany; Harris, Nyeema C.] Univ Michigan, Appl Wildlife Ecol Lab, Ecol & Evolutionary Biol, 1105 N Univ Ave, Ann Arbor, MI 48109 USA.
   [Schell, Justin] Univ Michigan, Shapiro Design Lab, Ann Arbor, MI 48109 USA.
   [Gadsden, Gabriel, I] Univ Michigan, Urban Energy Justice Lab, Sch Environm & Sustainabil, 440 Church St, Ann Arbor, MI 48109 USA.
   [Carey, Tiffany] Great Lakes Reg Ctr, Natl Wildlife Federat, Ann Arbor, MI 48109 USA.
RP Harris, NC (corresponding author), Univ Michigan, Appl Wildlife Ecol Lab, Ecol & Evolutionary Biol, 1105 N Univ Ave, Ann Arbor, MI 48109 USA.
EM nyeema@umich.edu
OI Harris, Nyeema/0000-0001-5174-2205
FU GoogleGoogle Incorporated; Alfred P. Sloan FoundationAlfred P. Sloan
   Foundation; Detroit Zoological Society; Huron Mountain Wildlife
   Foundation; University of Michigan Biological StationUniversity of
   Michigan System; National Science Foundation-Advancing Informal Learning
   in STEM [2005812]
FX First, we recognize implementing our field research of camera surveys
   was conducted on lands originally belonging to the People of the Three
   Fires. We thank the Shiawassee National Wildlife Refuge, the University
   of Michigan Biological Station, and the Huron Mountain Club for
   providing access to implement camera surveys for fieldwork on their
   property. We would like to thank the Applied Wildlife Ecology (AWE) Lab
   at the University of Michigan for assistance with validating
   classification. Our work is not human subjects research requiring IRB
   review, though we remain forever indebted to the nearly 4,000 anonymous
   volunteers that contributed their time to identify images on Michigan
   ZoomIN (MZI) and the dozens of colleagues that integrated MZI into
   curricula. Our publication used data generated via the platform,
   development of which was funded by generous support including a Global
   Impact Award from Google and by a grant from the Alfred P. Sloan
   Foundation. We appreciate technological support from University of
   Michigan LSA Information Technology and Technology Services teams
   especially A. Roelofs and P. Knoop. We thank G. Kuhnlein, Michigan News,
   and the University of Michigan Museum of Natural History for assistance
   with dissemination on multi-media platforms to recruit volunteers. We
   appreciate the funding provided by the Detroit Zoological Society, the
   Huron Mountain Wildlife Foundation, the University of Michigan
   Biological Station, and the National Science Foundation-Advancing
   Informal Learning in STEM (EHR #2005812). We also thank N. Peterson
   (Associate Editor), A. Knipps (Editorial Assistant), and 2 anonymous
   reviewers for their comments, which improved the manuscript.
CR Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Bonney R, 2016, PUBLIC UNDERST SCI, V25, P2, DOI 10.1177/0963662515607406
   Broeckhoven C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137428
   Brown G, 2018, BIOL CONSERV, V227, P141, DOI 10.1016/j.biocon.2018.09.016
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Colborn AS, 2020, J ANIM ECOL, V89, P1952, DOI 10.1111/1365-2656.13266
   Crall AW, 2011, CONSERV LETT, V4, P433, DOI 10.1111/j.1755-263X.2011.00196.x
   Danielsen F, 2005, BIODIVERS CONSERV, V14, P2507, DOI 10.1007/s10531-005-8375-0
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Egerer M, 2019, SCI TOTAL ENVIRON, V694, DOI 10.1016/j.scitotenv.2019.133738
   Freitag A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0064079
   Halpern BS, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8615
   Hochachka WM, 2012, TRENDS ECOL EVOL, V27, P130, DOI 10.1016/j.tree.2011.11.006
   Hofmeester TR, 2019, ECOL EVOL, V9, P2320, DOI 10.1002/ece3.4878
   Jacobs CE, 2018, REMOTE SENS ECOL CON, V4, P352, DOI 10.1002/rse2.81
   Katrak-Adefowora R., 2020, CITIZEN SCI THEORY P, V5, P1, DOI [10.5334/cstp.219, DOI 10.5334/CSTP.219]
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lintott C, 2019, CROWD COSMOS ADVENTU
   Locke CM, 2019, WILDLIFE SOC B, V43, P4, DOI 10.1002/wsb.943
   Maes D, 2015, BIOL J LINN SOC, V115, P690, DOI 10.1111/bij.12530
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Parrish JK, 2019, P NATL ACAD SCI USA, V116, P1894, DOI 10.1073/pnas.1807186115
   Peterson MN, 2019, BIOL CONSERV, V235, P290, DOI 10.1016/j.biocon.2019.05.013
   PIELOU EC, 1966, J THEOR BIOL, V13, P131, DOI 10.1016/0022-5193(66)90013-0
   Potter LC, 2019, AUSTRAL ECOL, V44, P473, DOI 10.1111/aec.12681
   Prugh LR, 2009, BIOSCIENCE, V59, P779, DOI 10.1525/bio.2009.59.9.9
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Raudsepp-Hearne C, 2010, P NATL ACAD SCI USA, V107, P5242, DOI 10.1073/pnas.0907284107
   Ries L, 2015, BIOSCIENCE, V65, P419, DOI 10.1093/biosci/biv011
   Schuttler SG, 2019, BIOSCIENCE, V69, P69, DOI 10.1093/biosci/biy141
   Schuttler SG, 2018, FRONT ECOL ENVIRON, V16, P405, DOI 10.1002/fee.1826
   Shipley JR, 2018, REMOTE SENS ECOL CON, V4, P127, DOI 10.1002/rse2.62
   Skarlatidou A, 2019, JCOM-J SCI COMMUN, V18, DOI 10.22323/2.18010202
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Steger C, 2017, J APPL ECOL, V54, P2053, DOI 10.1111/1365-2664.12921
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Theobald EJ, 2015, BIOL CONSERV, V181, P236, DOI 10.1016/j.biocon.2014.10.021
   Trumbull DJ, 2000, SCI EDUC, V84, P265, DOI 10.1002/(SICI)1098-237X(200003)84:2<265::AID-SCE7>3.0.CO;2-5
   Waetjen DP, 2017, FRONT ECOL EVOL, V5, DOI 10.3389/fevo.2017.00089
   Young HS, 2016, ANNU REV ECOL EVOL S, V47, P333, DOI 10.1146/annurev-ecolsys-112414-054142
NR 44
TC 0
Z9 0
U1 4
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2328-5540
J9 WILDLIFE SOC B
JI Wildl. Soc. Bull.
PD JUN
PY 2021
VL 45
IS 2
BP 221
EP 229
DI 10.1002/wsb.1175
EA MAY 2021
PG 9
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA TY3RZ
UT WOS:000653252600001
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Lopucki, R
   Kiersztyn, A
AF Lopucki, Rafal
   Kiersztyn, Adam
TI The city changes the daily activity of urban adapters: Camera-traps
   study of Apodemus agrarius behaviour and new approaches to data analysis
SO ECOLOGICAL INDICATORS
LA English
DT Article
DE Activity patterns; Urban; Indicators of urbanization; Behaviour;
   Particle swarm optimization; Neural networks; Camera-traps; Data mining
ID PREDATION; URBANIZATION; COMPLEXITY; OVERLAP; LAB
AB As a result of the widespread use of camera-traps, the analysis of the daily activity of animals based on field data has become a common practice, which is addressed in ecological studies. The more frequent consideration of this issue in ecological research, however, has not led to any advancement in the techniques of analysis of these activity patterns. In this work, we have two main aims: ecological and methodological. Firstly, using camera-traps in the winter period, we examine the differences in the daily activity of wild small mammal populations, which are affected or unaffected by urbanization; we treated changes in daily activity as indicators of species adaptation to urban conditions. Secondly, we test four different approaches to data analysis regarding the determination and comparison of activity patterns, which are not based on the traditional methods that have been used to date, such as particle swarm optimization (PSO), neural networks, decision trees and cluster analysis. We found that the urbanized environment modifies the daily activity patterns of the mammals studied. Animals from the urban population have a longer active period than their rural counterparts and can forage under more favourable thermal conditions, so that the energetic cost of foraging is lower. PSO and neural networks allow for a more detailed analysis of patterns of daily activity than traditional methods, and their results correspond well with each other. Daily activity analysis shows great potential in the application of new statistical approaches that could supplement and enrich the traditionally used methods (e.g. the kernel density estimation). Our approach may help researchers to gain a broader perspective during their analysis of daily activity patterns and lead to a better description of the ecology of the species or even to more balanced wildlife management decisions.
C1 [Lopucki, Rafal; Kiersztyn, Adam] John Paul II Catholic Univ Lublin, Ctr Interdisciplinary Res, Konstantynow 1J, PL-20708 Lublin, Poland.
   [Lopucki, Rafal; Kiersztyn, Adam] Lublin Univ Technol, Inst Comp Sci, Nadbystrzycka 36B, PL-20618 Lublin, Poland.
RP Lopucki, R (corresponding author), John Paul II Catholic Univ Lublin, Ctr Interdisciplinary Res, Konstantynow 1J, PL-20708 Lublin, Poland.
EM lopucki@kul.pl; a.kiersztyn@pollub.pl
RI Kiersztyn, Adam/T-3200-2018
OI Kiersztyn, Adam/0000-0001-5222-8101; Lopucki, Rafal/0000-0003-2137-8742
CR Bonier F, 2012, HORM BEHAV, V61, P763, DOI 10.1016/j.yhbeh.2012.03.016
   Calisi RM, 2009, HORM BEHAV, V56, P1, DOI 10.1016/j.yhbeh.2009.02.010
   Chace JF, 2006, LANDSCAPE URBAN PLAN, V74, P46, DOI 10.1016/j.landurbplan.2004.08.007
   Cruz P, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200806
   Daan S, 2011, SLEEP BIOL RHYTHMS, V9, P1, DOI 10.1111/j.1479-8425.2010.00482.x
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Dominoni DM, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0247
   Doody JS, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36384-2
   Eberhart RC, 2001, IEEE C EVOL COMPUTAT, P81, DOI 10.1109/CEC.2001.934374
   Emerson KJ, 2008, EVOLUTION, V62, P979, DOI 10.1111/j.1558-5646.2008.00324.x
   ERLINGE S, 1984, AM NAT, V123, P125, DOI 10.1086/284191
   ERLINGE S, 1983, OIKOS, V40, P36, DOI 10.2307/3544197
   Fischer JD, 2012, BIOSCIENCE, V62, P809, DOI 10.1525/bio.2012.62.9.6
   Gagne SA, 2011, ECOL APPL, V21, P2297, DOI 10.1890/09-1905.1
   George SL, 2006, BIOL CONSERV, V133, P107, DOI 10.1016/j.biocon.2006.05.024
   Gray EL, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181592
   Hall DM, 2017, CONSERV BIOL, V31, P24, DOI 10.1111/cobi.12840
   Hanya G, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190631
   Jones FM, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.124
   JONES MC, 1989, J AM STAT ASSOC, V84, P733, DOI 10.2307/2289655
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   Krauss SL, 2018, ECOL EVOL, V8, P9304, DOI 10.1002/ece3.4438
   Lashley MA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22638-6
   Lopucki R, 2007, POL J ECOL, V55, P543
   Lopucki R, 2019, ECOL INDIC, V101, P1026, DOI 10.1016/j.ecolind.2019.02.016
   Lopucki R, 2019, URBAN ECOSYST, V22, P435, DOI 10.1007/s11252-019-0832-8
   Lopucki R, 2015, URBAN FOR URBAN GREE, V14, P508, DOI 10.1016/j.ufug.2015.05.001
   Magle SB, 2012, BIOL CONSERV, V155, P23, DOI 10.1016/j.biocon.2012.06.018
   McDonald PJ, 2015, BIOL CONSERV, V191, P93, DOI 10.1016/j.biocon.2015.06.027
   McDonnell MJ, 2015, ANNU REV ECOL EVOL S, V46, P261, DOI 10.1146/annurev-ecolsys-112414-054258
   Moller AP, 2009, OECOLOGIA, V159, P849, DOI 10.1007/s00442-008-1259-8
   Nunez-Antonio G, 2018, ENVIRON ECOL STAT, V25, P471, DOI 10.1007/s10651-018-0414-6
   O'Brien TA, 2016, COMPUT STAT DATA AN, V101, P148, DOI 10.1016/j.csda.2016.02.014
   Papadimitriou F, 2013, J LAND USE SCI, V8, P234, DOI 10.1080/1747423X.2011.637136
   Papadimitriou F, 2012, LANDSCAPE RES, V37, P591, DOI 10.1080/01426397.2011.650628
   Patten MA, 2018, BIOL CONSERV, V218, P233, DOI 10.1016/j.biocon.2017.12.033
   Persons WE, 2017, ETHOLOGY, V123, P348, DOI 10.1111/eth.12604
   Ren ZM, 2019, ECOL INDIC, V105, P700, DOI 10.1016/j.ecolind.2018.08.058
   Rendall AR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086592
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Sengupta S, 2019, MACH LEARN KNOW EXTR, V1, P157, DOI 10.3390/make1010010
   Shamoon H, 2018, BIOL CONSERV, V226, P32, DOI 10.1016/j.biocon.2018.07.028
   Sherwin CM, 1998, ANIM BEHAV, V56, P11, DOI 10.1006/anbe.1998.0836
   Shi YH, 1998, IEEE C EVOL COMPUTAT, P69, DOI 10.1109/ICEC.1998.699146
   Sollmann R, 2018, AFR J ECOL, V56, P740, DOI 10.1111/aje.12557
   Swaddle JP, 2015, TRENDS ECOL EVOL, V30, P550, DOI 10.1016/j.tree.2015.06.009
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Tomas LS, 2018, HYSTRIX, V29, P175, DOI 10.4404/hystrix-00065-2018
   Vanin S, 2012, NATURE, V484, P371, DOI 10.1038/nature10991
   Yerushalmi S, 2009, ECOL LETT, V12, P970, DOI 10.1111/j.1461-0248.2009.01343.x
   Zub K, 2014, BIOL J LINN SOC, V113, P297, DOI 10.1111/bij.12306
NR 52
TC 8
Z9 8
U1 6
U2 19
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1470-160X
EI 1872-7034
J9 ECOL INDIC
JI Ecol. Indic.
PD MAR
PY 2020
VL 110
AR 105957
DI 10.1016/j.ecolind.2019.105957
PG 9
WC Biodiversity Conservation; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KC7VW
UT WOS:000507381800103
OA hybrid
DA 2022-02-10
ER

PT J
AU Kitzes, J
   Blake, R
   Bombaci, S
   Chapman, M
   Duran, SM
   Huang, T
   Joseph, MB
   Lapp, S
   Marconi, S
   Oestreich, WK
   Rhinehart, TA
   Schweiger, AK
   Song, YL
   Surasinghe, T
   Yang, D
   Yule, K
AF Kitzes, Justin
   Blake, Rachael
   Bombaci, Sara
   Chapman, Melissa
   Duran, Sandra M.
   Huang, Tao
   Joseph, Maxwell B.
   Lapp, Samuel
   Marconi, Sergio
   Oestreich, William K.
   Rhinehart, Tessa A.
   Schweiger, Anna K.
   Song, Yiluan
   Surasinghe, Thilina
   Yang, Di
   Yule, Kelsey
TI Expanding NEON biodiversity surveys with new instrumentation and machine
   learning approaches
SO ECOSPHERE
LA English
DT Article
DE biogeography; deep learning; macroecology; monitoring; neural network;
   sensor; Special Feature; Harnessing the NEON Data Revolution; species
ID SPECIES IDENTIFICATION; RANDOM FOREST; CAMERA; CLASSIFICATION;
   DIVERSITY; LIDAR; RECOGNITION; VEGETATION; SOUNDSCAPE; AUSTRALIA
AB A core goal of the National Ecological Observatory Network (NEON) is to measure changes in biodiversity across the 30-yr horizon of the network. In contrast to NEON's extensive use of automated instruments to collect environmental data, NEON's biodiversity surveys are almost entirely conducted using traditional human-centric field methods. We believe that the combination of instrumentation for remote data collection and machine learning models to process such data represents an important opportunity for NEON to expand the scope, scale, and usability of its biodiversity data collection while potentially reducing long-term costs. In this manuscript, we first review the current status of instrument-based biodiversity surveys within the NEON project and previous research at the intersection of biodiversity, instrumentation, and machine learning at NEON sites. We then survey methods that have been developed at other locations but could potentially be employed at NEON sites in future. Finally, we expand on these ideas in five case studies that we believe suggest particularly fruitful future paths for automated biodiversity measurement at NEON sites: acoustic recorders for sound-producing taxa, camera traps for medium and large mammals, hydroacoustic and remote imagery for aquatic diversity, expanded remote and ground-based measurements for plant biodiversity, and laboratory-based imaging for physical specimens and samples in the NEON biorepository. Through its data science-literate staff and user community, NEON has a unique role to play in supporting the growth of such automated biodiversity survey methods, as well as demonstrating their ability to help answer key ecological questions that cannot be answered at the more limited spatiotemporal scales of human-driven surveys.
C1 [Kitzes, Justin; Lapp, Samuel; Rhinehart, Tessa A.] Univ Pittsburgh, Dept Biol Sci, Pittsburgh, PA 15260 USA.
   [Blake, Rachael] Natl Socioenvironm Synth Ctr, Annapolis, MD USA.
   [Bombaci, Sara] Colorado State Univ, Dept Fish Wildlife & Conservat Biol, Ft Collins, CO 80523 USA.
   [Chapman, Melissa] Univ Calif Berkeley, Dept Environm Sci Policy & Management, Berkeley, CA 94720 USA.
   [Duran, Sandra M.] Univ Arizona, Dept Ecol & Evolutionary Biol, Tucson, AZ USA.
   [Huang, Tao] Boise State Univ, Human Environm Syst, Boise, ID 83725 USA.
   [Joseph, Maxwell B.] Univ Colorado, Cooperat Inst Res Environm Sci CIRES, Earth Lab, Boulder, CO 80309 USA.
   [Marconi, Sergio] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL USA.
   [Oestreich, William K.] Stanford Univ, Hopkins Marine Stn, Stanford, CA 94305 USA.
   [Schweiger, Anna K.] Univ Zurich, Dept Geog, Zurich, Switzerland.
   [Song, Yiluan] Univ Calif Santa Cruz, Environm Studies Dept, Santa Cruz, CA 95064 USA.
   [Surasinghe, Thilina] Bridgewater State Univ, Dept Biol Sci, Bridgewater, MA USA.
   [Yang, Di] Univ Wyoming, Wyoming Geog Informat Sci Ctr WyGISC, Laramie, WY 82071 USA.
   [Yule, Kelsey] Arizona State Univ, Natl Ecol Observ Network Biorepository, Tempe, AZ USA.
RP Kitzes, J (corresponding author), Univ Pittsburgh, Dept Biol Sci, Pittsburgh, PA 15260 USA.
EM justin.kitzes@pitt.edu
OI Marconi, Sergio/0000-0002-8096-754X; Oestreich,
   William/0000-0002-0137-5053; Blake, Rachael/0000-0003-0847-9100; Duran,
   Sandra M/0000-0003-2044-8139; Schweiger, Anna
   Katharina/0000-0002-5567-4200; Joseph, Maxwell/0000-0002-7745-9990
FU National Science FoundationNational Science Foundation (NSF) [1935507,
   1926542]; Department of Biological Sciences at the University of
   Pittsburgh; Mascaro Center for Sustainable Development at the University
   of Pittsburgh; Gordon and Betty Moore Foundation's Data-Driven Discovery
   Initiative [GBMF4563]; NSF Dimension of Biodiversity program grant
   [DEB-1442280]; University of Florida Informatics Institute (UFII)
   Graduate Fellowship; University of Zurich's University Research Priority
   Programme on Global Change and Biodiversity
FX We thank Lauren Chronister for assistance in preparing this manuscript
   as well as Daniel Gruner, Elizabeth LaRue, and Natalie Robinson for
   ideas and suggestions that improved earlier drafts. This material is
   based upon work supported by the National Science Foundation under Grant
   No. 1935507 and was also financially supported by the Department of
   Biological Sciences and the Mascaro Center for Sustainable Development
   at the University of Pittsburgh. This work was also supported by the
   Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative
   through grant GBMF4563 to E.P. White and by the National Science
   Foundation through grant 1926542 to E.P. White, S.A. Bohlman, A. Zare,
   D.Z. Wang, and A. Singh; by the NSF Dimension of Biodiversity program
   grant (DEB-1442280) and the University of Florida Informatics Institute
   (UFII) Graduate Fellowship to Sergio Marconi. Anna Schweiger was
   supported by the University of Zurich's University Research Priority
   Programme on Global Change and Biodiversity. APC charges for this
   article were fully paid by the University Library System, University of
   Pittsburgh.
CR Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005
   Adams MD, 2010, ACTA CHIROPTEROL, V12, P231, DOI 10.3161/150811010X504725
   Aguirre-Gutierrez J, 2021, REMOTE SENS ENVIRON, V252, DOI 10.1016/j.rse.2020.112122
   Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
   Alexander C, 2014, REMOTE SENS ENVIRON, V147, P156, DOI 10.1016/j.rse.2014.02.013
   Almeida J, 2014, ECOL INFORM, V23, P49, DOI 10.1016/j.ecoinf.2013.06.011
   Anderson CB, 2018, PEERJ, V6, DOI 10.7717/peerj.5666
   Asner GP, 2017, SCIENCE, V355, P385, DOI 10.1126/science.aaj1987
   Baishali B., 2019, THESIS U ARIZONA
   Barrett B, 2016, REMOTE SENS ECOL CON, V2, P212, DOI 10.1002/rse2.32
   Bedoya C, 2014, ECOL INFORM, V24, P200, DOI 10.1016/j.ecoinf.2014.08.009
   Beery S., 2019, ARXIV190706772
   Bermant PC, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-48909-4
   Blair J, 2020, ECOL EVOL, V10, P13143, DOI 10.1002/ece3.6905
   Blumstein DT, 2011, J APPL ECOL, V48, P758, DOI 10.1111/j.1365-2664.2011.01993.x
   Bonnet S, 2015, REMOTE SENS-BASEL, V7, P11267, DOI 10.3390/rs70911267
   Bortone SA, 2000, MAR SCI SER, P127
   Brumfield KD, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0228899
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Buetti-Dinh Antoine, 2019, Biotechnology Reports, V22, pe00321, DOI 10.1016/j.btre.2019.e00321
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cabezas J, 2016, IEEE GEOSCI REMOTE S, V13, P646, DOI 10.1109/LGRS.2016.2532743
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z
   Chadwick K.D., 2020, Site-level Foliar C, N, delta 13C data from samples collected during field survey associated with NEON AOP survey, East River, CO 2018, DOI 10.15485/1631278
   Chen L.-C., 2014, COMPUTER SCI
   Cheng KC, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219570
   Chollet F., 2018, DEEP LEARNING R MANN
   Colgan MS, 2012, REMOTE SENS-BASEL, V4, P3462, DOI 10.3390/rs4113462
   Corcoran E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-39917-5
   Dalponte M, 2019, PEERJ, V6, DOI 10.7717/peerj.6227
   Darras K, 2019, ECOL APPL, V29, DOI 10.1002/eap.1954
   Delancey ER, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0218165
   Diaz S, 2001, TRENDS ECOL EVOL, V16, P646, DOI 10.1016/S0169-5347(01)02283-2
   Diaz S, 2006, PLOS BIOL, V4, P1300, DOI 10.1371/journal.pbio.0040277
   Du XX, 2019, IEEE T GEOSCI REMOTE, V57, P2741, DOI 10.1109/TGRS.2018.2876687
   Duhart C., 2019, DEEP LEARNING WILDLI
   Duran SM, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw8114
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013
   Favret C, 2016, SYST ENTOMOL, V41, P133, DOI 10.1111/syen.12146
   Ferretti R, 2017, OCEANS-IEEE
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Forrester Tavis, 2013, P 98 ESA ANN CONV 20
   Franklin SE, 2018, INT J REMOTE SENS, V39, P5236, DOI 10.1080/01431161.2017.1363442
   Fricker GA, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192326
   Frommolt KH, 2014, ECOL INFORM, V21, P4, DOI 10.1016/j.ecoinf.2013.12.009
   Gage SH, 2014, ECOL INFORM, V21, P100, DOI 10.1016/j.ecoinf.2013.11.004
   Geronimo RC, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101604
   Gomez WE, 2018, ECOL INFORM, V45, P16, DOI 10.1016/j.ecoinf.2018.03.001
   Graves S., 2018, PEERJ PREPRINTS, DOI [10.7287/peerj.preprints.27182v1, DOI 10.7287/PEERJ.PREPRINTS.27182V1]
   Guirado E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50795-9
   Hess LL, 2003, HYDROBIOLOGIA, V500, P65, DOI 10.1023/A:1024665017985
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   Hooper DU, 2005, ECOL MONOGR, V75, P3, DOI 10.1890/04-0922
   Huesca M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091100
   Husson E, 2014, APPL VEG SCI, V17, P567, DOI 10.1111/avsc.12072
   Jalali MA, 2015, FISH RES, V169, P26, DOI 10.1016/j.fishres.2015.04.009
   Jiao C, 2018, ISPRS J PHOTOGRAMM, V146, P235, DOI 10.1016/j.isprsjprs.2018.08.012
   Jones FM, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.124
   Jones MO, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2430
   Jones TR, 2009, P NATL ACAD SCI USA, V106, P1826, DOI 10.1073/pnas.0808843106
   Kahl S., 2019, OVERVIEW BIRDCLEF 20
   Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236
   Kamoske AG, 2019, FOREST ECOL MANAG, V433, P364, DOI 10.1016/j.foreco.2018.11.017
   Kampe TU, 2010, J APPL REMOTE SENS, V4, DOI 10.1117/1.3361375
   Kays R., 2014, P N AM CONSERVATION, P80
   Kerkech M, 2018, COMPUT ELECTRON AGR, V155, P237, DOI 10.1016/j.compag.2018.10.006
   Kitzes J, 2019, ENVIRON CONSERV, V46, P247, DOI 10.1017/S0376892919000146
   Klemas V, 2012, BALTICA, V25, P99, DOI 10.5200/baltica.2012.25.10
   Korneliussen RJ, 2009, ICES J MAR SCI, V66, P1111, DOI 10.1093/icesjms/fsp119
   Kosmala M, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0209649
   Kosmala M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090726
   Kotta J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0063946
   Krause K.S., 2015, REMOTE SENSING VEGET
   Laliberte E, 2020, ECOL LETT, V23, P370, DOI 10.1111/ele.13429
   Lapp S, 2021, CONSERV BIOL, V35, P1659, DOI 10.1111/cobi.13718
   LaRue EA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091407
   LeBien J, 2020, ECOL INFORM, V59, DOI 10.1016/j.ecoinf.2020.101113
   Libbrecht MW, 2015, NAT REV GENET, V16, P321, DOI 10.1038/nrg3920
   Locke CM, 2019, WILDLIFE SOC B, V43, P4, DOI 10.1002/wsb.943
   Lopatin J, 2016, REMOTE SENS ENVIRON, V173, P200, DOI 10.1016/j.rse.2015.11.029
   Lorieul T, 2019, APPL PLANT SCI, V7, DOI 10.1002/aps3.1233
   Lostanlen V, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214168
   Lyons MB, 2020, REMOTE SENS ECOL CON, V6, P557, DOI 10.1002/rse2.157
   Ma XL, 2019, REMOTE SENS ENVIRON, V233, DOI 10.1016/j.rse.2019.111368
   Magle SB, 2019, FRONT ECOL ENVIRON, V17, P232, DOI 10.1002/fee.2030
   Marconi S., 2019, BIORXIV556472
   Marconi S, 2019, PEERJ, V7, DOI 10.7717/peerj.5843
   Marvin DC, 2016, GLOB ECOL CONSERV, V7, P262, DOI 10.1016/j.gecco.2016.07.002
   Mayo M, 2007, KNOWL-BASED SYST, V20, P195, DOI 10.1016/j.knosys.2006.11.012
   McCann E, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.190
   McMahon CA, 2019, PEERJ, V7, DOI 10.7717/peerj.5837
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meerdink SK, 2019, REMOTE SENS ENVIRON, V232, DOI 10.1016/j.rse.2019.111308
   Meineke E.K., 2019, BIORXIV790899
   Meng L, 2018, IEEE ACCESS, V6, P17880, DOI 10.1109/ACCESS.2018.2820326
   Mishra D, 2006, PHOTOGRAMM ENG REM S, V72, P1037, DOI 10.14358/PERS.72.9.1037
   Mo J., 2017, AI 2017 ADV ARTIFICI, V10400, P301, DOI [10.1007/978-3-319-63004-5_24, DOI 10.1007/978-3-319-63004-5_24]
   Moniruzzaman M, 2017, LECT NOTES COMPUT SC, V10617, P150, DOI 10.1007/978-3-319-70353-4_13
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Narayanan BN, 2019, PROC SPIE, V11139, DOI 10.1117/12.2524681
   National Ecological Observatory Network, 2019, NEON STRAT ENG PLAN
   National Ecological Observatory Network, PAP PUBL
   National Ecological Observatory Network, 2020, DAT PROD DP1 00033 0
   Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070
   Nia MS, 2015, J APPL REMOTE SENS, V9, DOI 10.1117/1.JRS.9.095990
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Obrist Martin K., 2010, Abc Taxa, V8, P68
   Park JY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131534
   Paz-Kagan T, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080953
   Pijanowski BC, 2011, LANDSCAPE ECOL, V26, P1213, DOI 10.1007/s10980-011-9600-8
   Pijanowski BC, 2011, BIOSCIENCE, V61, P203, DOI 10.1525/bio.2011.61.3.6
   Pizarro O, 2008, OCEANS-IEEE, P1863
   Plesoianu AI, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152426
   Pouliot DA, 2002, REMOTE SENS ENVIRON, V82, P322, DOI 10.1016/S0034-4257(02)00050-0
   Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
   Qiu ZF, 2018, OPT EXPRESS, V26, P26810, DOI 10.1364/OE.26.026810
   Raitoharju J, 2018, IMAGE VISION COMPUT, V78, P73, DOI 10.1016/j.imavis.2018.06.005
   Rehush N, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111735
   Rhinehart TA, 2020, ECOL EVOL, V10, P6794, DOI 10.1002/ece3.6216
   Richardson AD, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.28
   Ruppe L, 2015, P NATL ACAD SCI USA, V112, P6092, DOI 10.1073/pnas.1424667112
   Salman A, 2019, ECOL INFORM, V51, P44, DOI 10.1016/j.ecoinf.2019.02.011
   Santos MJ, 2009, INVAS PLANT SCI MANA, V2, P216, DOI 10.1614/IPSM-08-115.1
   Schneider FD, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01530-3
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Schuettpelz E, 2017, BIODIVERS DATA J, V5, DOI 10.3897/BDJ.5.e21139
   Schweiger AK, 2018, NAT ECOL EVOL, V2, P976, DOI 10.1038/s41559-018-0551-1
   Sebastia-Frasquet MT, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242926
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Shiferaw H, 2019, ECOL EVOL, V9, P2562, DOI 10.1002/ece3.4919
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Soueidan H., 2015, ARXIV PREPRINT ARXIV
   Stowell D, 2020, REMOTE SENS ECOL CON, V6, P217, DOI 10.1002/rse2.174
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Sueur J, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0004065
   Sumsion GR, 2019, PEERJ, V7, DOI 10.7717/peerj.6101
   Sung M, 2017, OCEANS-IEEE
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tonolla D, 2010, HYDROL PROCESS, V24, P3146, DOI 10.1002/hyp.7730
   Tornow J.S., 2019, DEAR COLLEAGUE LETT
   Ulloa JS, 2018, ECOL INDIC, V90, P346, DOI 10.1016/j.ecolind.2018.03.026
   Uranga J, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0171382
   Vatnehol S, 2018, ICES J MAR SCI, V75, P1803, DOI 10.1093/icesjms/fsy029
   Verrelst J, 2019, SURV GEOPHYS, V40, P589, DOI 10.1007/s10712-018-9478-y
   Vihervaara P, 2017, GLOB ECOL CONSERV, V10, P43, DOI 10.1016/j.gecco.2017.01.007
   Wang ZH, 2020, NEW PHYTOL, V228, P494, DOI 10.1111/nph.16711
   Weinstein B.G., 2020, BIORXIV2020090828783, DOI [10.1101/2020.09.08.287839, DOI 10.1101/2020.09.08.287839]
   Weinstein BG, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111309
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wolff LM, 2014, OCEANS-IEEE
   Wu CF, 2018, J FORESTRY RES, V29, P151, DOI 10.1007/s11676-017-0404-9
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang D, 2016, BIOSYST ENG, V145, P65, DOI 10.1016/j.biosystemseng.2016.02.013
   Zhou T, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10121949
   Zhou T, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010039
   Zlinszky A, 2016, INT ARCH PHOTOGRAMM, V41, P1293, DOI 10.5194/isprsarchives-XLI-B8-1293-2016
   Zou S, 2019, PEERJ, V7, DOI 10.7717/peerj.6405
NR 160
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2150-8925
J9 ECOSPHERE
JI Ecosphere
PD NOV
PY 2021
VL 12
IS 11
AR e03795
DI 10.1002/ecs2.3795
PG 20
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA XE1GA
UT WOS:000723142700022
OA gold
DA 2022-02-10
ER

PT J
AU Chauvenet, ALM
   Gill, RMA
   Smith, GC
   Ward, AI
   Massei, G
AF Chauvenet, Alienor L. M.
   Gill, Robin M. A.
   Smith, Graham C.
   Ward, Alastair I.
   Massei, Giovanna
TI Quantifying the bias in density estimated from distance sampling and
   camera trapping of unmarked individuals
SO ECOLOGICAL MODELLING
LA English
DT Article
DE Individual based model; Sus scrofa; Transects; Population monitoring;
   Population size; Random encounter model
ID BOAR SUS-SCROFA; WILD BOAR; SOUTHERN ENGLAND; ANIMAL DENSITY;
   POPULATIONS; MAMMALS; DIVERSITY; ABUNDANCE; WOODLAND; DESIGN
AB Population size estimates are an integral part of any species conservation or management project. They are often used to evaluate the impact of management intervention and can be critical for making decisions for future management. Distance sampling and camera trapping of unmarked populations are commonly used for such a task as they can yield rapid and relatively inexpensive estimates of density. Yet, while accuracy is key for decision-making, the potential bias associated with densities estimated with each method have seldom been investigated and compared. We built a spatially-explicit individual based model to investigate the accuracy and precision of both monitoring techniques in estimating known densities. We used the wild boar population of the Forest of Dean, UK, as a case study because both methods have been employed in situ and offer the chance of using real life parameters in the model. Moreover, this is an introduced species in the UK that has the potential to impact natural and agricultural ecosystems. Therefore, improving the accuracy of density estimates is a priority for the species' management. We found that both distance sampling and camera trapping produce biased density estimates for unmarked populations. Despite large uncertainties, distance sampling estimates were on average closer to known densities than those from camera trapping, and robust to group size. Camera trapping estimates were highly sensitive to group size but could be improved with better survey design. This is the first time that the amount of bias associated with each method is quantified. Our model could be used to correct estimated field-based densities from distance sampling and camera trapping of wild boar and other species with similar life-history traits. Our work serves to increase confidence in the results produced by these two commonly-used methods, ensuring they can in turn be relied upon by wildlife managers and conservationists. Crown Copyright (C) 2017 Published by Elsevier B.V. All rights reserved.
C1 [Chauvenet, Alienor L. M.; Smith, Graham C.; Ward, Alastair I.; Massei, Giovanna] Anim & Plant Hlth Agcy, Natl Wildlife Management Ctr, York Y041 1LZ, N Yorkshire, England.
   [Chauvenet, Alienor L. M.] Univ Queensland, Ctr Biodivers & Conservat Sci, Goddard 8,Level 5, St Lucia, Qld 4072, Australia.
   [Chauvenet, Alienor L. M.] Univ Queensland, ARC Ctr Excellence Environm Decis, Goddard 8,Level 5, St Lucia, Qld 4072, Australia.
   [Gill, Robin M. A.] Ctr Human & Ecol Sci, Forest Res, Farnham GU10 4LH, Surrey, England.
RP Chauvenet, ALM (corresponding author), Univ Queensland, Ctr Biodivers & Conservat Sci, Goddard 8,Level 5, St Lucia, Qld 4072, Australia.
EM a.chauvenet@uq.edu.au
RI Chauvenet, Alienor/L-9135-2015; Smith, Graham C/J-2593-2013
OI Chauvenet, Alienor/0000-0002-3743-7375; Smith, Graham
   C/0000-0002-9897-6794; Ward, Alastair/0000-0002-3305-3323
FU Department for the Environment, Food and Rural AffairsDepartment for
   Environment, Food & Rural Affairs (DEFRA)
FX We would like to thank Julia Coats, the Forest of Dean, George Watola
   and Sue Fox for their contribution towards estimating realistic rates
   used in the model. This project was funded by the Department for the
   Environment, Food and Rural Affairs.
CR Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Anderson K. A., 2002, MODEL SELECTION MULT
   Baker SJ, 2010, REV SCI TECH OIE, V29, P311, DOI 10.20506/rst.29.2.1981
   Bartolommei P., 2013, HYSTRIX ITALIAN J MA, V23, P91
   Buckland ST, 2000, BIOMETRICS, V56, P1, DOI 10.1111/j.0006-341X.2000.00001.x
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Engeman RM, 2013, ENVIRON SCI POLLUT R, V20, P8077, DOI 10.1007/s11356-013-2002-5
   Ferretti F, 2014, SPRINGERBRIEF LAW, P1, DOI 10.1007/978-3-319-08906-5
   Focardi S, 2000, J ZOOL, V250, P329, DOI 10.1111/j.1469-7998.2000.tb00777.x
   Focardi S., 2001, WILDI SOC B, P133
   Franzetti B, 2012, EUR J WILDLIFE RES, V58, P385, DOI 10.1007/s10344-011-0587-x
   Goulding MJ, 2003, WILDLIFE BIOL, V9, P15, DOI 10.2981/wlb.2003.059
   Hutchinson JMC, 2007, BIOL REV, V82, P335, DOI 10.1111/j.1469-185X.2007.00014.x
   Karanth KU, 2004, P NATL ACAD SCI USA, V101, P4854, DOI 10.1073/pnas.0306210101
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Li S, 2010, BIODIVERS CONSERV, V19, P3195, DOI 10.1007/s10531-010-9886-x
   Lindberg MS, 2012, J ORNITHOL, V152, pS355, DOI 10.1007/s10336-010-0533-9
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Massei Giovanna, 2004, Galemys, V16, P135
   Massei G, 2015, PEST MANAG SCI, V71, P492, DOI 10.1002/ps.3965
   Miller DL, 2014, DISTANCE SIMPLE WAY
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Parrott D, 2012, EUR J WILDLIFE RES, V58, P23, DOI 10.1007/s10344-011-0536-8
   Peres Carlos A., 1999, Neotropical Primates, V7, P11
   Plumptre AJ, 2000, J APPL ECOL, V37, P356, DOI 10.1046/j.1365-2664.2000.00499.x
   Pollock KH, 2002, ENVIRONMETRICS, V13, P105, DOI 10.1002/env.514
   Roberts Nathan James, 2011, Bioscience Horizons, V4, P40, DOI 10.1093/biohorizons/hzr006
   Rosenstock SS, 2002, AUK, V119, P46, DOI 10.1642/0004-8038(2002)119[0046:LCTCPA]2.0.CO;2
   Rossi S, 2005, VET RES, V36, P27, DOI 10.1051/vetres:2004050
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2013, J WILDLIFE MANAGE, V77, P876, DOI 10.1002/jwmg.533
   Ruette S, 2003, J APPL ECOL, V40, P32, DOI 10.1046/j.1365-2664.2003.00776.x
   Ruiz-Fons F, 2008, VET J, V176, P158, DOI 10.1016/j.tvjl.2007.02.017
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Smart JCR, 2004, MAMMAL REV, V34, P99, DOI 10.1046/j.0305-1838.2003.00026.x
   Team R.C., 2013, R LANG ENV STAT COMP
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Wilson C.J., 2014, Wildlife Biology in Practice, V10, P1
   Wilson CJ, 2004, MAMMAL REV, V34, P331, DOI 10.1111/j.1365-2907.2004.00050.x
   Wilson CJ, 2003, MAMMAL REV, V33, P302, DOI 10.1046/j.1365-2907.2003.00016.x
   Yoccoz NG, 2001, TRENDS ECOL EVOL, V16, P446, DOI 10.1016/S0169-5347(01)02205-4
NR 43
TC 20
Z9 20
U1 3
U2 97
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0304-3800
EI 1872-7026
J9 ECOL MODEL
JI Ecol. Model.
PD APR 24
PY 2017
VL 350
BP 79
EP 86
DI 10.1016/j.ecolmodel.2017.02.007
PG 8
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA ER5XE
UT WOS:000398876800007
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Rumelt, RB
   Basto, A
   Roncal, CM
AF Rumelt, Reid B.
   Basto, Arianna
   Roncal, Carla Mere
TI Automated audio recording as a means of surveying tinamous (Tinamidae)
   in the Peruvian Amazon
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE bioacoustics; bird biology; machine learning; Neotropics; Peru; tinamous
ID ATLANTIC FOREST; HABITAT; EBIRD
AB The use of machine learning technologies to process large quantities of remotely collected audio data is a powerful emerging research tool in ecology and conservation. We applied these methods to a field study of tinamou (Tinamidae) biology in Madre de Dios, Peru, a region expected to have high levels of interspecies competition and niche partitioning as a result of high tinamou alpha diversity. We used autonomous recording units to gather environmental audio over a period of several months at lowland rainforest sites in the Los Amigos Conservation Concession and developed a Convolutional Neural Network-based data processing pipeline to detect tinamou vocalizations in the dataset. The classified acoustic event data are comparable to similar metrics derived from an ongoing camera trapping survey at the same site, and it should be possible to combine the two datasets for future explorations of the target species' niche space parameters. Here, we provide an overview of the methodology used in the data collection and processing pipeline, offer general suggestions for processing large amounts of environmental audio data, and demonstrate how data collected in this manner can be used to answer questions about bird biology.
C1 [Rumelt, Reid B.] Cornell Univ, Coll Agr & Life Sci, Ithaca, NY 14853 USA.
   [Basto, Arianna] Colorado State Univ, Warner Coll Nat Resources, Ft Collins, CO 80523 USA.
   [Roncal, Carla Mere] Univ Florida, Sch Forest Fisheries & Geomat Sci, Gainesville, FL USA.
RP Rumelt, RB (corresponding author), Cornell Univ, Coll Agr & Life Sci, Ithaca, NY 14853 USA.
EM rbr73@cornell.edu
OI Rumelt, Reid/0000-0003-3551-0599
FU Amazon Conservation Association
FX Amazon Conservation Association
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Acevedo MA, 2006, WILDLIFE SOC B, V34, P211, DOI 10.2193/0091-7648(2006)34[211:UADRSA]2.0.CO;2
   Bertelli S, 2002, BIOL J LINN SOC, V77, P423, DOI 10.1046/j.1095-8312.2002.00112.x
   Brandes TS, 2008, BIRD CONSERV INT, V18, pS163, DOI 10.1017/S0959270908000415
   de Juana E, 2020, BIRDS OF THE WORLD, DOI [10.2173/bow.blctin1.01, DOI 10.2173/BOW.BLCTIN1.01]
   Ding J, 2016, IEEE GEOSCI REMOTE S, V13, P364, DOI 10.1109/LGRS.2015.2513754
   dos Anjos L, 2006, BIOTROPICA, V38, P229, DOI 10.1111/j.1744-7429.2006.00122.x
   eBird, 2017, EBIRD ONL DAT BIRD D
   Fink D. T., 2020, EBIRD STATUS TRENDS EBIRD STATUS TRENDS, DOI [10.2173/ebirdst.2018, DOI 10.2173/EBIRDST.2018]
   Guerta RS, 2014, ORNITOL NEOTROP, V25, P73
   Hussein H, 2017, WORKING NOTES CLEF
   Joly A., 2019, WORKING NOTES CLEF 2, V2380
   Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
   Kotsiantis SB, 2007, FRONT ARTIF INTEL AP, V160, P3
   LANDAU HJ, 1967, PR INST ELECTR ELECT, V55, P1701, DOI 10.1109/PROC.1967.5962
   Larsen TH, 2006, COLEOPTS BULL, V60, P315, DOI 10.1649/0010-065X(2006)60[315:ETAHSB]2.0.CO;2
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Nogueira F., 2014, BAYESIAN OPTIMIZATIO
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Perez-Granados C, 2020, BIOTROPICA, V52, P165, DOI 10.1111/btp.12742
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Reich BJ, 2018, METHODS ECOL EVOL, V9, P1626, DOI 10.1111/2041-210X.13002
   Roncal CM, 2019, J FIELD ORNITHOL, V90, P203, DOI 10.1111/jofo.12299
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Dias LCS, 2016, WILSON J ORNITHOL, V128, P885
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
   Sullivan BL, 2014, BIOL CONSERV, V169, P31, DOI 10.1016/j.biocon.2013.11.003
   Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006
   Thornton DH, 2012, ORYX, V46, P567, DOI 10.1017/S0030605311001451
NR 29
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD OCT
PY 2021
VL 11
IS 19
BP 13518
EP 13531
DI 10.1002/ece3.8078
EA SEP 2021
PG 14
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA WC5VO
UT WOS:000692685400001
PM 34646487
OA Green Submitted, gold, Green Published
DA 2022-02-10
ER

PT J
AU Rowcliffe, JM
   Jansen, PA
   Kays, R
   Kranstauber, B
   Carbone, C
AF Rowcliffe, J. Marcus
   Jansen, Patrick A.
   Kays, Roland
   Kranstauber, Bart
   Carbone, Chris
TI Wildlife speed cameras: measuring animal travel speed and day range
   using camera traps
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article
DE Animal tracking; image analysis; length-biased distributions; movement
   ecology; travel distance; video capture
ID ENERGY-COST; HOME-RANGE; BODY-SIZE; MOVEMENT; DENSITY; DIET; MASS
AB Travel speed (average speed of travel while active) and day range (average speed over the daily activity cycle) are behavioural metrics that influence processes including energy use, foraging success, disease transmission and human-wildlife interactions, and which can therefore be applied to a range of questions in ecology and conservation. These metrics are usually derived from telemetry or direct observations. Here, we describe and validate an entirely new alternative approach, using camera traps recording passing animals to measure movement paths at very fine scale. Dividing the length of a passage by its duration gives a speed observation, and average travel speed is estimated by fitting size-biased probability distributions to a sample of speed observations. Day range is then estimated as the product of travel speed and activity level (proportion of time spent active), which can also be estimated from camera-trap data. We field tested the procedure with data from a survey of terrestrial mammals on Barro Colorado Island, Panama. Travel speeds and day ranges estimated for 12 species scaled positively with body mass, and were higher in faunivores than in herbivores, patterns that are consistent with those obtained using independent estimates derived from tracked individuals. Comparisons of our day range estimates with independent telemetry-based estimates for three species also showed very similar values in absolute terms. We conclude that these methods are accurate and ready to use for estimating travel speed and day range in wildlife. Key advantages of the methods are that they are non-invasive, and that measurements are made at very high resolution in time and space, yielding estimates that are comparable across species and studies. Combined with emerging techniques in computer vision, we anticipate that these methods will help to expand the range of species for which we can estimate movement rate in the wild.
C1 [Rowcliffe, J. Marcus; Carbone, Chris] ZSL Inst Zool, Regents Pk, London NW1 4RY, England.
   [Jansen, Patrick A.; Kays, Roland] Smithsonian Trop Res Inst, Ctr Trop Forest Sci, Panama City, Panama.
   [Jansen, Patrick A.] Wageningen Univ, Dept Environm Sci, Wageningen, Netherlands.
   [Kays, Roland] North Carolina State Univ, Raleigh, NC 27695 USA.
   [Kays, Roland] Museum Nat Sci, Raleigh, NC USA.
   [Kranstauber, Bart] Max Planck Inst Ornithol, Dept Migrat & Immunoecol, Radolfzell am Bodensee, Germany.
   [Kranstauber, Bart] Univ Konstanz, Dept Biol, Constance, Germany.
RP Rowcliffe, JM (corresponding author), ZSL Inst Zool, Regents Pk, London NW1 4RY, England.
EM marcus.rowcliffe@ioz.ac.uk
RI Rowcliffe, Marcus/G-3713-2018; Jansen, Patrick/G-2545-2015
OI Rowcliffe, Marcus/0000-0002-4286-6887; Jansen,
   Patrick/0000-0002-4660-0314; Kays, Roland/0000-0002-2947-6665
FU National Science FoundationNational Science Foundation (NSF) [NSF-DEB
   0717071]; British Ecological Society; Netherlands Organisation for
   Scientific ResearchNetherlands Organization for Scientific Research
   (NWO) [NWO-ALW863-07-008]; Direct For Biological SciencesNational
   Science Foundation (NSF)NSF - Directorate for Biological Sciences (BIO)
   [1232442] Funding Source: National Science Foundation; Division of
   Computing and Communication FoundationsNational Science Foundation
   (NSF)NSF - Directorate for Computer & Information Science & Engineering
   (CISE) [1539622] Funding Source: National Science Foundation
FX The work was funded by the National Science Foundation (NSF-DEB
   0717071), the British Ecological Society, and the Netherlands
   Organisation for Scientific Research (NWO-ALW863-07-008).
CR Aliaga-Rossel E, 2008, J TROP ECOL, V24, P367, DOI 10.1017/S0266467408005129
   Anderson K. A., 2002, MODEL SELECTION MULT
   Bolker, 2008, ECOLOGICAL MODELS DA
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Carbone C, 2005, AM NAT, V165, P290, DOI 10.1086/426790
   Carbone C, 2014, ECOL LETT, V17, P1553, DOI 10.1111/ele.12375
   Core Team R, 2014, R LANG ENV STAT COMP
   Cross PC, 2005, ECOL LETT, V8, P587, DOI 10.1111/j.1461-0248.2005.00760.x
   Davis P. J., 1972, HDB MATH FUNCTIONS, P253
   Emmons L.H., 1990, NEOTROPICAL RAINFORE
   EMMONS LH, 1988, REV ECOL-TERRE VIE, V43, P133
   GALDIKAS BMF, 1988, INT J PRIMATOL, V9, P1, DOI 10.1007/BF02740195
   GARLAND T, 1983, J ZOOL, V199, P157, DOI 10.1111/j.1469-7998.1983.tb02087.x
   GARLAND T, 1983, AM NAT, V121, P571, DOI 10.1086/284084
   GOODMAN LA, 1960, J AM STAT ASSOC, V55, P708, DOI 10.2307/2281592
   Graham MD, 2009, ANIM CONSERV, V12, P445, DOI 10.1111/j.1469-1795.2009.00272.x
   HEGLUND NC, 1988, J EXP BIOL, V138, P301
   Hutchinson JMC, 2007, BIOL REV, V82, P335, DOI 10.1111/j.1469-185X.2007.00014.x
   Isaac NJB, 2013, GLOBAL ECOL BIOGEOGR, V22, P1, DOI 10.1111/j.1466-8238.2012.00782.x
   Jansen PA, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P263
   Jetz W, 2004, SCIENCE, V306, P266, DOI 10.1126/science.1102138
   Kays Roland, 2011, International Journal of Research and Reviews in Wireless Sensor Networks, V1, P19
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   Leigh E.G., 1999, TROPICAL FOREST ECOL
   MCMAHON TA, 1975, J APPL PHYSIOL, V39, P619, DOI 10.1152/jappl.1975.39.4.619
   Meek PD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110832
   Miller CS, 2014, BIOL CONSERV, V170, P120, DOI 10.1016/j.biocon.2013.12.012
   Pedersen MW, 2013, METHODS ECOL EVOL, V4, P920, DOI 10.1111/2041-210X.12086
   Piegorsch, 2002, ENCY ENV
   PYKE GH, 1981, AM NAT, V118, P475, DOI 10.1086/283842
   Reid, 1997, FIELD GUIDE MAMMALS
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2013, J WILDLIFE MANAGE, V77, P876, DOI 10.1002/jwmg.533
   Rowcliffe JM, 2012, METHODS ECOL EVOL, V3, P653, DOI 10.1111/j.2041-210X.2012.00197.x
   SCHMIDTNIELSEN K, 1972, SCIENCE, V177, P222, DOI 10.1126/science.177.4045.222
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   SIGG H, 1981, FOLIA PRIMATOL, V36, P40, DOI 10.1159/000156008
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Turchin Peter, 1998
   Vaughan CS, 1999, REV BIOL TROP, V47, P263
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   WERNER EE, 1993, AM NAT, V142, P242, DOI 10.1086/285537
   Woodroffe R, 1998, SCIENCE, V280, P2126, DOI 10.1126/science.280.5372.2126
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 47
TC 35
Z9 38
U1 4
U2 11
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
PD JUN
PY 2016
VL 2
IS 2
BP 84
EP 94
DI 10.1002/rse2.17
PG 11
WC Ecology; Remote Sensing
WE Emerging Sources Citation Index (ESCI)
SC Environmental Sciences & Ecology; Remote Sensing
GA VG7IN
UT WOS:000448241100002
OA gold, Green Published, Green Accepted
DA 2022-02-10
ER

PT J
AU Ahumada, JA
   Fegraus, E
   Birch, T
   Flores, N
   Kays, R
   O'Brien, TG
   Palmer, J
   Schuttler, S
   Zhao, JY
   Jetz, W
   Kinnaird, M
   Kulkarni, S
   Lyet, A
   Thau, D
   Duong, M
   Oliver, R
   Dancer, A
AF Ahumada, Jorge A.
   Fegraus, Eric
   Birch, Tanya
   Flores, Nicole
   Kays, Roland
   O'Brien, Timothy G.
   Palmer, Jonathan
   Schuttler, Stephanie
   Zhao, Jennifer Y.
   Jetz, Walter
   Kinnaird, Margaret
   Kulkarni, Sayali
   Lyet, Arnaud
   Thau, David
   Duong, Michelle
   Oliver, Ruth
   Dancer, Anthony
TI Wildlife Insights: A Platform to Maximize the Potential of Camera Trap
   and Other Passive Sensor Wildlife Data for the Planet
SO ENVIRONMENTAL CONSERVATION
LA English
DT Article
DE artificial intelligence; camera traps; data analysis; data sharing;
   protected areas; technology platform; wildlife
ID BIODIVERSITY; DEFAUNATION; MANAGEMENT; SOFTWARE; ECOLOGY; SCIENCE; TREE
AB Wildlife is an essential component of all ecosystems. Most places in the globe do not have local, timely information on which species are present or how their populations are changing. With the arrival of new technologies, camera traps have become a popular way to collect wildlife data. However, data collection has increased at a much faster rate than the development of tools to manage, process and analyse these data. Without these tools, wildlife managers and other stakeholders have little information to effectively manage, understand and monitor wildlife populations. We identify four barriers that are hindering the widespread use of camera trap data for conservation. We propose specific solutions to remove these barriers integrated in a modern technology platform called Wildlife Insights. We present an architecture for this platform and describe its main components. We recognize and discuss the potential risks of publishing shared biodiversity data and a framework to mitigate those risks. Finally, we discuss a strategy to ensure platforms like Wildlife Insights are sustainable and have an enduring impact on the conservation of wildlife.
C1 [Ahumada, Jorge A.; Flores, Nicole] Moore Ctr Sci Conservat Int, 2011 Crystal Dr Suite 600, Arlington, VA 22202 USA.
   [Ahumada, Jorge A.] Arizona State Univ, Ctr Biodivers Outcomes, Julia Ann Wrigley Global Inst Sustainabil, Tempe, AZ 85281 USA.
   [Fegraus, Eric] Conservat Int, Ctr Environm Leadership & Business, 2011 Crystal Dr Suite 600, Arlington, VA 22202 USA.
   [Birch, Tanya; Kulkarni, Sayali] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Kays, Roland; Schuttler, Stephanie] North Carolina Museum Nat Sci, 11 West Jones St, Raleigh, NC 27601 USA.
   [O'Brien, Timothy G.; Palmer, Jonathan] Wildlife Conservat Soc, 2300 Southern Blvd, Bronx, NY 10460 USA.
   [Zhao, Jennifer Y.] Smithsonian Conservat Biol Inst, 1500 Remount Rd, Front Royal, VA 22630 USA.
   [Jetz, Walter; Duong, Michelle; Oliver, Ruth] Yale Univ, Dept Ecol & Evolutionary Biol, 165 Prospect St, New Haven, CT 06520 USA.
   [Kinnaird, Margaret] WWF Int, Mvuli Rd, Westlands, Kenya.
   [Lyet, Arnaud; Thau, David] WWF, 1250 24th St NW, Washington, DC 20037 USA.
   [Dancer, Anthony] Zool Soc London, Conservat & Policy, Regents Pk, London NW1 4RY, England.
RP Ahumada, JA (corresponding author), Moore Ctr Sci Conservat Int, 2011 Crystal Dr Suite 600, Arlington, VA 22202 USA.; Ahumada, JA (corresponding author), Arizona State Univ, Ctr Biodivers Outcomes, Julia Ann Wrigley Global Inst Sustainabil, Tempe, AZ 85281 USA.
EM jahumada@conservation.org
RI Kulkarni, Sayali/AAD-6144-2022; Jetz, Walter/ABF-1517-2020
OI Kays, Roland/0000-0002-2947-6665; Ahumada, Jorge/0000-0003-0953-9101
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation; Lyda
   Hill; GoogleGoogle Incorporated
FX We thank The Gordon and Betty Moore Foundation, Lyda Hill and Google for
   partial support of this work.
CR Ahumada JA, 2016, CAMERA TRAPPING WILD, P196
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Farley SS, 2018, BIOSCIENCE, V68, P563, DOI 10.1093/biosci/biy068
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Griscom BW, 2017, P NATL ACAD SCI USA, V114, P11645, DOI 10.1073/pnas.1710465114
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Harrison RD, 2013, ECOL LETT, V16, P687, DOI 10.1111/ele.12102
   IUCN, 2017, IUCN GREEN LIST PROT
   Jetz W, 2019, NAT ECOL EVOL, V3, P539, DOI 10.1038/s41559-019-0826-1
   Jetz W, 2012, TRENDS ECOL EVOL, V27, P151, DOI 10.1016/j.tree.2011.09.007
   Kissling WD, 2018, BIOL REV, V93, P600, DOI 10.1111/brv.12359
   Kurten EL, 2013, BIOL CONSERV, V163, P22, DOI 10.1016/j.biocon.2013.04.025
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meyer C, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9221
   Michener WK, 2012, TRENDS ECOL EVOL, V27, P85, DOI 10.1016/j.tree.2011.11.016
   O'Brien TG, 2010, ANIM CONSERV, V13, P335, DOI 10.1111/j.1469-1795.2010.00357.x
   O'Brien Timothy G., 2013, P45
   Osuri AM, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11351
   Pereira HM, 2013, SCIENCE, V339, P277, DOI 10.1126/science.1229931
   Peres CA, 2016, P NATL ACAD SCI USA, V113, P892, DOI 10.1073/pnas.1516525113
   Ripple WJ, 2016, BIOSCIENCE, V66, P807, DOI 10.1093/biosci/biw092
   Schuttler SG, 2019, BIOSCIENCE, V69, P69, DOI 10.1093/biosci/biy141
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stephenson PJ, 2017, FRONT ECOL ENVIRON, V15, P124
   Szegedy C, 2016, RETHINKING INCEPTION
   TEAM NETWORK, 2011, TERR VERT MON PROT V
   Tulloch AIT, 2018, NAT ECOL EVOL, V2, P1209, DOI 10.1038/s41559-018-0608-1
   Wearn O.R., 2017, CAMERA TRAPPING CONS
NR 30
TC 21
Z9 24
U1 5
U2 16
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0376-8929
EI 1469-4387
J9 ENVIRON CONSERV
JI Environ. Conserv.
PD MAR
PY 2020
VL 47
IS 1
SI SI
BP 1
EP 6
AR PII S0376892919000298
DI 10.1017/S0376892919000298
PG 6
WC Biodiversity Conservation; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KK2WI
UT WOS:000512608100001
OA hybrid
DA 2022-02-10
ER

PT J
AU Cappelle, N
   Despres-Einspenner, ML
   Howe, EJ
   Boesch, C
   Kuhl, HS
AF Cappelle, Noemie
   Despres-Einspenner, Marie-Lyne
   Howe, Eric J.
   Boesch, Christophe
   Kuehl, Hjalmar S.
TI Validating camera trap distance sampling for chimpanzees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE animal survey; comparative evaluation; monitoring; Pan troglodytes
   verus; spatial-explicit capture-recapture
ID TAI-NATIONAL-PARK; POPULATION; DENSITY; ABUNDANCE; FOREST
AB The extension of distance sampling methods to accommodate observations from camera traps has recently enhanced the potential to remotely monitor multiple species without the need of additional data collection (sign production and decay rates) or individual identification. However, the method requires that the proportion of time is quantifiable when animals can be detected by the cameras. This can be problematic, for instance, when animals spend time above the ground, which is the case for most primates. In this study, we aimed to validate camera trap distance sampling (CTDS) for the semiarboreal western chimpanzee (Pan troglodytes verus) in Tai National Park, Cote d'Ivoire by estimating abundance of a population of known size and comparing estimates to those from other commonly applied methods. We estimated chimpanzee abundance using CTDS and accounted for limited availability for detection (semiarboreal). We evaluated bias and precision of estimates, as well as costs and efforts required to obtain them, and compared them to those from spatially explicit capture-recapture (SECR) and line transect nest surveys. Abundance estimates obtained by CTDS and SECR produced a similar negligible bias, but CTDS yielded a larger coefficient of variation (CV = 39.70% for CTDS vs. 1%/19% for SECR). Line transects generated the most biased abundance estimates but yielded a better coefficient of variation (27.40-27.85%) than CTDS. Camera trap surveys were twice more costly than line transects because of the initial cost of cameras, while line transects surveys required more than twice as much time in the field. This study demonstrates the potential to obtain unbiased estimates of the abundance of semiarboreal species like chimpanzees by CTDS.
C1 [Cappelle, Noemie; Despres-Einspenner, Marie-Lyne; Boesch, Christophe; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Howe, Eric J.] Observ Univ St Andrews, Ctr Res Ecol & Environm Modeling, St Andrews, Fife, Scotland.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Sustainabil & Complex Ape Habitat, Leipzig, Germany.
RP Cappelle, N (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM noemie_cappelle@eva.mpg.de
OI Cappelle, Noemie/0000-0002-9176-4609
FU Centre for Forest Research Fonds de Recherche Quebec Nature et
   Technologies International internship program; Max-Planck-Institut fur
   Evolutionare Anthropologie
FX Centre for Forest Research Fonds de Recherche Quebec Nature et
   Technologies International internship program; Max-Planck-Institut fur
   Evolutionare Anthropologie
CR Anderson DP, 2005, BIOTROPICA, V37, P631, DOI 10.1111/j.1744-7429.2005.00080.x
   Boesch C, 2006, AM J PHYS ANTHROPOL, V130, P103, DOI 10.1002/ajpa.20341
   Boesch C, 2008, AM J PRIMATOL, V70, P519, DOI 10.1002/ajp.20524
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Buckland S.T., 2001, pi
   Buckland S. T., 2010, INT J PRIMATOL, P1
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Doran D., 1989, CHIMPANZEE PYGMY CHI
   Freytag A, 2014, LECT NOTES COMPUT SC, V8753, P144, DOI 10.1007/978-3-319-11752-2_12
   Grooten M., 2018, LIV PLAN REP 2018 AI
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Herbinger I, 2001, INT J PRIMATOL, V22, P143, DOI 10.1023/A:1005663212997
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Kalan A.K., CURRENT BIOL
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl H., 2008, LIGNES DIRECTRICES M
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Mrovlje J., 2008, 9 INT PHD WORKSH SYS, P1
   Plumptre AJ, 2006, PRIMATES, V47, P65, DOI 10.1007/s10329-005-0146-8
   Plumptre AJ, 1996, INT J PRIMATOL, V17, P85, DOI 10.1007/BF02696160
   Rovero F, 2004, TROP ZOOL, V17, P267, DOI 10.1080/03946975.2004.10531208
   Rovero F., 2016, CAMERA TRAPPING WILD
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Soisalo MK, 2006, BIOL CONSERV, V129, P487, DOI 10.1016/j.biocon.2005.11.023
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Tjandranegara E., 2005, DISTANCE ESTIMATION
   Trolle M, 2005, MAMMALIA, V69, P409, DOI 10.1515/mamm.2005.032
   Wagenmakers EJ, 2004, PSYCHON B REV, V11, P192, DOI 10.3758/BF03206482
   Walsh PD, 2005, ECOL APPL, V15, P1342, DOI 10.1890/03-5283
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 38
TC 17
Z9 17
U1 3
U2 22
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD MAR
PY 2019
VL 81
IS 3
AR e22962
DI 10.1002/ajp.22962
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA HP7RX
UT WOS:000461887600004
PM 30811079
DA 2022-02-10
ER

PT C
AU Yousif, H
   He, ZH
   Kays, R
AF Yousif, Hayder
   He, Zhihai
   Kays, Roland
GP IEEE
TI OBJECT SEGMENTATION IN THE DEEP NEURAL NETWORK FEATURE DOMAIN FROM
   HIGHLY CLUTTERED NATURAL SCENES
SO 2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
SE IEEE International Conference on Image Processing ICIP
LA English
DT Proceedings Paper
CT 24th IEEE International Conference on Image Processing (ICIP)
CY SEP 17-20, 2017
CL Beijing, PEOPLES R CHINA
SP Inst Elect & Elect Engineers, Inst Elect & Elect Engineers Signal Proc Soc
DE Camera-trap; graph cut; Convolutional Neural Network; image
   segmentation; object detection
AB Deep convolutional neural networks (DCNNs) offer an effective hierarchical representation of images for various vision analysis tasks, including classification and detection. In this paper, we propose to study background modeling and object segmentation from highly cluttered natural scenes in the DCNN feature domain instead of traditional pixel domain Specifically, we first design and train a DCNN for animal human-background object classification, which is used to analyze the input image to generate multi-layer feature maps, representing the responses of different image regions to the animal-human-background classifier. From these feature maps, we construct the so-called deep abjectness graph for accurate animal-human object segmentation with graph cut. The segmented object regions from each image in the sequence are then verified and fused in the temporal domain using background modeling. Recognizing that the DCNN is very computation-intensive, we explore a fast and efficient design of the DCNN which finds a good trade-off between complexity and the classification-segmentation performance. Our experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods on the camera-trap dataset with highly cluttered natural scenes.
C1 [Yousif, Hayder; He, Zhihai] Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27695 USA.
RP Yousif, H (corresponding author), Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM hyypp5@mail.missouri.edu; hezhi@missouri.edu; rokays@gmail.com
RI Yousif, Hayder/AAG-2259-2020; He, Zhihai/A-5885-2019
OI Yousif, Hayder/0000-0002-7638-9505; Kays, Roland/0000-0002-2947-6665
CR Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Bubnicki J. W., 2016, METHODS ECOLOGY EVOL
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   He K., 2017, ARXIV170306870
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lin Z., 2011, ADV NEURAL INFORM PR, P612, DOI [10.1007/s11263-013-0611-6, DOI 10.1007/S11263-013-0611-6]
   Liu FY, 2015, PATTERN RECOGN, V48, P2983, DOI 10.1016/j.patcog.2015.04.019
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Ravi D, 2016, PATTERN RECOGN, V52, P260, DOI 10.1016/j.patcog.2015.10.021
   Rodriguez P, 2013, IEEE IMAGE PROC, P69, DOI 10.1109/ICIP.2013.6738015
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Z, 2015, SIAM J SCI COMPUT, V37, pA488, DOI 10.1137/130934271
NR 19
TC 1
Z9 1
U1 1
U2 7
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1522-4880
BN 978-1-5090-2175-8
J9 IEEE IMAGE PROC
PY 2017
BP 3095
EP 3099
PG 5
WC Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Imaging Science & Photographic Technology
GA BJ8KY
UT WOS:000428410703045
DA 2022-02-10
ER

PT C
AU Wang, HN
   Su, H
   Chen, P
   Hou, R
   Zhang, ZH
   Xie, WY
AF Wang, Hongnian
   Su, Han
   Chen, Peng
   Hou, Rong
   Zhang, Zhihe
   Xie, Weiyi
GP IEEE
TI Learning Deep Features for Giant Panda Gender Classification using Face
   Images
SO 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS
   (ICCVW)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF International Conference on Computer Vision (ICCV)
CY OCT 27-NOV 02, 2019
CL Seoul, SOUTH KOREA
SP IEEE, IEEE Comp Soc, CVF
ID FOOTPRINTS
AB Giant panda (panda) has lived on earth for at least eight million years and is known as the living fossil. It is also a vulnerable species which requires urgent protection. It is essential to conduct population survey collecting information of their population, density, age structure, and gender ratio so as to design protection schemes and measure their effectiveness. However, it is challenging to accurately and timely obtain gender ratio of pandas because their pelage lacks distinguishable gender patterns and panda is sparsely distributed population in large habitats. All current approaches rely heavily on manual collection of samples in the wild, which are time consuming, costly, or even dangerous. With the widely deployed camera traps, if the gender of pandas can be determined from images, it is possible to monitor panda gender ratio in different regions in real-time. However, no such study was done. In this paper, a deep learning method is developed to study the distinctiveness of panda face for gender classification, in which the largest panda image dataset with 6,549 panda face images collected from 100 male and 121 female pandas is established. The experimental results show that panda faces contain some gender information, although they look very similar to human vision.
C1 [Wang, Hongnian; Su, Han; Xie, Weiyi] Sichuan Normal Univ, Chengdu, Sichuan, Peoples R China.
   [Chen, Peng; Hou, Rong; Zhang, Zhihe] Chengdu Res Base Giant Panda Breeding, Chengdu, Sichuan, Peoples R China.
RP Wang, HN (corresponding author), Sichuan Normal Univ, Chengdu, Sichuan, Peoples R China.
EM hongnianwang@gmail.com; jkxy_sh@scinu.edu.cn; capricorncp@163.com;
   405536517@qq.com; zzh@panda.org.cn; xwylove@gmail.com
OI Wang, Hongnian/0000-0002-7543-3957
FU Chengdu Research Base of Giant Panda Breeding [CPB2018-02, CPB2018-01];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61403266, 61403196, 31300306]; Chinese
   overseas returnees science and technology activities project funding of
   Ministry of Human Resources and Social Security; Sichuan Science and
   Technology Program [2018JY0096]
FX This research is supported by Chengdu Research Base of Giant Panda
   Breeding (NO. CPB2018-02, CPB2018-01), the National Natural Science
   Foundation of China (61403266, 61403196, and 31300306), Chinese overseas
   returnees science and technology activities project funding of Ministry
   of Human Resources and Social Security, and the Sichuan Science and
   Technology Program (2018JY0096).
CR Alibhai S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172065
   [Anonymous], 2015, TECHNICAL REPORT
   Castrillon-Santana M, 2016, PATTERN RECOGN LETT, V82, P181, DOI 10.1016/j.patrec.2015.09.014
   Dehghan A., 2017, ARXIV170204280
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Gu J, 2014, WILDLIFE SOC B, V38, P495, DOI 10.1002/wsb.432
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He Q., 2019, ARXIV190803391
   IOFFE S, 2015, ARXIV 1502 03167, V1502, DOI DOI 10.1007/S13398-014-0173-7.2
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kumar S, 2017, IET BIOMETRICS, V6, P139, DOI 10.1049/iet-bmt.2016.0017
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li BBV, 2018, BIOL CONSERV, V218, P83, DOI 10.1016/j.biocon.2017.11.029
   Li S., 2019, ARXIV190605586
   Liu YZ, 2018, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2018.00093
   Matkowski W. M., 2019, ABS190511163 ARXIV
   Moorhouse TP, 2005, J APPL ECOL, V42, P91, DOI 10.1111/j.1365-2664.2005.00998.x
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Polzounov A., 2016, ARXIV160405605
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Su H, 2014, IEEE T INF FOREN SEC, V9, P666, DOI 10.1109/TIFS.2014.2306591
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villa A. Gomez, 2016, ECOLOGICAL INFORM, V41
   Wang HN, 2019, IEEE INT CONF MOB DA, P304, DOI 10.1109/MDM.2019.00-44
   Zhan XJ, 2007, MOL ECOL, V16, P3792, DOI 10.1111/j.1365-294X.2007.03450.x
   Zhan XJ, 2009, URSUS, V20, P56
NR 28
TC 5
Z9 5
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2473-9936
BN 978-1-7281-5023-9
J9 IEEE INT CONF COMP V
PY 2019
BP 279
EP 285
DI 10.1109/ICCVW.2019.00037
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BP4UH
UT WOS:000554591600031
DA 2022-02-10
ER

PT C
AU Brust, CA
   Burghardt, T
   Groenenberg, M
   Kading, C
   Kuhl, HS
   Manguette, ML
   Denzler, J
AF Brust, Clemens-Alexander
   Burghardt, Tilo
   Groenenberg, Milou
   Kaeding, Christoph
   Kuehl, Hjalmar S.
   Manguette, Marie L.
   Denzler, Joachim
GP IEEE
TI Towards Automated Visual Monitoring of Individual Gorillas in the Wild
SO 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT 16th IEEE International Conference on Computer Vision (ICCV)
CY OCT 22-29, 2017
CL Venice, ITALY
SP IEEE, IEEE Comp Soc
ID CAMERA TRAPS; FACE RECOGNITION; LIFE-HISTORY; HABITAT USE; GREAT APES;
   MBELI-BAI; NEST; CONSERVATION; CHIMPANZEE; ABUNDANCE
AB In this paper we report on the context and evaluation of a system for an automatic interpretation of sightings of individual western lowland gorillas (Gorilla gorilla gorilla) as captured in facial field photography in the wild. This effort aligns with a growing need for effective and integrated monitoring approaches for assessing the status of biodiversity at high spatio-temporal scales. Manual field photography and the utilisation of autonomous camera traps have already transformed the way ecological surveys are conducted. In principle, many environments can now be monitored continuously, and with a higher spatio-temporal resolution than ever before. Yet, the manual effort required to process photographic data to derive relevant information delimits any large scale application of this methodology.
   The described system applies existing computer vision techniques including deep convolutional neural networks to cover the tasks of detection and localisation, as well as individual identification of gorillas in a practically relevant setup. We evaluate the approach on a relatively large and challenging data corpus of 12,765 field images of 147 individual gorillas with image-level labels (i.e. missing bounding boxes) photographed at Mbeli Bai at the Nouabal-Ndoki National Park, Republic of Congo. Results indicate a facial detection rate of 90.8% AP and an individual identification accuracy for ranking within the Top 5 set of 80.3%. We conclude that, whilst keeping the human in the loop is critical, this result is practically relevant as it exemplifies model transferability and has the potential to assist manual identification efforts. We argue further that there is significant need towards integrating computer vision deeper into ecological sampling methodologies and field practice to move the discipline forward and open up new research horizons.
C1 [Brust, Clemens-Alexander; Kaeding, Christoph; Denzler, Joachim] Friedrich Schiller Univ Jena, Comp Vis Grp, Jena, Germany.
   [Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
   [Groenenberg, Milou; Manguette, Marie L.] Wildlife Conservat Soc, Congo Program, Mbeli Bai Study, Kodigehalli, Karnataka, India.
   [Groenenberg, Milou] Wildlife Conservat Soc, Global Conservat Program, Bronx, NY USA.
   [Kaeding, Christoph; Denzler, Joachim] Michael Stifel Ctr Jena, Jena, Germany.
   [Kuehl, Hjalmar S.; Manguette, Marie L.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Kuehl, Hjalmar S.; Denzler, Joachim] German Ctr Integrat Biodivers Res iDiv, Halle Jena Leipzig, Germany.
RP Brust, CA (corresponding author), Friedrich Schiller Univ Jena, Comp Vis Grp, Jena, Germany.
OI Brust, Clemens-Alexander/0000-0001-5419-1998
FU German Research Foundation (DFG)German Research Foundation (DFG) [DE
   735/101]
FX This research was partly supported by grant DE 735/101 of the German
   Research Foundation (DFG). We thank the Ministry of Forest Economy and
   Environment and the Ministry of Scientific Research in the Republic of
   Congo for permission to work in the Nouabale-Ndoki National Park. We are
   grateful to the Wildlife Conservation Society's Congo Program for
   crucial logistical and administrative support. We are indebted to all
   research assistants who contributed to the datasets of the Mbeli Bai
   Study, in particular, Jana Robeyst, Davy Ekouoth, Barbara Hendus, and
   Vidrige Kandza. We are grateful for the financial support provided by
   the funders of the study. The contents of this publication are the sole
   responsibility of its authors and can in no way be taken to reflect the
   views of the funders.
CR Alexander L., 2012, ACM INT WORKSH MULT, P19
   Arandjelovic M, 2010, BIOL CONSERV, V143, P1780, DOI 10.1016/j.biocon.2010.04.030
   Boyer-Ontl KM, 2014, INT J PRIMATOL, V35, P881, DOI 10.1007/s10764-014-9783-3
   Branson S., 2014, BRIT MACH VIS C BMVC
   Breuer T., 2008, THESIS
   Breuer T, 2009, AM J PRIMATOL, V71, P106, DOI 10.1002/ajp.20628
   Brust C.-A., 2015, C COMP VIS THEOR APP
   Buckland S.T., 2001, pi
   Caillaud D, 2006, CURR BIOL, V16, pR489, DOI 10.1016/j.cub.2006.06.017
   Carreira J, 2015, IEEE T PATTERN ANAL, V37, P1177, DOI 10.1109/TPAMI.2014.2361137
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Freytag A, 2014, LECT NOTES COMPUT SC, V8753, P144, DOI 10.1007/978-3-319-11752-2_12
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Girshick R., 2014, C COMP VIS PATT REC
   Goring C, 2014, PROC CVPR IEEE, P2489, DOI 10.1109/CVPR.2014.319
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   He XF, 2004, ADV NEUR IN, V16, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Head JS, 2012, J TROP ECOL, V28, P571, DOI 10.1017/S0266467412000612
   Howe E. J., 2017, METHODS ECOLOGY EVOL
   Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
   Irani R, 2015, IEEE COMPUT SOC CONF
   Jacobson SK, 1998, CONSERV BIOL, V12, P263, DOI 10.1046/j.1523-1739.1998.97235.x
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kding C., 2016, EUR S ART NEUR NETW
   Khosla A., 2011, C COMP VIS PATT REC
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kingma D. P., 2014, INT C LEARN REPR ICL
   Klailova M, 2012, FOLIA PRIMATOL, V83, P312, DOI 10.1159/000342143
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kuhl H., 2008, BEST PRACTICE GUIDEL
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Laing SE, 2003, J APPL ECOL, V40, P1102, DOI 10.1111/j.1365-2664.2003.00861.x
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu X., 2016, ARXIV160306765
   Long J., 2015, C COMP VIS PATT REC C COMP VIS PATT REC
   Loos A, 2011, EUR SIGNAL PR CONF, P922
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   Manly BF, 2010, HDB CAPTURE RECAPTUR
   McDonald-Madden E, 2008, J APPL ECOL, V45, P1630, DOI 10.1111/j.1365-2664.2008.01553.x
   Mehlman PT, 2002, INT J PRIMATOL, V23, P1257, DOI 10.1023/A:1021126920753
   Miura S., 1997, MALAYAN NATURE J MAL
   Morgan D, 2006, INT J PRIMATOL, V27, P147, DOI 10.1007/s10764-005-9013-0
   Nakashima Y, 2013, AM J PRIMATOL, V75, P1220, DOI 10.1002/ajp.22185
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Parkhi O. M., 2015, BRIT MACH VIS C BMVC
   Parnell RJ, 2002, AM J PRIMATOL, V56, P193, DOI 10.1002/ajp.1074
   Pebsworth PA, 2014, INT J PRIMATOL, V35, P825, DOI 10.1007/s10764-014-9802-4
   Prasad S, 2010, ECOL RES, V25, P225, DOI 10.1007/s11284-009-0650-1
   Redmon J., 2016, ARXIV161208242
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Robbins AM, 2016, ROY SOC OPEN SCI, V3, DOI 10.1098/rsos.160533
   Rodner E., 2015, C COMP VIS PATT REC
   Rodner E., 2016, ARXIV PREPRINT ARXIV
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Roy J, 2014, BIOL CONSERV, V180, P249, DOI 10.1016/j.biocon.2014.10.011
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Simon  Marcel, 2017, ARXIV170500487
   Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163
   Steinmetz R, 2014, J APPL ECOL, V51, P1469, DOI 10.1111/1365-2664.12239
   Stokes EJ, 2003, BEHAV ECOL SOCIOBIOL, V54, P329, DOI 10.1007/s00265-003-0630-3
   Stokes EJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010294
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   TUTIN CEG, 1984, AM J PRIMATOL, V6, P313, DOI 10.1002/ajp.1350060403
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vie J.-C., 2009, WILDLIFE CHANGING WO
   Wah C., 2011, TECHNICAL REPORT
   Walsh PD, 2005, ECOL APPL, V15, P1342, DOI 10.1890/03-5283
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Zhang N., 2014, EUR C COMP VIS ECCV
   Zhou K, 2016, DESTECH TRANS COMP
NR 86
TC 20
Z9 21
U1 0
U2 8
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2473-9936
BN 978-1-5386-1034-3
J9 IEEE INT CONF COMP V
PY 2017
BP 2820
EP 2830
DI 10.1109/ICCVW.2017.333
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4OB
UT WOS:000425239602105
OA Green Submitted
DA 2022-02-10
ER

PT J
AU O'Connor, KM
   Nathan, LR
   Liberati, MR
   Tingley, MW
   Vokoun, JC
   Rittenhouse, TAG
AF O'Connor, Kelly M.
   Nathan, Lucas R.
   Liberati, Marjorie R.
   Tingley, Morgan W.
   Vokoun, Jason C.
   Rittenhouse, Tracy A. G.
TI Camera trap arrays improve detection probability of wildlife:
   Investigating study design considerations using an empirical dataset
SO PLOS ONE
LA English
DT Article
ID AUTOMATICALLY TRIGGERED CAMERAS; ESTIMATING SITE OCCUPANCY; WHITE-TAILED
   DEER; IMPERFECT DETECTION; PHOTOGRAPHIC RATES; ESTIMATE DENSITIES;
   SAMPLING DESIGN; CRYPTIC MAMMALS; HABITAT; TIGERS
AB Camera trapping is a standard tool in ecological research and wildlife conservation. Study designs, particularly for small-bodied or cryptic wildlife species often attempt to boost low detection probabilities by using non-random camera placement or baited cameras, which may bias data, or incorrectly estimate detection and occupancy. We investigated the ability of non-baited, multi-camera arrays to increase detection probabilities of wildlife. Study design components were evaluated for their influence on wildlife detectability by iteratively parsing an empirical dataset (1) by different sizes of camera arrays deployed (1-10 cameras), and (2) by total season length (1-365 days). Four species from our dataset that represented a range of body sizes and differing degrees of presumed detectability based on life history traits were investigated: white-tailed deer (Odocoileus virginianus), bobcat (Lynx rufus), raccoon (Procyon lotor), and Virginia opossum (Didelphis virginiana). For all species, increasing from a single camera to a multi-camera array significantly improved detection probability across the range of season lengths and number of study sites evaluated. The use of a two camera array increased survey detection an average of 80% (range 40-128%) from the detection probability of a single camera across the four species. Species that were detected infrequently benefited most from a multiple-camera array, where the addition of up to eight cameras produced significant increases in detectability. However, for species detected at high frequencies, single cameras produced a season-long (i.e, the length of time over which cameras are deployed and actively monitored) detectability greater than 0.75. These results highlight the need for researchers to be critical about camera trap study designs based on their intended target species, as detectability for each focal species responded differently to array size and season length. We suggest that researchers a priori identify target species for which inference will be made, and then design camera trapping studies around the most difficult to detect of those species.
C1 [O'Connor, Kelly M.; Nathan, Lucas R.; Liberati, Marjorie R.; Vokoun, Jason C.; Rittenhouse, Tracy A. G.] Univ Connecticut, Wildlife & Fisheries Conservat Ctr, Dept Nat Resources & Environm, Storrs, CT 06269 USA.
   [Tingley, Morgan W.] Univ Connecticut, Ecol & Evolutionary Biol, Storrs, CT USA.
   [O'Connor, Kelly M.] Archbold Biol Stn, Venus, FL 33960 USA.
RP O'Connor, KM (corresponding author), Univ Connecticut, Wildlife & Fisheries Conservat Ctr, Dept Nat Resources & Environm, Storrs, CT 06269 USA.; O'Connor, KM (corresponding author), Archbold Biol Stn, Venus, FL 33960 USA.
EM kelly.oconnor8@gmail.com
RI Tingley, Morgan W/F-8519-2011
OI Tingley, Morgan W/0000-0002-1477-2218; Liberati,
   Marjorie/0000-0002-8434-1659
FU Federal Aid in Wildlife Restoration Act [W-49-R]; USDA National
   Institute of Food and Agricultural Hatch [C0NS00920]; Connecticut
   Department of Energy and Environmental Protection [2014-38420-21802]
FX Funding provided by Federal Aid in Wildlife Restoration Act under
   Project W-49-R "Wildlife Investigations" administered by the Connecticut
   Department of Energy and Environmental Protection, Wildlife Division.
   USDA National Institute of Food and Agricultural Hatch Project C0NS00920
   to TR. Connecticut Department of Energy and Environmental Protection
   2014-38420-21802 to ML. The funders had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Bailey LL, 2007, ECOL APPL, V17, P281, DOI 10.1890/1051-0761(2007)017[0281:SDTIOS]2.0.CO;2
   Bailey LL, 2014, METHODS ECOL EVOL, V5, P1269, DOI 10.1111/2041-210X.12100
   Braczkowski AR, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151033
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   Curtis PD, 2009, 25 HUM WILDL INT
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   ESRI (Environmental Systems Resource Institute), 2015, ARCMAP 10 1
   Guillera-Arroita G, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099571
   Guillera-Arroita G, 2012, METHODS ECOL EVOL, V3, P860, DOI 10.1111/j.2041-210X.2012.00225.x
   Guillera-Arroita G, 2010, METHODS ECOL EVOL, V1, P131, DOI 10.1111/j.2041-210X.2010.00017.x
   Hamel S, 2013, METHODS ECOL EVOL, V4, P105, DOI 10.1111/j.2041-210x.2012.00262.x
   Heilbrun RD, 2003, WILDLIFE SOC B, V31, P748
   Heilbrun RD, 2006, WILDLIFE SOC B, V34, P69, DOI 10.2193/0091-7648(2006)34[69:EBAUAT]2.0.CO;2
   Jennelle CS, 2002, ANIM CONSERV, V5, P119, DOI 10.1017/S1367943002002160
   Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
   Kelly MJ, 2003, CARIBB GEOGR, V13, P19
   Long RA, 2011, LANDSCAPE ECOL, V26, P327, DOI 10.1007/s10980-010-9547-1
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2004, J ANIM ECOL, V73, P546, DOI 10.1111/j.0021-8790.2004.00828.x
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   O'Brien TG, 2010, ANIM CONSERV, V13, P335, DOI 10.1111/j.1469-1795.2010.00357.x
   O'Connell AF, 2006, J WILDLIFE MANAGE, V70, P1625, DOI 10.2193/0022-541X(2006)70[1625:ESOADP]2.0.CO;2
   O'Connor KM, 2017, AM MIDL NAT, V177, P15, DOI 10.1674/0003-0031-177.1.15
   R Development Core Team, 2012, R LANG ENV STAT COMP
   Roberts CW, 2006, J WILDLIFE MANAGE, V70, P263, DOI 10.2193/0022-541X(2006)70[263:COCARS]2.0.CO;2
   Rota CT, 2009, J APPL ECOL, V46, P1173, DOI 10.1111/j.1365-2664.2009.01734.x
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Ruell EW, 2009, J MAMMAL, V90, P129, DOI 10.1644/07-MAMM-A-249.1
   SARGEANT GA, 2003, SWIFT FOX ECOLOGY CO, P99
   Shannon G, 2014, PEERJ, V2, DOI 10.7717/peerj.532
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wearn OR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077598
   Williams SC, 2013, WILDLIFE SOC B, V37, P137, DOI 10.1002/wsb.236
NR 40
TC 28
Z9 29
U1 0
U2 61
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD APR 19
PY 2017
VL 12
IS 4
AR e0175684
DI 10.1371/journal.pone.0175684
PG 12
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA ES9KK
UT WOS:000399875600041
PM 28422973
OA Green Published, gold, Green Submitted
DA 2022-02-10
ER

PT C
AU Cheema, GS
   Anand, S
AF Cheema, Gullal Singh
   Anand, Saket
BE Altun, Y
   Das, K
   Mielikainen, T
   Malerba, D
   Stefanowski, J
   Read, J
   Zitnik, M
   Ceci, M
   Dzeroski, S
TI Automatic Detection and Recognition of Individuals in Patterned Species
SO MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, ECML PKDD 2017,
   PT III
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECML PKDD)
CY SEP 18-22, 2017
CL Skopje, MACEDONIA
SP Deutsche Post DHL Grp, Google, AGT, ASML, Deloitte, NEC Europe Ltd, Siemens, Cambridge Univ Press, IEEE CAA Journal Automatica Sinica, Springer, IBM Res, Data Min & Knowledge Discovery, Machine Learning, EurAi, GrabIT
DE Animal biometrics; Wildlife monitoring; Detection; Recognition;
   Convolutional neural network; Computer vision
AB Visual animal biometrics is rapidly gaining popularity as it enables a non-invasive and cost-effective approach for wildlife monitoring applications. Widespread usage of camera traps has led to large volumes of collected images, making manual processing of visual content hard to manage. In this work, we develop a framework for automatic detection and recognition of individuals in different patterned species like tigers, zebras and jaguars. Most existing systems primarily rely on manual input for localizing the animal, which does not scale well to large datasets. In order to automate the detection process while retaining robustness to blur, partial occlusion, illumination and pose variations, we use the recently proposed Faster-RCNN object detection framework to efficiently detect animals in images. We further extract features from AlexNet of the animal's flank and train a logistic regression (or Linear SVM) classifier to recognize the individuals. We primarily test and evaluate our framework on a camera trap tiger image dataset that contains images that vary in overall image quality, animal pose, scale and lighting. We also evaluate our recognition system on zebra and jaguar images to show generalization to other patterned species. Our framework gives perfect detection results in camera trapped tiger images and a similar or better individual recognition performance when compared with state-of-the-art recognition techniques.
C1 [Cheema, Gullal Singh; Anand, Saket] IIIT Delhi, New Delhi, India.
RP Cheema, GS (corresponding author), IIIT Delhi, New Delhi, India.
EM gullal1408@iiitd.ac.in; anands@iiitd.ac.in
OI Cheema, Gullal Singh/0000-0003-4354-9629
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Burghardt T., 2004, EWIMT
   Burghardt T, 2006, NEUREL 2006: EIGHT SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL ENGINEERING, PROCEEDINGS, P27
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Jain AK, 2000, IEEE T IMAGE PROCESS, V9, P846, DOI 10.1109/83.841531
   Jiang XD, 2000, INT C PATT RECOG, P1038, DOI 10.1109/ICPR.2000.906252
   KLINGEL H, 1974, Zeitschrift fuer Tierpsychologie, V36, P37
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lahiri M., 2011, P 1 ACM INT C MULT R, P6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mizroch S. A., 2003, MAR FISH REV, V65, P25
   Norouzzadeh MS, 2017, ARXIV170305830
   Prodger Phillip, 2009, DARWINS CAMERA ART P
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Scott D. K., 1978, RECOGNITION MARKING, P160
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tisse C.-L., 2002, P VISION INTERFACE, P294
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 31
TC 11
Z9 13
U1 1
U2 10
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-71273-4; 978-3-319-71272-7
J9 LECT NOTES ARTIF INT
PY 2017
VL 10536
BP 27
EP 38
DI 10.1007/978-3-319-71273-4_3
PN III
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BK8MC
UT WOS:000443111100003
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Nazir, S
   Kaleem, M
AF Nazir, Sajid
   Kaleem, Muhammad
TI Advances in image acquisition and processing technologies transforming
   animal ecological studies
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Animal behaviour; Big data; Computer vision; Deep learning; Image
   processing; Population monitoring; Drone; Robot
ID CAMERA TRAPS; HABITAT; CONSERVATION; BIODIVERSITY; BEHAVIOR;
   PERSPECTIVE; DISTURBANCE; NETWORKS; INSIGHTS
AB Images and videos have become pervasive in ecological research and the ease of acquiring image data and its subsequent processing can provide answers in research areas such as species recognition, animal behaviour, and population studies which are critical for animal conservation and biodiversity. Technological advances in imaging are enabling data collection from new areas such as from underwater, new modalities such as thermal and new ways of processing such as deep learning. These advances are accelerating due to ease of data collection, better storage and processing technologies with associated lowering costs. The advancements in state-of-the-art machine learning for image and video classification and analysis can directly be applied in ecology. Ecological applications are generally conducted in remote and harsh deployment environments, and therefore present formidable challenges that require appreciation of the limitations of such technologies. The ecological field is poised to make use of images acquired through drones, robotics, and satellites through machine learning for rapid advancements in critical research areas. Timely insights from such data help to understand and protect the species and environment. This paper provides a review of the advancements in image acquisition and processing technologies used in animal ecological studies. We also discuss concepts and technologies that would help foster future ecological research methodologies potentially opening new insights and quickening growth to an already rich and data-intensive field.
C1 [Nazir, Sajid] Glasgow Caledonian Univ, Sch Comp Engn & Built Environm, Glasgow, Lanark, Scotland.
   [Kaleem, Muhammad] COMSATS Univ, Dept Elect & Comp Engn, Islamabad, Pakistan.
RP Nazir, S (corresponding author), Glasgow Caledonian Univ, Sch Comp Engn & Built Environm, Glasgow, Lanark, Scotland.
EM sajid.nazir@gcu.ac.uk
CR Andersen GE, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0230216
   [Anonymous], PHIL T R SOC B, DOI [10.1098/rstb.2013.0197., DOI 10.1098/RSTB.2013.0197]
   [Anonymous], AMBIO S4, DOI [10.1007/s13280-015-0711-3., DOI 10.1007/S13280-015-0711-3]
   [Anonymous], METHODS ECOL EVOL
   Balch T, 2006, P IEEE, V94, P1445, DOI 10.1109/JPROC.2006.876969
   Beauxis-Aussalet E, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (IEEE DSAA 2015), P900
   Branson K, 2014, NAT METHODS, V11, P721, DOI 10.1038/nmeth.3004
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Crutsinger GM, 2016, J UNMANNED VEH SYST, V4, P161, DOI 10.1139/juvs-2016-0008
   Dahlen B., 2017, SUCCESSFUL AERIAL SU
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   Dietterich TG, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P8
   Ditria EM, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.00429
   Evans LJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16091527
   Farley SS, 2018, BIOSCIENCE, V68, P563, DOI 10.1093/biosci/biy068
   Fretwell PT, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088655
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
   Gremillet D., 2012, Open Journal of Ecology, V2, P49, DOI 10.4236/oje.2012.22006
   Handcock RN, 2009, SENSORS-BASEL, V9, P3586, DOI 10.3390/s90503586
   Harmsen BJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179505
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
   Huang ZP, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087318
   Jeantet L, 2020, ROY SOC OPEN SCI, V7, DOI 10.1098/rsos.200139
   Lopez JJ, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010010
   Joo D, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077686
   Kays R, 2019, INT J REMOTE SENS, V40, P407, DOI 10.1080/01431161.2018.1523580
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   Koniar D, 2016, COMPUT METH PROG BIO, V127, P258, DOI 10.1016/j.cmpb.2015.12.009
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kucera TE, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P9, DOI 10.1007/978-4-431-99495-4_2
   Kwok R, 2018, NATURE, V556, P137, DOI 10.1038/d41586-018-03924-9
   Lawson, 2008, INT WORKSH DISTR SEN
   LTER, 2020, LONG TERM ECOLOGICAL
   Mapes KL, 2020, DRONES-BASEL, V4, DOI 10.3390/drones4020012
   Mattern T, 2018, PEERJ, V6, DOI 10.7717/peerj.5459
   McMahon CR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092613
   Mellin C, 2012, ECOL APPL, V22, P792, DOI 10.1890/11-2105.1
   NAZIR S, 2017, PLOS, P69758
   Nazir S, 2017, INT J SATELL COMM N, V35, P201, DOI 10.1002/sat.1176
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   Peters DPC, 2014, ECOSPHERE, V5, DOI 10.1890/ES13-00359.1
   Rafiq K, 2019, METHODS ECOL EVOL, V10, P1517, DOI 10.1111/2041-210X.13231
   Recknagel F., 2018, ECOLOGICAL INFORM DA
   Rovero F, 2013, J MAMMAL, V94, P792, DOI 10.1644/12-MAMM-A-235.1
   Rush GP, 2018, ECOL EVOL, V8, P12322, DOI 10.1002/ece3.4495
   Schmaljohann H, 2020, ECOGRAPHY, V43, P236, DOI 10.1111/ecog.04807
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Schofield G, 2017, FUNCT ECOL, V31, P2310, DOI 10.1111/1365-2435.12930
   Shan Y, 2006, ECOL MODEL, V195, P129, DOI 10.1016/j.ecolmodel.2005.11.015
   St-Louis V, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2013.0197
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stepanian PM, 2014, METHODS ECOL EVOL, V5, P730, DOI 10.1111/2041-210X.12214
   Suraci JP, 2017, METHODS ECOL EVOL, V8, P957, DOI 10.1111/2041-210X.12711
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   TABAK MA, METHODS ECOL EVOL
   Troscianko J, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2015.0777
   Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
   van der Wal R, 2015, AMBIO, V44, pS612, DOI 10.1007/s13280-015-0711-3
   van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
   Vas E, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2014.0754
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wang HQ, 2020, IEEE T NEUR NET LEAR, V31, P972, DOI 10.1109/TNNLS.2019.2912082
   Wang K, 2010, SENSORS-BASEL, V10, P9647, DOI 10.3390/s101109647
   Wich S, 2016, J UNMANNED VEH SYST, V4, P45, DOI 10.1139/juvs-2015-0015
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Wildlife Insights, 2020, WILDLIFE INSIGHTS
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 75
TC 1
Z9 1
U1 4
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAR
PY 2021
VL 61
AR 101212
DI 10.1016/j.ecoinf.2021.101212
PG 11
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QT6DJ
UT WOS:000626676100003
OA Green Published
DA 2022-02-10
ER

PT C
AU Song, YL
   Wang, HP
   Li, S
   Xu, FL
   Liu, JT
AF Song, Yulin
   Wang, Hongpeng
   Li, Sheng
   Xu, Fulai
   Liu, Jingtai
GP IEEE
TI CNN based Wildlife Recognition with Super-pixel Segmentation for
   Ecological Surveillance
SO 2018 IEEE 8TH ANNUAL INTERNATIONAL CONFERENCE ON CYBER TECHNOLOGY IN
   AUTOMATION, CONTROL, AND INTELLIGENT SYSTEMS (IEEE-CYBER)
SE IEEE Annual International Conference on Cyber Technology in Automation
   Control and Intelligent Systems
LA English
DT Proceedings Paper
CT 8th IEEE Annual International Conference on Cyber Technology in
   Automation, Control, and Intelligent Systems (IEEE-CYBER)
CY JUL 19-23, 2018
CL Tianjin, PEOPLES R CHINA
SP IEEE, IEEE Robot & Automat Soc, Nankai Univ, Chinese Assoc Automat, Tech Comm Robot, CINGAI, Hebei Univ Technol, K C Wong Educ Fdn
DE wildlife monitoring; super-pixel; convolutional neural network; low
   resolution image
AB Recent years, the convolutional neural network have shown to provide excellent results on recognition in different competitions. However, challenges in specific missions still exist. The cluttered backgrounds and rich feature changes of wild environment bring great challenges to the problem of species recognition of wild animals. To address these problems, this paper proposes a novel and effective combination to learn a CNN model. This is achieved by apply simple linear iterative clustering (SLIC) super-pixel segmentation method to unified data dimension during the process of making raw image data (captured by camera-traps) into a dataset. In short, the super pixel-divided images provides the input of the convolutional neural network. In order to verify the application, we conducted a comprehensive performance comparisons between our SLIC-dataset and generally used Resize-dataset over CNN networks. Results proved that our proposed method performs exceptionally well in low-resolution data when it is crucial to take full advantage of the edge information of original images. In addition, we collected and annotated a standard camera-trap dataset of 14 common wildlife species in China, which contains 16,480 training images and 4,120 testing images.
C1 [Song, Yulin; Wang, Hongpeng; Xu, Fulai; Liu, Jingtai] Nankai Univ, Inst Robot & Automat Informat Syst, Tianjin 300353, Peoples R China.
   [Song, Yulin; Wang, Hongpeng; Xu, Fulai; Liu, Jingtai] Tianjin Key Lab Intelligent Robot, Tianjin 300353, Peoples R China.
   [Li, Sheng] Peking Univ, Sch Life Sci, Beijing 100871, Peoples R China.
RP Wang, HP (corresponding author), Nankai Univ, Inst Robot & Automat Informat Syst, Tianjin 300353, Peoples R China.; Wang, HP (corresponding author), Tianjin Key Lab Intelligent Robot, Tianjin 300353, Peoples R China.
EM songyulin@mail.nankai.edu.cn; hpwang@nankai.edu.cn; shengli@pku.edu.cn;
   xufl@mail.nankai.edu.cn; liujt@nankai.edu.cn
FU China Scholarship CouncilChina Scholarship Council; Key Program of
   Natural Science Foundation of Tianjin [15JCZDJC31200]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   (NSFC) [61375087]
FX This work is supported by China Scholarship Council, National Natural
   Science Foundation of China (Grant No. 61375087) and Key Program of
   Natural Science Foundation of Tianjin (Grant No. 15JCZDJC31200).
CR Achanta R., 2010, 149300 EPFL
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Borman S., 1998, CIRC SYST 1998 P 199, P374
   Boser, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Huang JH, 2010, IEEE SYST J, V4, P198, DOI 10.1109/JSYST.2010.2047294
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Krizhevsky A., 2010, CONVOLUTIONAL DEEP B, V40, P7
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lee H., 2009, P ANN INT C MACH LEA, P609, DOI DOI 10.1145/1553374.1553453
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Linares OAC, 2017, IET IMAGE PROCESS, V11, P1219, DOI 10.1049/iet-ipr.2016.0072
   Monteiro Sildomar T., 2016, IM PROC ICIP 2016 IE
   Moore A.P., 2008, IEEE C COMP VIS PATT, P1
   Ren Xiaofeng, 2003, LEARNING CLASSIFICAT
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Tian Luchao, 2017, MULT EXP ICME 2017 I
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Zhang PJ, 2014, IEEE C ELEC DEVICES
   Zhou Peicheng, 2016, CVPR
NR 25
TC 0
Z9 0
U1 0
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2379-7711
BN 978-1-5386-7057-6
J9 IEEE ANN INT CONF CY
PY 2018
BP 132
EP 137
PG 6
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science
GA BM8EA
UT WOS:000468941800023
DA 2022-02-10
ER

PT C
AU Hines, G
   Swanson, A
   Kosmala, M
   Lintott, C
AF Hines, Greg
   Swanson, Alexandra
   Kosmala, Margaret
   Lintott, Chris
GP AAAI
TI Aggregating User Input in Ecology Citizen Science Projects
SO PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL
   INTELLIGENCE
LA English
DT Proceedings Paper
CT 29th Association-for-the-Advancement-of-Artificial-Intelligence (AAAI)
   Conference on Artificial Intelligence
CY JAN 25-30, 2015
CL Austin, TX
SP Assoc Advancement Artificial Intelligence
AB Camera traps (remote, automatic cameras) are revolutionizing large-scale studies in ecology. The Serengeti Lion Project has used camera traps to produce over 1.5 million pictures of animals in the Serengeti. To analyze these pictures, the Project created Snapshot Serengeti, a citizen science website where volunteers can help classify animals. To increase accuracy, each photo is shown to multiple users and a critical step is aggregating individual classifications. In this paper, we present a new aggregation algorithm which achieves an accuracy of 98.6%, better than many human experts. Our algorithm also requires fewer users per photo than existing methods. The algorithm is intuitive and designed so that non experts can understand the end results.
C1 [Hines, Greg; Swanson, Alexandra; Lintott, Chris] Univ Oxford, Dept Phys, Citizen Sci Grp, Oxford, England.
   [Kosmala, Margaret] Harvard Univ, Dept Organism & Evolutionary Biol, Cambridge, MA 02138 USA.
RP Hines, G (corresponding author), Univ Oxford, Dept Phys, Citizen Sci Grp, Oxford, England.
EM greg@zooniverse.org; ali@zooniverse.org; kosmala@fas.harvard.edu;
   cjl@astro.ox.ac.uk
FU MICO
FX These authors are part of the Zooniverse project, funded in part by
   MICO.
CR Dalvi N., 2013, P 22 INT C WORLD WID
   Dawid A. P, 1977, J ROYAL STAT SOC C
   Karger D. R., 2011, ADV NEURAL INFORM PR
   Kim H.-C., 2012, P 15 INT C ART INT S
   Kosmala M., 2013, SUMMARY EXPERTS
   Littlestone N, 1994, INFORM COMPUTATION
   Liu Q., 2012, NIPS, V25, P692
   Simpson E., 2013, DECISION MAKING IMPE, P1, DOI [DOI 10.1007/978-3-642-36406-8_1, DOI 10.1007/978-3-642-36406-8]
   Swanson A, 2014, ECOLOGY EVOLUTION
   Venanzi M., 2014, P 23 INT C WORLD WID
NR 10
TC 8
Z9 8
U1 0
U2 2
PU ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI PALO ALTO
PA 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
PY 2015
BP 3975
EP 3980
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BN6JS
UT WOS:000485625504003
DA 2022-02-10
ER

PT J
AU Guo, YH
   Rothfus, TA
   Ashour, AS
   Si, L
   Du, CL
   Ting, TF
AF Guo, Yanhui
   Rothfus, Thomas A.
   Ashour, Amira S.
   Si, Lei
   Du, Chunlai
   Ting, Tih-Fen
TI Varied channels region proposal and classification network for wildlife
   image classification under complex environment
SO IET IMAGE PROCESSING
LA English
DT Article
DE feature extraction; object recognition; object detection; learning
   (artificial intelligence); neural nets; image segmentation; image
   classification; convolution; cameras; varied channels region proposal;
   classification network; wildlife image classification; deep
   convolutional neural network; automatic wildlife animal classification;
   camera trapped images; different aims; background images; region
   proposal component; region candidates; classification component;
   animals; potential animal regions; low contrast animal images; object
   detection network; faster region convolutional neural network
AB A varied channels region proposal and classification network (VCRPCN) is developed based on a deep convolutional neural network (DCNN) and the characteristics of the animals appearing for automatic wildlife animal classification in camera trapped images, the architecture of the network is improved by feeding different channels into different components of the network to accomplish different aims, i.e. the animal images and their background images are employed in the region proposal component to extract region candidates for the animal's location, and the animal images combined with the region candidates are fed into the classification component to identify their categories. This novel architecture considers changes to the image due to the animals' appearances, and identifies potential animal regions in images and extracts their local features to describe and classify them. Five hundred low contrast animal images have been collected. All images have low contrast due to being acquired during the night. Cross-validation is employed to statistically measure the performance of the proposed algorithm. The experimental results demonstrate that in comparison with the well-known object detection network, faster R-CNN, the proposed VCRPCN achieved higher accuracy with the same dataset and training configuration with an average accuracy improvement of 21%.
C1 [Guo, Yanhui; Si, Lei] Univ Illinois, Dept Comp Sci, Springfield, IL 62703 USA.
   [Rothfus, Thomas A.] Univ Illinois, Therkildsen Field Stn Emiquon, Springfield, IL USA.
   [Rothfus, Thomas A.; Ting, Tih-Fen] Univ Illinois, Dept Environm Studies, Springfield, IL USA.
   [Ashour, Amira S.] Tanta Univ, Dept Elect & Elect Commun Engn, Tanta, Egypt.
   [Du, Chunlai] North China Univ Technol, Sch Comp Sci, Beijing, Peoples R China.
RP Guo, YH (corresponding author), Univ Illinois, Dept Comp Sci, Springfield, IL 62703 USA.
EM yguo56@uis.edu
RI ; Guo, Yanhui/L-3267-2013
OI Si, Lei/0000-0002-1886-0362; Guo, Yanhui/0000-0003-1814-9682; Ashour,
   Amira/0000-0003-3217-6185
CR Borji A, 2014, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2014.22
   Bridle J. S., 1990, NEUROCOMPUTING, V4, P227, DOI DOI 10.1007/978-3-642-76153-9_28
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
   Darrell, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Kays R., 2010, INT J RES REV WIREL, V1, P19
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI [10.1007/s11263-019-01204-1, 10.1007/978-3-030-01264-9_45]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Matuska S, 2016, RADIOENGINEERING, V25, P161, DOI 10.13164/re.2016.0161
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   REDMON J, 2016, PROC CVPR IEEE, P779, DOI DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vitousek PM, 1997, SCIENCE, V277, P494, DOI 10.1126/science.277.5325.494
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Yu X., 2013, J IMAGE VIDEO PROCES, P561
   Zhang S, 2015, IEEE SENS J, V15, P2679, DOI 10.1109/JSEN.2014.2382174
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
NR 32
TC 2
Z9 2
U1 5
U2 23
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1751-9659
EI 1751-9667
J9 IET IMAGE PROCESS
JI IET Image Process.
PD MAR 27
PY 2020
VL 14
IS 4
BP 585
EP 591
DI 10.1049/iet-ipr.2019.1042
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA KW1WN
UT WOS:000520961700001
DA 2022-02-10
ER

PT J
AU Kalan, AK
   Hohmann, G
   Arandjelovic, M
   Boesch, C
   McCarthy, MS
   Agbor, A
   Angedakin, S
   Bailey, E
   Balongelwa, CW
   Bessone, M
   Bocksberger, G
   Coxe, SJ
   Deschner, T
   Despres-Einspenner, ML
   Dieguez, P
   Fruth, B
   Herbinger, I
   Granjon, AC
   Head, J
   Kablan, YA
   Langergraber, KE
   Lokasola, AL
   Maretti, G
   Marrocoli, S
   Mbende, M
   Moustgaard, J
   N'Goran, PK
   Robbins, MM
   van Schijndel, J
   Sommer, V
   Surbeck, M
   Tagg, N
   Willie, J
   Wittig, RM
   Kuhl, HS
AF Kalan, Ammie K.
   Hohmann, Gottfried
   Arandjelovic, Mimi
   Boesch, Christophe
   McCarthy, Maureen S.
   Agbor, Anthony
   Angedakin, Samuel
   Bailey, Emma
   Balongelwa, Cosma Wilungula
   Bessone, Mattia
   Bocksberger, Gaelle
   Coxe, Sally Jewel
   Deschner, Tobias
   Despres-Einspenner, Marie-Lyne
   Dieguez, Paula
   Fruth, Barbara
   Herbinger, Ilka
   Granjon, Anne-Celine
   Head, Josephine
   Kablan, Yves Aka
   Langergraber, Kevin E.
   Lokasola, Albert Lotana
   Maretti, Giovanna
   Marrocoli, Sergio
   Mbende, Menard
   Moustgaard, Jennifer
   N'Goran, Paul Kouame
   Robbins, Martha M.
   van Schijndel, Joost
   Sommer, Volker
   Surbeck, Martin
   Tagg, Nikki
   Willie, Jacob
   Wittig, Roman M.
   Kuehl, Hjalmar S.
TI Novelty Response of Wild African Apes to Camera Traps
SO CURRENT BIOLOGY
LA English
DT Article
ID PERSONALITY-DEVELOPMENT; CHIMPANZEES; EVOLUTION; RISK; TEMPERAMENT;
   PREFERENCES; EXPLORATION; NEOPHOBIA; CURIOSITY; BOLDNESS
AB Temperament and personality research in humans and nonhuman animals measures behavioral variation in individual, population, or species-specific traits with implications for survival and fitness, such as social status, foraging, and mating success [1-5]. Curiosity and risk-taking tendencies have been studied extensively across taxa by measuring boldness and exploration responses to experimental novelty exposure [3, 4, 6-15]. Here, we conduct a natural field experiment using wildlife monitoring technology to test variation in the reaction of wild great apes (43 groups of naive chimpanzees, bonobos, and western gorillas across 14 field sites in Africa) to a novel object, the camera trap. Bonobo and gorilla groups demonstrated a stronger looking impulse toward the camera trap device compared to chimpanzees, suggesting higher visual attention and curiosity. Bonobos were also more likely to show alarm and other fearful behaviors, although such neophobic (and conversely, neophilic) responses were generally rare. Among all three species, individuals looked at cameras longer when they were young, were associating with fewer individuals, and did not live near a long-term research site. Overall, these findings partially validate results from great ape novelty paradigms in captivity [7, 8]. We further suggest that species-typical leadership styles [16] and social and environmental effects, including familiarity with humans, best explain novelty responses of wild great apes. In sum, this study illustrates the feasibility of large-scale field experiments and the importance of both intrinsic and extrinsic factors in shaping animal curiosity.
C1 [Kalan, Ammie K.; Hohmann, Gottfried; Arandjelovic, Mimi; Boesch, Christophe; McCarthy, Maureen S.; Agbor, Anthony; Angedakin, Samuel; Bailey, Emma; Bocksberger, Gaelle; Deschner, Tobias; Despres-Einspenner, Marie-Lyne; Dieguez, Paula; Granjon, Anne-Celine; Head, Josephine; Kablan, Yves Aka; Maretti, Giovanna; Marrocoli, Sergio; Robbins, Martha M.; van Schijndel, Joost; Surbeck, Martin; Wittig, Roman M.; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Boesch, Christophe; Kablan, Yves Aka] WCF, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Balongelwa, Cosma Wilungula] ICCN, 13 Ave Clin, Kinshasa, DEM REP CONGO.
   [Bessone, Mattia] Ludwig Maximilians Univ Munchen, Fac Biol, Dept Neurobiol, Grossaderner Str 2, D-82152 Planegg Martinsried, Germany.
   [Coxe, Sally Jewel; Lokasola, Albert Lotana; Moustgaard, Jennifer; Surbeck, Martin] Bonobo Conservat Initiat, 2701 Connecticut Ave,NW 702, Washington, DC 20008 USA.
   [Fruth, Barbara] Liverpool John Moores Univ, Sch Nat Sci & Psychol, Liverpool L3 3AF, Merseyside, England.
   [Fruth, Barbara; Tagg, Nikki; Willie, Jacob] Royal Zool Soc Antwerp, Ctr Res & Conservat, B-2018 Antwerp, Belgium.
   [Herbinger, Ilka] WWF Germany, Dept Africa & South Amer, Reinhardtstr 18, D-10117 Berlin, Germany.
   [Langergraber, Kevin E.] Arizona State Univ, Sch Human Evolut & Social Change, 900 Cady Mall, Tempe, AZ 85287 USA.
   [Langergraber, Kevin E.] Arizona State Univ, Inst Human Origins, 900 Cady Mall, Tempe, AZ 85287 USA.
   [Mbende, Menard] WWF Democrat Republ Congo DRC, 14 Ave Sergent Moke, Kinshasa, DEM REP CONGO.
   [N'Goran, Paul Kouame] WWF Reg Off Africa Yaounde Hub, POB 6776, Yaounde, Cameroon.
   [Sommer, Volker] UCL, Dept Anthropol, London WC1H 0BW, England.
   [Wittig, Roman M.] Ctr Suisse Rech Scient, Tai Chimpanzee Project, BP 1301, Abidjan 01, Cote Ivoire.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv Halle Leip, D-04103 Leipzig, Germany.
RP Kalan, AK (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM ammie_kalan@eva.mpg.de
RI Bessone, Mattia/AAL-6716-2021; Kalan, Ammie/M-8027-2019
OI Bessone, Mattia/0000-0002-8066-6413; Kalan, Ammie/0000-0003-1542-7077;
   Bocksberger, Gaelle/0000-0002-3399-0405
FU Max Planck SocietyMax Planck SocietyFoundation CELLEX; Max Planck
   Society Innovation Fund; Heinz L. Krekeler Foundation; Kreditanstalt fur
   Wiederaufbau (KfW Group); WWF Germany
FX This work was funded by the Max Planck Society, Max Planck Society
   Innovation Fund, and Heinz L. Krekeler Foundation, and data related to
   the Salonga National Park in the Democratic Republic of the Congo was
   funded by the Kreditanstalt fur Wiederaufbau (KfW Group) on behalf of
   the German Government and WWF Germany. We sincerely thank Karsten
   Dierks, Henk Eshuis, Veerle Hermans, Sonja Nicholl, Luc Tedonzong, and
   Rodolphe Violleau for assistance in the field; Andrew Dunn, John Hart,
   Martin Ter Heegde, Thurston Cleveland Hicks, and Inaoyom Imong for
   facilitating field work; Chimp&See citizen scientists for help with
   video coding for multiple PanAf sites
   (https://www.chimpandsee.org/#/about/authors); Kristin Havercamp,
   Stefano Lucchesi, Anna Preis, Liran Samuni, and Claudia Wilke for help
   with the identification of individuals; and Vittoria Estienne for her
   participation in the interobserver reliability test. We also thank the
   following wildlife and government authorities for permissions to conduct
   and host research sites in their countries: Ministere de la Recherche
   Scientifique et de l'Innovation, Ministere des Forets et de la Faune in
   Cameroon, Ministere de la Recherche Scientifique, and Ministere des Eaux
   et Forets in Cote d'Ivoire; Institut Congolais pour la Conservation de
   la Nature and Ministere de la Recherche Scientifique in DR Congo; Agence
   Nationale des Parcs Nationaux, Centre National de la Recherche
   Scientifique et Technologique, and Societe Equatoriale d'Exploitation
   Forestiere in Gabon; Ministere de l'Agriculture de l'Elevage et des Eaux
   et Forets in Guinea; Instituto da Biodiversidade e das Areas Protegidas
   in Guinea-Bissau; Forestry Development Authority in Liberia; National
   Park Service and Conservation Society of Mbe Mountains in Nigeria;
   Direction des Eaux, Forets Chasses et de la Conservation des Sols, and
   Reserve Naturelle Communautaire de Dindefelo in Senegal; and Uganda
   National Council for Science and Technology, Uganda Wildlife Authority
   in Uganda. We also thank three anonymous reviewers for their feedback,
   which significantly improved this manuscript.
CR Almeling L, 2016, CURR BIOL, V26, P1744, DOI 10.1016/j.cub.2016.04.066
   Anderson JR, 2017, PRIMATES, V58, P51, DOI 10.1007/s10329-016-0574-7
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   BERLYNE DE, 1966, SCIENCE, V153, P25, DOI 10.1126/science.153.3731.25
   Boesch C, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22613
   Boesch L, 2017, ECOL SOC, V22, DOI 10.5751/ES-09516-220436
   Boinski S., 2000, MOVE WHY ANIMALS TRA
   Bowerman B.L., 2000, LINEAR STAT MODELS A
   Brown GE, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2712
   Byrnes JP, 1999, PSYCHOL BULL, V125, P367, DOI 10.1037/0033-2909.125.3.367
   Damerius LA, 2017, ANIM BEHAV, V134, P57, DOI 10.1016/j.anbehav.2017.10.005
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Dobson A.J., 2008, INTRO GEN LINEAR MOD, Vthird
   Duda P, 2013, J HUM EVOL, V65, P424, DOI 10.1016/j.jhevol.2013.07.009
   Forss SIF, 2015, AM J PRIMATOL, V77, P1109, DOI 10.1002/ajp.22445
   Forstmeier W, 2011, BEHAV ECOL SOCIOBIOL, V65, P47, DOI 10.1007/s00265-010-1038-5
   Fox J, 2001, R COMPANION APPL REG
   Friard O, 2016, METHODS ECOL EVOL, V7, P1325, DOI 10.1111/2041-210X.12584
   Greenberg JR, 2017, ANIM BEHAV, V132, P303, DOI 10.1016/j.anbehav.2017.08.023
   Greenberg R, 2001, CURR ORNITHOL, V16, P119
   Greenberg R., 1990, STUD AVIAN BIOL, V13, P431
   Hare B, 2012, ANIM BEHAV, V83, P573, DOI 10.1016/j.anbehav.2011.12.007
   HAUDE RH, 1976, ANIM LEARN BEHAV, V4, P163, DOI 10.3758/BF03214028
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Heilbronner SR, 2008, BIOL LETTERS, V4, P246, DOI 10.1098/rsbl.2008.0081
   Herrmann E, 2011, DEVELOPMENTAL SCI, V14, P1393, DOI 10.1111/j.1467-7687.2011.01082.x
   Hohmann G., 2006, FEEDING ECOLOGY APES
   Kablan YA, 2019, ORYX, V53, P469, DOI 10.1017/S0030605317001272
   King AJ, 2009, CURR BIOL, V19, pR911, DOI 10.1016/j.cub.2009.07.027
   Kuhl HS, 2016, SCI REP-UK, V6, DOI 10.1038/srep22219
   Kurvers RHJM, 2010, P ROY SOC B-BIOL SCI, V277, P601, DOI 10.1098/rspb.2009.1474
   Kurvers RHJM, 2009, ANIM BEHAV, V78, P447, DOI 10.1016/j.anbehav.2009.06.002
   Laird, 2004, ANAL LONGITUDINAL CL
   Lefebvre L, 2004, BRAIN BEHAV EVOLUT, V63, P233, DOI 10.1159/000076784
   Levin LE, 1996, BEHAV PROCESS, V37, P1, DOI 10.1016/0376-6357(95)00067-4
   Lucas RE, 2011, J PERS SOC PSYCHOL, V101, P847, DOI 10.1037/a0024298
   Biondi LM, 2010, ANIM COGN, V13, P701, DOI 10.1007/s10071-010-0319-8
   Massen JJM, 2013, AM J PRIMATOL, V75, P947, DOI 10.1002/ajp.22159
   McCarthy MS, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22904
   Moretti L, 2015, ANIM BEHAV, V107, P159, DOI 10.1016/j.anbehav.2015.06.008
   PULLIAM HR, 1973, J THEOR BIOL, V38, P419, DOI 10.1016/0022-5193(73)90184-7
   R Development Core Team, 2017, R LANG ENV STAT COMP
   Reale D, 2007, BIOL REV, V82, P291, DOI 10.1111/j.1469-185X.2007.00010.x
   Rosati AG, 2012, ANIM BEHAV, V84, P869, DOI 10.1016/j.anbehav.2012.07.010
   Schielzeth H, 2010, METHODS ECOL EVOL, V1, P103, DOI 10.1111/j.2041-210X.2010.00012.x
   Schielzeth H, 2009, BEHAV ECOL, V20, P416, DOI 10.1093/beheco/arn145
   Sih A, 2004, TRENDS ECOL EVOL, V19, P372, DOI 10.1016/j.tree.2004.04.009
   Surbeck M, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.161081
   Tagg N, 2018, AM J PHYS ANTHROPOL, V166, P510, DOI 10.1002/ajpa.23478
   Tokuyama N, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2277-5
   Visalberghi E, 2003, INT J PRIMATOL, V24, P653, DOI 10.1023/A:1023700800113
   WILSON DS, 1994, TRENDS ECOL EVOL, V9, P442, DOI 10.1016/0169-5347(94)90134-1
   Wright TF, 2010, ETHOL ECOL EVOL, V22, P393, DOI 10.1080/03949370.2010.505580
NR 54
TC 16
Z9 16
U1 1
U2 37
PU CELL PRESS
PI CAMBRIDGE
PA 50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA
SN 0960-9822
EI 1879-0445
J9 CURR BIOL
JI Curr. Biol.
PD APR 1
PY 2019
VL 29
IS 7
BP 1211
EP +
DI 10.1016/j.cub.2019.02.024
PG 10
WC Biochemistry & Molecular Biology; Biology; Cell Biology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Biochemistry & Molecular Biology; Life Sciences & Biomedicine - Other
   Topics; Cell Biology
GA HR1YD
UT WOS:000462931400030
PM 30880013
OA Green Accepted, Green Submitted, Bronze
DA 2022-02-10
ER

PT J
AU Murphy, SM
   Wilckens, DT
   Augustine, BC
   Peyton, MA
   Harper, GC
AF Murphy, Sean M.
   Wilckens, David T.
   Augustine, Ben C.
   Peyton, Mark A.
   Harper, Glenn C.
TI Improving estimation of puma (Puma concolor) population density:
   clustered camera-trapping, telemetry data, and generalized spatial
   mark-resight models
SO SCIENTIFIC REPORTS
LA English
DT Article
ID CAPTURE-RECAPTURE; MOUNTAIN LIONS; HAIR SNARES; COUGAR; EFFICACY;
   DESIGNS; TRAPS; RECOMMENDATIONS; INFERENCE; ABUNDANCE
AB Obtaining reliable population density estimates for pumas (Puma concolor) and other cryptic, wideranging large carnivores is challenging. Recent advancements in spatially explicit capture-recapture models have facilitated development of novel survey approaches, such as clustered sampling designs, which can provide reliable density estimation for expansive areas with reduced effort. We applied clustered sampling to camera-traps to detect marked (collared) and unmarked pumas, and used generalized spatial mark-resight (SMR) models to estimate puma population density across 15,314 km(2 )in the southwestern USA. Generalized SMR models outperformed conventional SMR models. Integrating telemetry data from collars on marked pumas with detection data from camera-traps substantially improved density estimates by informing cryptic activity (home range) center transiency and improving estimation of the SMR home range parameter. Modeling sex of unmarked pumas as a partially identifying categorical covariate further improved estimates. Our density estimates (0.84-1.65 puma/100 km(2)) were generally more precise (CV= 0.24-0.31) than spatially explicit estimates produced from other puma sampling methods, including biopsy darting, scat detection dogs, and regular camera-trapping. This study provides an illustrative example of the effectiveness and flexibility of our combined sampling and analytical approach for reliably estimating density of pumas and other wildlife across geographically expansive areas.
C1 [Murphy, Sean M.; Wilckens, David T.] New Mexico Dept Game & Fish, Wildlife Management Div, Santa Fe, NM 87507 USA.
   [Augustine, Ben C.] Cornell Univ, Dept Nat Resources, Atkinson Ctr Sustainable Future, Fernow Hall, Ithaca, NY 14853 USA.
   [Peyton, Mark A.] Natl Pk Serv, Valles Caldera Natl Preserve, Jemez Springs, NM 87025 USA.
   [Harper, Glenn C.] Dept Nat Resources Pueblo Santa Ana, Santa Ana Pueblo, NM 87004 USA.
   [Murphy, Sean M.] Univ Kentucky, Dept Forestry & Nat Resources, Lexington, KY 40546 USA.
RP Murphy, SM (corresponding author), New Mexico Dept Game & Fish, Wildlife Management Div, Santa Fe, NM 87507 USA.; Murphy, SM (corresponding author), Univ Kentucky, Dept Forestry & Nat Resources, Lexington, KY 40546 USA.
EM smmurp2@uky.edu
RI Murphy, Sean/AAM-4692-2021
OI Murphy, Sean/0000-0002-9404-8878
FU New Mexico Department of Game Fish; U.S. Fish and Wildlife Service via
   the Federal Aid in Wildlife Restoration Program [W-93-R]; U.S. Fish and
   Wildlife Service Tribal Wildlife Grant; U.S. National Park Service; U.S.
   Forest ServiceUnited States Department of Agriculture (USDA)United
   States Forest Service; Atkinson Center for a Sustainable Future
FX Data collection was funded by New Mexico Department of Game & Fish and
   U.S. Fish and Wildlife Service via the Federal Aid in Wildlife
   Restoration Program (Grant #W-93-R). Supplemental funds were provided by
   a U.S. Fish and Wildlife Service Tribal Wildlife Grant, U.S. National
   Park Service, and U.S. Forest Service. B.C.A. was supported by a
   fellowship from the Atkinson Center for a Sustainable Future. A. Anaya,
   S. Bard, K. Blue-Sky, E. Duvuvuei, T. Frybarger, D. Ginter, M. Martinez,
   R. Passering, B. Van Der Werff, and F. Winslow assisted with aspects of
   data collection, and J. Cain III provided a portion of telemetry data.
   S. Bard, J. Cain III, T. Chaudhry, K. Philbrook, and M. Wrigley provided
   constructive comments on early drafts of this article. Pueblo of Santa
   Ana, Pueblo de Cochiti, Valles Caldera National Preserve, Santa Fe
   National Forest, E. Goldstein, K. Mower, R. Parmenter, and R. Rowe
   provided logistical support. The conclusions and any opinions expressed
   herein are those of the authors and do not necessarily reflect the views
   of New Mexico Department of Game & Fish, U.S. National Park Service, or
   Pueblo of Santa Ana. Any use of trade, firm, or product names is for
   descriptive purposes only and does not imply endorsement by the U.S.
   Government or State of New Mexico.
CR Alexander PD, 2018, WILDLIFE RES, V45, P274, DOI 10.1071/WR17044
   Apker J, 2017, P 12 MOUNT LION WORK, P37
   Augustine B. C, 2018, SPIM SPATIAL PARTIAL
   Augustine B. C., 2018, BIORXIV, DOI [10.1101/299982, DOI 10.1101/299982]
   Augustine B, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2627
   Beausoleil R. A, 2017, P 12 MOUNT LION WORK, P35
   Beausoleil R. A, 2008, P 9 MOUNT LION WORKS, P205
   Beausoleil RA, 2016, WILDLIFE SOC B, V40, P583, DOI 10.1002/wsb.675
   Beausoleil RA, 2013, WILDLIFE SOC B, V37, P680, DOI 10.1002/wsb.299
   Benson JF, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.0957
   Braczkowski AR, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151033
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cagnacci F, 2010, PHILOS T R SOC B, V365, P2157, DOI 10.1098/rstb.2010.0107
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Choate DM, 2006, WILDLIFE SOC B, V34, P782, DOI 10.2193/0091-7648(2006)34[782:EOCPEI]2.0.CO;2
   Clark JD, 2019, POPUL ECOL, V61, P93, DOI 10.1002/1438-390X.1011
   Darimont CT, 2018, CONSERV BIOL, V32, P747, DOI 10.1111/cobi.13065
   Davidson GA, 2014, J WILDLIFE MANAGE, V78, P1104, DOI 10.1002/jwmg.758
   Dickson BG, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0081898
   Efford MG, 2014, ECOLOGY, V95, P1341, DOI 10.1890/13-1497.1
   Efford MG, 2018, BIOMETRICS, V74, P411, DOI 10.1111/biom.12766
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Fecske D. M, 2011, MANAGING COUGARS N A, P6
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Frakes RA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133044
   Gonzalez--Borrajo N, 2017, MAMMAL REV, V47, P62, DOI 10.1111/mam.12081
   Hebblewhite M, 2010, PHILOS T R SOC B, V365, P2303, DOI 10.1098/rstb.2010.0087
   Homer C, 2015, PHOTOGRAMM ENG REM S, V81, P345, DOI 10.14358/PERS.81.5.345
   Humm JM, 2017, J WILDLIFE MANAGE, V81, P1187, DOI 10.1002/jwmg.21294
   Jedrzejewski W, 2017, MAMMAL RES, V62, P9, DOI 10.1007/s13364-016-0300-2
   Jimenez J, 2017, HYSTRIX, V28, P208, DOI 10.4404/hystrix-28.2-12141
   Johnson WE, 2010, SCIENCE, V329, P1641, DOI 10.1126/science.1192891
   Kane MD, 2015, BIODIVERS CONSERV, V24, P3527, DOI 10.1007/s10531-015-1012-7
   Kreeger T. J., 2012, HDB WILDLIFE CHEM IM, V4th
   Larue MA, 2012, J WILDLIFE MANAGE, V76, P1364, DOI 10.1002/jwmg.396
   Laundre JW, 2000, WILDLIFE SOC B, V28, P963
   Logan K. A, 1996, W128R U ID HORN WILD
   Logan KA, 1999, WILDLIFE SOC B, V27, P201
   Long ES, 2003, WEST N AM NATURALIST, V63, P529
   Long RA, 2007, J WILDLIFE MANAGE, V71, P2018, DOI 10.2193/2006-292
   Maletzke BT, 2014, ECOL EVOL, V4, P2178, DOI 10.1002/ece3.1089
   McBride R, 2015, SOUTHEAST NAT, V14, P351, DOI 10.1656/058.014.0215
   McBride R, 2010, SOUTHEAST NAT, V9, P629, DOI 10.1656/058.009.0319
   McClintock BT, 2015, J APPL ECOL, V52, P893, DOI 10.1111/1365-2664.12438
   Morehouse AT, 2017, ECOL SOC, V22, DOI [10.5751/ES-09415-220304, 10.5751/es-09415-220304]
   Murphy MA, 2007, CONSERV GENET, V8, P1219, DOI 10.1007/s10592-006-9264-0
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   Pease BS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0166689
   Portella TP, 2013, ZOOLOGIA-CURITIBA, V30, P49, DOI 10.1590/S1984-46702013000100006
   Proffitt KM, 2015, ECOSPHERE, V6, DOI 10.1890/ES15-00001.1
   Puckett EE, 2017, CONSERV GENET RESOUR, V9, P289, DOI 10.1007/s12686-016-0643-7
   Quiroga VA, 2016, J NAT CONSERV, V31, P9, DOI 10.1016/j.jnc.2016.02.004
   R Core Team, 2018, STATS PACK LANG ENV
   Rich LN, 2014, J MAMMAL, V95, P382, DOI 10.1644/13-MAMM-A-126
   Ripple WJ, 2008, BIOL CONSERV, V141, P1249, DOI 10.1016/j.biocon.2008.02.028
   Ripple WJ, 2006, BIOL CONSERV, V133, P397, DOI 10.1016/j.biocon.2006.07.002
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Rominger EM, 2018, J WILDLIFE MANAGE, V82, P19, DOI 10.1002/jwmg.21396
   Rowe C. B, 2017, P 12 MOUNT LION WORK, P32
   Royle JA, 2016, POPUL ECOL, V58, P53, DOI 10.1007/s10144-015-0524-z
   Royle JA, 2009, ECOLOGY, V90, P3233, DOI 10.1890/08-1481.1
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Russell RE, 2012, J WILDLIFE MANAGE, V76, P1551, DOI 10.1002/jwmg.412
   Schaub M, 2014, METHODS ECOL EVOL, V5, P1316, DOI 10.1111/2041-210X.12134
   Sikes RS, 2016, J MAMMAL, V97, P663, DOI 10.1093/jmammal/gyw078
   Sollmann R, 2013, J APPL ECOL, V50, P961, DOI 10.1111/1365-2664.12098
   Sollmann R, 2013, BIOL CONSERV, V159, P405, DOI 10.1016/j.biocon.2012.12.025
   Sollmann R, 2013, ECOLOGY, V94, P553, DOI 10.1890/12-1256.1
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Sunquist M., 2002, WILD CATS WORLD, DOI 10.1644/1545-1542(2004)0852.0.co;2
   Thompson DJ, 2010, ECOSPHERE, V1, DOI 10.1890/ES10-00028.1
   Tobler MW, 2013, BIOL CONSERV, V159, P109, DOI 10.1016/j.biocon.2012.12.009
   Western Regional Climate Center, 2018, COOP CLIM DAT SUMM M
   Whittaker D, 2011, MANAGING COUGARS N A, P74
   Whittington J, 2018, J APPL ECOL, V55, P157, DOI 10.1111/1365-2664.12954
   Wilckens DT, 2016, J MAMMAL, V97, P373, DOI 10.1093/jmammal/gyv183
   Wilton CM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111257
   Zanon-Martinez JI, 2016, WILDLIFE RES, V43, P449, DOI 10.1071/WR16056
NR 79
TC 10
Z9 10
U1 3
U2 23
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD MAR 14
PY 2019
VL 9
AR 4590
DI 10.1038/s41598-019-40926-7
PG 13
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA HO7WB
UT WOS:000461159600067
PM 30872785
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Chen, P
   Swarup, P
   Matkowski, WM
   Kong, AWK
   Han, S
   Zhang, ZH
   Rong, H
AF Chen, Peng
   Swarup, Pranjal
   Matkowski, Wojciech Michal
   Kong, Adams Wai Kin
   Han, Su
   Zhang, Zhihe
   Rong, Hou
TI A study on giant panda recognition based on images of a large proportion
   of captive pandas
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE giant panda; individual identification; panda face recognition;
   population estimation
ID CAMERA TRAP; DESIGN
AB As a highly endangered species, the giant panda (panda) has attracted significant attention in the past decades. Considerable efforts have been put on panda conservation and reproduction, offering the promising outcome of maintaining the population size of pandas. To evaluate the effectiveness of conservation and management strategies, recognizing individual pandas is critical. However, it remains a challenging task because the existing methods, such as traditional tracking method, discrimination method based on footprint identification, and molecular biology method, are invasive, inaccurate, expensive, or challenging to perform. The advances of imaging technologies have led to the wide applications of digital images and videos in panda conservation and management, which makes it possible for individual panda recognition in a noninvasive manner by using image-based panda face recognition method.
   In recent years, deep learning has achieved great success in the field of computer vision and pattern recognition. For panda face recognition, a fully automatic deep learning algorithm which consists of a sequence of deep neural networks (DNNs) used for panda face detection, segmentation, alignment, and identity prediction is developed in this study. To develop and evaluate the algorithm, the largest panda image dataset containing 6,441 images from 218 different pandas, which is 39.78% of captive pandas in the world, is established.
   The algorithm achieved 96.27% accuracy in panda recognition and 100% accuracy in detection.
   This study shows that panda faces can be used for panda recognition. It enables the use of the cameras installed in their habitat for monitoring their population and behavior. This noninvasive approach is much more cost-effective than the approaches used in the previous panda surveys.
C1 [Chen, Peng; Zhang, Zhihe; Rong, Hou] Chengdu Res Base Giant Panda Breeding, Chengdu, Peoples R China.
   [Chen, Peng; Zhang, Zhihe; Rong, Hou] Sichuan Key Lab Conservat Biol Endangered Wildlif, Chengdu, Sichuan, Peoples R China.
   [Chen, Peng; Zhang, Zhihe; Rong, Hou] Sichuan Acad Giant Panda, Chengdu, Peoples R China.
   [Swarup, Pranjal; Matkowski, Wojciech Michal; Kong, Adams Wai Kin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Han, Su] Sichuan Normal Univ, Coll Comp Sci, Chengdu, Peoples R China.
RP Zhang, ZH (corresponding author), Chengdu Res Base Giant Panda Breeding, Chengdu, Peoples R China.; Swarup, P (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM pswarup@ntu.edu.sg; zzh@panda.org.cn
RI Matkowski, Wojciech/AAX-5437-2020
OI Swarup, Pranjal/0000-0002-1766-7413
CR [Anonymous], 2018, PEOPLES DAILY ONLINE
   Burghardt T., 2007, INT C COMP VIS SYST
   Cheema GS, 2017, LECT NOTES ARTIF INT, V10536, P27, DOI 10.1007/978-3-319-71273-4_3
   Chen J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P911
   Chen J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P814
   Deb D., 2018, 2018 IEEE 9 INT C BI, P1, DOI DOI 10.1109/BTAS.2018.8698538
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guan TP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0159738
   Hansen ME, 2018, COMPUT IND, V98, P145, DOI 10.1016/j.compind.2018.02.016
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343
   Juan Chen, 2012, 2012 International Conference on Computational Problem-Solving (ICCP), P358, DOI 10.1109/ICCPS.2012.6384309
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Kumar S, 2017, IET BIOMETRICS, V6, P139, DOI 10.1049/iet-bmt.2016.0017
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Matkowski WM, 2019, IEEE IMAGE PROC, P1680, DOI 10.1109/ICIP.2019.8803125
   McNeely JA, 1990, CONSERVING WORLDS BI
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Miller CR, 2005, MOL ECOL, V14, P1991, DOI 10.1111/j.1365-294X.2005.02577.x
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Pollard KA, 2010, J APPL ECOL, V47, P1103, DOI 10.1111/j.1365-2664.2010.01851.x
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Smallwood KS, 1998, OECOLOGIA, V113, P474, DOI 10.1007/s004420050400
   Solberg KH, 2006, BIOL CONSERV, V128, P158, DOI 10.1016/j.biocon.2005.09.025
   State Forestry Administration, 2015, GIANT PAND SICH 4 GI
   State Forestry Administration, 2006, 3 NAT SURV REP GIANT
   State Forestry Administration, 2015, REL 4 NAT SURV REP G
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Zhan XJ, 2006, CURR BIOL, V16, pR451, DOI 10.1016/j.cub.2006.05.042
   Zhan XJ, 2009, URSUS, V20, P56
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
   Zheng X, 2016, J ZOOL, V300, P247, DOI 10.1111/jzo.12377
NR 43
TC 8
Z9 8
U1 11
U2 27
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD APR
PY 2020
VL 10
IS 7
BP 3561
EP 3573
DI 10.1002/ece3.6152
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA LB1TE
UT WOS:000524417200033
PM 32274009
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Burns, PA
   Parrott, ML
   Rowe, KC
   Phillips, BL
AF Burns, Phoebe A.
   Parrott, Marissa L.
   Rowe, Kevin C.
   Phillips, Benjamin L.
TI Identification of threatened rodent species using infrared and
   white-flash camera traps
SO AUSTRALIAN MAMMALOGY
LA English
DT Article
DE Muridae; Pseudomys fumeus; P. novaehollandiae; sensitivity; small
   mammals; specificity
ID ANURAN CALL SURVEYS; MANAGEMENT; AUSTRALIA; IMPACTS; MAMMALS; ERROR;
   RATES; BIAS
AB Camera trapping has evolved into an efficient technique for gathering presence/absence data for many species; however, smaller mammals such as rodents are often difficult to identify in images. Identification is inhibited by co-occurrence with similar-sized small mammal species and by camera set-ups that do not provide adequate image quality. Here we describe survey procedures for identification of two small, threatened rodent species - smoky mouse (Pseudomys fumeus) and New Holland mouse (P. novaehollandiae) - using white-flash and infrared camera traps. We tested whether observers could accurately identify each species and whether experience with small mammals influenced accuracy. Pseudomys fumeus was 20 times less likely to be misidentified on white-flash images than infrared, and observer experience affected accuracy only for infrared images, where it accounted for all observer variance. Misidentifications of P. novaehollandiae were more common across both flash types: false positives (>0.21) were more common than false negatives (<0.09), and experience accounted for only 31% of variance in observer accuracy. For this species, accurate identification appears to be, in part, an innate skill. Nonetheless, using an appropriate setup, camera trapping clearly has potential to provide broad-scale occurrence data for these and other small mammal species.
C1 [Burns, Phoebe A.; Phillips, Benjamin L.] Univ Melbourne, Sch BioSci, Parkville, Vic 3010, Australia.
   [Burns, Phoebe A.; Rowe, Kevin C.] Museum Victoria, Sci Dept, GPO Box 666, Melbourne, Vic 3001, Australia.
   [Parrott, Marissa L.] Zoos Victoria, Wildlife & Conservat Sci, Elliott Ave, Parkville, Vic 3010, Australia.
RP Burns, PA (corresponding author), Univ Melbourne, Sch BioSci, Parkville, Vic 3010, Australia.; Burns, PA (corresponding author), Museum Victoria, Sci Dept, GPO Box 666, Melbourne, Vic 3001, Australia.
EM pburns@museum.vic.gov.au
RI Burns, Phoebe A./U-6746-2019
OI Burns, Phoebe A./0000-0003-1015-3775
FU Australian Postgraduate AwardAustralian Government; Zoos Victoria; Parks
   Victoria; Holsworth Wildlife Research Endowment; Royal Zoological
   Society of NSW Ethel Mary Read Research Fund; Linnaean Society of NSW
   Joyce W. Vickery Scientific Research Fund; Field Naturalists Victorian
   Environment Fund
FX Surveys were conducted under DELWP Research Permits nos 10007493,
   10007606, 10007934 and approval from the Zoos Victoria and Museum
   Victoria Animal Ethics Committees. This research was conducted with
   support awarded to PAB from an Australian Postgraduate Award, Zoos
   Victoria, Parks Victoria, the Holsworth Wildlife Research Endowment, the
   Royal Zoological Society of NSW Ethel Mary Read Research Fund, the
   Linnaean Society of NSW Joyce W. Vickery Scientific Research Fund, and
   the Field Naturalists Victorian Environment Fund. Thank you to Zoos
   Victoria, Wildlife Unlimited Pty Ltd, and the Department of Environment,
   Land, Water and Planning (Victoria) for lending cameras and bait
   stations. Thank you to the observers who participated in the trials and
   to two anonymous reviewers for helpful comments and suggestions on the
   manuscript.
CR Ballard G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P189
   Burns PA, 2015, WILDLIFE RES, V42, P668, DOI 10.1071/WR15096
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chades I, 2008, P NATL ACAD SCI USA, V105, P13936, DOI 10.1073/pnas.0805265105
   Clemann Nick, 2014, Memoirs of Museum Victoria, V72, P141
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Diefenbach DR, 2003, AUK, V120, P1168, DOI 10.1642/0004-8038(2003)120[1168:VIGBCR]2.0.CO;2
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Fancourt BA, 2018, AUST MAMMAL, V40, P118, DOI 10.1071/AM17004
   Farmer RG, 2012, AUK, V129, P76, DOI 10.1525/auk.2012.11129
   Field SA, 2004, ECOL LETT, V7, P669, DOI 10.1111/j.1461-0248.2004.00625.x
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Gu WD, 2004, BIOL CONSERV, V116, P195, DOI 10.1016/S0006-3207(03)00190-3
   Harley DKP, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P233
   Hollis G. J., 1999, PRE POSTFIRE TRAPPIN
   Holmes B, 2012, DETECTION DISTRIBUTI
   Lock M, 2017, AUST J ZOOL, V65, P60, DOI 10.1071/ZO16084
   Lotz A, 2007, J WILDLIFE MANAGE, V71, P675, DOI 10.2193/2005-759
   Manzo E, 2012, ACTA THERIOL, V57, P165, DOI 10.1007/s13364-011-0055-8
   McCall C., 2015, ASSESSMENT STATUS NE
   Mcclintock BT, 2010, J WILDLIFE MANAGE, V74, P1882, DOI 10.2193/2009-321
   McDonald PJ, 2015, BIOL CONSERV, V191, P93, DOI 10.1016/j.biocon.2015.06.027
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Meek PD, 2016, AUST MAMMAL, V38, P44, DOI 10.1071/AM15016
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Meek PD, 2015, AUST MAMMAL, V37, P1, DOI 10.1071/AM14021
   Miller DAW, 2012, ECOL APPL, V22, P1665
   Nelson J., 2009, STATUS SMOKY MOUSE P
   Nelson J. L., 2010, STATUS SMOKY MOUSE P
   Oliveira-Santos LGR, 2010, MAMM BIOL, V75, P375, DOI 10.1016/j.mambio.2009.08.005
   Paull DJ, 2012, WILDLIFE RES, V39, P546, DOI 10.1071/WR12034
   Quin Bruce R., 1996, Victorian Naturalist (Blackburn), V113, P236
   Quin Bruce R., 1996, Victorian Naturalist (Blackburn), V113, P281
   R Core Team, 2017, R LANG ENV STAT COMP
   Roberts Nathan James, 2011, Bioscience Horizons, V4, P40, DOI 10.1093/biohorizons/hzr006
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Russell R, 2009, PSYCHON B REV, V16, P252, DOI 10.3758/PBR.16.2.252
   Swan M, 2014, BIODIVERS CONSERV, V23, P343, DOI 10.1007/s10531-013-0604-3
   TAYLOR BL, 1993, CONSERV BIOL, V7, P489, DOI 10.1046/j.1523-1739.1993.07030489.x
   Taylor BD, 2014, AUST MAMMAL, V36, P60, DOI 10.1071/AM13012
   Tyre AJ, 2003, ECOL APPL, V13, P1790, DOI 10.1890/02-5078
   Villette P, 2016, J MAMMAL, V97, P32, DOI 10.1093/jmammal/gyv150
NR 43
TC 11
Z9 12
U1 1
U2 22
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 0310-0049
EI 1836-7402
J9 AUST MAMMAL
JI Aust. Mammal.
PY 2018
VL 40
IS 2
BP 188
EP 197
DI 10.1071/AM17016
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA GO1FC
UT WOS:000439693400008
DA 2022-02-10
ER

PT J
AU Crunchant, AS
   Egerer, M
   Loos, A
   Burghardt, T
   Zuberbuhler, K
   Corogenes, K
   Leinert, V
   Kulik, L
   Kuhl, HS
AF Crunchant, Anne-Sophie
   Egerer, Monika
   Loos, Alexander
   Burghardt, Tilo
   Zuberbuhler, Klaus
   Corogenes, Katherine
   Leinert, Vera
   Kulik, Lars
   Kuehl, Hjalmar S.
TI Automated face detection for occurrence and occupancy estimation in
   chimpanzees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE animal biometrics; apes; automated image recognition; camera placement;
   site use
ID ESTIMATING SITE OCCUPANCY; DENSITY-ESTIMATION; BUDONGO FOREST; GREAT
   APES; DECLINE; CONSERVATION; MODELS; DUNG
AB Surveying endangered species is necessary to evaluate conservation effectiveness. Camera trapping and biometric computer vision are recent technological advances. They have impacted on the methods applicable to field surveys and these methods have gained significant momentum over the last decade. Yet, most researchers inspect footage manually and few studies have used automated semantic processing of video trap data from the field. The particular aim of this study is to evaluate methods that incorporate automated face detection technology as an aid to estimate site use of two chimpanzee communities based on camera trapping. As a comparative baseline we employ traditional manual inspection of footage. Our analysis focuses specifically on the basic parameter of occurrence where we assess the performance and practical value of chimpanzee face detection software. We found that the semi-automated data processing required only 2-4% of the time compared to the purely manual analysis. This is a non-negligible increase in efficiency that is critical when assessing the feasibility of camera trap occupancy surveys. Our evaluations suggest that our methodology estimates the proportion of sites used relatively reliably. Chimpanzees are mostly detected when they are present and when videos are filmed in high-resolution: the highest recall rate was 77%, for a false alarm rate of 2.8% for videos containing only chimpanzee frontal face views. Certainly, our study is only a first step for transferring face detection software from the lab into field application. Our results are promising and indicate that the current limitation of detecting chimpanzees in camera trap footage due to lack of suitable face views can be easily overcome on the level of field data collection, that is, by the combined placement of multiple high-resolution cameras facing reverse directions. This will enable to routinely conduct chimpanzee occupancy surveys based on camera trapping and semi-automated processing of footage.
C1 [Crunchant, Anne-Sophie; Egerer, Monika; Corogenes, Katherine; Leinert, Vera; Kulik, Lars; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Loos, Alexander] Fraunhofer Inst Digital Media Technol IDMT, Ilmenau, Germany.
   [Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
   [Zuberbuhler, Klaus] Univ Neuchatel, Dept Comparat Cognit, Neuchatel, Switzerland.
   [Zuberbuhler, Klaus] Univ St Andrews, Sch Psychol & Neurosci, St Andrews, Scotland.
   [Zuberbuhler, Klaus] Budongo Conservat Field Stn, Masindi, Uganda.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res IDiv Halle Leip, Leipzig, Germany.
RP Crunchant, AS (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM as.crunchant@gmail.com
RI Zuberbühler, Klaus/A-9053-2011; Egerer, Monika/AAV-6902-2021
OI Zuberbühler, Klaus/0000-0001-8378-088X; Egerer,
   Monika/0000-0002-3304-0725
FU Max Planck Society Innovation Fund; Krekeler Foundation; Robert Bosch
   Foundation; Pact for Research and Innovation
FX Max Planck Society Innovation Fund; Krekeler Foundation; Robert Bosch
   Foundation; Pact for Research and Innovation
CR Andresen L, 2014, J ZOOL, V292, P212, DOI 10.1111/jzo.12098
   Araabi BN, 2000, ANN BIOMED ENG, V28, P1269, DOI 10.1114/1.1317532
   Arandjelovic M, 2010, BIOL CONSERV, V143, P1780, DOI 10.1016/j.biocon.2010.04.030
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Bengsen AJ, 2011, J WILDLIFE MANAGE, V75, P1222, DOI 10.1002/jwmg.132
   Bermejo M, 2006, SCIENCE, V314, P1564, DOI 10.1126/science.1133105
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers D. L., 2002, ESTIMATING ANIMAL AB, P314
   Buckland S.T., 2001, pi
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Campbell G, 2008, CURR BIOL, V18, pR903, DOI 10.1016/j.cub.2008.08.015
   Carlsen F., 2012, W CHIMPANZEE POPULAT, P124
   Dunn A., 2014, REVISED REGIONAL ACT, P49
   EGGELING WJ, 1947, J ECOL, V34, P20, DOI 10.2307/2256760
   Ernst Andreas, 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P279, DOI 10.1109/AVSS.2011.6027337
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Funwi-Gabga N., 2014, STATE APES 2013 EXTR, P252
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
   Greengrass Elizabeth J., 2009, Primate Conservation, V24, P77
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Hughes B., 2015, 26 BRIT MACH VIS C B
   IUCN & ICCN, 2012, BON PAN PAN CONS STR, P65
   Junker J, 2012, DIVERS DISTRIB, V18, P1077, DOI 10.1111/ddi.12005
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Kondgen S, 2008, CURR BIOL, V18, P260, DOI 10.1016/j.cub.2008.01.012
   Kormos R., 2003, REGIONAL ACTION PLAN, P24
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kublbeck C, 2006, IMAGE VISION COMPUT, V24, P564, DOI 10.1016/j.imavis.2005.08.005
   Kuehl H. S., 2008, OCCASIONAL PAPERS IU, P32
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kuehl HS, 2009, BIOL CONSERV, V142, P1500, DOI 10.1016/j.biocon.2009.02.032
   Lahiri M., 2011, P 1 ACM INT C MULT R
   Leendertz FH, 2006, BIOL CONSERV, V131, P325, DOI 10.1016/j.biocon.2006.05.002
   Leendertz FH, 2004, NATURE, V430, P451, DOI 10.1038/nature02722
   Loos A., 2016, THESIS, P279
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Loos A, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P116, DOI 10.1109/ISM.2012.30
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   MacKenzie DI., 2006, OCCUPANCY ESTIMATION, P324
   Macmillan N. A., 2004, DETECTION THEORY USE, P512
   Maldonado O., 2012, GRAUERS GORILLAS AND, P66
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Morgan B, 2011, REGIONAL ACTION PLAN
   Murai M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075024
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   O'Connell A. F., 2012, ANIMAL CONSERVATION, V15, P527
   O'Connell A. F., 2010, CAMERA TRAPS ANIMAL, P271
   Oates J., 2007, Regional action plan for the conservation of the Cross River Gorilla (Gorilla gorilla diehli)
   Oates JF, 1996, AUST J ECOL, V21, P1, DOI 10.1111/j.1442-9993.1996.tb00580.x
   Plumptre A. J., 2010, E CHIMPANZEE PAN TRO, P52
   Plumptre AJ, 2006, PRIMATES, V47, P65, DOI 10.1007/s10329-005-0146-8
   Plumptre AJ, 1996, FOREST ECOL MANAG, V89, P101, DOI 10.1016/S0378-1127(96)03854-6
   Plumptre AJ, 1996, INT J PRIMATOL, V17, P85, DOI 10.1007/BF02696160
   Robinson P.T., 1981, Zoonooz, V54, P7
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Sherley Richard B., 2010, Endangered Species Research, V11, P101, DOI 10.3354/esr00267
   Todd AF, 2008, INT J PRIMATOL, V29, P549, DOI 10.1007/s10764-008-9247-8
   Tuyttens FAM, 2014, ANIM BEHAV, V90, P273, DOI 10.1016/j.anbehav.2014.02.007
   Tweh CG, 2015, ORYX, V49, P710, DOI 10.1017/S0030605313001191
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Walsh PD, 2003, NATURE, V422, P611, DOI 10.1038/nature01566
   Welch G., 2006, TECH REP
   Wich SA, 2008, ORYX, V42, P329, DOI 10.1017/S003060530800197X
   Wich SA, 2014, CURR BIOL, V24, P1659, DOI 10.1016/j.cub.2014.05.077
   Williamson E., 2011, REGIONAL ACTION PLAN, P49
   Williamson E. A., 2014, REVISED REGIONAL ACT, P49
   Woodford MH, 2002, ORYX, V36, P153, DOI 10.1017/S0030605302000224
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151
NR 74
TC 11
Z9 12
U1 1
U2 25
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD MAR
PY 2017
VL 79
IS 3
AR e22627
DI 10.1002/ajp.22627
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA EL5HG
UT WOS:000394651600013
PM 28095593
OA Green Submitted, Green Accepted
DA 2022-02-10
ER

PT C
AU Koh, PW
   Sagawa, S
   Marklund, H
   Xie, SM
   Zhang, M
   Balsubramani, A
   Hu, WH
   Yasunaga, M
   Phillips, RL
   Gao, I
   Lee, T
   David, E
   Stavness, I
   Guo, W
   Earnshaw, BA
   Haque, IS
   Beery, S
   Leskovec, J
   Kundaje, A
   Pierson, E
   Levine, S
   Finn, C
   Liang, P
AF Koh, Pang Wei
   Sagawa, Shiori
   Marklund, Henrik
   Xie, Sang Michael
   Zhang, Marvin
   Balsubramani, Akshay
   Hu, Weihua
   Yasunaga, Michihiro
   Phillips, Richard Lanas
   Gao, Irena
   Lee, Tony
   David, Etienne
   Stavness, Ian
   Guo, Wei
   Earnshaw, Berton A.
   Haque, Imran S.
   Beery, Sara
   Leskovec, Jure
   Kundaje, Anshul
   Pierson, Emma
   Levine, Sergey
   Finn, Chelsea
   Liang, Percy
BE Meila, M
   Zhang, T
TI WILDS: A Benchmark of in-the-Wild Distribution Shifts
SO INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 139
SE Proceedings of Machine Learning Research
LA English
DT Proceedings Paper
CT International Conference on Machine Learning (ICML)
CY JUL 18-24, 2021
CL ELECTR NETWORK
ID RACIAL DISPARITIES; CLASSIFICATION; ART; IMAGERY; CANCER; HEALTH; DRIFT
AB Distribution shifts-where the training distribution differs from the test distribution-can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an opensource package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.
C1 [Koh, Pang Wei; Sagawa, Shiori; Marklund, Henrik; Xie, Sang Michael; Balsubramani, Akshay; Hu, Weihua; Yasunaga, Michihiro; Gao, Irena; Lee, Tony; Leskovec, Jure; Kundaje, Anshul; Finn, Chelsea; Liang, Percy] Stanford, Stanford, CA 94305 USA.
   [Zhang, Marvin; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA USA.
   [Phillips, Richard Lanas; Pierson, Emma] Cornell, Ithaca, NY USA.
   [David, Etienne] INRAE, Paris, France.
   [Stavness, Ian] USask, Saskatoon, SK, Canada.
   [Guo, Wei] UTokyo, Tokyo, Japan.
   [Earnshaw, Berton A.; Haque, Imran S.] Recursion, Cambridge, MA USA.
   [Beery, Sara] CALTECH, Pasadena, CA 91125 USA.
   [Pierson, Emma] Microsoft Res, Redmond, WA USA.
RP Koh, PW; Sagawa, S; Liang, P (corresponding author), Stanford, Stanford, CA 94305 USA.
EM pangwei@cs.stanford.edu; ssagawa@cs.stanford.edu; pliang@cs.stanford.edu
OI Kundaje, Anshul/0000-0003-3084-2287
FU Open Philanthropy Project Award; NSFNational Science Foundation (NSF)
   [OAC-1835598, OAC-1934578, CCF-1918940, IIS-2030477, 1805310]; Herbert
   Kunzel Stanford Graduate Fellowship; Dr. Tech. Marcus Wallenberg
   Foundation for Education in International Industrial Entrepreneurship;
   CIFARCanadian Institute for Advanced Research (CIFAR); GoogleGoogle
   Incorporated; NDSEG Graduate Fellowships; Funai Overseas Scholarship;
   Masason Foundation Fellowship; NSF Graduate Research FellowshipNational
   Science Foundation (NSF); DARPAUnited States Department of
   DefenseDefense Advanced Research Projects Agency (DARPA)
   [N660011924033]; ARO [W911NF-16-1-0342, W911NF-16-1-0171]; Stanford Data
   Science Initiative; Wu Tsai Neurosciences Institute; Chan Zuckerberg
   Biohub; Amazon; JPMorgan Chase; Docomo; Hitachi; JD.com; KDDIKDDI
   Corporation; NVIDIA; Dell; Toshiba; UnitedHealth Group
FX This project was funded by an Open Philanthropy Project Award and NSF
   Award Grant No. 1805310. Shiori Sagawa was supported by the Herbert
   Kunzel Stanford Graduate Fellowship. Henrik Marklund was supported by
   the Dr. Tech. Marcus Wallenberg Foundation for Education in
   International Industrial Entrepreneurship, CIFAR, and Google. Sang
   Michael Xie and Marvin Zhang were supported by NDSEG Graduate
   Fellowships. Weihua Hu was supported by the Funai Overseas Scholarship
   and the Masason Foundation Fellowship. Sara Beery was supported by an
   NSF Graduate Research Fellowship and is a PIMCO Fellow in Data Science.
   Jure Leskovec is a Chan Zuckerberg Biohub investigator. Chelsea Finn is
   a CIFAR Fellow in the Learning in Machines and Brains Program.; We also
   gratefully acknowledge the support of DARPA under Nos. N660011924033
   (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP);
   NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940
   (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu
   Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan
   Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, and
   UnitedHealth Group.
CR Abelson B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1563, DOI 10.1145/2623330.2623335
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Aguet F, 2020, SCIENCE, V369, P1318, DOI 10.1126/science.aaz1776
   Ahadi A., 2015, P 11 ANN INT C INT C, P121, DOI [10.1145/2787622.2787717, DOI 10.1145/2787622.2787717]
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Aich S, 2018, IEEE WINT CONF APPL, P323, DOI 10.1109/WACV.2018.00042
   AlBadawy EA, 2018, MED PHYS, V45, P1150, DOI 10.1002/mp.12752
   Alexandari AM, 2020, PR MACH LEARN RES, V119
   Allamanis M., 2017, ARXIV170507867
   Allamanis M, 2015, 2015 10TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE 2015) PROCEEDINGS, P38, DOI 10.1145/2786805.2786849
   AMORIM LA, 2018, ASS COMPUTATIONAL LI, P229, DOI DOI 10.1109/WSCAD.2018.00043
   Nguyen AT, 2015, 2015 IEEE/ACM 37TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, VOL 1, P858, DOI 10.1109/ICSE.2015.336
   [Anonymous], 2020, BBC
   [Anonymous], 2019, ARXIV191007113
   Ardila R, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P4218
   Arjovsky Martin, 2019, ARXIV190702893
   Attene-Ramos MS, 2013, DRUG DISCOV TODAY, V18, P716, DOI 10.1016/j.drudis.2013.05.015
   Avsec Z., 2019, BIORXIV, DOI [10.1101/737981, DOI 10.1101/737981]
   Ayalew Tewodros W., 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P330, DOI 10.1007/978-3-030-65414-6_23
   Azizzadenesheli Kamyar, 2019, INT C LEARN REPR
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Badgeley MA, 2019, NPJ DIGIT MED, V2, DOI 10.1038/s41746-019-0105-1
   Balaji Y., 2018, NEURIPS, P1006
   Bandi P, 2019, IEEE T MED IMAGING, V38, P550, DOI 10.1109/TMI.2018.2867350
   Barbu A, 2019, ADV NEUR IN, V32
   Bartlett PL, 2008, J MACH LEARN RES, V9, P1823
   Baumann T, 2019, LANG RESOUR EVAL, V53, P303, DOI 10.1007/s10579-017-9410-y
   Beck AH, 2011, SCI TRANSL MED, V3, DOI 10.1126/scitranslmed.3002564
   Becke AD, 2014, J CHEM PHYS, V140, DOI 10.1063/1.4869598
   Beede E, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376718
   Beery S., 2020, ARXIV200410340
   Beery S., 2019, ARXIV190706772
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Bejnordi BE, 2017, JAMA-J AM MED ASSOC, V318, P2199, DOI 10.1001/jama.2017.14585
   Bellamy D., 2020, ABS201001149
   Bellemare MG, 2020, NATURE, V588, P77, DOI 10.1038/s41586-020-2939-8
   Ben-David S., 2006, NIPS, V19, P137
   Bender E. M., 2018, T ASSOC COMPUT LING, V6, P587, DOI [10.1162/tacl_a_00041, DOI 10.1162/TACL_A_00041]
   BenTaieb A, 2018, IEEE T MED IMAGING, V37, P792, DOI 10.1109/TMI.2017.2781228
   Berman G., 2018, ETHICAL CONSIDERATIO
   Beyene AA, 2015, KNOWL INF SYST, V44, P177, DOI 10.1007/s10115-014-0756-9
   Blanchard G., 2011, ADV NEURAL INF PROCE, P1
   Blitzer J., 2007, P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Blodgett SL, 2016, P 2016 C EMP METH, P1119, DOI DOI 10.18653/V1/D16-1120
   Blodgett SL, 2017, P WORKSH FAIRN ACC T
   Blumenstock J, 2015, SCIENCE, V350, P1073, DOI 10.1126/science.aac4420
   Blundell, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387
   Board Editorial, 2016, NY TIMES
   Bohacek RS, 1996, MED RES REV, V16, P3, DOI 10.1002/(SICI)1098-1128(199601)16:1<3::AID-MED1>3.3.CO;2-D
   Borkan D., 2019, ARXIV190302088
   Borkan D, 2019, COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 ), P491, DOI 10.1145/3308560.3317593
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Boutros M, 2015, CELL, V163, P1314, DOI 10.1016/j.cell.2015.11.007
   Bray MA, 2016, NAT PROTOC, V11, P1757, DOI 10.1038/nprot.2016.105
   Broach JR, 1996, NATURE, V384, P14
   Broussard M., 2020, NY TIMES
   Bruch M, 2009, 7TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING, P213, DOI 10.1145/1595696.1595728
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Bug D, 2017, LECT NOTES COMPUT SC, V10553, P135, DOI 10.1007/978-3-319-67558-9_16
   Bunel Rudy, 2018, INT C LEARN REPR ICL
   Buolamwini J., 2018, P C FAIRN ACC TRANSP, P77
   BURKE L, 2016, LANCET GLOB HEALTH, V4, DOI DOI 10.3389/FCELL.2016.00103
   Byrd J, 2019, PR MACH LEARN RES, V97
   Caicedo JC, 2018, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2018.00970
   Caicedo JC, 2017, NAT METHODS, V14, P849, DOI [10.1038/NMETH.4397, 10.1038/nmeth.4397]
   Caldas S., 2018, LEAF BENCHMARK FEDER
   Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1
   Cao KD, 2019, ADV NEUR IN, V32
   Cao Kaidi, 2020, ARXIV200615766
   CARLUCCI FM, 2019, COMPUTER VISION PATT, P2224, DOI DOI 10.1109/CVPR.2019.00233
   Cavalli-Sforza V., 2020, INT C ED DAT MIN, P257
   Chanussot L., 2020, ARXIV201009990
   Chen I.Y., 2020, ANN REV BIOMEDICAL D
   Chen Irene Y, 2019, AMA J Ethics, V21, pE167, DOI 10.1001/amajethics.2019.167
   Chen VS, 2019, ADV NEUR IN, V32
   Ching T, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0387
   Christie G, 2018, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR.2018.00646
   Chung JS, 2018, INTERSPEECH, P1086
   Clark JH, 2020, T ASSOC COMPUT LING, V8, P454, DOI 10.1162/tacl_a_00317
   Codella Noel, 2019, ARXIV190203368
   Conneau Alexis, 2018, ARXIV180905053
   Consortium H., 2019, NATURE, V574
   Corbett-Davies S., 2018, MEASURE MISMEASURE F
   Corbett-Davies S., 2016, WASH POST
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   CORDELLA LP, 1995, IEEE T NEURAL NETWOR, V6, P1140, DOI 10.1109/72.410358
   Courtiol P, 2019, NAT MED, V25, P1519, DOI 10.1038/s41591-019-0583-3
   Croce F., 2020, ARXIV201009670
   Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Cynthia Dwork, 2012, P 3 INN THEOR COMP C, P214
   D'Amour A., 2020, ARXIV PREPRINT ARXIV
   D'Amour A, 2020, FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P525, DOI 10.1145/3351095.3372878
   Dai DX, 2018, IEEE INT C INTELL TR, P3819, DOI 10.1109/ITSC.2018.8569387
   David E., 2021, GLOBAL WHEAT HEAD DA
   David E, 2020, PLANT PHENOMICS, V2020, DOI 10.34133/2020/3521852
   Davis SE, 2017, J AM MED INFORM ASSN, V24, P1052, DOI 10.1093/jamia/ocx030
   DeGrave Alex J, 2020, medRxiv, DOI 10.1101/2020.09.13.20193565
   Delangue Clement, 2019, ARXIV PREPRINT ARXIV
   Desmarais MC, 2012, USER MODEL USER-ADAP, V22, P9, DOI 10.1007/s11257-011-9106-8
   DEVKOTA P, 2018, EMPIRICAL METHODS NA, P2799
   DigitalGlobe N., 2016, SPAC
   Dill KA, 2012, SCIENCE, V338, P1042, DOI 10.1126/science.1219021
   Dixon L, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P67, DOI 10.1145/3278721.3278729
   Djolonga Josip, 2020, ARXIV200708558
   Dodge S., 2017, 2017 26 INT C COMP C, P1, DOI DOI 10.1109/ICCCN.2017.8038465
   Dou Q., 2019, NEURIPS, P6447
   Dreccer MF, 2019, PLANT SCI, V282, P73, DOI 10.1016/j.plantsci.2018.06.008
   Dressel J, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aao5580
   Duchi J., 2020, ARXIV200713982
   Duchi JC, 2021, ANN STAT, V49, P1378, DOI 10.1214/20-AOS2004
   Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247
   Echeverri CJ, 2006, NAT REV GENET, V7, P373, DOI 10.1038/nrg1836
   El-Yaniv R., 2018, INT C LEARN REPR ICL
   Elvidge CD, 2009, COMPUT GEOSCI-UK, V35, P1652, DOI 10.1016/j.cageo.2009.01.009
   Eraslan G, 2019, NAT REV GENET, V20, P389, DOI 10.1038/s41576-019-0122-6
   Espey J., 2015, SUSTAINABLE DEV SOLU
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   FAN QC, 2018, ADV NEURAL INFORM PR, P3539
   Fan Z, 2018, IEEE J-STARS, V11, P876, DOI 10.1109/JSTARS.2018.2793849
   Fang C, 2013, IEEE I CONF COMP VIS, P1657, DOI 10.1109/ICCV.2013.208
   Feng J., 2019, ARXIV190605473
   Filmer D., 2011, DEMOGRAPHY, V49
   Franks Christine, 2015, P 37 INT C SOFTW ENG
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022
   Futoma J, 2020, LANCET DIGIT HEALTH, V2, pE489, DOI 10.1016/S2589-7500(20)30186-2
   Gal Y, 2016, PR MACH LEARN RES, V48
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Garg Saurabh, 2020, ADV NEUR IN, V33
   Gebru T., 2018, DATASHEETS DATASETS
   Geifman Y, 2019, PR MACH LEARN RES, V97
   Geifman Y, 2017, ADV NEUR IN, V30
   Geirhos R, 2020, ABS200407780 ARXIV
   Geirhos R., 2018, NEURIPS, P7538, DOI DOI 10.5555/3327757.3327854
   Geirhos Robert, 2018, ARXIV PREPRINT ARXIV
   Gelman A, 2007, J AM STAT ASSOC, V102, P813, DOI 10.1198/016214506000001040
   Geva Mor, 2019, P 2019 C EMP METH NA, P1161
   Gibson C.C, BIORXIV, DOI [10.1101/2020.08.02.233064, DOI 10.1101/2020.08.02.233064]
   GILMER J, 2017, INT C MACH LEARN ICM, V70
   Godinez W. J., 2018, BIORXIV
   Goel K., 2020, ARXIV200806775
   Goel S, 2016, ANN APPL STAT, V10, P365, DOI 10.1214/15-AOAS897
   Gogoll D, 2020, IEEE INT C INT ROBOT, P2636, DOI 10.1109/IROS45743.2020.9341277
   Goh WWB, 2017, TRENDS BIOTECHNOL, V35, P498, DOI 10.1016/j.tibtech.2017.02.012
   Goodfellow I., 2015, P 3 INT C LEARN REPR
   Graetz N, 2018, NATURE, V555, P48, DOI 10.1038/nature25761
   Grooten M., 2020, LIVING PLANET REPORT
   Gulrajani Ishaan, 2020, ARXIV200701434
   Guo J., 2018, ARXIV PREPRINT ARXIV, P4694
   Gupta A, 2018, ADV NEUR IN, V31
   Gurcan Metin N, 2009, IEEE Rev Biomed Eng, V2, P147, DOI 10.1109/RBME.2009.2034865
   Halpern Y., 2020, NEURIPS 18 COMPETITI, P155
   Han X., 2020, P 2020 C EMP METH NA, P7732, DOI [10.18653/v1/2020.emnlp-main.622, DOI 10.18653/V1/2020.EMNLP-MAIN.622]
   Hand DJ, 2006, STAT SCI, V21, P1, DOI 10.1214/088342306000000060
   Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1215904, 10.1126/science.1244693]
   Harrill Joshua, 2019, Current Opinion in Toxicology, V15, P64, DOI 10.1016/j.cotox.2019.05.004
   Hashimoto TB, 2018, PR MACH LEARN RES, V80
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He Luheng, 2019, P 2019 C N AM CHAPT
   He Y, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107383
   Heinze-Deml C., 2017, ARXIV171011469
   Hellendoorn VJ, 2019, PROC INT CONF SOFTW, P960, DOI 10.1109/ICSE.2019.00101
   Henderson BE, 2012, NAT REV CANCER, V12, P648, DOI 10.1038/nrc3341
   Hendrycks D., 2020, ARXIV200616241
   Hendrycks D., 2020, ARXIV191111132
   Hendrycks D, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2744
   Hendrycks Dan, 2017, 5 INT C LEARN REPR I
   Hendrycks Dan, 2019, 7 INT C LEARN REPR I
   Ho JWK, 2014, NATURE, V512, P449, DOI 10.1038/nature13415
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hovy D, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P591
   Hu JJ, 2020, PR MACH LEARN RES, V119
   Hu W., 2020, NEURIPS
   Hu WH, 2018, PR MACH LEARN RES, V80
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang J., 2020, P IEEE CVF C COMP VI, P13075
   Hughes JP, 2011, BRIT J PHARMACOL, V162, P1239, DOI 10.1111/j.1476-5381.2010.01127.x
   Husain H., 2019, ARXIV190909436
   Jaganathan K, 2019, CELL, V176, P535, DOI 10.1016/j.cell.2018.12.015
   Jean N, 2018, ADV NEUR IN, V31
   Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894
   Jin W., 2020, ARXIV200603908
   Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Jones E., 2021, INT C LEARN REPR ICL, P2021
   Jorgensen A., 2015, P WORKSH NOIS US GEN, P9
   Jumper J., 2020, 14 CRITICAL ASSESSME
   Kahn G., 2020, ARXIV200205700
   Kallus N, 2018, PR MACH LEARN RES, V80
   Kamath Amita, 2020, ACL
   Kearns M, 2018, PR MACH LEARN RES, V80
   Keilwagen J, 2019, GENOME BIOL, V20, DOI 10.1186/s13059-018-1614-y
   Kelley DR, 2016, GENOME RES, V26, P990, DOI 10.1101/gr.200535.115
   Kim J. H., 2016, INCORPORATING SPATIA
   Kim Najoung, 2020, P 2020 C EMP METH NA, P9087
   Kim S, 2016, NUCLEIC ACIDS RES, V44, pD1202, DOI 10.1093/nar/gkv951
   Koenecke A, 2020, P NATL ACAD SCI USA, V117, P7684, DOI 10.1073/pnas.1915768117
   Koh PW, 2020, PR MACH LEARN RES, V119
   Kompa B., 2020, ARXIV201003039
   Komura D, 2018, COMPUT STRUCT BIOTEC, V16, P34, DOI 10.1016/j.csbj.2018.01.001
   Kulal S, 2019, ADV NEUR IN, V32
   Kulkarni C, 2015, UNDERST INNOV, P131, DOI 10.1007/978-3-319-06823-7_9
   Kulkarni Chinmay E., 2014, P 1 ACM C LEARN SCAL, P99, DOI 10.1145/2556325.2566238
   Kumar A, 2020, PR MACH LEARN RES, V119
   Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248
   Kuznichov D, 2019, IEEE COMPUT SOC CONF, P2580, DOI 10.1109/CVPRW.2019.00314
   Lake B, 2018, PR MACH LEARN RES, V80
   Lample Guillaume, 2019, ADV NEURAL INFORM PR
   Landrum G., 2006, RDKIT OPEN SOURCE CH
   LANGMEAD B, 2010, NAT REV GENET, V11, DOI DOI 10.1186/GB-2010-11-8-R83
   Larrazabal AJ, 2020, P NATL ACAD SCI USA, V117, P12592, DOI 10.1073/pnas.1919012117
   Latessa E.J., 2010, FEDERAL PROBATION, V74, P16
   Lau RYK, 2014, DECIS SUPPORT SYST, V65, P80, DOI 10.1016/j.dss.2014.05.005
   LeCun Y, 1998, MNIST DATABASE HANDW
   Levine, 2017, 2017 IEEE INT C ROB, P3389, DOI DOI 10.1109/ICRA.2017.7989385
   Li D, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li H.M.Y.T. Yongya, 2019, BIORXIV
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li HY, 2019, GENOME RES, V29, P281, DOI 10.1101/gr.237156.118
   Li T., 2019, ARXIV PREPRINT ARXIV
   Li Y, 2017, P 5 INT C LEARN REPR
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Liang Shiyu, 2018, 6 INT C LEARN REPR I
   Libbrecht MW, 2015, NAT REV GENET, V16, P321, DOI 10.1038/nrg3920
   Lin ZY, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz0652
   Lipton Z. C., 2018, ARXIV PREPRINT ARXIV
   Lipton Zachary, 2019, INT C LEARN REPR ICL
   Liu LT, 2018, PR MACH LEARN RES, V80
   Liu Y, 2017, ARXIV170302442
   Ljosa V, 2012, NAT METHODS, V9, P637, DOI 10.1038/nmeth.2083
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Loshchilov Ilya, 2019, ARXIV171105101
   Lu S., 2021, ARXIV210204664V2
   Lum K, 2016, SIGNIFICANCE, V13, P14, DOI DOI 10.1111/J.1740-9713.2016.00960.X
   Lum K., 2019, MEASURES FAIRNESS NE, P21
   Lyu J, 2019, NATURE, V566, P224, DOI 10.1038/s41586-019-0917-9
   Macarron R, 2011, NAT REV DRUG DISCOV, V10, P188, DOI 10.1038/nrd3368
   Macenko M, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO TO MACRO, VOLS 1 AND 2, P1107, DOI 10.1109/ISBI.2009.5193250
   Madec S, 2019, AGR FOREST METEOROL, V264, P225, DOI 10.1016/j.agrformet.2018.10.013
   Malloy BA, 2017, INT SYMP EMP SOFTWAR, P314, DOI 10.1109/ESEM.2017.45
   Mansour Y., 2009, ADV NEURAL INF PROCE, P1041
   Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556
   Mattu, 2016, PROPUBLICA
   McCloskey K, 2020, J MED CHEM, V63, P8857, DOI 10.1021/acs.jmedchem.0c00452
   McCoy R.T., 2019, ARXIV PREPRINT ARXIV
   McCoy R. Thomas, 2019, ARXIV191102969
   McKinney SM, 2020, NATURE, V577, P89, DOI 10.1038/s41586-019-1799-6
   Mehrabi N., 2019, ARXIV190809635CS
   Meinshausen N, 2015, ANN STAT, V43, P1801, DOI 10.1214/15-AOS1325
   Michael Ando D, 2017, BIORXIV, DOI [10.1101/161422, DOI 10.1101/161422]
   Miller J., 2020, ARXIV200414444
   Mirowski Piotr, 2017, ICLR
   Moore JE, 2020, NATURE, V583, P699, DOI 10.1038/s41586-020-2493-4
   MOULT J, 1995, PROTEINS, V23, pR2, DOI 10.1002/prot.340230303
   Nekoto Wilhelmina, 2020, FINDINGS ASS COMPUTA, P2144
   Nestor B, 2019, FEATURE ROBUSTNESS N
   Newman D, 2007, UCI MACHINE LEARNING
   Ng, 2019, RADIOLOGY, DOI [DOI 10.1148/RADIOL.2020201160, 10.1148/radiol.2020201160]
   Ni J., 2019, P 2019 C EMP METH NA, P188, DOI DOI 10.18653/V1/D19-1018
   NITA M, 2010, 2010 ACM IEEE 32 INT, V0001, P00205
   Noor Abdisalan M, 2008, Popul Health Metr, V6, P5, DOI 10.1186/1478-7954-6-5
   Norouzzadeh M. S., 2019, ARXIV191009716
   Nygaard V, 2016, BIOSTATISTICS, V17, P29, DOI 10.1093/biostatistics/kxv027
   Obermeyer Z, 2019, SCIENCE, V366, P447, DOI 10.1126/science.aax2342
   Oren Y., 2019, EMPIRICAL METHODS NA
   Osgood-Zimmerman A, 2018, NATURE, V555, P41, DOI 10.1038/nature25760
   Ovadia Y, 2019, ADV NEUR IN, V32
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Parham J., 2017, ASS ADVANCEMENT ARTI, DOI [10.1016/j.wasman.2010.12.019, DOI 10.1016/J.WASMAN.2010.12.019.]
   Parker HS, 2012, STAT APPL GENET MOL, V11, DOI 10.1515/1544-6115.1766
   Patro GK, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1194, DOI 10.1145/3366423.3380196
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Peng XB, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Perelman L, 2014, ASSESS WRIT, V21, P104, DOI 10.1016/j.asw.2014.05.001
   Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Phillips, 2020, ARXIV200706199 CORR
   Piech C., 2013, ED DATA MINING
   Pierson E, 2018, PR MACH LEARN RES, V84
   Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026
   Pipal KA, 2012, N AM J FISH MANAGE, V32, P880, DOI 10.1080/02755947.2012.697096
   Price WN, 2019, NAT MED, V25, P37, DOI 10.1038/s41591-018-0272-7
   Proksch S., 2016, 2016 31 IEEE ACM INT
   Proksch S, 2015, ACM T SOFTW ENG METH, V25, DOI 10.1145/2744200
   Quang D, 2019, METHODS, V166, P40, DOI 10.1016/j.ymeth.2019.03.020
   Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Raychev V, 2016, ACM SIGPLAN NOTICES, V51, P731, DOI 10.1145/3022671.2984041
   Raychev V, 2014, ACM SIGPLAN NOTICES, V49, P419, DOI [10.1145/2666356.2594321, 10.1145/2594291.2594321]
   Re C., 2019, ARXIV190905372
   Recht B, 2019, PR MACH LEARN RES, V97
   Reiner RC, 2018, NEW ENGL J MED, V379, P1128, DOI 10.1056/NEJMoa1716766
   Reker D., 2020, DRUG DISCOV TODAY
   Ren SQ, 2015, ADV NEUR IN, V28
   Reynolds M, 2020, PLANT SCI, V295, DOI 10.1016/j.plantsci.2019.110396
   Ribeiro Marco Tulio, 2020, ARXIV PREPRINT ARXIV, P4902, DOI 10.18653/v1/2020.acl-main.442
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Rigaki M, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P70, DOI 10.1109/SPW.2018.00019
   Robbes Romain, 2008, 2008 23rd IEEE/ACM International Conference on Automated Software Engineering, P317, DOI 10.1109/ASE.2008.42
   Rolf E, 2020, PR MACH LEARN RES, V108, P1759
   ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352
   Rosenfeld A., 2018, ARXIV180803305
   Ruszczynski, 2014, LECT STOCHASTIC PROG
   Sadeghi F, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII
   Sadeghi-Tehran P, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0253-8
   SAEED U, 2017, ELIFE, V6, DOI DOI 10.1186/S40035-017-0076-6
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446
   Sagawa S., 2020, PROCEEDINGS OF THE 3, P8346
   Sagawa S, 2020, INT C LEARN REPR
   Sahn DE, 2003, REV INCOME WEALTH, P463
   Sanh Victor, 2019, EMC2
   Santurkar Shibani, 2020, BREEDS BENCHMARKS SU, V8
   Sap M, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1668
   Schneider S., 2020, ARXIV200712808
   Seyyed-Kalantari L., 2020, CHEXCLUSION FAIRNESS
   Shakoor N, 2017, CURR OPIN PLANT BIOL, V38, P184, DOI 10.1016/j.pbi.2017.05.006
   Shankar Shreya, 2017, ARXIV PREPRINT ARXIV
   Shankar V., 2019, ARXIV190602168
   Shen J, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P4058
   Shermis MD, 2014, ASSESS WRIT, V20, P53, DOI 10.1016/j.asw.2013.04.001
   Shetty R, 2019, PROC CVPR IEEE, P8210, DOI 10.1109/CVPR.2019.00841
   Shi Y, 2016, PLOS ONE, V11, P1, DOI DOI 10.1371/J0URNAL.P0NE.0157259
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
   Shoichet BK, 2004, NATURE, V432, P862, DOI 10.1038/nature03197
   Slack D., 2019, ARXIV190809092CSSTAT
   SOHONI NS, 2020, ADV NEURAL INFORM PR, V33
   Soneson C, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0100335
   Song, 2019, INT C LEARN REPR
   Srivastava D, 2020, BBA-GENE REGUL MECH, V1863, DOI 10.1016/j.bbagrm.2019.194443
   SRIVASTAVA M, 2020, INT C MACH LEARN, P109
   Sterling T, 2015, J CHEM INF MODEL, V55, P2324, DOI 10.1021/acs.jcim.5b00559
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Subbaswamy A., 2020, ARXIV201015100
   Sun BC, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun Pei, 2020, P IEEE CVF C COMP VI
   Sun Yu, 2020, INT C MACH LEARN ICM, P9229
   Svyatkovskiy A, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2727
   Swinney DC, 2011, NAT REV DRUG DISCOV, V10, P507, DOI 10.1038/nrd3480
   Tabak G, 2020, PEERJ, V8, DOI 10.7717/peerj.8594
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Taghipour K., 2016, P 2016 C EMP METH NA, P1882, DOI [10.18653/v1/D16-1193, DOI 10.18653/V1/D16-1193]
   Taori Rohan, 2020, ARXIV200700644, V33
   Tatman R., 2017, P 1 ACL WORKSH ETH N, P53
   Taylor J., 2019, INT C LEARN REPR ICL
   Taylor MJ, 2021, J AM SOC MASS SPECTR, V32, P872, DOI 10.1021/jasms.0c00439
   Tellez D, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101544
   Tellez D, 2018, IEEE T MED IMAGING, V37, P2126, DOI 10.1109/TMI.2018.2820199
   Temel D, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P137, DOI 10.1109/ICMLA.2018.00028
   Thorp KR, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111682
   Tiecke TG, 2017, MAPPING WORLD POPULA
   Tobin Joshua, 2017, ARXIV170306907
   Toda Y, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/9237136
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tuschl T, 2001, CHEMBIOCHEM, V2, P239, DOI 10.1002/1439-7633(20010401)2:4<239::AID-CBIC239>3.3.CO;2-I
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Ubbens Jordan R., 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P391, DOI 10.1007/978-3-030-65414-6_27
   Uzkent Burak, 2020, P IEEE CVF C COMP VI
   Vasic M., 2019, ARXIV PREPRINT ARXIV
   Vatnehol S, 2018, ICES J MAR SCI, V75, P1803, DOI 10.1093/icesjms/fsy029
   Veeling BS, 2018, LECT NOTES COMPUT SC, V11071, P210, DOI 10.1007/978-3-030-00934-2_24
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Veta M, 2019, MED IMAGE ANAL, V54, P111, DOI 10.1016/j.media.2019.02.012
   Veta M, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161286
   Volpi R., 2018, ADV NEURAL INFORM PR
   Wang A, 2019, ADV NEUR IN, V32
   Wang Alex, 2019, 7 INT C LEARNING REP
   Wang Dong, 2020, ARXIV200610726
   Wang Haohan, 2019, NEURIPS, P10506
   Wang SL, 2017, IEEE I CONF COMP VIS, P3028, DOI 10.1109/ICCV.2017.327
   Wang S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020207
   Ward D, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103009
   Wearn OR, 2017, WWF CONSERVATION TEC, V1
   Weinberger S., 2015, SPEECH ACCENT ARCHIV
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weinstein JN, 2013, NAT GENET, V45, P1113, DOI 10.1038/ng.2764
   West R., 2014, T ASSOC COMPUT LING, V2, P297, DOI [10.1162/tacl_a_00184, DOI 10.1162/TACL_A_00184]
   Weston Jason, 2017, INT C LEARN REPR ICL
   Widmer G, 1996, MACH LEARN, V23, P69, DOI 10.1007/BF00116900
   Williams JJ, 2016, PROCEEDINGS OF THE THIRD (2016) ACM CONFERENCE ON LEARNING @ SCALE (L@S 2016), P379, DOI 10.1145/2876034.2876042
   Wilson Benjamin, 2019, ARXIV190211097
   Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758
   Wu MK, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P782
   Wu YF, 2019, PR MACH LEARN RES, V97
   Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a
   Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489
   Xiao Kai Y., 2020, ARXIV200609994
   Xie M, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3929
   Xie S. M., 2020, IN N OUT PRETRAINING
   Xiong HP, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0537-2
   Xu K., 2018, ARXIV PREPRINT ARXIV
   Yang Y., 2019, ASIAN BUS MANAG, P1
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270
   Yasunaga Michihiro, 2020, ARXIV PREPRINT ARXIV
   Yeh C, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16185-w
   You J., 2017, 31 AAAI C ART INT SA
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yuval N., 2011, NIPS WORKSH DEEP LEA
   Zafar M. B., 2017, ADV NEURAL INFORM PR, P229
   Zech JR, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002683
   Zemel Richard, 2020, ARXIV PREPRINT ARXIV
   Zeng J., 2018, MIAM BEH FIN C, P1
   ZHANG L, 2013, INT C MACH LEARN, P819
   Zhang M., 2020, ARXIV200702931
   Zhao Jieyu, 2018, P 2018 C N AM CHAPT, V2, P15
   Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/NMETH.3547, 10.1038/nmeth.3547]
   Zhou Xiang, 2020, P 2020 C EMP METH NA, P8215
   Zhou YX, 2014, NATURE, V509, P487, DOI 10.1038/nature13166
   Zitnick C.L., 2020, ARXIV201009435CONDMA
NR 413
TC 4
Z9 4
U1 1
U2 1
PU JMLR-JOURNAL MACHINE LEARNING RESEARCH
PI SAN DIEGO
PA 1269 LAW ST, SAN DIEGO, CA, UNITED STATES
SN 2640-3498
J9 PR MACH LEARN RES
PY 2021
VL 139
PG 28
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS0JP
UT WOS:000683104605062
DA 2022-02-10
ER

PT J
AU Piel, AK
   Crunchant, A
   Knot, IE
   Chalmers, C
   Fergus, P
   Mulero-Pazmany, M
   Wich, SA
AF Piel, A. K.
   Crunchant, A.
   Knot, I. E.
   Chalmers, C.
   Fergus, P.
   Mulero-Pazmany, M.
   Wich, S. A.
TI Noninvasive Technologies for Primate Conservation in the 21st Century
SO INTERNATIONAL JOURNAL OF PRIMATOLOGY
LA English
DT Article; Early Access
DE Endangered; Methods; Monitoring; Remote Sensing; Tools
ID ESTIMATING ANIMAL DENSITY; CAMERA TRAPS; HABITAT USE; NATIONAL-PARK;
   LOUD CALLS; RESPIRATORY-DISEASE; ENDANGERED PRIMATE; WILDLIFE RESEARCH;
   REAL-TIME; OCCUPANCY
AB Observing and quantifying primate behavior in the wild is challenging. Human presence affects primate behavior and habituation of new, especially terrestrial, individuals is a time-intensive process that carries with it ethical and health concerns, especially during the recent pandemic when primates are at even greater risk than usual. As a result, wildlife researchers, including primatologists, have increasingly turned to new technologies to answer questions and provide important data related to primate conservation. Tools and methods should be chosen carefully to maximize and improve the data that will be used to answer the research questions. We review here the role of four indirect methods-camera traps, acoustic monitoring, drones, and portable field labs- and improvements in machine learning that offer rapid, reliable means of combing through large datasets that these methods generate. We describe key applications and limitations of each tool in primate conservation, and where we anticipate primate conservation technology moving forward in the coming years.
C1 [Piel, A. K.] UCL, Dept Anthropol, London, England.
   [Crunchant, A.; Chalmers, C.; Fergus, P.; Mulero-Pazmany, M.; Wich, S. A.] Liverpool John Moores Univ, Sch Biol & Environm Sci, Liverpool, Merseyside, England.
   [Knot, I. E.; Wich, S. A.] Univ Amsterdam, Inst Biodivers & Ecosyst Dynam, Amsterdam, Netherlands.
RP Piel, AK (corresponding author), UCL, Dept Anthropol, London, England.
EM a.piel@ucl.ac.uk
OI Knot, Ineke/0000-0001-9886-2597; Piel, Alexander/0000-0002-4674-537X;
   Wich, Serge/0000-0003-3954-5174; Chalmers, Carl/0000-0003-0822-1150
CR ADAMS W., 2007, CONSERV SOC, V5, P147, DOI DOI 10.2307/26392879
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Allan A. T. L., 2020, SCI ADV, P1
   Ancrenaz M, 2005, PLOS BIOL, V3, P30, DOI 10.1371/journal.pbio.0030003
   [Anonymous], 2011, QUANTIFYING SENSITIV, DOI [10.1111/j.2041-210X.2011.00094.x, DOI 10.1111/J.2041-210X.2011.00094.X]
   Apolo-Apolo OE, 2020, EUR J AGRON, V115, DOI 10.1016/j.eja.2020.126030
   Arandjelovic M, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22743
   Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Assmann JJ, 2019, J UNMANNED VEH SYST, V7, P54, DOI 10.1139/juvs-2018-0018
   August T., 2019, AUTONOMOUS DRONES AR
   Balantic C, 2019, ECOL APPL, V29, DOI 10.1002/eap.1854
   Baldi P, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.570862
   Barasona JA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115608
   Beehner JC, 2017, HORM BEHAV, V91, P68, DOI 10.1016/j.yhbeh.2017.03.003
   Berger-Tal O, 2018, CONSERV LETT, V11, DOI 10.1111/conl.12458
   Bessone M, 2020, J APPL ECOL, V57, P963, DOI 10.1111/1365-2664.13602
   Bianco MJ, 2019, J ACOUST SOC AM, V146, P3590, DOI 10.1121/1.5133944
   Blanco MB, 2020, CONSERV GENET, V21, P785, DOI 10.1007/s10592-020-01296-0
   Blumstein DT, 2011, J APPL ECOL, V48, P758, DOI 10.1111/j.1365-2664.2011.01993.x
   Bondi E., 2018, IJCAI INT JOINT C AR, P5814
   Bondi E, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P7741
   Bondi Elizabeth, 2019, P77
   Bonnin N, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2020017
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers DL, 2015, J AM STAT ASSOC, V110, P195, DOI 10.1080/01621459.2014.893884
   Bouchet H, 2012, J COMP PSYCHOL, V126, P45, DOI 10.1037/a0025018
   Brauer CL, 2016, WILDLIFE SOC B, V40, P140, DOI 10.1002/wsb.619
   Brede B, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071032
   Brittain S, 2020, CONSERV BIOL, V34, P925, DOI 10.1111/cobi.13464
   Brown AT, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232044
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Buckland ST, 2010, INT J PRIMATOL, V31, P485, DOI 10.1007/s10764-010-9408-4
   Buehler P, 2019, ECOL INFORM, V50, P191, DOI 10.1016/j.ecoinf.2019.02.003
   Burke C, 2019, J UNMANNED VEH SYST, V7, P235, DOI 10.1139/juvs-2018-0035
   Burke C, 2018, PROC SPIE, V10709, DOI 10.1117/12.2311673
   Buxton RT, 2018, GLOB ECOL CONSERV, V16, DOI 10.1016/j.gecco.2018.e00493
   Campos-Cerqueira M, 2016, METHODS ECOL EVOL, V7, P1340, DOI 10.1111/2041-210X.12599
   Cardenosa D, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.39
   Carvalho F, 2020, ENVIRON CONSERV, V47, P113, DOI 10.1017/S0376892920000090
   Chabot D, 2019, BIOL CONSERV, V237, P125, DOI 10.1016/j.biocon.2019.06.022
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chalmers C., 2019, ARXIV191007360
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chang JJM, 2020, GENES-BASEL, V11, DOI 10.3390/genes11101121
   Chapman CA, 2005, EVOL ANTHROPOL, V14, P134, DOI 10.1002/evan.20068
   Chen DM, 2021, AM J PRIMATOL, V83, DOI 10.1002/ajp.23270
   Chen RL, 2019, ECOL EVOL, V9, P9453, DOI 10.1002/ece3.5410
   Clark DA, 2019, ARCT SCI, V5, P62, DOI 10.1139/as-2018-0013
   Clink DJ, 2019, BIOACOUSTICS, V28, P193, DOI 10.1080/09524622.2018.1426042
   Comer CE, 2014, WILDLIFE SOC B, V38, P103, DOI 10.1002/wsb.375
   Comstock KE, 2003, CONSERV BIOL, V17, P1840, DOI 10.1111/j.1523-1739.2003.00358.x
   Coulter LL, 2009, ENVIRON MONIT ASSESS, V152, P343, DOI 10.1007/s10661-008-0320-8
   Cove MV, 2013, TROP CONSERV SCI, V6, P781, DOI 10.1177/194008291300600606
   Crofoot MC, 2010, ANIM BEHAV, V80, P475, DOI 10.1016/j.anbehav.2010.06.006
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   D'Odorico P, 2020, NEW PHYTOL, V226, P1667, DOI 10.1111/nph.16488
   Damas Joana, 2020, bioRxiv, DOI [10.1073/pnas.2010146117, 10.1101/2020.04.16.045302]
   Dawson DK, 2009, J APPL ECOL, V46, P1201, DOI 10.1111/j.1365-2664.2009.01731.x
   Delgado RA, 2006, INT J PRIMATOL, V27, P5, DOI 10.1007/s10764-005-9001-4
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Diaz-Delgado R, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010003
   Doran-Sheehy DM, 2007, AM J PRIMATOL, V69, P1354, DOI 10.1002/ajp.20442
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford MG, 2009, ECOLOGY, V90, P2676, DOI 10.1890/08-1735.1
   Elsey RM, 2016, SOUTHEAST NAT, V15, P76, DOI 10.1656/058.015.0106
   Enari H, 2019, ECOL INDIC, V98, P753, DOI 10.1016/j.ecolind.2018.11.062
   Eriksson J, 2004, MOL ECOL, V13, P3425, DOI 10.1111/j.1365-294X.2004.02332.x
   Estrada A, 2018, PEERJ, V6, DOI 10.7717/peerj.4869
   Estrada A, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1600946
   Fang YH, 2020, PRIMATES, V61, P151, DOI 10.1007/s10329-019-00774-5
   Fauver JR, 2020, CELL, V181, P990, DOI 10.1016/j.cell.2020.04.021
   Frankham R., 2010, INTRO CONSERVATION G
   Fruth B, 2018, AM J PHYS ANTHROPOL, V166, P499, DOI 10.1002/ajpa.23373
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Garriga RM, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0215545
   Gazagne E, 2020, RAFFLES B ZOOL, V68, P735, DOI 10.26107/RBZ-2020-0085
   Gerber B, 2010, ORYX, V44, P219, DOI 10.1017/S0030605309991037
   Gilardi K. V, 1999, BEST PRACTICE GUIDEL
   Gillespie TR, 2020, NATURE, V579, P497, DOI 10.1038/d41586-020-00859-y
   Gogarten JF, 2020, MOL ECOL RESOUR, V20, P204, DOI 10.1111/1755-0998.13101
   Goordial J, 2017, FRONT MICROBIOL, V8, DOI 10.3389/fmicb.2017.02594
   Goossens B, 2006, PLOS BIOL, V4, P285, DOI 10.1371/journal.pbio.0040025
   Gowers GOF, 2019, GENES-BASEL, V10, DOI 10.3390/genes10110902
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   Groves CR, 2002, BIOSCIENCE, V52, P499, DOI 10.1641/0006-3568(2002)052[0499:PFBCPC]2.0.CO;2
   Grutzmacher KS, 2016, ECOHEALTH, V13, P499, DOI 10.1007/s10393-016-1144-6
   Guevara EE, 2018, CONSERV GENET RESOUR, V10, P119, DOI 10.1007/s12686-017-0758-5
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   Hambrecht L, 2019, BIOL CONSERV, V233, P109, DOI 10.1016/j.biocon.2019.02.017
   Hanya G, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190631
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Heinicke S, 2015, METHODS ECOL EVOL, V6, P753, DOI 10.1111/2041-210X.12384
   Higham JP, 2016, HORM BEHAV, V84, P145, DOI 10.1016/j.yhbeh.2016.07.001
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   Hines D.I, 2017, OCCUPANCY ESTIMATION
   Hoban SM, 2013, CONSERV GENET RESOUR, V5, P593, DOI 10.1007/s12686-013-9859-y
   Hongo S, 2018, INT J PRIMATOL, V39, P27, DOI 10.1007/s10764-017-0007-5
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Isaac NJB, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000296
   Jack KM, 2008, AM J PRIMATOL, V70, P490, DOI 10.1002/ajp.20512
   Jain M, 2016, GENOME BIOL, V17, DOI 10.1186/s13059-016-1103-0
   Lopez JJ, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010010
   Johnson CL, 2020, ORYX, V54, P784, DOI 10.1017/S0030605319000851
   Johnson Sarah S, 2017, J Biomol Tech, V28, P2, DOI 10.7171/jbt.17-2801-009
   Joppa LN, 2017, NATURE, V552, P325, DOI 10.1038/d41586-017-08675-7
   Kalan AK, 2015, ECOL INDIC, V54, P217, DOI 10.1016/j.ecolind.2015.02.023
   Kalbitzer U, 2019, AFR J ECOL, V57, P190, DOI 10.1111/aje.12589
   Kappeler PM, 1998, AM J PRIMATOL, V46, P7
   Kauffman MJ, 2007, ORYX, V41, P70, DOI 10.1017/S0030605306001414
   Kays R, 2019, INT J REMOTE SENS, V40, P407, DOI 10.1080/01431161.2018.1523580
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Kidney D, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0155066
   Kimwele C. N., 2012, African Journal of Biotechnology, V11, P14276
   Klosterman S, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122852
   Knot IE, 2020, FRONT ECOL EVOL, V8, DOI 10.3389/fevo.2020.00100
   Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
   Kolowski JM, 2012, INT J PRIMATOL, V33, P958, DOI 10.1007/s10764-012-9627-y
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Koustubh Sharma, 2020, Ecological Solutions and Evidence, V1, DOI 10.1002/2688-8319.12033
   Krehenwinkel H, 2019, GENES-BASEL, V10, DOI 10.3390/genes10110858
   Krehenwinkel H, 2019, GIGASCIENCE, V8, DOI 10.1093/gigascience/giz006
   Krief S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0109925
   KUCERA TE, 2011, ANIMAL ECOLOGY METHO, P00009, DOI DOI 10.1007/978-4-431-99495-4_2
   Kwok R, 2019, NATURE, V567, P133, DOI 10.1038/d41586-019-00746-1
   Lehman SM, 2006, DEV PRIMATOL, P1
   Leyequien E, 2007, INT J APPL EARTH OBS, V9, P1, DOI 10.1016/j.jag.2006.08.002
   Lilly AA, 2002, INT J PRIMATOL, V23, P555, DOI 10.1023/A:1014969617036
   Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
   Loit K, 2019, APPL ENVIRON MICROB, V85, DOI 10.1128/AEM.01368-19
   Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Lu S, 2020, COMP SARS COV 2 INFE, DOI [10.1101/2020.04.08.031807, DOI 10.1101/2020.04.08.031807]
   Lynn MS, 2016, INTRODUCTION TO PRIMATE CONSERVATION, P53, DOI 10.1093/acprof:oso/9780198703389.003.0005
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   Marques TA, 2013, BIOL REV, V88, P287, DOI 10.1111/brv.12001
   Marques TA, 2009, J ACOUST SOC AM, V125, P1982, DOI 10.1121/1.3089590
   Marshall AJ, 2013, TECH ECOL CONSERVAT, P103
   Martin, 2012, ESTIMATING MINKE WHA, DOI DOI 10.1111/J.1748-7692.2011.00561.X
   Marvin DC, 2016, GLOB ECOL CONSERV, V7, P262, DOI 10.1016/j.gecco.2016.07.002
   Marx V, 2015, NAT METHODS, V12, P393, DOI 10.1038/nmeth.3369
   Massawe E. A., 2017, INT J ADV SMART SENS, V7, P1, DOI [10.5121/ ijassn.2017.7401., DOI 10.5121/IJASSN.2017.7401]
   Masters A, 2019, FORENSIC SCI INT, V301, P231, DOI 10.1016/j.forsciint.2019.05.041
   McDonald MA, 1999, J ACOUST SOC AM, V105, P2643, DOI 10.1121/1.426880
   McDowall IL, 2008, APPL HERPETOL, V5, P371, DOI 10.1163/157075408786532057
   McMahon BJ, 2014, EVOL APPL, V7, P999, DOI 10.1111/eva.12193
   Measey GJ, 2017, J APPL ECOL, V54, P894, DOI 10.1111/1365-2664.12810
   Medeiros K, 2019, INT J PRIMATOL, V40, P511, DOI 10.1007/s10764-019-00103-z
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Meek PD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110832
   Meijaard E, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027491
   Meijaard E, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0012042
   Melin Amanda D, 2020, bioRxiv, DOI 10.1101/2020.04.09.034967
   Menegon M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0184741
   Mitani JC, 1998, PRIMATES, V39, P171, DOI 10.1007/BF02557729
   Mondol S, 2015, CONSERV BIOL, V29, P556, DOI 10.1111/cobi.12393
   Moore JF, 2020, ANIM CONSERV, V23, P561, DOI 10.1111/acv.12569
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Morgan D, 2006, INT J PRIMATOL, V27, P147, DOI 10.1007/s10764-005-9013-0
   Mporas I, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10207379
   Mulero-Pazmany M, 2015, ECOL EVOL, V5, P4808, DOI 10.1002/ece3.1744
   Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   Negrey JD, 2019, EMERG MICROBES INFEC, V8, P139, DOI [10.1080/22221751.2018.1563456, 10.1080/22221751.201]
   Ni QY, 2018, PEERJ, V6, DOI 10.7717/peerj.6069
   NIJMAN V, 2017, AM J PRIMATOL, V79, DOI DOI 10.1002/AJP.22517
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Nowak K, 2014, BEHAV ECOL, V25, P1199, DOI 10.1093/beheco/aru110
   Nowak Maciej M., 2018, European Journal of Ecology, V4, P56, DOI 10.2478/eje-2018-0012
   Nunn CL, 2016, INTRODUCTION TO PRIMATE CONSERVATION, P157, DOI 10.1093/acprof:oso/9780198703389.003.0010
   O'Donoghue P, 2016, J APPL ECOL, V53, P5, DOI 10.1111/1365-2664.12452
   Oklander LI, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60569-3
   Olivares-Mendez MA, 2015, SENSORS-BASEL, V15, P31362, DOI 10.3390/s151229861
   Olson ER, 2012, ORYX, V46, P593, DOI 10.1017/S0030605312000488
   Ondei S, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2859
   Munnink BOB, 2020, NAT MED, V26, DOI 10.1038/s41591-020-0997-y
   Ouso DO, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-61600-3
   Pafco B, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24126-3
   Park JY, 2017, SCI REP-UK, V7, DOI [10.1038/srep43270, 10.1177/2158244016684912]
   Patrono LV, 2020, NAT MICROBIOL, V5, P955, DOI 10.1038/s41564-020-0706-0
   Pebsworth PA, 2014, INT J PRIMATOL, V35, P825, DOI 10.1007/s10764-014-9802-4
   PERES CA, 1994, BIOTROPICA, V26, P98, DOI 10.2307/2389114
   Pettorelli N, 2013, NORMALIZED DIFFERENCE VEGETATION INDEX, P1, DOI 10.1093/acprof:osobl/9780199693160.001.0001
   Piel AK, 2018, AM J PHYS ANTHROPOL, V166, P530, DOI 10.1002/ajpa.23609
   Piel AK, 2015, AM J PRIMATOL, V77, P1027, DOI 10.1002/ajp.22438
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Pomerantz A, 2018, GIGASCIENCE, V7, DOI 10.1093/gigascience/giy033
   Quick J, 2017, NAT PROTOC, V12, P1261, DOI 10.1038/nprot.2017.066
   Quick J, 2016, NATURE, V530, P228, DOI 10.1038/nature16996
   Ren W., 2020, FUNCTIONAL GENETIC A, DOI [10.1101/2020.04.22.046565, DOI 10.1101/2020.04.22.046565]
   Rodriguez A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0050336
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Sandbrook C, 2018, CONSERV SOC, V16, P493, DOI 10.4103/cs.cs_17_165
   Sarron J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10121900
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Seah A, 2020, GENES-BASEL, V11, DOI 10.3390/genes11040445
   Sebastian-Gonzalez E, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01224-130207
   Senthilnath J, 2016, BIOSYST ENG, V146, P16, DOI 10.1016/j.biosystemseng.2015.12.003
   Sousa-Lima RS, 2013, AQUAT MAMM, V39, P23, DOI 10.1578/AM.39.1.2013.23
   Spaan D, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3020034
   Spehar SN, 2017, INT J PRIMATOL, V38, P358, DOI 10.1007/s10764-017-9959-8
   Spillmann B, 2017, BIOACOUSTICS, V26, P109, DOI 10.1080/09524622.2016.1216802
   Spillmann B, 2015, AM J PRIMATOL, V77, P767, DOI 10.1002/ajp.22398
   Srivathsan A, 2019, BMC BIOL, V17, DOI 10.1186/s12915-019-0706-9
   Srivathsan A, 2016, FRONT ZOOL, V13, DOI 10.1186/s12983-016-0150-4
   Stevenson BC, 2015, METHODS ECOL EVOL, V6, P38, DOI 10.1111/2041-210X.12291
   STRIER KB, 1991, CONSERV BIOL, V5, P214, DOI 10.1111/j.1523-1739.1991.tb00126.x
   Szantoi Z, 2017, INT J REMOTE SENS, V38, P2231, DOI 10.1080/01431161.2017.1280638
   Tan TF, 2016, 2016 IEEE CONFERENCE ON SYSTEMS, PROCESS AND CONTROL (ICSPC), P37, DOI 10.1109/SPC.2016.7920700
   Teelen S, 2007, AM J PRIMATOL, V69, P1030, DOI 10.1002/ajp.20417
   Thompson ME, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.180840
   Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
   Van Andel AC, 2015, AM J PRIMATOL, V77, P1122, DOI 10.1002/ajp.22446
   Vigilant L, 2009, PRIMATES, V50, P105, DOI 10.1007/s10329-008-0124-z
   Voigt M, 2018, CURR BIOL, V28, P761, DOI 10.1016/j.cub.2018.01.053
   Wang YF, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P1401
   Wasser SK, 2004, P NATL ACAD SCI USA, V101, P14847, DOI 10.1073/pnas.0403170101
   Wearn OR, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181748
   Welbourne DJ, 2019, ANIMALS-BASEL, V9, DOI 10.3390/ani9060388
   Whitworth A, 2016, TROP CONSERV SCI, V9, P675, DOI 10.1177/194008291600900208
   Whytock RC, 2017, METHODS ECOL EVOL, V8, P308, DOI 10.1111/2041-210X.12678
   Wich S. A., 2018, NEW GEOSPATIAL APPRO, P121
   Wich SA, 2018, CONSERVATION DRONES: MAPPING AND MONITORING BIODIVERSITY, P1, DOI 10.1093/oso/9780198787617.001.0001
   Wich SA, 2002, BEHAV ECOL SOCIOBIOL, V52, P474, DOI 10.1007/s00265-002-0541-8
   WICH SA, 2016, PRIMATE CONSERVATION, P00001, DOI DOI 10.1093/ACPROF:OSO/9780198703389.003.0001
   Wich S, 2016, J UNMANNED VEH SYST, V4, P45, DOI 10.1139/juvs-2015-0015
   Wijers M, 2021, BIOACOUSTICS, V30, P41, DOI 10.1080/09524622.2019.1685408
   Williamson E. A., 2003, FIELD LAB METHODS PR, P33
   Wilson AM, 2017, AUK, V134, P350, DOI 10.1642/AUK-16-216.1
   Zak AA, 2017, INT J PRIMATOL, V38, P224, DOI 10.1007/s10764-016-9945-6
   Zhang H, 2020, GLOB ECOL CONSERV, V23, DOI 10.1016/j.gecco.2020.e01101
   ZIMMERMANN E, 1993, ETHOLOGY, V93, P211
NR 241
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0164-0291
EI 1573-8604
J9 INT J PRIMATOL
JI Int. J. Primatol.
DI 10.1007/s10764-021-00245-z
EA OCT 2021
PG 35
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA WK3QY
UT WOS:000709644700001
OA hybrid, Green Accepted, Green Published
DA 2022-02-10
ER

PT J
AU Ahmed, A
   Yousif, H
   Kays, R
   He, ZH
AF Ahmed, Ahmed
   Yousif, Hayder
   Kays, Roland
   He, Zhihai
TI Animal species classification using deep neural networks with noise
   labels
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Noisy labels; Deep neural networks; Clean network; K-means clustering
AB In this paper, we developed a robust learning method for animal classification from camera-trap images collected in highly cluttered natural scenes and annotated with noisy labels. We proposed two different network structures with and without clean samples to handle noisy labels. We use k-means clustering to divide the training samples into groups with different characteristics, which are then used to train different networks. These networks with enhanced diversity are then used to jointly predict or correct sample labels using max voting. We evaluate the performance of the proposed method on two public available camera-trap image datasets: Snapshot Serengeti and Panama-Netherlands datasets. Our experimental results demonstrate that our method outperforms the state-of-the-art methods from the literature and achieved improved accuracy on animal species classification from camera-trap images with high levels of label noise.
C1 [Ahmed, Ahmed; Yousif, Hayder; He, Zhihai] Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
   [Kays, Roland] North Carolina Museum Nat Sci, Raleigh, NC 27601 USA.
RP He, ZH (corresponding author), Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
EM hezhi@missouri.edu
RI Ahmed, Ahmed Q./ABH-6918-2020
OI Kays, Roland/0000-0002-2947-6665; Yousif, Hayder/0000-0002-7638-9505
FU National Science FoundationNational Science Foundation (NSF) [1539389]
FX This work has been supported in part by National Science Foundation
   under grant 1539389.
CR Barandela R, 2000, LECT NOTES COMPUT SC, V1876, P621
   Beigman E., 2009, ANN C ASS COMP LING
   Brodley CE, 1999, J ARTIF INTELL RES, V11, P131, DOI 10.1613/jair.606
   Choh Man Teng, 2001, Proceedings of the Fourteenth International Florida Artificial Intelligence Research Society Conference, P269
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everitt BS., 2011, INTRO CLASSIFICATION
   Fegraus A.J.A., 2019, ENV CONSERV PAGE, DOI [10. 1017/S0376892919000298, DOI 10.1017/S0376892919000298]
   Fergus R., 2009, ADV NEURAL INFORM PR
   Fergus R, 2010, P IEEE, V98, P1453, DOI 10.1109/JPROC.2010.2048990
   Goldberg AB., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [DOI 10.2200/S00196ED1V01Y200906AIM006, 10.2200/S00196ED1V01Y200906AIM006]
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Jiang L., 2017, INT C MACH LEARN
   Jindal I, 2016, IEEE DATA MINING, P967, DOI [10.1109/ICDM.2016.0121, 10.1109/ICDM.2016.124]
   Kingma DP., 2014, ADV NEURAL INFORM PR, V2, P3581
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460
   Miranda ALB, 2009, LECT NOTES ARTIF INT, V5572, P417, DOI 10.1007/978-3-642-02319-4_50
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894
   Ren Mengye, 2018, ARXIV180309050
   Rolnick D., 2017, DEEP LEARNING IS ROB
   Sukhbaatar S., 2014, INT C LEARN REPR ICL
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C., 2013, INTRIGUING PROPERTIE
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yuan BD, 2018, IEEE WINT CONF APPL, P757, DOI 10.1109/WACV.2018.00088
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhou B., 2014, ADV NEURAL INFORM PR, P487
   Zhu X., 2002, CMUCALD02107
NR 33
TC 2
Z9 2
U1 3
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAY
PY 2020
VL 57
AR 101063
DI 10.1016/j.ecoinf.2020.101063
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA LG6NY
UT WOS:000528216500003
DA 2022-02-10
ER

PT J
AU Avrin, AC
   Pekins, CE
   Sperry, JH
   Allen, ML
AF Avrin, Alexandra C.
   Pekins, Charles E.
   Sperry, Jinelle H.
   Allen, Maximilian L.
TI Evaluating the efficacy and decay of lures for improving carnivore
   detections with camera traps
SO ECOSPHERE
LA English
DT Article
DE attractant; bait; detection; fatty acid tablets; mesocarnivore;
   predator; sardines
ID CENTRAL NEW-MEXICO; OCCUPANCY; CAPTURE; RATES; BAIT; ATTRACTANTS;
   MOUNTAINS; DENSITY; MODELS; SCENT
AB Abundance and occupancy estimates are essential to wildlife research, but are often hampered by limited detections, especially for cryptic species like carnivores. While scientists can account for limited detections during statistical analyses, increasing detections in the field is the best way to reduce uncertainty. Camera traps are an effective, noninvasive method of monitoring wildlife, and using attractants with camera traps can increase the likelihood of detecting carnivores. We tested two scent lures (sardines and fatty acid tablets) against a control of no lure to determine whether either lure increased detections of six carnivore species, bobcat (Lynx rufus), coyote (Canis latrans), gray fox (Urocyon cinereoargenteus), raccoon (Procyon lotor), striped skunk (Memphitis memphitis), and ringtail (Bassariscus astutus). We also examined how detection of carnivores was affected as the lure decayed over time. We used occupancy modeling for each species to determine whether either lure increased detection probability. We then modeled how lure decay affected carnivore detections and determined the optimal length of deployment using generalized linear mixed models. Sardines increased detections across all carnivores, but also had a high rate of decay and were no different than the control at day 18. Fatty acid tablets decayed more slowly, but were not significantly different from the control at any point. Among species, detections of gray foxes and raccoons increased with both sardines and fatty acid tablets, while detections of ringtails increased only with sardines, and other species did not respond significantly to either lure. Our analysis shows that lures can increase detections of carnivores, but species-specific responses and study objectives must be considered when choosing a lure. These results will allow future researchers to improve the accuracy of abundance and occupancy estimates through increased detections of difficult to study species which ultimately leads to better conservation and management of those species.
C1 [Avrin, Alexandra C.; Sperry, Jinelle H.; Allen, Maximilian L.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin, Urbana, IL 61801 USA.
   [Pekins, Charles E.] US Army Garrison, Fort Hood Nat Resources Management Branch, Bldg 1939 Rod & Gun Club Loop, Ft Hood, TX 76544 USA.
   [Sperry, Jinelle H.] US Army Corps Engineers, Engn Res & Dev Ctr, 2902 Newmark Dr, Champaign, IL 61822 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
RP Avrin, AC (corresponding author), Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin, Urbana, IL 61801 USA.
EM alex.avrin@gmail.com
OI Avrin, Alexandra/0000-0003-4037-1685
CR Allen ML, 2020, BIODIVERS CONSERV, V29, P3591, DOI 10.1007/s10531-020-02039-w
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Bahaa-el-din L, 2015, MAMMAL REV, V45, P63, DOI 10.1111/mam.12033
   Bender LC, 2017, MAMMAL RES, V62, P323, DOI 10.1007/s13364-017-0318-0
   Brooks ME, 2017, R J, V9, P378, DOI 10.32614/RJ-2017-066
   Colyn RB, 2019, BIRD CONSERV INT, V29, P463, DOI 10.1017/S0959270918000400
   Cove M, 2014, HYSTRIX, V25, P113, DOI 10.4404/hystrix-25.2-9945
   Cove MV, 2012, AM MIDL NAT, V168, P456, DOI 10.1674/0003-0031-168.2.456
   Eckrich GH, 1999, STUD AVIAN BIOL, P267
   Edwards Cody W., 1998, Occasional Papers Museum of Texas Tech University, V185, P1
   Evans BE, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0217543
   Ferreira-Rodriguez N, 2019, MAMMAL RES, V64, P155, DOI 10.1007/s13364-018-00414-1
   Findlay MA, 2020, MAMMAL RES, V65, P167, DOI 10.1007/s13364-020-00478-y
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Gerber BD, 2012, POPUL ECOL, V54, P43, DOI 10.1007/s10144-011-0276-3
   Gese EM, 2001, CONSERV BIOL SER, V5, P372
   Gilbert A, 2018, J WILDLIFE DIS, V54, P122, DOI 10.7589/2017-04-073
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Hackett HM, 2007, AM MIDL NAT, V158, P123, DOI 10.1674/0003-0031(2007)158[123:DROESS]2.0.CO;2
   Haidir IA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202876
   Harrison RL, 2013, WEST N AM NATURALIST, V73, P365, DOI 10.3398/064.073.0313
   Hayden TJ, 2000, ECOLOGY AND MANAGEMENT OF COWBIRDS AND THEIR HOSTS, P357
   Hearn AJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151046
   Heinlein BW, 2020, WILDLIFE RES, V47, P338, DOI 10.1071/WR19117
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Kilshaw K, 2015, ORYX, V49, P207, DOI 10.1017/S0030605313001154
   Larson RN, 2015, WEST N AM NATURALIST, V75, P339, DOI 10.3398/064.075.0311
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Melo GL, 2012, IHERINGIA SER ZOOL, V102, P88, DOI 10.1590/S0073-47212012000100012
   Mills D, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216447
   O'Connor KM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175684
   Osorio-Olvera L, 2019, ECOGRAPHY, V42, P1415, DOI 10.1111/ecog.04442
   R Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
   Rocha DG, 2016, J ZOOL, V300, P205, DOI 10.1111/jzo.12372
   ROUGHTON RD, 1982, J WILDLIFE MANAGE, V46, P217, DOI 10.2307/3808424
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x
   Russel L., 2020, EMMEANS ESTIMATED MA
   Schlexer Fredrick V., 2008, P263
   Sebastian-Gonzalez E., 2019, GLOBAL CHANGE BIOL, V25, P3005, DOI [10.1111/gcb.14708, DOI 10.1111/gcb.14708]
   Sebastian-Gonzalez E, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-01439-1
   Stewart FEC, 2019, J WILDLIFE MANAGE, V83, P985, DOI 10.1002/jwmg.21657
   Suarez-Tangil BD, 2017, EUR J WILDLIFE RES, V63, DOI 10.1007/s10344-017-1150-1
   Tanner D, 2012, WILDLIFE SOC B, V36, P594, DOI 10.1002/wsb.160
   Thorn M, 2009, S AFR J WILDL RES, V39, P1, DOI 10.3957/056.039.0101
   Thornton DH, 2015, WILDLIFE RES, V42, P394, DOI 10.1071/WR15092
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   White GC, 2005, WILDLIFE RES, V32, P211, DOI 10.1071/WR03123
   Zielinski WJ, 2006, WILDLIFE SOC B, V34, P1152, DOI 10.2193/0091-7648(2006)34[1152:TEOWAG]2.0.CO;2
NR 51
TC 0
Z9 0
U1 7
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2150-8925
J9 ECOSPHERE
JI Ecosphere
PD AUG
PY 2021
VL 12
IS 8
AR e03710
DI 10.1002/ecs2.3710
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA UG7VB
UT WOS:000689453800017
OA gold
DA 2022-02-10
ER

PT J
AU Bohm, T
   Hofer, H
AF Bohm, Torsten
   Hofer, Heribert
TI Population numbers, density and activity patterns of servals in savannah
   patches of Odzala-Kokoua National Park, Republic of Congo
SO AFRICAN JOURNAL OF ECOLOGY
LA English
DT Article
DE camera-trapping; Central Africa; Crocuta crocuta; Leptailurus serval;
   spatial capture-recapture
ID CAMERA-TRAP; FOREST; CONSEQUENCES; CARNIVORES; LANDSCAPE; ABUNDANCE;
   SELECTION; WILDLIFE; ECOLOGY; HABITAT
AB Despite its wide distribution in continental Africa, the serval (Leptailurus serval Schreber) has received relatively little scientific attention so far. We did camera-trapping in the forest-savannah mosaic of the Odzala-Kokoua National Park, Republic of Congo. The park's savannahs represent the northernmost extension of the savannahs of the Bateke Plateaux, a large ecoregion of open habitat in Central Africa. During 8 months of camera-trapping, we recorded 51 individuals. Almost two-thirds of individuals recorded belonged to the servaline morph, with a pattern mutation of small "freckled" spots. Using maximum likelihood (ML) and Bayesian spatially explicit capture-recapture methods serval density was 7.7-9.8 individuals/100 km(2). ML analyses favoured a model with trap placement and gender as covariates. Serval males were largely nocturnal whereas females were mainly diurnal. Differences in activity patterns were likely related to the occurrence of spotted hyaenas (Crocuta crocuta Erxleben). Spotted hyaenas were highly nocturnal and, consequently, had a higher overlap in activity patterns with male servals. Our study provided the first robust density estimates for this medium-sized carnivore in Central Africa. To achieve sufficient precision in density estimates, we recommend that future studies also include individual and trap placement covariates in analyses.
C1 [Bohm, Torsten; Hofer, Heribert] Leibniz Inst Zoo & Wildlife Res, Dept Evolutionary Ecol, Berlin, Germany.
   [Bohm, Torsten] African Pk, Brazzaville, Rep Congo.
RP Bohm, T (corresponding author), Leibniz Inst Zoo & Wildlife Res, Dept Evolutionary Ecol, Berlin, Germany.; Bohm, T (corresponding author), African Pk, Brazzaville, Rep Congo.
EM torstenb@african-parks.org
RI Hofer, Heribert/AAF-7854-2021
OI Hofer, Heribert/0000-0002-2813-7442; Bohm, Torsten/0000-0001-5446-2321
FU SAVE - Wildlife Conservation Fund
FX We thank the Direction Generale de l'Economie Forestiere and Direction
   de la Faune et des Aires Protegees for the permit to conduct this study
   in the OKNP, L. Lamprecht and D. Zeller, directors of the OKNP, for
   their support during fieldwork. We especially thank our Congolese
   research assistants for their assistance in the field and the staff of
   African Parks, in particular, G. LeFlohic and G.-A. Malanda for their
   support in Congo. We are in particular grateful to the SAVE - Wildlife
   Conservation Fund and F. Weiss for providing financial support. The
   authors further thank Dr. T. Breuer and Dr. R. Sollmann for their
   reviews of an earlier version of this paper.
CR Abernethy KA, 2013, PHILOS T R SOC B, V368, DOI 10.1098/rstb.2012.0303
   Allen J. A., 1924, Bulletin of the American Museum of Natural History, V47, P73
   Anderson K. A., 2002, MODEL SELECTION MULT
   Blake S, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003546
   Bohm T., 2014, SPOTTED HYENA UNPUB
   Bongui A., 2015, AIRES PROTEGEES AFRI, P89
   Bout N., 2006, PARC NATL PLATEAUX B
   Bout N, 2011, AFR J ECOL, V49, P127, DOI 10.1111/j.1365-2028.2010.01240.x
   Breuer T., 2015, STUDYING FOREST ELEP, P14
   Breuer T, 2016, CONSERV BIOL, V30, P1019, DOI 10.1111/cobi.12679
   Di Silvestre I, 2000, AFR J ECOL, V38, P102
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   Dowsett-Lemaire F., 1996, Bulletin du Jardin Botanique National de Belgique, V65, P253, DOI 10.2307/3668453
   Gray TNE, 2012, J WILDLIFE MANAGE, V76, P163, DOI 10.1002/jwmg.230
   Efford M.G., 2018, secr: spatially explicit capture-recapture models. R package version 3.1.6
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   GEERTSEMA AA, 1985, NETH J ZOOL, V35, P527
   Gopalaswamy AM, 2012, METHODS ECOL EVOL, V3, P1067, DOI 10.1111/j.2041-210X.2012.00241.x
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Hecketsweiler P., 1991, PARC NATL ODZALA CON
   Henschel P, 2011, J ZOOL, V285, P11, DOI 10.1111/j.1469-7998.2011.00826.x
   Henschel P, 2014, J MAMMAL, V95, P882, DOI 10.1644/13-MAMM-A-306
   Henschel Philipp, 2009, P206, DOI 10.1002/9781444312034.ch10
   Honer OP, 2005, OIKOS, V108, P544, DOI 10.1111/j.0030-1299.2005.13533.x
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kingdon J., 2004, KINGDON POCKET GUIDE
   Laurance WF, 2015, CURR BIOL, V25, P3202, DOI 10.1016/j.cub.2015.10.046
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Linkie M, 2008, BIOL CONSERV, V141, P2410, DOI 10.1016/j.biocon.2008.07.002
   Maisels F, 1996, SYNTHESIS INFORM PAR
   Maisels F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059469
   Malbrant R., 1949, FAUNE EQUATEUR AFRIC, VII
   Mathot L., 2006, RAPPORT ANN MONITORI
   Mills MGL, 1990, KALAHARI HYAENAS COM, DOI [10.1007/978-94-015-1101-8, DOI 10.1007/978-94-015-1101-8]
   Nasi R, 2011, INT FOREST REV, V13, P355, DOI 10.1505/146554811798293872
   Nowell K., 1996, WILD CATS STATUS SUR
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Olson DM, 2001, BIOSCIENCE, V51, P933, DOI 10.1641/0006-3568(2001)051[0933:TEOTWA]2.0.CO;2
   Pettorelli N, 2010, ANIM CONSERV, V13, P131, DOI 10.1111/j.1469-1795.2009.00309.x
   Pocock RI, 1907, P ZOOL SOC LOND, V1907, P656
   R Core Team, 2015, R LANG ENV STAT COMP
   Ramesh T, 2015, ECOL INDIC, V52, P8, DOI 10.1016/j.ecolind.2014.11.021
   Ramesh T, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2271-y
   Ramesh T, 2016, J MAMMAL, V97, P554, DOI 10.1093/jmammal/gyv201
   Ramesh T, 2016, FOREST ECOL MANAG, V360, P20, DOI 10.1016/j.foreco.2015.10.005
   Ramesh T, 2015, MAMMALIA, V79, P399, DOI 10.1515/mammalia-2014-0053
   Ramesh T, 2013, J MAMMAL, V94, P1460, DOI 10.1644/13-MAMM-A-063.1
   Ray JC, 1997, AFR J ECOL, V35, P237, DOI 10.1111/j.1365-2028.1997.086-89086.x
   Ray JC, 2001, OECOLOGIA, V127, P395, DOI 10.1007/s004420000604
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Simcharoen S, 2008, BIOL CONSERV, V141, P2242, DOI 10.1016/j.biocon.2008.06.015
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Stokes EJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010294
   Thiel C., 2011, THESIS
   Thiel C., 2015, IUCN RED LIST THREAT, DOI [10. 2305/iucn. uk. 2015-2. rlts. t11638a50654625. en, DOI 10.2305/IUCN.UK.2015-2.RLTS.T11638A50654625.EN]
   Treves A, 2010, BIOL CONSERV, V143, P521, DOI 10.1016/j.biocon.2009.11.025
   White G., 1982, CAPTURE RECAPTURE RE
NR 59
TC 7
Z9 7
U1 2
U2 13
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0141-6707
EI 1365-2028
J9 AFR J ECOL
JI Afr. J. Ecol.
PD DEC
PY 2018
VL 56
IS 4
SI SI
BP 841
EP 849
DI 10.1111/aje.12520
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HC1PV
UT WOS:000451574200017
DA 2022-02-10
ER

PT J
AU Jackson, K
   Wilmers, CC
   Wittmer, HU
   Allen, ML
AF Jackson, Kathrina
   Wilmers, Christopher C.
   Wittmer, Heiko U.
   Allen, Maximilian L.
TI First documentation of scent-marking behaviors in striped skunks
   (Mephitis mephitis)
SO MAMMAL RESEARCH
LA English
DT Article
DE Camera trap; Chemical communication; Communication; Mephitis mephitis;
   Novel behaviors; Scent marking; Striped skunk
AB Communication behaviors play a critical role in both an individual's fitness as well as the viability of populations. Solitary animals use chemical communication (i.e., scent marking) to locate mates and defend their territory to increase their own fitness. Previous research has suggested that striped skunks (Mephitis mephitis) do not perform scent-marking behaviors, despite being best known for using odor as chemical defense. We used video camera traps to document behaviors exhibited by striped skunks at a remote site in coastal California between January 2012 and April 2015. Our camera traps captured a total of 71 visits by striped skunks, the majority of which (73%) included a striped skunk exhibiting scent-marking behaviors. Overall, we documented 8 different scent-marking behaviors. The most frequent behaviors we documented were cheek rubbing (45.1%), investigating (40.8%), and claw marking (35.2%). The behaviors exhibited for the longest durations on average were grooming (x = 34. 4 s) and investigating (x = 21.2 s). Although previous research suggested that striped skunks do not scent mark, we documented that at least some populations do and our findings suggest that certain sites are used for communication via scent marking. Our study further highlights how camera traps allow researchers to discover previously undocumented animal behaviors.
C1 [Jackson, Kathrina] Univ Illinois, Dept Anim Sci, 1816 S Oak St, Champaign, IL 61820 USA.
   [Wilmers, Christopher C.] Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Environm Studies Dept, Santa Cruz, CA 95064 USA.
   [Wittmer, Heiko U.] Victoria Univ Wellington, Sch Biol Sci, Box 600,PO 6140, Wellington, New Zealand.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
RP Jackson, K (corresponding author), Univ Illinois, Dept Anim Sci, 1816 S Oak St, Champaign, IL 61820 USA.
EM kj11@illinois.edu
RI Wittmer, Heiko U/D-4172-2015
OI Wittmer, Heiko U/0000-0002-8861-188X
FU NSFNational Science Foundation (NSF) [0963022, 1255913]; Gordon and
   Betty Moore FoundationGordon and Betty Moore Foundation; University of
   California at Santa Cruz; Illinois Natural History Survey; University of
   Illinois Champaign-Urbana
FX Funding was provided by NSF Grants 0963022 and 1255913, the Gordon and
   Betty Moore Foundation, the University of California at Santa Cruz, the
   Illinois Natural History Survey, and the University of Illinois
   Champaign-Urbana.
CR Allen ML, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2366-5
   Allen ML, 2017, J ETHOL, V35, P13, DOI 10.1007/s10164-016-0492-6
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep27257
   Allen ML, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139087
   Allen ML, 2015, J ETHOL, V33, P9, DOI 10.1007/s10164-014-0418-0
   Allen ML, 2014, BEHAVIOUR, V151, P819, DOI 10.1163/1568539X-00003173
   BAILEY TN, 1974, J WILDLIFE MANAGE, V38, P435, DOI 10.2307/3800874
   Baldwin RA, 2015, UC ANR PUBLICATION, V74118
   Fisher KA, 2018, ANIM BEHAV, V143, P25, DOI 10.1016/j.anbehav.2018.06.023
   Gosling LM, 2001, ADV STUD BEHAV, V30, P169, DOI 10.1016/S0065-3454(01)80007-3
   Krofel M, 2017, MAMM BIOL, V87, P36, DOI 10.1016/j.mambio.2017.05.003
   Lariviere S, 1998, J APPL ECOL, V35, P207, DOI 10.1046/j.1365-2664.1998.00301.x
   Lariviere S, 1998, J WILDLIFE MANAGE, V62, P199, DOI 10.2307/3802279
   Lariviere S, 1996, ETHOLOGY, V102, P986
   MELLEN JD, 1993, AM ZOOL, V33, P151
   NAMS VO, 1991, BEHAVIOUR, V119, P267, DOI 10.1163/156853991X00472
   R Core Team, 2017, R LANG ENV STAT COMP
   RALLS K, 1971, SCIENCE, V171, P443, DOI 10.1126/science.171.3970.443
   Russell AF, 2003, BEHAV ECOL, V14, P486, DOI 10.1093/beheco/arg022
   SMITH JLD, 1989, ANIM BEHAV, V37, P1
   Sokal R.R., 2012, BIOMETRY PRINCIPLES, V4
   Steiger S, 2011, P ROY SOC B-BIOL SCI, V278, P970, DOI 10.1098/rspb.2010.2285
   Taylor AP, 2015, BEHAVIOUR, V152, P1097, DOI 10.1163/1568539X-00003270
   Verts B. J., 1967, BIOL STRIPED SKUNK
   Vogt K, 2014, BEHAV PROCESS, V106, P98, DOI 10.1016/j.beproc.2014.04.017
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wooldridge RL, 2019, J MAMMAL, V100, P445, DOI 10.1093/jmammal/gyz055
NR 28
TC 0
Z9 0
U1 4
U2 7
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2199-2401
EI 2199-241X
J9 MAMMAL RES
JI Mammal Res.
PD APR
PY 2021
VL 66
IS 2
BP 399
EP 404
DI 10.1007/s13364-021-00565-8
EA APR 2021
PG 6
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA RQ2DF
UT WOS:000639073000001
DA 2022-02-10
ER

PT J
AU Allen, ML
   Peterson, B
   Krofel, M
AF Allen, Maximilian L.
   Peterson, Brittany
   Krofel, Miha
TI No respect for apex carnivores: Distribution and activity patterns of
   honey badgers in the Serengeti
SO MAMMALIAN BIOLOGY
LA English
DT Article
DE Abundance; Activity patterns; Distribution; Honey badger; Interspecific
   interactions; Mellivora capensis
ID SCENT-MARKING; COMMUNICATION BEHAVIORS; MELLIVORA-CAPENSIS; CAMERA TRAP;
   ABUNDANCE; DENSITY; MOUNTAINS; MODELS; LYNX; SIZE
AB Honey badgers are cryptic carnivores that occur at low densities and range across large areas. The processes behind site-level honey badger abundance and detection rates are poorly understood, and there are conflicting results about their avoidance of larger carnivores from different regions. We used data from 224 camera traps set up in the Serengeti National Park, Tanzania to evaluate patterns in detection rates, spatial distribution, and activity patterns of honey badgers. Our top models showed that the relative abundance of larger carnivores (e.g., African lions, Panthera leo, and spotted hyenas, Crocuta crocuta) was important, but surprisingly was positively related to honey badger distribution. These results suggest that honey badgers were not avoiding larger carnivores, but were instead potentially seeking out similar habitats and niches. We also found no temporal avoidance of larger carnivores. Honey badgers exhibited seasonal variation in activity patterns, being active at all times during the wet season with peaks during crepuscular hours, but having a strong nocturnal peak during the dry season. Our detection rates of honey badgers at individual camera traps were low ( 3402 trap nights/detection), but our study shows that with adequate effort camera traps can be used successfully as a research tool for this elusive mustelid. (c) 2018 Deutsche Gesellschaft fur Saugetierkunde. Published by Elsevier GmbH. All rights reserved.
C1 [Allen, Maximilian L.; Peterson, Brittany] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Wildlife Ecol Res Grp, Vena Pot 83, SI-1000 Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM maximilian.allen@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X; Krofel, Miha/0000-0002-2010-5219
CR Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2015, J ETHOL, V33, P9, DOI 10.1007/s10164-014-0418-0
   Baha-El-Din Laila, 2013, Small Carnivore Conservation, V48, P19
   Balme GA, 2017, BEHAV ECOL, V28, P1348, DOI 10.1093/beheco/arx098
   Begg C. M., 2001, THESIS
   Begg CM, 2005, J ZOOL, V265, P23, DOI 10.1017/S0952836904005989
   Begg CM, 2003, J ZOOL, V260, P301, DOI 10.1017/S0952836903003789
   Bird Tania L. F., 2013, Small Carnivore Conservation, V48, P47
   BROWN JH, 1984, AM NAT, V124, P255, DOI 10.1086/284267
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Do Linh San, 2016, BADGERS SYSTEMATICS, P161
   Do Linh San E, 2016, IUCN RED LIST THREAT
   Durant SM, 2010, J ANIM ECOL, V79, P1012, DOI 10.1111/j.1365-2656.2010.01717.x
   Durant SM, 1998, J ANIM ECOL, V67, P370, DOI 10.1046/j.1365-2656.1998.00202.x
   Estes R.D., 1992, BEHAV GUIDE AFRICAN
   Greengrass Elizabeth J., 2013, Small Carnivore Conservation, V48, P30
   Hayward MW, 2009, S AFR J WILDL RES, V39, P109, DOI 10.3957/056.039.0207
   Heurich M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114143
   Hopcraft JGC, 2005, J ANIM ECOL, V74, P559, DOI 10.1111/j.1365-2656.2005.00955.x
   Johnson DDP, 2000, MAMMAL REV, V30, P171, DOI 10.1046/j.1365-2907.2000.00066.x
   Lynam AJ, 2013, RAFFLES B ZOOL, V61, P407
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Newsome TM, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15469
   NORTON-GRIFFITHS M, 1975, East African Wildlife Journal, V13, P347, DOI 10.1111/j.1365-2028.1975.tb00144.x
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Proulx G., 2016, BADGERS SYSTEMATICS, P31
   R Core Team, 2017, R LANG ENV STAT COMP
   Rabinowitz D. S., 1986, BIOL ASPECTS RARE PL, P182
   Ramesh T, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2271-y
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Sagarin RD, 2006, TRENDS ECOL EVOL, V21, P524, DOI 10.1016/j.tree.2006.06.008
   Sinclair A. R. E., 1979, SERENGETI DYNAMICS E, P31
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Vogt K, 2014, BEHAV PROCESS, V106, P98, DOI 10.1016/j.beproc.2014.04.017
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wegge P, 2004, ANIM CONSERV, V7, P251, DOI 10.1017/S1367943004001441
NR 39
TC 15
Z9 16
U1 8
U2 59
PU ELSEVIER GMBH, URBAN & FISCHER VERLAG
PI JENA
PA OFFICE JENA, P O BOX 100537, 07705 JENA, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
PD MAR
PY 2018
VL 89
BP 90
EP 94
DI 10.1016/j.mambio.2018.01.001
PG 5
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA FY0ZA
UT WOS:000426540000012
OA Green Published
DA 2022-02-10
ER

PT C
AU Zotin, AG
   Proskurin, AV
AF Zotin, A. G.
   Proskurin, A., V
BE Zheltov, S
   Knyaz, V
TI ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING
   CONDITIONS
SO INTERNATIONAL WORKSHOP ON PHOTOGRAMMETRIC AND COMPUTER VISION TECHNIQUES
   FOR VIDEO SURVEILLANCE, BIOMETRICS AND BIOMEDICINE
SE International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences
LA English
DT Proceedings Paper
CT International Workshop on Photogrammetric and Computer Vision Techniques
   for Video Surveillance, Biometrics and Biomedicine
CY MAY 13-15, 2019
CL Moscow, RUSSIA
DE Animal Detection; Background Modeling; Camera Traps; MSR Algorithm
ID BACKGROUND SUBTRACTION; TRACKING
AB Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection.
C1 [Zotin, A. G.; Proskurin, A., V] Reshetnev Siberian State Univ Sci & Technol, Inst Comp Sci & Telecommun, Krasnoyarsk, Russia.
RP Proskurin, AV (corresponding author), Reshetnev Siberian State Univ Sci & Technol, Inst Comp Sci & Telecommun, Krasnoyarsk, Russia.
EM zotin@sibsau.ru; proskurin.av.wof@gmail.com
RI Zotin, Aleksandr G/Q-3028-2018
OI Zotin, Aleksandr G/0000-0001-9954-9826
FU Russian Foundation for Basic Research, Government of Krasnoyarsk
   Territory, Krasnoyarsk Regional Fund of Science [18-47-240001]
FX The reported study was funded by Russian Foundation for Basic Research,
   Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of
   Science, to the research project 18-47-240001.
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bouwmans T., 2019, ABS190103577 CORR
   Bouwmans T., 2010, HDB PATTERN RECOGNIT, P181
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Chen ML, 2014, LECT NOTES COMPUT SC, V8695, P521, DOI 10.1007/978-3-319-10584-0_34
   Favorskaya M, 2016, SMART INNOV SYST TEC, V55, P121, DOI 10.1007/978-3-319-39345-2_11
   Gonzalez R. C, 2008, DIGITAL IMAGE PROCES, V3rd
   Heikkila M., 2004, BRIT MACH VIS C, P187, DOI DOI 10.5244/C.18.21
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Liao SJ, 2017, PROC SPIE, V10605, DOI 10.1117/12.2295105
   Liu HJ, 2016, CHIN CONT DECIS CONF, P3712, DOI 10.1109/CCDC.2016.7531629
   Manzanera A, 2007, LECT NOTES COMPUT SC, V4756, P42
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Castelblanco LP, 2017, PROC SPIE, V10341, DOI 10.1117/12.2268732
   Raju A., 2013, International Journal of Signal Processing, Image Processing and Pattern Recognition, V6, P353, DOI [10.14257/ijsip.2013.6.5.31, DOI 10.14257/IJSIP.2013.6.5.31]
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Van Droogenbroeck M., 2014, VIBE DISRUPTIVE METH
   Wang B, 2014, IEEE COMPUT SOC CONF, P401, DOI 10.1109/CVPRW.2014.64
   Wang HZ, 2007, PATTERN RECOGN, V40, P1091, DOI 10.1016/j.patcog.2006.05.024
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zotin Alexander, 2018, Procedia Computer Science, V131, P6, DOI 10.1016/j.procs.2018.04.179
NR 26
TC 1
Z9 1
U1 2
U2 2
PU INTL SOC PHOTOGRAMMETRY & REMOTE SENSING-ISPRS
PI HANNOVER
PA LEIBNIZ UNIV HANNOVER, INST PHOTOGRAMMETRY  & GEOINFORMATION, NIENBURGER
   STR 1, HANNOVER, HANNOVER, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PY 2019
VL 42-2
IS W12
BP 249
EP 257
DI 10.5194/isprs-archives-XLII-2-W12-249-2019
PG 9
WC Computer Science, Artificial Intelligence; Mathematical & Computational
   Biology; Remote Sensing; Imaging Science & Photographic Technology;
   Radiology, Nuclear Medicine & Medical Imaging
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematical & Computational Biology; Remote Sensing;
   Imaging Science & Photographic Technology; Radiology, Nuclear Medicine &
   Medical Imaging
GA BQ2ZX
UT WOS:000582728500039
OA Green Submitted, gold
DA 2022-02-10
ER

PT C
AU Fassold, H
AF Fassold, Hannes
BE Bebis, G
   Boyle, R
   Parvin, B
   Koracin, D
   Ushizima, D
   Chai, S
   Sueda, S
   Lin, X
   Lu, A
   Thalmann, D
   Wang, C
   Xu, P
TI Automatic Camera Path Generation from 360 degrees Video
SO ADVANCES IN VISUAL COMPUTING, ISVC 2019, PT I
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 14th International Symposium on Visual Computing (ISVC)
CY OCT 07-09, 2019
CL NV
SP UNR Comp Vis Lab, Desert Res Inst, NASA, Ford, HP, Intel, BAE Syst, Delphi, Mitsubishi Elect Res Labs, GE, Toyota
DE 360 degrees video; Object detection; Tracking; VR; Storytelling
AB Omnidirectional (360 degrees) video is a novel media format, rapidly becoming adopted in media production and consumption as part of today's ongoing virtual reality revolution. The goal of automatic camera path generation is to calculate automatically a visually interesting camera path from a 360 degrees video in order to provide a traditional, TV-like consumption experience. In this work, we describe our algorithm for automatic camera path generation, based on extraction of the information of the scene objects with deep learning based methods.
C1 [Fassold, Hannes] JOANNEUM RES, DIGITAL Inst Informat & Commun Technol, Graz, Austria.
RP Fassold, H (corresponding author), JOANNEUM RES, DIGITAL Inst Informat & Commun Technol, Graz, Austria.
EM hannes.fassold@joanneum.at
FU European Union's Horizon 2020 research and innovation programme [761934]
FX This work has received funding from the European Union's Horizon 2020
   research and innovation programme, grant n. 761934, Hyper360 ("Enriching
   360 media with 3D storytelling and personalisation elements"). Thanks to
   Rundfunk Berlin-Brandenburg (RBB) for providing the 360. video content.
CR Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Galvane Q., 2017, EUR WORKSH INT CIN E, DOI [10.2312/wiced.20171065, DOI 10.2312/WICED.20171065]
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Kuhn H. W., 1955, NAV RES LOG, V2, P83, DOI 10.1002/nav.3800020109
   Redmon J., 2018, ABS180402767 CORR
   Su Y.C., 2017, EUR WORKSH INT CIN E, DOI [10.2312/wiced.20171071, DOI 10.2312/WICED.20171071]
   Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150
   Sun N, 2010, 2010 INTERNATIONAL SYMPOSIUM ON VLSI DESIGN AUTOMATION AND TEST (VLSI-DAT), P121, DOI 10.1109/VDAT.2010.5496706
   Suzuki T, 2018, IEEE SYS MAN CYBERN, P2079, DOI 10.1109/SMC.2018.00358
   Truong A., 2018, HUMAN COMPUTER INTER
   Werlberger M., 2009, P BRIT MACH VIS C BM
NR 11
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-33720-9; 978-3-030-33719-3
J9 LECT NOTES COMPUT SC
PY 2020
VL 11844
BP 505
EP 514
DI 10.1007/978-3-030-33720-9_39
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ2NC
UT WOS:000582481300039
DA 2022-02-10
ER

PT C
AU Favorskaya, M
   Pakhirka, A
AF Favorskaya, Margarita
   Pakhirka, Andrey
BE Rudas, IJ
   Janos, C
   Toro, C
   Botzheim, J
   Howlett, RJ
   Jain, LC
TI Animal species recognition in the wildlife based on muzzle and shape
   features using joint CNN
SO KNOWLEDGE-BASED AND INTELLIGENT INFORMATION & ENGINEERING SYSTEMS (KES
   2019)
SE Procedia Computer Science
LA English
DT Proceedings Paper
CT 23rd KES International Conference on Knowledge-Based and Intelligent
   Information and Engineering Systems (KES)
CY SEP 04-06, 2019
CL Budapest, HUNGARY
SP KES Int
DE Animal recognition; shape features; muzzle features;deep learning
ID NETWORK
AB Monitoring of animal behavior in the wild supposes the reliable techniques for their species recognition using, mainly, visual data captured by camera traps. In this paper, we propose to extent Convolutional Neural Network (CNN) VGG by three branches, two of which are VGG16 for the muzzle and part of shape recognition and one is VGG19 for the whole shape recognition. A necessity of such branched CNN structure is caused by great variety of the animal poses fixed by a camera trap. Also, here we met with an objective problem of the unbalanced dataset due to different behavior of animals in nature. Preliminary categorization procedure of images helps to obtain better recognition results. Experiments were conducted using the dataset obtained from Ergaki national park, Krasnoyarsky Kray, Russia, 2012-2018. The joint CNN shows good accuracy results on the balanced dataset achieving 80.6% Top-1 and 94.1% Top-5, respectively. In the case of the unbalanced training dataset, we obtained 38.7% Top-1 and 54.8% Top-5 accuracy. (C) 2019 The Authors. Published by Elsevier B.V.
C1 [Favorskaya, Margarita; Pakhirka, Andrey] Reshetnev Siberian State Univ Sci & Technol, 31 Krasnoyarsky Rabochy Ave, Krasnoyarsk 660037, Russia.
RP Favorskaya, M (corresponding author), Reshetnev Siberian State Univ Sci & Technol, 31 Krasnoyarsky Rabochy Ave, Krasnoyarsk 660037, Russia.
EM favorskaya@sibsau.ru
RI Favorskaya, Margarita N./N-1840-2017
OI Favorskaya, Margarita/0000-0002-2181-0454
FU Russian Foundation for Basic Research, Government of Krasnoyarsk
   Territory, Krasnoyarsk Regional Fund of Science [18-47-240001]
FX The reported study was funded by Russian Foundation for Basic Research,
   Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of
   Science to the research project No. 18-47-240001.
CR Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Duyck J, 2015, PATTERN RECOGN, V48, P1059, DOI 10.1016/j.patcog.2014.07.017
   [Фаворская М.Н. Favorskaya M.N.], 2018, [Информационно-управляющие системы, Informatsionno-upravlyayushchie sistemy], P35, DOI 10.31799/1684-8853-2018-6-35-45
   Favorskaya Margarita N., 2019, P 11 KES INT C INT D
   Giraldo-Zuluaga J.-H., 2017, VISUAL COMPUT, P1
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Huang G, 2018, ANN OPER RES, P1, DOI DOI 10.1007/S10479-018-2973-1
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Li S, 2018, PATTERN RECOGN, V81, P294, DOI 10.1016/j.patcog.2018.03.035
   Lin MH, 2014, ASIA-PAC EDUC RES, V23, P577, DOI 10.1007/s40299-013-0131-8
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   TensorFlow, IMAGE RECOGNITION
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zotin Alexander, 2019, 3 INT ISPRS WORKSH P
NR 18
TC 5
Z9 5
U1 2
U2 4
PU ELSEVIER
PI AMSTERDAM
PA Radarweg 29, PO Box 211, AMSTERDAM, NETHERLANDS
SN 1877-0509
J9 PROCEDIA COMPUT SCI
PY 2019
VL 159
BP 933
EP 942
DI 10.1016/j.procs.2019.09.260
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Software Engineering; Computer Science,
   Theory & Methods; Engineering, Manufacturing; Engineering, Electrical &
   Electronic; Operations Research & Management Science
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Operations Research & Management Science
GA BP9ZA
UT WOS:000571151500096
OA gold
DA 2022-02-10
ER

PT J
AU Torney, CJ
   Dobson, AP
   Borner, F
   Lloyd-Jones, DJ
   Moyer, D
   Maliti, HT
   Mwita, M
   Fredrick, H
   Borner, M
   Hopcraft, JGC
AF Torney, Colin J.
   Dobson, Andrew P.
   Borner, Felix
   Lloyd-Jones, David J.
   Moyer, David
   Maliti, Honori T.
   Mwita, Machoke
   Fredrick, Howard
   Borner, Markus
   Hopcraft, J. Grant C.
TI Assessing Rotation-Invariant Feature Classification for Automated
   Wildebeest Population Counts
SO PLOS ONE
LA English
DT Article
AB Accurate and on-demand animal population counts are the holy grail for wildlife conservation organizations throughout the world because they enable fast and responsive adaptive management policies. While the collection of image data from camera traps, satellites, and manned or unmanned aircraft has advanced significantly, the detection and identification of animals within images remains a major bottleneck since counting is primarily conducted by dedicated enumerators or citizen scientists. Recent developments in the field of computer vision suggest a potential resolution to this issue through the use of rotation-invariant object descriptors combined with machine learning algorithms. Here we implement an algorithm to detect and count wildebeest from aerial images collected in the Serengeti National Park in 2009 as part of the biennial wildebeest count. We find that the per image error rates are greater than, but comparable to, two separate human counts. For the total count, the algorithm is more accurate than both manual counts, suggesting that human counters have a tendency to systematically over or under count images. While the accuracy of the algorithm is not yet at an acceptable level for fully automatic counts, our results show this method is a promising avenue for further research and we highlight specific areas where future research should focus in order to develop fast and accurate enumeration of aerial count data. If combined with a bespoke image collection protocol, this approach may yield a fully automated wildebeest count in the near future.
C1 [Torney, Colin J.] Univ Exeter, Ctr Math & Environm, Penryn Campus, Penryn, Cornwall, England.
   [Dobson, Andrew P.] Princeton Univ, Dept Ecol & Evolutionary Biol, Princeton, NJ 08544 USA.
   [Borner, Felix] Serengeti Natl Pk, Frankfurt Zool Soc, Seronera, Tanzania.
   [Lloyd-Jones, David J.] POB 1272, Iringa, Tanzania.
   [Moyer, David] Field Museum Nat Hist, Integrated Res Ctr, 1400 S Lake Shore Dr, Chicago, IL 60605 USA.
   [Maliti, Honori T.; Mwita, Machoke] Tanzania Wildlife Res Inst, POB 661, Arusha, Tanzania.
   [Fredrick, Howard] Tanzania Conservat Resource Ctr, Arusha, Tanzania.
   [Borner, Markus; Hopcraft, J. Grant C.] Univ Glasgow, Inst Biodivers Anim Hlth & Comparat Med, Glasgow, Lanark, Scotland.
   [Borner, Markus; Hopcraft, J. Grant C.] Univ Glasgow, Boyd Orr Ctr Populat & Ecosyst Hlth, Glasgow, Lanark, Scotland.
RP Torney, CJ (corresponding author), Univ Exeter, Ctr Math & Environm, Penryn Campus, Penryn, Cornwall, England.
EM colin.j.torney@gmail.com
OI Lloyd-Jones, David/0000-0001-7880-5659
FU James S. McDonnell Foundation; Lord Kelvin Adam Smith Fellowship;
   British Ecological Society; European UnionEuropean Commission [641918];
   NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCESUnited States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USANIH National Center for Advancing Translational Sciences (NCATS)
   [UL1TR001422] Funding Source: NIH RePORTER
FX CJT is supported by a Complex Systems Scholar Award from the James S.
   McDonnell Foundation. JGCH is supported by a Lord Kelvin Adam Smith
   Fellowship, funding from the British Ecological Society and the European
   Union's Horizon 2020 research and innovation programme under grant
   agreement No 641918 AfricanBioServices. The funders had no role in study
   design, data collection and analysis, decision to publish, or
   preparation of the manuscript.
CR Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005
   BAJZAK D, 1990, WILDLIFE SOC B, V18, P125
   Bidder OR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088609
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Bradski G, 2000, OPENCV LIB
   BRIGGS KT, 1985, J WILDLIFE MANAGE, V49, P405, DOI 10.2307/3801542
   CAUGHLEY G, 1976, J WILDLIFE MANAGE, V40, P290, DOI 10.2307/3800428
   Caughley G, 1979, SAMPLING TECHNIQUES, P15
   Chen Ch., 2010, HDB PATTERN RECOGNIT, V27
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   EVANS CD, 1966, J WILDLIFE MANAGE, V30, P767, DOI 10.2307/3798283
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   GODDARD J, 1969, East African Wildlife Journal, V7, P105
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Klockner A, 2012, PARALLEL COMPUT, V38, P157, DOI 10.1016/j.parco.2011.09.001
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Laliberte AS, 2003, WILDLIFE SOC B, V31, P362
   Lin Y, 2011, IEEE GEOSCI REMOTE S, V8, P426, DOI 10.1109/LGRS.2010.2079913
   Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z
   MARSH H, 1989, J WILDLIFE MANAGE, V53, P1017, DOI 10.2307/3809604
   McNeill S, 2011, INT GEOSCI REMOTE SE, P4312, DOI 10.1109/IGARSS.2011.6050185
   Michel P., 2003, P 5 INT C MULT INT, P258, DOI DOI 10.1145/958432.958479
   MORTON SR, 1990, AUST J ECOL, V15, P307, DOI 10.1111/j.1442-9993.1990.tb01035.x
   Mosbech A, 1999, ARCTIC, V52, P188
   NORTON-GRIFFITHS M, 1973, East African Wildlife Journal, V11, P135
   Oritsland T, 1995, DEV MAR BIO, V4, P77
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Perez-Escudero A, 2014, NAT METHODS, V11, P743, DOI [10.1038/NMETH.2994, 10.1038/nmeth.2994]
   Russell J., 1996, Rangifer, V16, P319
   Sherley Richard B., 2010, Endangered Species Research, V11, P101, DOI 10.3354/esr00267
   Sirmacek B, 2012, MAN RES LTD PLAN 6 B
   ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006
   Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
   XU L, 1992, IEEE T SYST MAN CYB, V22, P418, DOI 10.1109/21.155943
   Yang Z, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115989
   [No title captured]
NR 36
TC 15
Z9 16
U1 5
U2 28
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD MAY 26
PY 2016
VL 11
IS 5
AR e0156342
DI 10.1371/journal.pone.0156342
PG 10
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA DN2GQ
UT WOS:000376882500119
PM 27227888
OA Green Submitted, Green Published, Green Accepted, gold
DA 2022-02-10
ER

PT J
AU Schindler, F
   Steinhage, V
AF Schindler, Frank
   Steinhage, Volker
TI Identification of animals and recognition of their actions in wildlife
   videos using deep learning techniques
SO ECOLOGICAL INFORMATICS
LA English
DT Article
ID IMAGES
AB Biodiversity crisis has continued to accelerate. Studying animal distribution, movement and behaviour is of critical importance to address environmental challenges such as spreading of diseases, invasive species, climate and land-use change. Camera traps are an appropriate technique for continuous animal monitoring in an automated 24/7/52 documentation. This study shows a proof-of-concept for an end-to-end pipeline to detect and classify animals and their behaviour in video clips. Video clips are captured with 8 frames per second by camera traps using infrared cameras and infrared flash-lights. The clips show deer, boars, foxes and hares - mostly at night time. Our approach shows an average precision of 63.8% for animal detection and identification. For action recognition the achieved accuracies range between 88.4% and 94.1%.
C1 [Schindler, Frank; Steinhage, Volker] Univ Bonn, Dept Comp Sci 4, Endenicher Allee 19A, D-53115 Bonn, Germany.
RP Schindler, F (corresponding author), Univ Bonn, Dept Comp Sci 4, Endenicher Allee 19A, D-53115 Bonn, Germany.
EM schindl@cs.uni-bonn.de
CR Barz B., 2018, ARXIV181204418
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Burghardt T, 2006, IEE P-VIS IMAGE SIGN, V153, P305, DOI 10.1049/ip-vis:20050052
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen RL, 2019, ECOL EVOL, V9, P9453, DOI 10.1002/ece3.5410
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Follmann P., 2018, Pattern Recognition and Image Analysis, V28, P605, DOI 10.1134/S1054661818040107
   Furnari A, 2018, J VIS COMMUN IMAGE R, V52, P1, DOI 10.1016/j.jvcir.2018.01.019
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI 10.1109/TPAMI.2018.2844175
   Hu YT, 2017, ADV NEUR IN, V30
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jung A.B., 2020, IMGAUG
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Paszke A, 2019, ADV NEUR IN, V32
   Pawara P., 2016, P 2016 IEEE S SER CO, P1, DOI 10.1109/ SSCI.2016.7850111
   Raman B, 2018, P 2 INT C COMP VIS I
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Schiele B., COMPUTER VISION ECCV
   Sudhakaran S, 2020, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR42600.2020.00118
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   United Nations, 1992, CONV BIOL DIV TEXT A
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vora A., 2018, COMPUTER VISION PATT, P34
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xiong Y., 2017
   Xu N, 2019, IEEE INT C COMP VIS
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 41
TC 1
Z9 1
U1 6
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAR
PY 2021
VL 61
AR 101215
DI 10.1016/j.ecoinf.2021.101215
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA RC2AC
UT WOS:000632605400002
DA 2022-02-10
ER

PT C
AU Beery, S
   Van Horn, G
   Perona, P
AF Beery, Sara
   Van Horn, Grant
   Perona, Pietro
BE Ferrari, V
   Hebert, M
   Sminchisescu, C
   Weiss, Y
TI Recognition in Terra Incognita
SO COMPUTER VISION - ECCV 2018, PT XVI
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 15th European Conference on Computer Vision (ECCV)
CY SEP 08-14, 2018
CL Munich, GERMANY
DE Recognition; Transfer learning; Domain adaptation; Context; Dataset;
   Benchmark
AB It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https:// beerys.github.io/CaltechCameraTraps/)
C1 [Beery, Sara; Van Horn, Grant; Perona, Pietro] CALTECH, Pasadena, CA 91125 USA.
RP Beery, S (corresponding author), CALTECH, Pasadena, CA 91125 USA.
EM sbeery@caltech.edu; gvanhorn@caltech.edu; perona@caltech.edu
FU NSFGRFPNational Science Foundation (NSF)NSF - Office of the Director
   (OD) [1745301]; AWS Research Grant
FX We would like to thank the USGS and NPS for providing data. This work
   was supported by NSFGRFP Grant No. 1745301, the views are those of the
   authors and do not necessarily reflect the views of the NSF. Compute
   time was provided by an AWS Research Grant.
CR [Anonymous], 2017, IEEE ICC
   Babaee M., 2017, ARXIV170201731
   Benedek C, 2008, INT C PATT RECOG, P1686
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Busto P. P., 2017, IEEE INT C COMP VIS, V1
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Chen Y., 2017, ARXIV171111556
   Csurka G., 2017, ARXIV170205374
   Deng J., 2009, CVPR09
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151
   Giraldo-Zuluaga J.-H., 2017, VISUAL COMPUT, P1
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006
   He, 2017, 2017 IEEE INT S CIRC, P1, DOI DOI 10.1109/ISCAS.2017.8050762
   Hoffman J., 2016, ARXIV161202649
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krasin I., 2017, OPENIMAGES PUBLIC DA
   Kumar N, 2012, 12 EUR C COMP VIS EC
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin KH, 2014, IEEE IMAGE PROC, P1125, DOI 10.1109/ICIP.2014.7025224
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Miyake, 1982, COMPETITION COOPERAT, P267, DOI DOI 10.1007/978-3-642-46466-9_18
   Murphy GL., 2004, BIG BOOK CONCEPTS
   Nilsback M.E., 2006, P 2006 IEEE COMP SOC, P1447, DOI DOI 10.1109/CVPR.2006.42
   Norouzzadeh MS, 2017, ARXIV170305830
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151
   Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29
   Poplin R., 2018, NAT BIOMED ENG, V1
   Raj A., 2015, ARXIV150705578
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Schaller RR, 1997, IEEE SPECTRUM, V34, P52, DOI 10.1109/6.591665
   Spain M, 2008, LECT NOTES COMPUT SC, V5302, P523, DOI 10.1007/978-3-540-88682-2_40
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Sun B., 2014, BMVC, V1, P3
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang K., 2012, ADV NEURAL INFORM PR, V25, P638
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   van Horn G., MERLIN BIRD ID SMART
   Van Horn G., 2018, COMPUT VIS PATTERN R
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Van Horn Grant, 2017, ARXIV170706642
   Van Horn Grant, 2017, ARXIV170901450
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Welinder P, 2013, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2013.419
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Xu JL, 2014, IEEE T PATTERN ANAL, V36, P2367, DOI 10.1109/TPAMI.2014.2327973
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhan Y, 2017, IEEE GEOSCI REMOTE S, V14, P1845, DOI 10.1109/LGRS.2017.2738149
   Zhang Y., 2017, P IEEE INT C COMP VI
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 57
TC 33
Z9 33
U1 3
U2 4
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-01270-0; 978-3-030-01269-4
J9 LECT NOTES COMPUT SC
PY 2018
VL 11220
BP 472
EP 489
DI 10.1007/978-3-030-01270-0_28
PG 18
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BQ5DJ
UT WOS:000603403700028
OA Green Submitted, Green Accepted
DA 2022-02-10
ER

PT J
AU Aximoff, I
   Neto, EP
   de Paula, W
   Hofmann, GS
   Keuroghlian, A
   Jorge, ML
   Lima, E
   Barquero, G
AF Aximoff, Izar
   Neto, Ennio Painkow
   de Paula, Wlainer
   Hofmann, Gabriel Selbach
   Keuroghlian, Alexine
   Jorge, Maria Luisa
   Lima, Edson
   Barquero, Gonzalo
TI Sticking out in a herd? Records of anomalous pigmentation in a social
   herd- forming ungulate (Tayassu pecari)
SO NORTH-WESTERN JOURNAL OF ZOOLOGY
LA English
DT Article
DE mammals; anomalous pigmentation; white-lipped peccary; camera traps;
   Brazilian biomes
ID HOME-RANGE; 1ST RECORDS; PECCARY; ALBINISM; MAMMALS; COLOR; BEHAVIOR;
   TAJACU; MONKEY; AREA
AB Piebaldism, leucism or albinism are different types of pigmentation anomalies related to the excess or deficit of pigmentation in some regions or in the whole body of an animal. It is extremely rare in peccaries and has never been reported for Tayassu, a genus endemic to Neotropical region. Tayassu pecari, known as white-lipped peccaries (WLP's), have behaviors and ecology well documented in literature, but there is no published scientific information about pigmentation anomalies. Here, we report the first records of anomalous individuals (leucism and piebaldism) for WLP's in locations of Central Brazil. During fieldwork for mammal survey (2010-2020) search for individuals with pigmentation anomalies were carried out in ten sites spread across four Brazilian biomes: Atlantic Forest, Cerrado, Amazon Forest, and Pantanal. Thirteen individuals were registered throughout eight locations in the last three biomes. All these individuals were part of a herd in which most individuals occurred with normal pigmentation. The Cerrado was the most representative biome in which individuals with abnormal pigmentation were recorded (46.2%) followed by the Amazon (30.8 %) and the Pantanal (23.0%). Despite the long-term survey by camera traps (70,000 nights of camera traps), only 0.1% of the records were of WLP with anomalous coloration. Our work presents, for the first time, WLP with leucism and piebaldism, and a range of chromatic variations. It was not possible to determine how leucism affects the health and survival of the individuals that have it, therefore, our work encourages future field studies that can monitor the behavior and fitness of those individuals for extended periods.
C1 [Aximoff, Izar] Univ Estado Rio De Janeiro, Radioecol & Global Changes Lab, 524 PHLC Subsolo, Rio De Janeiro, Brazil.
   [Neto, Ennio Painkow; Barquero, Gonzalo] Trop Sustainabil Inst, Estr Tambau 9, Sao Paulo, Brazil.
   [de Paula, Wlainer] Neofauna Ambiental Quadra 406 Norte, Qc 02,Alameda 07,Lote 03, Palmas, Tocantins, Brazil.
   [Hofmann, Gabriel Selbach] Rio Grande Sul Fed Univ, Inst Geosci, Ave Bento Goncalves 9500,Bldg 43113, Porto Alegre, RS, Brazil.
   [Keuroghlian, Alexine] IUCN SSC Peccary Specialist Grp, Queixada Project, Peccary Project, R Spipe Calarge 2355, Campo Grande, MS, Brazil.
   [Jorge, Maria Luisa] Vanderbilt Univ, Earth & Environm Sci, PMB 351805,2301 Vanderbilt Pl, Nashville, TN 37235 USA.
   [Lima, Edson] Cachorro Vinagre Program, Pernambuco St 179, Nova Xavantina, Mato Grosso, Brazil.
RP Aximoff, I (corresponding author), Univ Estado Rio De Janeiro, Radioecol & Global Changes Lab, 524 PHLC Subsolo, Rio De Janeiro, Brazil.
EM izar.aximoff@gmail.com
FU Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior -Brasil
   (CAPES)Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior
   (CAPES) [001]; Innovation Department -InovUERJ
FX We thank Luiz Flamarion B. de Oliveira (MN/UFRJ), Igor P. Coelho,
   Guilherme Ribeiro and SESC Pantanal team for the partnership in our
   projects. Julia Oshima for assisting and registering the capture and
   contributing with the photos (2C and 2D). Dorival Miguel Stank for
   contributing with the photo (2H). We want to thank the Peccary Project
   staff for support in the field and during the capture. We also thank
   UNESP-Rio Claro (LEEC) for their field support in Corguinho, MS. To
   Sindicato Rural de Chapadao do Ceu, management of Emas National Park,
   ICMBio, Centro Nacional de Pesquisa e Conservacao de Mamiferos
   Carnivoros (CENAP), to team of Tropical Sustainability Institute and
   Neofauna Ambiental for supporting the use of records. GSH was supported
   by Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior -Brasil
   (CAPES) -Finance Code 001. We also thank the Innovation Department
   -InovUERJ, for the granting (Qualitec Superior) to first author, and we
   also thank the editors and anonymous reviewers for the important
   contributions.
CR Abreu MSL, 2013, BRAZ J BIOL, V73, P185, DOI 10.1590/S1519-69842013000100020
   Acevedo J, 2008, REV BIOL MAR OCEANOG, V43, P413
   Aximoff I, 2020, FOLIA PRIMATOL, V91, P149, DOI 10.1159/000501186
   Aximoff Izar, 2016, Oecologia Australis, V20, P122
   Aximoff Izar A., 2016, Oecologia Australis, V20, P526
   Beck H, 2018, ECOLOGY, CONSERVATION AND MANAGEMENT OF WILD PIGS AND PECCARIES, P265
   Borteiro C, 2021, SALAMANDRA, V57, P124
   Brito Jorge, 2016, Therya, V7, P483, DOI 10.12933/therya-16-408
   Cavalcanti SMC, 2010, J MAMMAL, V91, P722, DOI 10.1644/09-MAMM-A-171.1
   da Silva VL, 2019, TROP ECOL, V60, P303, DOI 10.1007/s42965-019-00036-x
   Jacomo ATD, 2013, J MAMMAL, V94, P137, DOI 10.1644/11-MAMM-A-411.1
   de Mello Luciano Moura, 2020, Oecologia Australis, V24, P191, DOI 10.4257/oeco.2020.2401.17
   Desbiez Arnaud Leonard Jean, 2009, International Journal of Biodiversity and Conservation, V1, P11
   DONKIN RA, 1985, T AM PHILOS SOC, V75, P1, DOI 10.2307/1006340
   Eaton DP, 2017, BIOL CONSERV, V208, P29, DOI 10.1016/j.biocon.2016.09.010
   Espinal Mario, 2016, Mastozool. neotrop., V23, P63
   Fertl D, 2009, ENCYCLOPEDIA OF MARINE MAMMALS, 2ND EDITION, P24
   Fowler ME., 2015, FOWLERS ZOO WILD ANI, P568, DOI [DOI 10.1016/B978-1-4557-7397-8.00058-X, 10.1016/B978-1-4557-7397-8.00058-X]
   Garcia-Casimiro Erika, 2020, Neotropical Biology and Conservation, V15, P195, DOI 10.3897/neotropical.15.e50951
   Hendges CD, 2019, J MAMMAL, V100, P475, DOI 10.1093/jmammal/gyz061
   Hofreiter M, 2010, CELL MOL LIFE SCI, V67, P2591, DOI 10.1007/s00018-010-0333-7
   Hubel M., 2020, OECOL AUST
   Jorge M.L.S., 2019, MOVEMENT ECOLOGY NEO, P39, DOI [10.1007/978-3-030-03463-4_4, DOI 10.1007/978-3-030-03463-4_4]
   Keuroghlian A, 2004, BIOL CONSERV, V120, P411, DOI 10.1016/j.biocon.2004.03.016
   Keuroghlian A, 2008, BIOTROPICA, V40, P62, DOI 10.1111/j.1744-7429.2007.00351.x
   Keuroghlian A, 2018, ECOLOGY, CONSERVATION AND MANAGEMENT OF WILD PIGS AND PECCARIES, P277
   Keuroghlian A, 2015, MAMMALIA, V79, P491, DOI 10.1515/mammalia-2014-0094
   Keuroghlian Alexine, 2012, Biodiversidade Brasileira, V1, P84
   Keuroghlian A, 2009, BIODIVERS CONSERV, V18, P1733, DOI 10.1007/s10531-008-9554-6
   Landis MB, 2020, MAMMALIA, V84, P601, DOI 10.1515/mammalia-2019-0084
   Leite DA, 2018, MAMM BIOL, V92, P111, DOI 10.1016/j.mambio.2018.05.005
   Lima M, 2019, PERSPECT ECOL CONSER, V17, P36, DOI 10.1016/j.pecon.2018.12.001
   Mazza Isabela, 2018, Oecologia Australis, V22, P74, DOI 10.4257/oeco.2018.2201.07
   Mello L.M., 2016, REV CIENCIAS AMBIENT, V10, P157, DOI [10.18316/1981-8858.16.37, DOI 10.18316/1981-8858.16.37]
   Paviolo A, 2016, SCI REP-UK, V6, DOI 10.1038/srep37147
   Reyna-Hurtado R, 2009, J MAMMAL, V90, P1199, DOI 10.1644/08-MAMM-A-246.1
   Ribeiro Raquel, 2020, Acta Scientiarum Biological Sciences, V42, pe46734, DOI 10.4025/actascibiolsci.v42i2.46734
   Sazima I., 1992, Memorias do Instituto Butantan (Sao Paulo), V53, P167
   Sobroza TV, 2016, STUD NEOTROP FAUNA E, V51, P231, DOI 10.1080/01650521.2016.1227137
   Sowls L.K., 1997, JAVELINAS OTHER PECC
   Sowls L. K., 1984, PECCARIES
   Summers CG, 2009, OPTOMETRY VISION SCI, V86, P659, DOI 10.1097/OPX.0b013e3181a5254c
   Taber A., 2016, TROPICAL CONSERVATIO, P255
   Veiga L. A., 1994, Revista Brasileira de Zoologia, V11, P341
NR 44
TC 0
Z9 0
U1 0
U2 0
PU UNIV ORADEA PUBL HOUSE
PI ORADEA
PA UNIVERSITATII NR 1, ORADEA, 410087, ROMANIA
SN 1584-9074
EI 1842-6441
J9 NORTH-WEST J ZOOL
JI North-West. J. Zool.
PD DEC
PY 2021
VL 17
IS 2
BP 288
EP 293
AR e211702
PG 6
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA XQ0YR
UT WOS:000731281200003
DA 2022-02-10
ER

PT J
AU Cappele, N
   Howe, EJ
   Boesch, C
   Kuhl, HS
AF Cappele, Noemie
   Howe, Eric J.
   Boesch, Christophe
   Kuehl, Hjalmar S.
TI Estimating animal abundance and effort-precision relationship with
   camera trap distance sampling
SO ECOSPHERE
LA English
DT Article
DE chimpanzee; design; elephant; leopard; Maxwell's duiker; monitoring;
   precision; sampling effort
AB Effective monitoring methods are needed for assessing the state of biodiversity and detecting population trends. The popularity of camera trapping in wildlife surveys continues to increase as they are able to detect species in remote and difficult-to-access areas. As a result, several statistical estimators of the abundance of unmarked animal populations have been developed, but none have been widely tested. Even where the potential for accurate estimation has been demonstrated, whether these methods estimators can yield estimates of sufficient precision to detect trends and inform conservation action remains questionable. Here, we assess the effort-precision relationship of camera trap distance sampling (CTDS) in order to help researchers design efficient surveys. A total of 200 cameras were deployed for 10 months across 200 km(2) in the Tai National Park, Cote d'Ivoire. We estimated abundance of Maxwell's duikers, western chimpanzees, leopards, and forest elephants that are challenging to enumerate due to rarity or semi-arboreality. To test the effects of spatial and temporal survey effort on the precision of CTDS estimates, we calculated coefficient of variation (CV) of the encounter rate from subsets of our complete data sets. Estimated abundance of leopard and Maxwell's duiker density (20% < CV < 30% and CV = 11%, respectively) were similar to prior estimates from the same area. Abundances of chimpanzees (20% < CV < 30%) were underestimated, but the quality of inference was similar to that reported after labor-intensive line transect surveys to nests. Estimates for the rare forest elephants were potentially unreliable since they were too imprecise (60% < CV < 200%). Generalized linear models coefficients indicated that for relatively common, ground-dwelling species, CVs between 10% and 20% are achievable from a variety of survey designs, including long-term (6+ months) surveys at few locations (50), or short term (2-week to 2-month) surveys at 100-150 locations. We conclude that CTDS can efficiently provide estimates of abundance of multiple species of sufficient quality and precision to inform conservation decisions. However, estimates for the rarest species will be imprecise even from ambitious surveys and may be biased for species that exhibit strong reactions to cameras.
C1 [Cappele, Noemie; Boesch, Christophe; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Howe, Eric J.] Univ St Andrews, Ctr Res Ecol & Environm Modeling, St Andrews, Fife, Scotland.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP Cappele, N (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
EM noemie_cappelle@eva.mpg.de
FU Max Planck SocietyMax Planck SocietyFoundation CELLEX; ARCUS foundation
FX The authors thank the Max Planck Society in Germany, The Robert Bosch
   Foundation, the Wild Chimpanzee Foundation, the Ministere des Eaux et
   Forets, Ministere de l'Enseignement superieur et de la Recherche
   scientifique, and the Office Ivoirien des Parcs et Reserves for
   permitting this research, as well as the Tai Chimpanzee Project. This
   study was conducted with the financial support of the Max Planck Society
   and ARCUS foundation. We thank Roger Mundry for the statistical support.
   We particularly want to thank Serge N'Goran for his invaluable help in
   supervising field and data processing assistants. We thank Alphonse,
   Emile, Eric, Lambert, Martin, and Nicaise for their support in
   fieldwork, Alejandro Estrella, Adiko Noel, Ange, Benjamin Debetencourt,
   Christina, Coulibaly, Daniel, Diomande, Hanna, Heather Cohen, Iko Noel,
   Ines, Kouadio, Lisa Orth, Amoakon et Sita Scherer for their
CR Anderson DP, 2005, BIOTROPICA, V37, P631, DOI 10.1111/j.1744-7429.2005.00080.x
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155
   Anile S, 2014, J ZOOL, V293, P252, DOI 10.1111/jzo.12141
   Balestrieri A., 2016, THESIS U MILAN MILAN
   Bessone M, 2020, J APPL ECOL, V57, P963, DOI 10.1111/1365-2664.13602
   Boesch C, 2006, AM J PHYS ANTHROPOL, V130, P103, DOI 10.1002/ajpa.20341
   Boesch C, 2008, AM J PRIMATOL, V70, P519, DOI 10.1002/ajp.20524
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Buckland S.T., 2001, pi
   Buckland ST., 2004, ADV DISTANCE SAMPLIN
   Buckland ST, 2006, AUK, V123, P345, DOI 10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campos-Candela A., 2018, J ANIM ECOL, V38, P42
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Doran D., 1989, THESIS U NEW YORK NE
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Gilbert NA, 2021, CONSERV BIOL, V35, P88, DOI 10.1111/cobi.13517
   Hoppe-Dominik B, 2011, AFR J ECOL, V49, P450, DOI 10.1111/j.1365-2028.2011.01277.x
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Hutchinson JMC, 2007, BIOL REV, V82, P335, DOI 10.1111/j.1469-185X.2007.00014.x
   Jenny D, 1996, J ZOOL, V240, P427, DOI 10.1111/j.1469-7998.1996.tb05296.x
   Jourdain NOAS, 2020, J AGR BIOL ENVIR ST, V25, P148, DOI 10.1007/s13253-020-00385-4
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   Luo G, 2020, WILDLIFE SOC B, V44, P173, DOI 10.1002/wsb.1060
   Marini F, 2009, EUR J WILDLIFE RES, V55, P107, DOI 10.1007/s10344-008-0222-7
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Marshall AR, 2008, AM J PRIMATOL, V70, P452, DOI 10.1002/ajp.20516
   McCullagh P., 1989, GEN LINEAR MODELS, V2
   Moeller AK, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2331
   Nakashima Y, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108381
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero Francesco, 2010, Abc Taxa, V8, P100
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2013, J WILDLIFE MANAGE, V77, P876, DOI 10.1002/jwmg.533
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Si XF, 2014, PEERJ, V2, DOI 10.7717/peerj.374
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Tiedoue M.R., 2016, ETAT CONSERVATION PA
   WCF, 2016, RAPP ANN 2016
   Zero VH, 2013, ORYX, V47, P410, DOI 10.1017/S0030605312000324
NR 52
TC 4
Z9 4
U1 5
U2 13
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2150-8925
J9 ECOSPHERE
JI Ecosphere
PD JAN
PY 2021
VL 12
IS 1
AR e03299
DI 10.1002/ecs2.3299
PG 16
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QB4KR
UT WOS:000614109800003
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Gooliaff, TJ
   Hodges, KE
AF Gooliaff, T. J.
   Hodges, Karen E.
TI Measuring agreement among experts in classifying camera images of
   similar species
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE bobcat; Canada lynx; expert identification; image classification; Lynx
   canadensis; Lynx rufus
ID CAPTURE-RECAPTURE; CONSERVATION; TRAPS; MANAGEMENT; OCCUPANCY; MODELS
AB Camera trapping and solicitation of wildlife images through citizen science have become common tools in ecological research. Such studies collect many wildlife images for which correct species classification is crucial; even low misclassification rates can result in erroneous estimation of the geographic range or habitat use of a species, potentially hindering conservation or management efforts. However, some species are difficult to tell apart, making species classification challenging-but the literature on classification agreement rates among experts remains sparse. Here, we measure agreement among experts in distinguishing between images of two similar congeneric species, bobcats (Lynx rufus) and Canada lynx (Lynx canadensis). We asked experts to classify the species in selected images to test whether the season, background habitat, time of day, and the visible features of each animal (e.g., face, legs, tail) affected agreement among experts about the species in each image. Overall, experts had moderate agreement (Fleiss' kappa = 0.64), but experts had varying levels of agreement depending on these image characteristics. Most images (71%) had >= 1 expert classification of "unknown," and many images (39%) had some experts classify the image as "bobcat" while others classified it as "lynx." Further, experts were inconsistent even with themselves, changing their classifications of numerous images when they were asked to reclassify the same images months later. These results suggest that classification of images by a single expert is unreliable for similar-looking species. Most of the images did obtain a clear majority classification from the experts, although we emphasize that even majority classifications may be incorrect. We recommend that researchers using wildlife images consult multiple species experts to increase confidence in their image classifications of similar sympatric species. Still, when the presence of a species with similar sympatrics must be conclusive, physical or genetic evidence should be required.
C1 [Gooliaff, T. J.; Hodges, Karen E.] Univ British Columbia Okanagan, Dept Biol, Kelowna, BC, Canada.
RP Gooliaff, TJ (corresponding author), Univ British Columbia Okanagan, Dept Biol, Kelowna, BC, Canada.
EM tjgooliaff@hotmail.com
RI Hodges, Karen E/K-5598-2012
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)CGIAR
   [312222]; University of British Columbia
FX Natural Sciences and Engineering Research Council of Canada, Grant/Award
   Number: 312222; University of British Columbia
CR Alexander PD, 2018, WILDLIFE RES, V45, P274, DOI 10.1071/WR17044
   Austen GE, 2016, SCI REP-UK, V6, DOI 10.1038/srep33634
   Barnett CW, 2018, J ENDODONT, V44, P938, DOI 10.1016/j.joen.2017.12.022
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Canty A., 2017, PACKAGE BOOT
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Chambert T, 2018, METHODS ECOL EVOL, V9, P560, DOI 10.1111/2041-210X.12910
   Costa H, 2015, ISPRS INT J GEO-INF, V4, P2496, DOI 10.3390/ijgi4042496
   Farr S, 2018, J WRIST SURG, V7, P227, DOI 10.1055/s-0037-1612636
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Furnas BJ, 2017, ECOSPHERE, V8, DOI 10.1002/ecs2.1747
   Gibbon GEM, 2015, PEERJ, V3, DOI 10.7717/peerj.1303
   Gooliaff T, 2018, J WILDLIFE MANAGE, V82, P810, DOI 10.1002/jwmg.21437
   Gwet K. L., 2010, HDB INTERRATER RELIA
   Hansen K., 2007, BOBCAT MASTER SURVIV
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Jiang GS, 2015, SCI REP-UK, V5, DOI 10.1038/srep15475
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lewis JC, 2016, DRAFT PERIODIC STATU
   Mammal Watch South East, 2018, MAMMAL TRACKER
   McKelvey KS, 2008, BIOSCIENCE, V58, P549, DOI 10.1641/B580611
   McKelvey Kevin S., 2000, P207
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Miller DA, 2011, ECOLOGY, V92, P1422, DOI 10.1890/10-1396.1
   Molinari-Jobin A, 2012, ANIM CONSERV, V15, P266, DOI 10.1111/j.1469-1795.2011.00511.x
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Rich LN, 2017, GLOBAL ECOL BIOGEOGR, V26, P918, DOI 10.1111/geb.12600
   Rich LN, 2014, J MAMMAL, V95, P382, DOI 10.1644/13-MAMM-A-126
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Sim J, 2005, PHYS THER, V85, P257
   Singh, 2014, PACKAGE IRR
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stewart FEC, 2016, ECOL EVOL, V6, P1493, DOI 10.1002/ece3.1921
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   US Fish and Wildlife Service, 2000, END THREAT WILD LIF, V65, P16052
   Vandenberk B, 2018, J ELECTROCARDIOL, V51, P549, DOI 10.1016/j.jelectrocard.2017.12.002
   Wearn O.R., 2017, CAMERA TRAPPING CONS
   Weingarth K, 2012, ANIM BIODIV CONSERV, V35, P197
   Wisconsin Department of Natural Resources, 2018, SNAPSH WISC
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 45
TC 8
Z9 8
U1 0
U2 8
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD NOV
PY 2018
VL 8
IS 22
BP 11009
EP 11021
DI 10.1002/ece3.4567
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA HC2CY
UT WOS:000451611000030
PM 30519423
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Egna, N
   O'Connor, D
   Stacy-Dawes, J
   Tobler, MW
   Pilfold, N
   Neilson, K
   Simmons, B
   Davis, EO
   Bowler, M
   Fennessy, J
   Glikman, JA
   Larpei, L
   Lekalgitele, J
   Lekupanai, R
   Lekushan, J
   Lemingani, L
   Lemirgishan, J
   Lenaipa, D
   Lenyakopiro, J
   Lesipiti, RL
   Lororua, M
   Muneza, A
   Rabhayo, S
   Ranah, SMO
   Ruppert, K
   Owen, M
AF Egna, Nicole
   O'Connor, David
   Stacy-Dawes, Jenna
   Tobler, Mathias W.
   Pilfold, Nicholas
   Neilson, Kristin
   Simmons, Brooke
   Davis, Elizabeth Oneita
   Bowler, Mark
   Fennessy, Julian
   Glikman, Jenny Anne
   Larpei, Lexson
   Lekalgitele, Jesus
   Lekupanai, Ruth
   Lekushan, Johnson
   Lemingani, Lekuran
   Lemirgishan, Joseph
   Lenaipa, Daniel
   Lenyakopiro, Jonathan
   Lesipiti, Ranis Lenalakiti
   Lororua, Masenge
   Muneza, Arthur
   Rabhayo, Sebastian
   Ole Ranah, Symon Masiaine
   Ruppert, Kirstie
   Owen, Megan
TI Camera settings and biome influence the accuracy of citizen science
   approaches to camera trap image classification
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE amazon; crowdsource; image processing; kenya; serengeti; trail camera;
   volunteer
ID WILDLIFE; TOOL
AB Scientists are increasingly using volunteer efforts of citizen scientists to classify images captured by motion-activated trail cameras. The rising popularity of citizen science reflects its potential to engage the public in conservation science and accelerate processing of the large volume of images generated by trail cameras. While image classification accuracy by citizen scientists can vary across species, the influence of other factors on accuracy is poorly understood. Inaccuracy diminishes the value of citizen science derived data and prompts the need for specific best-practice protocols to decrease error. We compare the accuracy between three programs that use crowdsourced citizen scientists to process images online: Snapshot Serengeti, Wildwatch Kenya, and AmazonCam Tambopata. We hypothesized that habitat type and camera settings would influence accuracy. To evaluate these factors, each photograph was circulated to multiple volunteers. All volunteer classifications were aggregated to a single best answer for each photograph using a plurality algorithm. Subsequently, a subset of these images underwent expert review and were compared to the citizen scientist results. Classification errors were categorized by the nature of the error (e.g., false species or false empty), and reason for the false classification (e.g., misidentification). Our results show that Snapshot Serengeti had the highest accuracy (97.9%), followed by AmazonCam Tambopata (93.5%), then Wildwatch Kenya (83.4%). Error type was influenced by habitat, with false empty images more prevalent in open-grassy habitat (27%) compared to woodlands (10%). For medium to large animal surveys across all habitat types, our results suggest that to significantly improve accuracy in crowdsourced projects, researchers should use a trail camera set up protocol with a burst of three consecutive photographs, a short field of view, and determine camera sensitivity settings based on in situ testing. Accuracy level comparisons such as this study can improve reliability of future citizen science projects, and subsequently encourage the increased use of such data.
C1 [Egna, Nicole] Duke Univ, Nicholas Sch Environm, Durham, NC 27708 USA.
   [Egna, Nicole; O'Connor, David; Stacy-Dawes, Jenna; Tobler, Mathias W.; Pilfold, Nicholas; Neilson, Kristin; Davis, Elizabeth Oneita; Glikman, Jenny Anne; Ole Ranah, Symon Masiaine; Ruppert, Kirstie; Owen, Megan] San Diego Zoo Inst Conservat Res, Escondido, CA USA.
   [O'Connor, David] Save Giraffe Now, Dallas, TX USA.
   [O'Connor, David] Goethe Univ, Fac Biol Sci, Frankfurt, Germany.
   [Simmons, Brooke] Univ Lancaster, Dept Phys, Lancaster, England.
   [Bowler, Mark] Univ Suffolk, Sci & Technol, Ipswich, Suffolk, England.
   [Fennessy, Julian; Muneza, Arthur] Giraffe Conservat Fdn, Windhoek, Namibia.
   [Glikman, Jenny Anne] Inst Estudios Sociales Avanzados IESA CSIC, Cordoba, Spain.
   [Larpei, Lexson] Loisaba Conservancy, Nanyuki, Kenya.
   [Lekalgitele, Jesus; Lekupanai, Ruth; Lekushan, Johnson; Lemingani, Lekuran; Lemirgishan, Joseph; Lenaipa, Daniel; Lenyakopiro, Jonathan; Lesipiti, Ranis Lenalakiti; Lororua, Masenge; Rabhayo, Sebastian] Namunyak Wildlife Conservat Trust, Archers Post, Kenya.
RP Egna, N (corresponding author), POB 1636, Port Washington, NY 11050 USA.
EM nicoleegna@gmail.com
OI Tobler, Mathias/0000-0002-8587-0560; Egna, Nicole/0000-0001-7417-8168;
   Ruppert, Kirstie A./0000-0002-6263-7600; Stacy-Dawes,
   Jenna/0000-0001-5828-6476; Pilfold, Nicholas/0000-0001-5324-5499;
   Bowler, Mark/0000-0001-5236-3477; Simmons, Brooke/0000-0001-5882-3323;
   O'Connor, David/0000-0001-8604-1812; Glikman, Jenny
   Anne/0000-0002-0208-5488
FU Leiden Foundation
FX Carolyn Barkley; Victoria and Allan Peacock; The Leiden Foundation
CR Agha M, 2018, AFR J ECOL, V56, P694, DOI 10.1111/aje.12565
   [Anonymous], **NON-TRADITIONAL**
   [Anonymous], **NON-TRADITIONAL**
   Busnell Outdoor Products, 2014, BUSHNELL TROPHY CAM
   Chafota J., 1998, EFFECTS CHANGES ELEP
   Cox J, 2015, COMPUT SCI ENG, V17, P28, DOI 10.1109/MCSE.2015.65
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Dickinson JL, 2015, CITIZEN SCI PUBLIC P
   Ellwood ER, 2017, BIOL CONSERV, V208, P1, DOI 10.1016/j.biocon.2016.10.014
   Fennessy J, 2016, CURR BIOL, V26, P2543, DOI 10.1016/j.cub.2016.07.036
   Ford C., 2016, PAIRWISE COMP PROPOR
   Foster-Smith J, 2003, BIOL CONSERV, V113, P199, DOI 10.1016/S0006-3207(02)00373-7
   Jones FM, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.124
   Jordan RC, 2011, CONSERV BIOL, V25, P1148, DOI 10.1111/j.1523-1739.2011.01745.x
   Koivuniemi M, 2016, ENDANGER SPECIES RES, V30, P29, DOI 10.3354/esr00723
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek P., 2012, INTRO CAMERA TRAPPIN
   Mitchell N, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186285
   Moo SSB, 2018, ORYX, V52, P537, DOI 10.1017/S0030605316001113
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell A. F., 2010, CAMERA TRAPS ECOLOGY
   O'connor D, 2019, MAMMAL REV, V49, P285, DOI 10.1111/mam.12165
   Pilfold NW, 2019, AFR J ECOL, V57, P270, DOI 10.1111/aje.12586
   R Core Team, 2018, STATS PACK LANG ENV
   Raddick M Jordan, 2010, Astronomy Education Review, V9, DOI 10.3847/AER2009036
   Rosenberg KV, 2019, SCIENCE, V366, P120, DOI 10.1126/science.aaw1313
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Sauermann H, 2015, P NATL ACAD SCI USA, V112, P679, DOI 10.1073/pnas.1408907112
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Spielman SE, 2014, CARTOGR GEOGR INF SC, V41, P115, DOI 10.1080/15230406.2013.874200
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Torney CJ, 2019, METHODS ECOL EVOL, V10, P779, DOI 10.1111/2041-210X.13165
   Tulloch AIT, 2013, BIOL CONSERV, V165, P128, DOI 10.1016/j.biocon.2013.05.025
   Wearn OR, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181748
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
NR 41
TC 0
Z9 0
U1 7
U2 13
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD NOV
PY 2020
VL 10
IS 21
BP 11954
EP 11965
DI 10.1002/ece3.6722
EA OCT 2020
PG 12
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA OQ4KX
UT WOS:000575196000001
PM 33209262
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Rather, TA
   Kumar, S
   Khan, JA
AF Rather, Tahir A.
   Kumar, Sharad
   Khan, Jamal A.
TI Multi-scale habitat modelling and predicting change in the distribution
   of tiger and leopard using random forest algorithm
SO SCIENTIFIC REPORTS
LA English
DT Article
ID SPECIES DISTRIBUTION MODELS; SMALL-MAMMAL COMMUNITIES; CLIMATE-CHANGE;
   PREY SELECTION; PANTHERA-TIGRIS; LANDSCAPE; SCALE; CONSERVATION;
   SUITABILITY; CARNIVORES
AB Tigers and leopards have experienced considerable declines in their population due to habitat loss and fragmentation across their historical ranges. Multi-scale habitat suitability models (HSM) can inform forest managers to aim their conservation efforts at increasing the suitable habitat for tigers by providing information regarding the scale-dependent habitat-species relationships. However the current gap of knowledge about ecological relationships driving species distribution reduces the applicability of traditional and classical statistical approaches such as generalized linear models (GLMs), or occupancy surveys to produce accurate predictive maps. This study investigates the multi-scale habitat relationships of tigers and leopards and the impacts of future climate change on their distribution using a machine-learning algorithm random forest (RF). The recent advancements in the machine-learning algorithms provide a powerful tool for building accurate predictive models of species distribution and their habitat relationships even when little ecological knowledge is available about the species. We collected species occurrence data using camera traps and indirect evidence of animal presences (scats) in the field over 2 years of rigorous sampling and used a machine-learning algorithm random forest (RF) to predict the habitat suitability maps of tiger and leopard under current and future climatic scenarios. We developed niche overlap models based on the recently developed statistical approaches to assess the patterns of niche similarity between tigers and leopards. Tiger and leopard utilized habitat resources at the broadest spatial scales (28,000 m). Our model predicted a 23% loss in the suitable habitat of tigers under the RCP 8.5 Scenario (2050). Our study of multi-scale habitat suitability modeling provides valuable information on the species habitat relationships in disturbed and human-dominated landscapes concerning two large felid species of conservation importance. These areas may act as refugee habitats for large carnivores in the future and thus should be the focus of conservation importance. This study may also provide a methodological framework for similar multi-scale and multi-species monitoring programs using robust and more accurate machine learning algorithms such as random forest.
C1 [Rather, Tahir A.; Kumar, Sharad; Khan, Jamal A.] Aligarh Muslim Univ, Dept Wildlife Sci, Aligarh 202002, Uttar Pradesh, India.
   [Rather, Tahir A.; Kumar, Sharad] Corbett Fdn, 81-88 Atlanta Bldg, Mumbai 400021, Maharashtra, India.
RP Rather, TA (corresponding author), Aligarh Muslim Univ, Dept Wildlife Sci, Aligarh 202002, Uttar Pradesh, India.; Rather, TA (corresponding author), Corbett Fdn, 81-88 Atlanta Bldg, Mumbai 400021, Maharashtra, India.
EM murtuzatahiri@gmail.com
RI Rather, Tahir Ali/AAX-7605-2021
OI Rather, Tahir Ali/0000-0003-2204-6870
CR Andheria AP, 2007, J ZOOL, V273, P169, DOI 10.1111/j.1469-7998.2007.00310.x
   [Anonymous], 2018, GLOB WARM 15 C 26
   Ashrafzadeh MR, 2019, MAMMAL RES, V64, P39, DOI 10.1007/s13364-018-0384-y
   Athreya V., 2006, CONSERV SOC, V4, P419
   Bagchi S, 2003, J ZOOL, V260, P285, DOI 10.1017/S0952836903003765
   Beaumont LJ, 2008, ECOL LETT, V11, P1135, DOI 10.1111/j.1461-0248.2008.01231.x
   Bellard C, 2012, ECOL LETT, V15, P365, DOI 10.1111/j.1461-0248.2011.01736.x
   Bradter U, 2013, METHODS ECOL EVOL, V4, P167, DOI 10.1111/j.2041-210x.2012.00253.x
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Broennimann O, 2012, GLOBAL ECOL BIOGEOGR, V21, P481, DOI 10.1111/j.1466-8238.2011.00698.x
   Brown JL, 2014, METHODS ECOL EVOL, V5, P694, DOI 10.1111/2041-210X.12200
   Buermann W, 2008, J BIOGEOGR, V35, P1160, DOI 10.1111/j.1365-2699.2007.01858.x
   Burns CE, 2003, P NATL ACAD SCI USA, V100, P11474, DOI 10.1073/pnas.1635115100
   Cahill AE, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.1890
   Carnaval A.C., 2019, FRONTIERS BIOGEOGRAP, V11
   Champion H.G., 1968, REVISED SURVEY FORES
   Cunningham MA, 2006, ECOL APPL, V16, P1062, DOI 10.1890/1051-0761(2006)016[1062:PALFIG]2.0.CO;2
   Cushman S, 2017, LANDSCAPE ECOL, V32, P1581, DOI 10.1007/s10980-017-0520-0
   Cushman SA, 2010, SPATIAL COMPLEXITY, INFORMATICS, AND WILDLIFE CONSERVATION, P83, DOI 10.1007/978-4-431-87771-4_5
   Di Cola V, 2017, ECOGRAPHY, V40, P774, DOI 10.1111/ecog.02671
   Diaz-Uriarte R, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-3
   Dinerstein E, 2007, BIOSCIENCE, V57, P508, DOI 10.1641/B570608
   Dormann CF, 2013, ECOGRAPHY, V36, P27, DOI 10.1111/j.1600-0587.2012.07348.x
   Drew CA, 2011, PREDICTIVE SPECIES AND HABITAT MODELING IN LANDSCAPE ECOLOOGY: CONCEPTS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-7390-0
   Elith J, 1998, QUANTITATIVE METHODS FOR CONSERVATION BIOLOGY, P39
   Elith J, 2006, ECOGRAPHY, V29, P129, DOI 10.1111/j.2006.0906-7590.04596.x
   Evans JS, 2011, PREDICTIVE SPECIES AND HABITAT MODELING IN LANDSCAPE ECOLOOGY: CONCEPTS AND APPLICATIONS, P139, DOI 10.1007/978-1-4419-7390-0_8
   Evans JS, 2009, LANDSCAPE ECOL, V24, P673, DOI 10.1007/s10980-009-9341-0
   Farr TG, 2007, REV GEOPHYS, V45, DOI 10.1029/2005RG000183
   Fedriani JM, 1999, OECOLOGIA, V121, P138, DOI 10.1007/s004420050915
   Gaston KJ., 2003, STRUCTURE DYNAMICS G
   Genuer R, 2010, PATTERN RECOGN LETT, V31, P2225, DOI 10.1016/j.patrec.2010.03.014
   Gienapp P, 2008, MOL ECOL, V17, P167, DOI 10.1111/j.1365-294X.2007.03413.x
   Graham CH, 2006, GLOBAL ECOL BIOGEOGR, V15, P578, DOI 10.1111/j.1466-8238.2006.00257.x
   Graham CH, 2004, EVOLUTION, V58, P1781, DOI 10.1554/03-274
   Guisan A, 2005, ECOL LETT, V8, P993, DOI 10.1111/j.1461-0248.2005.00792.x
   Hamilton P.H., 1976, THESIS
   Hayward MW, 2012, J ZOOL, V286, P221, DOI 10.1111/j.1469-7998.2011.00871.x
   Heikkinen RK, 2006, PROG PHYS GEOG, V30, P751, DOI 10.1177/0309133306071957
   Hof C, 2010, ECOGRAPHY, V33, P242, DOI 10.1111/j.1600-0587.2010.06309.x
   Holland JD, 2004, BIOSCIENCE, V54, P227, DOI 10.1641/0006-3568(2004)054[0227:DTSSOS]2.0.CO;2
   Huettmann F., 2018, MACHINE LEARNING ECO, P185
   Jhala Y, 2011, J APPL ECOL, V48, P14, DOI 10.1111/j.1365-2664.2010.01901.x
   JOHNSINGH A J T, 1983, Journal of the Bombay Natural History Society, V80, P1
   JOHNSINGH AJT, 1992, MAMMALIA, V56, P517, DOI 10.1515/mamm.1992.56.4.517
   Karanth K. Ullas, 1999, P100
   KARANTH KU, 1995, J ANIM ECOL, V64, P439, DOI 10.2307/5647
   Karanth KU, 2004, P NATL ACAD SCI USA, V101, P4854, DOI 10.1073/pnas.0306210101
   Khan JA, 1996, J TROP ECOL, V12, P149, DOI 10.1017/S0266467400009366
   Khosravi R, 2019, LANDSCAPE ECOL, V34, P2451, DOI 10.1007/s10980-019-00900-0
   Lentz DL, 2008, INT J PLANT SCI, V169, P541, DOI 10.1086/528754
   Liaw A., 2002, R NEWS, V2, P18, DOI DOI 10.1177/154405910408300516
   Loveridge AJ, 2003, J ZOOL, V259, P143, DOI 10.1017/S0952836902003114
   Mateo RG, 2010, DIVERS DISTRIB, V16, P84, DOI 10.1111/j.1472-4642.2009.00617.x
   Sanchez MCM, 2014, INT J GEOGR INF SCI, V28, P1531, DOI 10.1080/13658816.2013.776684
   MCDOUGAL C, 1988, Journal of the Bombay Natural History Society, V85, P609
   McGarigal K, 2016, LANDSCAPE ECOL, V31, P1161, DOI 10.1007/s10980-016-0374-x
   Calvente MEM, 2009, ACTA BOT GALLICA, V156, P63, DOI 10.1080/12538078.2009.10516142
   Moritz C, 2008, SCIENCE, V322, P261, DOI 10.1126/science.1163428
   Myers N., 1986, P437
   Myers P, 2009, GLOBAL CHANGE BIOL, V15, P1434, DOI 10.1111/j.1365-2486.2009.01846.x
   Odden M, 2010, ECOL RES, V25, P875, DOI 10.1007/s11284-010-0723-1
   Pandey R, 2018, REG ENVIRON CHANGE, V18, P1223, DOI 10.1007/s10113-017-1265-7
   Pearman PB, 2008, TRENDS ECOL EVOL, V23, P149, DOI 10.1016/j.tree.2007.11.005
   Perkins SE, 2007, J CLIMATE, V20, P4356, DOI 10.1175/JCLI4253.1
   Peterson AT, 1999, SCIENCE, V285, P1265, DOI 10.1126/science.285.5431.1265
   Phillips SJ, 2006, ECOL MODEL, V190, P231, DOI 10.1016/j.ecolmodel.2005.03.026
   Phillips SJ, 2008, ECOGRAPHY, V31, P161, DOI 10.1111/j.0906-7590.2008.5203.x
   Pontius RG, 2014, INT J GEOGR INF SCI, V28, P570, DOI 10.1080/13658816.2013.862623
   Pontius RG, 2011, INT J REMOTE SENS, V32, P4407, DOI 10.1080/01431161.2011.552923
   Qiao HJ, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-01313-2
   R Core Team, 2019, R LANG ENV STAT COMP
   Rabus B, 2003, ISPRS J PHOTOGRAMM, V57, P241, DOI 10.1016/S0924-2716(02)00124-7
   Rodriguez-Galiano VF, 2012, ISPRS J PHOTOGRAMM, V67, P93, DOI 10.1016/j.isprsjprs.2011.11.002
   Rogelj J, 2012, NAT CLIM CHANGE, V2, P248, DOI [10.1038/NCLIMATE1385, 10.1038/nclimate1385]
   Rowe KC, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2014.1857
   Sanderson E, 2006, TIGERS WORLD SCI POL, P143, DOI DOI 10.1016/B978-0-8155-1570-8.00009-8
   Sandri M, 2006, STUD CLASS DATA ANAL, P263, DOI 10.1007/3-540-35978-8_30
   SAUPE EE, 2017, SYSTEMATIC BIOL, V0067
   Schneider A, 2012, REMOTE SENS ENVIRON, V124, P689, DOI 10.1016/j.rse.2012.06.006
   SCHOENER TW, 1983, AM NAT, V122, P240, DOI 10.1086/284133
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Seidensticker J., 1990, P415
   SEIDENSTICKER J, 1976, BIOTROPICA, V8, P225, DOI 10.2307/2989714
   Singh H.S., 2005, Cat News, V42, P15
   SMITH JLD, 1993, BEHAVIOUR, V124, P165, DOI 10.1163/156853993X00560
   Steffan-Dewenter I, 2002, ECOLOGY, V83, P1421, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Strobl C, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-307
   Thogmartin WE, 2007, LANDSCAPE ECOL, V22, P61, DOI 10.1007/s10980-006-9005-2
   Tian Y, 2014, LANDSCAPE ECOL, V29, P621, DOI 10.1007/s10980-014-0009-z
   Titeux N, 2006, THESIS
   van Vuuren DP, 2011, CLIMATIC CHANGE, V109, P1, DOI [10.1007/s10584-011-0148-z, 10.1007/s10584-011-0157-y]
   Vergara M, 2016, LANDSCAPE ECOL, V31, P1241, DOI 10.1007/s10980-015-0307-0
   Vieira EM, 2007, J ZOOL, V272, P57, DOI 10.1111/j.1469-7998.2006.00237.x
   Walston J, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000485
   Wan JZ, 2017, ECOL EVOL, V7, P1541, DOI 10.1002/ece3.2684
   Warren DL, 2008, EVOLUTION, V62, P2868, DOI 10.1111/j.1558-5646.2008.00482.x
   Warren DL, 2010, ECOGRAPHY, V33, P607, DOI 10.1111/j.1600-0587.2009.06142.x
   Wasserman, 2012, MULTISCALE HABITAT R
   Watanabe M, 2010, J CLIMATE, V23, P6312, DOI 10.1175/2010JCLI3679.1
   Wayne GP, 2013, BEGINNERS GUIDE REPR
   WIENS JA, 1989, FUNCT ECOL, V3, P385, DOI 10.2307/2389612
   Wikramanayake ED, 1998, CONSERV BIOL, V12, P865, DOI 10.1046/j.1523-1739.1998.96428.x
   Wilson DE, 2009, HDB MAMMALS WORLD
   Wilson DE, 2009, HDB MAMMALS WORLD CA
NR 106
TC 6
Z9 6
U1 3
U2 7
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD JUL 10
PY 2020
VL 10
IS 1
DI 10.1038/s41598-020-68167-z
PG 19
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA MJ7ZR
UT WOS:000548308300012
PM 32651414
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Chen, X
   Zhao, J
   Chen, YH
   Zhou, W
   Hughes, AC
AF Chen, Xing
   Zhao, Jun
   Chen, Yan-hua
   Zhou, Wei
   Hughes, Alice C.
TI Automatic standardized processing and identification of tropical bat
   calls using deep learning approaches
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Bats; Bioacoustics; Automated monitoring; Algorithms; Deep learning;
   Neural network; Automatic processing; Biodiversity metrics; Machine
   learning; Calls; Echolocation; Monitoring protocol
ID ECHOLOCATION CALLS; SIGNALS; CHIROPTERA; DIVERGENCE; OCCUPANCY;
   DIVERSITY; FREQUENCY; PATTERNS; SOUND; TOOL
AB Consistent and comparable metrics to automatically monitor biodiversity across the landscape remain a gold-standard for biodiversity research, yet such approaches have frequently been limited to a very small selection of species for which visual approaches (e.g., camera traps) make continuous monitoring possible. Acoustic-based methods have been widely applied in the monitoring of bats and some other taxa across extended spatial scales, but are have yet to be applied to diverse tropical communities.
   In this study, we developed a software program "Waveman" and prepared a reference library using over 880 audio-files from 36 Asian bat species. The software incorporated a novel network "BatNet" and a re-checking strategy (ReChk) to maximize accuracy. In Waveman, BatNet outperforms three other published networks: CNNFULL, VggNet and ResNet_v2, with over 90% overall accuracy and 0.94 AUC on the ROC plot. The classification accuracy rates for all 36 species are at least 86% when analysed in combination. Moreover, our library preparation and ReChk greatly improved the sensitivity and reduced the false positive rate, when tested with 15 species for which more detailed and situationally diverse records were available. Finally, BatNet was successfully used to identify Hipposideros larvatus and Rhinolophus siamensis from three different environments. We hope this pipeline is useful tool to process bioacoustic data accurately, effectively and automatically, therefore allowing for greater standardization and comparability for researchers to understand bat activities across space and time and therefore provide a consistent tool for monitoring biodiversity for management and conservation.
C1 [Chen, Xing; Chen, Yan-hua; Hughes, Alice C.] Chinese Acad Sci, Ctr Integrat Conservat, Xishuangbanna Trop Bot Garden, Menglun 666303, Peoples R China.
   [Zhao, Jun; Zhou, Wei] Yunnan Univ, Software Sch, Kunming 650500, Yunnan, Peoples R China.
RP Hughes, AC (corresponding author), Chinese Acad Sci, Ctr Integrat Conservat, Xishuangbanna Trop Bot Garden, Menglun 666303, Peoples R China.; Zhou, W (corresponding author), Yunnan Univ, Software Sch, Kunming 650500, Yunnan, Peoples R China.
EM zwei@ynu.edu.cn; ACHughes@xtbg.cas.cn
OI hughes, Alice/0000-0002-4899-3158
FU Chinese National Natural Science FoundationNational Natural Science
   Foundation of China (NSFC) [U1602265]; Strategic Priority Research
   Program of the Chinese Academy of SciencesChinese Academy of Sciences
   [XDA20050202]; High-End Foreign Experts Program of Yunnan Province
   [Y9YN021B01]; CAS 135 program [2017XTBG-T03]
FX Supported by Chinese National Natural Science Foundation (Grant #:
   U1602265, Mapping Karst Biodiversity in Yunnan). Supported by the
   Strategic Priority Research Program of the Chinese Academy of Sciences
   (Grant No. XDA20050202). Supported by the High-End Foreign Experts
   Program of Yunnan Province (Grant #: Y9YN021B01, Yunnan Bioacoustic
   monitoring program). Supported by the CAS 135 program (No.
   2017XTBG-T03).
CR ALTES RA, 1970, J ACOUST SOC AM, V48, P1014, DOI 10.1121/1.1912222
   Astaras C, 2017, FRONT ECOL ENVIRON, V15, P233, DOI 10.1002/fee.1495
   Baker E, 2015, DATABASE-OXFORD, DOI 10.1093/database/bav054
   Barratt EM, 1997, NATURE, V387, P138, DOI 10.1038/387138b0
   Benson DA, 2015, NUCLEIC ACIDS RES, V43, pD30, DOI 10.1093/nar/gku1216
   Boonman A, 2005, J COMP PHYSIOL A, V191, P13, DOI 10.1007/s00359-004-0566-8
   Cardinale BJ, 2018, BIOL CONSERV, V219, P175, DOI 10.1016/j.biocon.2017.12.021
   Christin S., 2018, APPL DEEP LEARNING E
   Clement MJ, 2014, J APPL ECOL, V51, P1460, DOI 10.1111/1365-2664.12303
   Gager Y, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150780
   Gasc A, 2013, ECOL INDIC, V25, P279, DOI 10.1016/j.ecolind.2012.10.009
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   Hughes AC, 2012, GLOBAL CHANGE BIOL, V18, P1854, DOI 10.1111/j.1365-2486.2012.02641.x
   Hughes AC, 2011, ACTA CHIROPTEROL, V13, P447, DOI 10.3161/150811011X624938
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   IOFFE S, 2015, ARXIV 1502 03167, V1502, DOI DOI 10.1007/S13398-014-0173-7.2
   Jacobs DS, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187769
   Jiang TL, 2010, ETHOLOGY, V116, P691, DOI 10.1111/j.1439-0310.2010.01785.x
   Kembel SW, 2010, BIOINFORMATICS, V26, P1463, DOI 10.1093/bioinformatics/btq166
   Kingston T, 2004, NATURE, V429, P654, DOI 10.1038/nature02487
   Kiskin I, 2020, NEURAL COMPUT APPL, V32, P915, DOI 10.1007/s00521-018-3626-7
   Mac Aodha O, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005995
   Mao XG, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0056786
   Marques A, 2014, BASIC APPL ECOL, V15, P633, DOI 10.1016/j.baae.2014.09.004
   Meagher JP, 2018, STATISTICAL DATA SCIENCE, P111
   Meyer CFJ, 2010, BIOL CONSERV, V143, P2797, DOI 10.1016/j.biocon.2010.07.029
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Parsons S, 2000, J EXP BIOL, V203, P2641
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pennell MW, 2014, BIOINFORMATICS, V30, P2216, DOI 10.1093/bioinformatics/btu181
   Proenca V, 2017, BIOL CONSERV, V213, P256, DOI 10.1016/j.biocon.2016.07.014
   R Core Team, 2019, R LANGUAGE ENV STAT
   Rich LN, 2017, GLOBAL ECOL BIOGEOGR, V26, P918, DOI 10.1111/geb.12600
   Russo D, 2002, J ZOOL, V258, P91, DOI 10.1017/S0952836902001231
   Russo D, 2018, CAN J ZOOL, V96, P63, DOI 10.1139/cjz-2017-0089
   Russo D, 2016, ECOL INDIC, V66, P598, DOI 10.1016/j.ecolind.2016.02.036
   Rydell J, 2017, ECOL INDIC, V78, P416, DOI 10.1016/j.ecolind.2017.03.023
   Silberman N., 2017, TF SLIM LIGHTWEIGHT
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Stamatakis A, 2014, BIOINFORMATICS, V30, P1312, DOI 10.1093/bioinformatics/btu033
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Thabah A, 2006, BIOL J LINN SOC, V88, P119, DOI 10.1111/j.1095-8312.2006.00602.x
   Trolle M, 2003, J MAMMAL, V84, P607, DOI 10.1644/1545-1542(2003)084<0607:EOODIT>2.0.CO;2
   Walters Charlotte L., 2013, P479
   Wilkins MR, 2013, TRENDS ECOL EVOL, V28, P156, DOI 10.1016/j.tree.2012.10.002
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   ZINGG P.E., 2019, REV SUISSE ZOOL, P263
NR 47
TC 6
Z9 7
U1 9
U2 13
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD JAN
PY 2020
VL 241
AR 108269
DI 10.1016/j.biocon.2019.108269
PG 10
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KT0IV
UT WOS:000518695100048
DA 2022-02-10
ER

PT J
AU Rather, TA
   Kumar, S
   Khan, JA
AF Rather, Tahir Ali
   Kumar, Sharad
   Khan, Jamal Ahmad
TI Using machine learning to predict habitat suitability of sloth bears at
   multiple spatial scales
SO ECOLOGICAL PROCESSES
LA English
DT Article
DE Bandhavgarh; Melursus ursinus; Multi-scale; Habitat selection; Random
   forest; Sloth bear; Species distribution models
ID DISTRIBUTION MODELS; MELURSUS-URSINUS; NATIONAL-PARK; HOME RANGES;
   SELECTION; CONSERVATION; LANDSCAPE
AB Background: Habitat resources occur across the range of spatial scales in the environment. The environmental resources are characterized by upper and lower limits, which define organisms' distribution in their communities. Animals respond to these resources at the optimal spatial scale. Therefore, multi-scale assessments are critical to identifying the correct spatial scale at which habitat resources are most influential in determining the species-habitat relationships. This study used a machine learning algorithm random forest (RF), to evaluate the scale-dependent habitat selection of sloth bears (Melursus ursinus) in and around Bandhavgarh Tiger Reserve, Madhya Pradesh, India.
   Results: We used 155 spatially rarified occurrences out of 248 occurrence records of sloth bears obtained from camera trap captures (n = 36) and scats located (n = 212) in the field. We calculated focal statistics for 13 habitat variables across ten spatial scales surrounding each presence-absence record of sloth bears. Large (> 5000 m) and small (1000-2000 m) spatial scales were the most dominant scales at which sloth bears perceived the habitat features. Among the habitat covariates, farmlands and degraded forests were the essential patches associated with sloth bear occurrences, followed by sal and dry deciduous forests. The final habitat suitability model was highly accurate and had a very low out-of-bag (OOB) error rate. The high accuracy rate was also obtained using alternate validation matrices.
   Conclusions: Human-dominated landscapes are characterized by expanding human populations, changing land-use patterns, and increasing habitat fragmentation. Farmland and degraded habitats constitute similar to 40% of the landform in the buffer zone of the reserve. One of the management implications may be identifying the highly suitable bear habitats in human-modified landscapes and integrating them with the existing conservation landscapes.
C1 [Rather, Tahir Ali; Kumar, Sharad; Khan, Jamal Ahmad] Aligarh Muslim Univ, Dept Wildlife Sci, Aligarh 202002, Uttar Pradesh, India.
   [Rather, Tahir Ali; Kumar, Sharad] Corbett Fdn, 81-88,Atlanta,Nariman Point, Mumbai 400021, Maharashtra, India.
RP Rather, TA (corresponding author), Aligarh Muslim Univ, Dept Wildlife Sci, Aligarh 202002, Uttar Pradesh, India.; Rather, TA (corresponding author), Corbett Fdn, 81-88,Atlanta,Nariman Point, Mumbai 400021, Maharashtra, India.
EM murtuzatahiri@gmail.com
RI Rather, Tahir Ali/AAX-7605-2021
OI Rather, Tahir Ali/0000-0003-2204-6870
FU TCF, Shri Kedar Gore; Bandhavgarh Tiger Reserve
FX We are thankful to The Corbett Foundation (TCF) for facilitating this
   study. We wish to thank the Director of the TCF, Shri Kedar Gore, for
   his support. We are grateful to the Madhya Pradesh Forest Department for
   the necessary permission to conduct this study. Our acknowledgments are
   with the administrative body of Bandhavgarh Tiger Reserve for their
   support. The first author is thankful to Mr. Shahid A. Dar for
   troubleshooting and suggestions with the analysis. The first author also
   thanks Ms. Shaizah Tajdar for her support.
CR Akhtar N, 2004, URSUS, V15, P203, DOI 10.2192/1537-6176(2004)015<0203:SBHUID>2.0.CO;2
   Akhtar N, 2007, URSUS, V18, P203, DOI 10.2192/1537-6176(2007)18[203:COSBDD]2.0.CO;2
   [Anonymous], 2017, R LANG ENV STAT COMP, DOI DOI 10.2788/95827
   Ash E, 2021, LANDSCAPE ECOL, V36, P455, DOI 10.1007/s10980-020-01105-6
   Atzeni L, 2020, ECOL EVOL, V10, P7686, DOI 10.1002/ece3.6492
   Boyce MS, 2003, ECOSCIENCE, V10, P421, DOI 10.1080/11956860.2003.11682790
   Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Breiman L., 1996, OUT OF BAG ESTIMATIO, P1
   Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45
   Chawla NV, 2003, LECT NOTES ARTIF INT, V2838, P107, DOI 10.1007/978-3-540-39804-2_12
   Chen C., 2004, USING RANDOM FOREST
   Ciarniello LM, 2007, ECOL APPL, V17, P1424, DOI 10.1890/06-1100.1
   Cushman SA, 2004, OIKOS, V105, P117, DOI 10.1111/j.0030-1299.2004.12524.x
   Cushman SA, 2018, MACHINE LEARNING ECO, DOI [10.1007/978-3-319-96978-7_9, DOI 10.1007/978-3-319-96978-7_9]
   Cushman S, 2017, LANDSCAPE ECOL, V32, P1581, DOI 10.1007/s10980-017-0520-0
   Cutler DR, 2007, ECOLOGY, V88, P2783, DOI 10.1890/07-0539.1
   Dar SA, 2021, ANIM CONSERV, V24, P659, DOI 10.1111/acv.12671
   Das S, 2014, URSUS, V25, P111, DOI 10.2192/URSUS-D-14-00008.1
   DHARAIYA N., 2016, IUCN RED LIST THREAT, V2016, DOI 10.2305/IUCN.UK.2016
   Drew CA, 2011, PREDICTIVE SPECIES AND HABITAT MODELING IN LANDSCAPE ECOLOOGY: CONCEPTS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-7390-0
   Elith J, 2006, ECOGRAPHY, V29, P129, DOI 10.1111/j.2006.0906-7590.04596.x
   Evans J.S., 2018, rfUtilities. R package version 2.1-3
   Evans JS, 2011, PREDICTIVE SPECIES AND HABITAT MODELING IN LANDSCAPE ECOLOOGY: CONCEPTS AND APPLICATIONS, P139, DOI 10.1007/978-1-4419-7390-0_8
   Fisher JT, 2011, ECOL EVOL, V1, DOI 10.1002/ece3.45
   Garshelis David L., 1999, P225
   Guisan A, 2000, ECOL MODEL, V135, P147, DOI 10.1016/S0304-3800(00)00354-9
   Hegel TM, 2010, SPATIAL COMPLEXITY, INFORMATICS, AND WILDLIFE CONSERVATION, P273, DOI 10.1007/978-4-431-87771-4_16
   Hostetler Mark, 2000, Urban Ecosystems, V4, P25, DOI 10.1023/A:1009587719462
   Johnsingh A. J. T., 2003, Journal of the Bombay Natural History Society, V100, P190
   JOHNSON DH, 1980, ECOLOGY, V61, P65, DOI 10.2307/1937156
   JOSHI AR, 1995, J WILDLIFE MANAGE, V59, P204, DOI 10.2307/3808932
   Khosravi R, 2019, LANDSCAPE ECOL, V34, P2451, DOI 10.1007/s10980-019-00900-0
   Klaassen B, 2018, ECOL EVOL, V8, P7611, DOI 10.1002/ece3.4269
   Liaw A., 2002, R NEWS, V2, P18, DOI DOI 10.1177/154405910408300516
   Manly BFJ., 1993, RESOURCE SELECTION A, DOI [10.1007/978-94-011-1558-2, DOI 10.1007/978-94-011-1558-2]
   Martin J, 2012, J APPL ECOL, V49, P621, DOI 10.1111/j.1365-2664.2012.02139.x
   Sanchez MCM, 2014, INT J GEOGR INF SCI, V28, P1531, DOI 10.1080/13658816.2013.776684
   Mayer AL, 2003, LANDSCAPE URBAN PLAN, V65, P201, DOI 10.1016/S0169-2046(03)00057-4
   Mayor SJ, 2007, ECOLOGY, V88, P1634, DOI 10.1890/06-1672.1
   Mayor SJ, 2009, ECOSCIENCE, V16, P238, DOI 10.2980/16-2-3238
   McGarigal K, 2016, LANDSCAPE ECOL, V31, P1161, DOI 10.1007/s10980-016-0374-x
   Mi CR, 2017, PEERJ, V5, DOI 10.7717/peerj.2849
   Murphy MA, 2010, ECOLOGY, V91, P252, DOI 10.1890/08-0879.1
   Puri M, 2015, DIVERS DISTRIB, V21, P1087, DOI 10.1111/ddi.12335
   Ramesh T, 2012, URSUS, V23, P78, DOI 10.2192/URSUS-D-11-00006.1
   Rather TA, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-68167-z
   Rather TA, 2021, PEERJ, V9, DOI 10.7717/peerj.10634
   Rather TA, 2020, ECOL PROCESS, V9, DOI 10.1186/s13717-020-00265-2
   Rather TA, 2020, URSUS, V31, DOI 10.2192/URSUS-D-19-00013.2
   Ratnayeke S, 2007, WILDLIFE BIOL, V13, P272, DOI 10.2981/0909-6396(2007)13[272:HRAHUO]2.0.CO;2
   Sathyakumar S., 2012, NATL BEAR CONSERVATI
   Schaefer JA, 1995, ECOGRAPHY, V18, P333, DOI 10.1111/j.1600-0587.1995.tb00136.x
   Schneider, 1994, QUANTITATIVE ECOLOGY
   Schneider DC, 1998, COM ECO SYS, P253
   Schneider DC, 2001, BIOSCIENCE, V51, P545, DOI 10.1641/0006-3568(2001)051[0545:TROTCO]2.0.CO;2
   Schneider DC, 1997, J EXP MAR BIOL ECOL, V216, P129, DOI 10.1016/S0022-0981(97)00093-2
   Shirk AJ., 2012, SCALE DEPENDENCY AM
   Shirk AJ, 2014, ECOL APPL, V24, P1434, DOI 10.1890/13-1510.1
   Wan HY, 2017, CONDOR, V119, P641, DOI 10.1650/CONDOR-17-32.1
   Wasserman TN, 2012, RMRSRP94 USDA FOR SE, P21
   WIENS JA, 1989, FUNCT ECOL, V3, P385, DOI 10.2307/2389612
   Yoganand K., 2006, Journal of the Bombay Natural History Society, V103, P172
   Yoganand K., 2005, THESIS SAURASHTRA U
NR 65
TC 0
Z9 0
U1 4
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
EI 2192-1709
J9 ECOL PROCESS
JI Ecol. Process.
PD JUN 29
PY 2021
VL 10
IS 1
AR 48
DI 10.1186/s13717-021-00323-3
PG 12
WC Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA TD7IL
UT WOS:000669495700001
OA gold
DA 2022-02-10
ER

PT J
AU Miller, GA
   Hyslop, JJ
   Barclay, D
   Edwards, A
   Thomson, W
   Duthie, CA
AF Miller, Gemma A.
   Hyslop, James J.
   Barclay, David
   Edwards, Andrew
   Thomson, William
   Duthie, Carol-Anne
TI Using 3D Imaging and Machine Learning to Predict Liveweight and Carcass
   Characteristics of Live Finishing Beef Cattle
SO FRONTIERS IN SUSTAINABLE FOOD SYSTEMS
LA English
DT Article
DE finishing beef cattle; 3D imaging; carcass characteristics; machine
   learning; precision livestock farming
ID BODY CONDITION SCORE; WEIGHT ESTIMATION; RETAIL PRODUCT; CAMERA SYSTEM;
   PIGS; ULTRASOUND; VISION
AB Selection of finishing beef cattle for slaughter and evaluation of performance is currently achieved through visual assessment and/or by weighing through a crush. Consequently, large numbers of cattle are not meeting target specification at the abattoir. Video imaging analysis (VIA) is increasingly used in abattoirs to grade carcasses with high accuracy. There is potential for three-dimensional (3D) imaging to be used on farm to predict carcass characteristics of live animals and to optimise slaughter selections. The objectives of this study were to predict liveweight (LW) and carcass characteristics of live animals using 3D imaging technology and machine learning algorithms (artificial neural networks). Three dimensional images and LW's were passively collected from finishing steer and heifer beef cattle of a variety of breeds pre-slaughter (either on farm or after entry to the abattoir lairage) using an automated camera system. Sixty potential predictor variables were automatically extracted from the live animal 3D images using bespoke algorithms; these variables included lengths, heights, widths, areas, volumes, and ratios and were used to develop predictive models for liveweight and carcass characteristics. Cold carcass weights (CCW) for each animal were provided by the abattoir. Saleable meat yield (SMY) and EUROP fat and conformation grades were also determined for each individual by VIA of half of the carcass. Performance of prediction models was assessed using R-2 and RMSE parameters following regression of predicted and actual variables for LW (R-2 = 0.7, RMSE = 42), CCW (R-2 = 0.88, RMSE = 14) and SMY (R-2 = 0.72, RMSE = 14). The models predicted EUROP fat and conformation grades with 54 and 55% accuracy (R-2), respectively. This study demonstrated that 3D imaging coupled with machine learning analytics can be used to predict LW, SMY and traditional carcass characteristics of live animals. This system presents an opportunity to reduce a considerable inefficiency in beef production enterprises through autonomous monitoring of finishing cattle on the farm and marketing of animals at the optimal time.
C1 [Miller, Gemma A.; Duthie, Carol-Anne] Scotlands Rural Coll, Future Farming Syst, Edinburgh, Midlothian, Scotland.
   [Hyslop, James J.] SAC Consulting Ltd, SRUC, Edinburgh, Midlothian, Scotland.
   [Barclay, David] Innovent Technol Ltd, Turriff, Scotland.
   [Edwards, Andrew] Ritchie Ltd, Forfar, Scotland.
   [Thomson, William] Harbro Ltd, Turriff, Scotland.
RP Miller, GA (corresponding author), Scotlands Rural Coll, Future Farming Syst, Edinburgh, Midlothian, Scotland.
EM gemma.miller@sruc.ac.uk
OI Duthie, Carol-Anne/0000-0003-3020-9078; /0000-0001-7810-9987
FU Innovate UKUK Research & Innovation (UKRI)Innovate UK [101812]
FX This research was funded by Innovate UK (Grant No. 101812).
CR Afolayan RA, 2006, CZECH J ANIM SCI, V51, P343, DOI 10.17221/3948-CJAS
   Agatonovic-Kustrin S, 2000, J PHARMACEUT BIOMED, V22, P717, DOI 10.1016/S0731-7085(99)00272-1
   AHDB, 2018, UK YB 2018 CATTL
   AHDB, 2018, MARK PRIM BEEF CATTL
   Aydin A, 2017, COMPUT ELECTRON AGR, V135, P4, DOI 10.1016/j.compag.2017.01.024
   Bewley JM, 2008, J DAIRY SCI, V91, P3439, DOI 10.3168/jds.2007-0836
   Craigie CR, 2012, MEAT SCI, V92, P307, DOI 10.1016/j.meatsci.2012.05.028
   de Vries M, 2010, LIVEST SCI, V128, P1, DOI 10.1016/j.livsci.2009.11.007
   Fischer A, 2015, J DAIRY SCI, V98, P4465, DOI 10.3168/jds.2014-8969
   Gevrey M, 2003, ECOL MODEL, V160, P249, DOI 10.1016/S0304-3800(02)00257-0
   Greiner SP, 2003, J ANIM SCI, V81, P1736
   Hyslop J. J., 2008, P BRIT SOC AN SCI 20
   Hyslop J. J., 2009, P BRIT SOC AN SCI 20
   Kashiha M, 2014, COMPUT ELECTRON AGR, V107, P38, DOI 10.1016/j.compag.2014.06.003
   Kuzuhara Y, 2015, COMPUT ELECTRON AGR, V111, P186, DOI 10.1016/j.compag.2014.12.020
   Lambe NR, 2008, MEAT SCI, V80, P1138, DOI 10.1016/j.meatsci.2008.05.026
   Lambe NR, 2010, LIVEST SCI, V131, P193, DOI 10.1016/j.livsci.2010.03.019
   Minchin W, 2009, IRISH J AGR FOOD RES, V48, P75
   Mortensen AK, 2016, COMPUT ELECTRON AGR, V123, P319, DOI 10.1016/j.compag.2016.03.011
   Mullah MBR, 2010, COMPUT ELECTRON AGR, V72, P48, DOI 10.1016/j.compag.2010.02.002
   Ozkaya S, 2016, ANIM PROD SCI, V56, P2060, DOI 10.1071/AN14943
   Pogorzelska-Przybylek P, 2014, ANN ANIM SCI, V14, P429, DOI 10.2478/aoas-2014-0004
   R Core Team, 2017, R LANG ENV STAT COMP
   Realini CE, 2001, J ANIM SCI, V79, P1378
   Roehe R., 2013, UNDERSTANDING INEFFI
   Shi C, 2016, COMPUT ELECTRON AGR, V129, P37, DOI 10.1016/j.compag.2016.08.012
   Tasdemir S, 2011, COMPUT ELECTRON AGR, V76, P189, DOI 10.1016/j.compag.2011.02.001
   Van Hertem T, 2014, BIOSYST ENG, V119, P108, DOI 10.1016/j.biosystemseng.2014.01.009
   Viazzi S, 2014, COMPUT ELECTRON AGR, V100, P139, DOI 10.1016/j.compag.2013.11.005
   Wang Y, 2008, BIOSYST ENG, V100, P117, DOI 10.1016/j.biosystemseng.2007.08.008
   Weber A, 2014, LIVEST SCI, V165, P129, DOI 10.1016/j.livsci.2014.03.022
   Wongsriworaphon A, 2015, COMPUT ELECTRON AGR, V115, P26, DOI 10.1016/j.compag.2015.05.004
NR 32
TC 8
Z9 8
U1 5
U2 11
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
EI 2571-581X
J9 FRONT SUSTAIN FOOD S
JI Front. Sustain. Food Syst.
PD MAY 1
PY 2019
VL 3
AR 30
DI 10.3389/fsufs.2019.00030
PG 9
WC Food Science & Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Food Science & Technology
GA LR3IM
UT WOS:000535587100001
OA gold
DA 2022-02-10
ER

PT J
AU Allen, ML
   Sibarani, MC
   Krofel, M
AF Allen, Maximilian L.
   Sibarani, Marsya C.
   Krofel, Miha
TI Predicting preferred prey of Sumatran tigers Panthera tigris sumatrae
   via spatio-temporal overlap
SO ORYX
LA English
DT Article
DE Activity patterns; composite score; Panthera tigris; prey preference;
   spatial overlap; Sumatra; temporal overlap; tiger
AB Encounter rates of carnivores with prey are dependent on spatial and temporal overlap, and are often highest with their preferred prey. The Critically Endangered Sumatran tiger Panthera tigris sumatrae is dependent on prey populations, but little is known about its prey preferences. We collected camera-trap data for 7 years (2010-2016) in Bukit Barisan Selatan National Park, Sumatra, to investigate spatial and temporal overlap of tigers with potential prey species. We also developed a novel method to predict predator-prey encounter rates and potential prey preferences from camera-trap data. We documented at least 10 individual tigers, with an overall detection rate of 0.24 detections/100 trap nights. Tigers exhibited a diurnal activity pattern and had highest temporal overlap with wild boar Sus scrofa and pig-tailed macaques Macaca nemestrina, but highest spatial overlap with wild boar and sambar deer Rusa unicolor. We created a spatial and temporal composite score and three additional composite scores with adjustments for the spatial overlap and preferred prey mass. Wild boars ranked highest for all composite scores, followed by sambar deer, and both are known as preferred tiger prey in other areas. Spatial and temporal overlaps are often considered as separate indices, but a composite score may facilitate better predictions of encounter rates and potential prey preferences. Our findings suggest that prey management efforts in this area should focus on wild boar and sambar deer, to ensure a robust prey base for this Critically Endangered tiger population.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816S Oak St, Champaign, IL 61820 USA.
   [Sibarani, Marsya C.] Wildlife Conservat Soc Indonesia Program, Bogor, West Java, Indonesia.
   [Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
OI Krofel, Miha/0000-0002-2010-5219
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation;
   Illinois Natural History Survey; Slovenian Research AgencySlovenian
   Research Agency - Slovenia [P4-0059]
FX All data used in this study were collected by the Tropical Ecology
   Assessment and Monitoring Network, a collaboration between Conservation
   International, the Missouri Botanical Garden, the Smithsonian
   Institution and the Wildlife Conservation Society. The work was
   partially funded by these institutions, the Gordon and Betty Moore
   Foundation, the Illinois Natural History Survey, the Slovenian Research
   Agency (P4-0059), and other donors. Monitoring activities were managed
   by the Wildlife Conservation Society in collaboration with the Bukit
   Barisan Selatan National Park and the Ministry of Environment and
   Forestry, Republic of Indonesia. We thank all field staff and forest
   rangers involved in camera-trap deployment, and W. Marthy for help in
   the field and coordination.
CR Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Allen ML, 2017, J ETHOL, V35, P13, DOI 10.1007/s10164-016-0492-6
   Allen ML, 2016, ECOLOGY, V97, P1905, DOI 10.1002/ecy.1462
   Barber-Meyer SM, 2013, J ZOOL, V289, P10, DOI 10.1111/j.1469-7998.2012.00956.x
   Basak Krishnendu, 2018, Proceedings of the Zoological Society (Calcutta), V71, P92, DOI 10.1007/s12595-016-0196-5
   Begon M., 2006, ECOLOGY INDIVIDUALS
   Clinchy M, 2016, BEHAV ECOL, V27, P1826, DOI 10.1093/beheco/arw117
   du Preez B, 2017, J ZOOL, V302, P149, DOI 10.1111/jzo.12443
   Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088
   Fortin D, 2015, P ROY SOC B-BIOL SCI, V282, P99, DOI 10.1098/rspb.2015.0973
   Hayward MW, 2012, J ZOOL, V286, P221, DOI 10.1111/j.1469-7998.2011.00871.x
   HOLLING C. S., 1959, CANADIAN ENT, V91, P293
   Karanth KU, 2004, P NATL ACAD SCI USA, V101, P4854, DOI 10.1073/pnas.0306210101
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Linkie M, 2003, ORYX, V37, P41, DOI 10.1017/S0030605303000103
   Linkie M., 2008, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2008.RLTS.T15966A5334836.en, DOI 10.2305/IUCN.UK.2008.RLTS.T15966A5334836.EN]
   Meredith M., 2017, OVERVIEW OVERLAP PAC
   Miquelle Dale G., 1999, P71
   Ngoprasert D, 2012, BIOTROPICA, V44, P810, DOI 10.1111/j.1744-7429.2012.00878.x
   Nowak R.M., 1999, WALKERS MAMMALS WORL
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien Timothy G., 1996, Oryx, V30, P207
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Pattanavibool A, 2015, IUCN RED LIST THREAT
   Pusparini W, 2018, ORYX, V52, P25, DOI 10.1017/S0030605317001144
   Ramakrishnan U, 1999, BIOL CONSERV, V89, P113, DOI 10.1016/S0006-3207(98)00159-1
   Rich LN, 2019, BIOL CONSERV, V233, P12, DOI 10.1016/j.biocon.2019.02.018
   Rich LN, 2016, J APPL ECOL, V53, P1225, DOI 10.1111/1365-2664.12650
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rovero F., 2016, CAMERA TRAPPING WILD
   Saggiomo L., 2017, FOOD WEBS, V12, P35, DOI DOI 10.1016/J.FOOWEB.2017.01.001
   Sanderson EW, 2010, TIGERS OF THE WORLD: THE SCIENCE, POLITICS, AND CONSERVATION OF PANTHERA TIGRIS, 2ND EDITION, P143, DOI 10.1016/B978-0-8155-1570-8.00009-8
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   SEIDENSTICKER J, 1993, SYM ZOOL S, P105
   Seidensticker J, 2010, INTEGR ZOOL, V5, P285, DOI 10.1111/j.1749-4877.2010.00214.x
   Sibarani MC, 2019, J APPL ECOL, V56, P1220, DOI 10.1111/1365-2664.13360
   Song MK, 2013, NURS RES, V62, P45, DOI 10.1097/NNR.0b013e3182741948
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   TEAM Network, 2011, TERRESTRIAL VERTEBRA
   Team RC, 2014, R LANG ENV STAT COMP
   Walston J, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000485
NR 41
TC 0
Z9 0
U1 9
U2 11
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0030-6053
EI 1365-3008
J9 ORYX
JI Oryx
PD MAR
PY 2021
VL 55
IS 2
BP 197
EP 203
DI 10.1017/S0030605319000577
PG 7
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA RF0WN
UT WOS:000634568800011
OA Green Published, hybrid
DA 2022-02-10
ER

PT J
AU Bain, M
   Nagrani, A
   Schofield, D
   Berdugo, S
   Bessa, J
   Owen, J
   Hockings, KJ
   Matsuzawa, T
   Hayashi, M
   Biro, D
   Carvalho, S
   Zisserman, A
AF Bain, Max
   Nagrani, Arsha
   Schofield, Daniel
   Berdugo, Sophie
   Bessa, Joana
   Owen, Jake
   Hockings, Kimberley J.
   Matsuzawa, Tetsuro
   Hayashi, Misato
   Biro, Dora
   Carvalho, Susana
   Zisserman, Andrew
TI Automated audiovisual behavior recognition in wild primates
SO SCIENCE ADVANCES
LA English
DT Article
ID CHIMPANZEES
AB Large video datasets of wild animal behavior are crucial to produce longitudinal research and accelerate conservation efforts; however, large-scale behavior analyses continue to be severely constrained by time and resources. We present a deep convolutional neural network approach and fully automated pipeline to detect and track two audiovisually distinctive actions in wild chimpanzees: buttress drumming and nut cracking. Using camera trap and direct video recordings, we train action recognition models using audio and visual signatures of both behaviors, attaining high average precision (buttress drumming: 0.87 and nut cracking: 0.85), and demonstrate the potential for behavioral analysis using the automatically parsed video. Our approach produces the first automated audiovisual action recognition of wild primate behavior, setting a milestone for exploiting large datasets in ethology and conservation.
C1 [Bain, Max; Nagrani, Arsha; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Visual Geometry Grp, Oxford, England.
   [Schofield, Daniel; Berdugo, Sophie; Carvalho, Susana] Univ Oxford, Sch Anthropol & Museum Ethnog, Inst Human Sci, Primate Models Behav Evolut Lab, Oxford, England.
   [Berdugo, Sophie] Univ Oxford, Sch Anthropol & Museum Ethnog, Inst Human Sci, Social Body Lab, Oxford, England.
   [Bessa, Joana; Owen, Jake; Biro, Dora] Univ Oxford, Dept Zool, Oxford, England.
   [Hockings, Kimberley J.] Univ Exeter, Coll Life & Environm Sci, Ctr Ecol & Conservat, Exeter, Devon, England.
   [Matsuzawa, Tetsuro] CALTECH, Div Humanities & Social Sci, 1200 E Calif Blvd,MC 228-77, Pasadena, CA 91125 USA.
   [Biro, Dora] Univ Rochester, Dept Brain & Cognit Sci, Rochester, NY USA.
   [Carvalho, Susana] Gorongosa Natl Pk, Sofala, Mozambique.
   [Carvalho, Susana] Coimbra Univ, Ctr Funct Ecol, Dept Life Sci, Coimbra, Portugal.
   [Carvalho, Susana] Algarve Univ, Interdisciplinary Ctr Archaeol & Evolut Human Beh, Faro, Portugal.
   [Hayashi, Misato] Chubu Gakuin Univ, 30-1 Naka Oida Cho, Kakamigahara, Gifu 5040837, Japan.
   [Hayashi, Misato] Japan Monkey Ctr, Kanrin 26, Inuyama, Aichi 4840081, Japan.
RP Bain, M (corresponding author), Univ Oxford, Dept Engn Sci, Visual Geometry Grp, Oxford, England.
EM maxbain@robots.ox.ac.uk
RI Matsuzawa, Tetsuro/ABE-9654-2021
OI Matsuzawa, Tetsuro/0000-0002-8147-2725; Biro, Dora/0000-0002-3408-6274;
   Bain, Max/0000-0002-2345-5441; Hayashi, Misato/0000-0001-7289-6414;
   Bessa, Joana/0000-0001-7196-5369; Berdugo, Sophie/0000-0003-2162-1087;
   Nagrani, Arsha/0000-0003-2190-9013; Owen, Jake/0000-0002-8490-3949;
   Schofield, Daniel/0000-0002-3308-0209; Hockings,
   Kimberley/0000-0002-6187-644X
FU EPSRCUK Research & Innovation (UKRI)Engineering & Physical Sciences
   Research Council (EPSRC) [Seebibyte EP/M013774/1, Visual AI
   EP/T028572/1]; Google PhD FellowshipGoogle Incorporated; Clarendon Fund;
   Boise Trust Fund; Wolfson College, University of Oxford; Keble College
   Sloane-Robinson Clarendon Scholarship, University of Oxford; Fundacao
   para a Ciencia e a Tecnologia, PortugalPortuguese Foundation for Science
   and Technology [SFRH/BD/108185/2015]; Templeton World Charity Foundation
   [TWCF0316]; National Geographic SocietyNational Geographic Society; St
   Hugh's College, University of Oxford; Kyoto University Primate Research
   Institute for Cooperative Research Program; MEXT-JSPSMinistry of
   Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of Science [16H06283]; Japan Society for the
   Promotion of ScienceMinistry of Education, Culture, Sports, Science and
   Technology, Japan (MEXT)Japan Society for the Promotion of Science;
   Darwin Initiative [26-018]; LGP-U04
FX This study was supported by EPSRC Programme Grants Seebibyte
   EP/M013774/1 and Visual AI EP/T028572/1; Google PhD Fellowship (to
   A.N.); Clarendon Fund (to D.S. and S.B.); Boise Trust Fund (to D.S.,
   S.B., and J.B.); Wolfson College, University of Oxford (to D.S.); Keble
   College Sloane-Robinson Clarendon Scholarship, University of Oxford (to
   S.B.); Fundacao para a Ciencia e a Tecnologia, Portugal
   SFRH/BD/108185/2015 (to J.B.); Templeton World Charity Foundation grant
   no. TWCF0316 (to D.B.); National Geographic Society (to S.C.); St Hugh's
   College, University of Oxford (to S.C.); Kyoto University Primate
   Research Institute for Cooperative Research Program (to M.H. and D.S.);
   MEXT-JSPS (no. 16H06283), LGP-U04, the Japan Society for the Promotion
   of Science (to T.M.); and Darwin Initiative funding grant number 26-018
   (to K.J.H.).
CR Anderson DJ, 2014, NEURON, V84, P18, DOI 10.1016/j.neuron.2014.09.005
   Arcadi AC, 1998, PRIMATES, V39, P505, DOI 10.1007/BF02557572
   Babiszewska M, 2015, AM J PHYS ANTHROPOL, V156, P125, DOI 10.1002/ajpa.22634
   Bessa J, 2021, FRONT ECOL EVOL, V9, DOI 10.3389/fevo.2021.625303
   Biro D, 2003, ANIM COGN, V6, P213, DOI 10.1007/s10071-003-0183-x
   Boesch C., 1991, Human Evolution, V6, P81, DOI 10.1007/BF02435610
   Burghardt T, 2020, ARXIV201110759CSCV
   Cantor M, 2020, J ANIM ECOL, DOI 10.1111/1365-2656.13336
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Carvalho S, 2008, J HUM EVOL, V55, P148, DOI 10.1016/j.jhevol.2008.02.005
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI 10.1109/ICASSP40776.2020.9053174
   Chen P, 2020, ECOL EVOL, V10, P3561, DOI 10.1002/ece3.6152
   Christiansen F, 2013, BEHAV ECOL, V24, P1415, DOI 10.1093/beheco/art086
   DEB D, 2018, ARXIV180408790CSCV
   Dominoni DM, 2020, NAT ECOL EVOL, V4, P502, DOI 10.1038/s41559-020-1135-4
   Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   Huang K, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-22970-y
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kaufhold SP, 2019, BIOL LETTERS, V15, DOI 10.1098/rsbl.2019.0695
   Krause J, 2013, TRENDS ECOL EVOL, V28, P541, DOI 10.1016/j.tree.2013.06.002
   Kuehl HS, 2019, SCIENCE, V363, P1453, DOI 10.1126/science.aau4532
   Mathis A, 2018, NAT NEUROSCI, V21, P1281, DOI 10.1038/s41593-018-0209-y
   Matsuzawa T, 2011, PRIMATOL MONOGR, P1, DOI 10.1007/978-4-431-53921-6
   Matsuzawa Tetsuro, 1994, P351
   Nishida T., 2011, CHIMPANZEES LAKESHOR
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Reynolds V., 2005, CHIMPANZEES BUDONGO
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Sturman O, 2020, NEUROPSYCHOPHARMACOL, V45, P1942, DOI 10.1038/s41386-020-0776-y
   Swarup P, 2021, GLOB ECOL CONSERV, V26, DOI 10.1016/j.gecco.2021.e01510
   Tinbergen N., 1963, Zeitschrift fuer Tierpsychologie, V20, P410
   van Dam EA, 2020, J NEUROSCI METH, V332, DOI 10.1016/j.jneumeth.2019.108536
   Whiten A, 2001, BEHAVIOUR, V138, P1481, DOI 10.1163/156853901317367717
   Yu JW, 2019, IEEE INT CONF COMP V, P302, DOI 10.1109/ICCVW.2019.00040
   Zhou K, 2016, DESTECH TRANS COMP
   Zisserman, 2019, WORKSH COMP VIS WILD
NR 42
TC 0
Z9 0
U1 4
U2 4
PU AMER ASSOC ADVANCEMENT SCIENCE
PI WASHINGTON
PA 1200 NEW YORK AVE, NW, WASHINGTON, DC 20005 USA
SN 2375-2548
J9 SCI ADV
JI Sci. Adv.
PD NOV
PY 2021
VL 7
IS 46
AR eabi4883
DI 10.1126/sciadv.abi4883
PG 11
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA XA0JL
UT WOS:000720344200005
PM 34767448
OA Green Published, gold, Green Accepted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Harris, RE
   Olson, LO
   Olson, ER
   Van Stappen, J
   Van Deelen, TR
AF Allen, Maximilian L.
   Harris, Rachel E.
   Olson, Lucas O.
   Olson, Erik R.
   Van Stappen, Julie
   Van Deelen, Timothy R.
TI Resource limitations and competitive interactions affect carnivore
   community composition at different ecological scales in a temperate
   island system
SO MAMMALIA
LA English
DT Article
DE American marten; camera trap; carnivores; community ecology; island
   biogeography; Martes americana; scale
ID MARTENS MARTES-AMERICANA; POPULATION; FISHERS; GUILD
AB Selective pressures (i.e. resource limitation and competitive interaction) that drive the composition of ecological communities vary, and often operate on different ecological scales (ecological variables across varying spatial scales) than observed patterns. We studied the drivers of distribution and abundance of the American marten (Martes americana) and the carnivore community at three ecological scales on a Great Lakes island archipelago using camera traps. We found different drivers appeared important at each ecological scale and studying any of the three scales alone would give a biased understanding of the process driving the system. Island biogeography (resource limitation) was most important for carnivore richness, with higher richness on larger islands and lower richness as distance from the mainland increased. Marten presence on individual islands appeared to be driven by island size (resource limitation) and human avoidance (competitive interaction). Marten abundance at camera trap sites was driven by the cascading effect of coyotes (Canis latrans) on fishers (Pekania pennanti) (competitive interaction). Incorporating three ecological scales gave novel insights into the varying effects of resource limitation and competitive interaction processes. Our data suggests that ecological communities are structured through multiple competing ecological forces, and effective management and conservation relies on our ability to understand ecological forces operating at multiple ecological scales.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Harris, Rachel E.; Olson, Lucas O.; Van Deelen, Timothy R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, Madison, WI USA.
   [Olson, Erik R.] Northland Coll, Nat Resources, 1411 Ellis Ave S, Ashland, WI 54806 USA.
   [Van Stappen, Julie] Apostle Isl Natl Lakeshore, Planning & Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Apostle Islands National Lakeshore (GLNF CESU) [P14AC01180]; Northland
   College (Department of Natural Resources); Northland College (Sigurd
   Olson Professorship in the Natural Sciences); Northland College (Morris
   O. Ristvedt Professorship in the Natural Sciences); University of
   Wisconsin (Schorger fund, Department of Forest and Wildlife Ecology);
   University of Wisconsin (Beers-Bascom Professorship in Conservation)
FX This project was supported by the Apostle Islands National Lakeshore
   (GLNF CESU Agreement P14AC01180), Northland College (Department of
   Natural Resources; Sigurd Olson Professorship in the Natural Sciences;
   Morris O. Ristvedt Professorship in the Natural Sciences), and the
   University of Wisconsin (Schorger fund, Department of Forest and
   Wildlife Ecology; Beers-Bascom Professorship in Conservation). We thank
   students and technicians from the National Park Service, University of
   Wisconsin-Madison, and Northland College for their assistance with
   fieldwork.
CR Allen ML, 2018, COMMUNITY ECOL, V19, P272, DOI 10.1556/168.2018.19.3.8
   Allen ML, 2018, AM MIDL NAT, V179, P294
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Anderson K. A., 2002, MODEL SELECTION MULT
   [Anonymous], 2016, WISCLAND 2 0 USER GU
   Brown JS, 1999, J MAMMAL, V80, P385, DOI 10.2307/1383287
   Carlson JE, 2014, J WILDLIFE MANAGE, V78, P1499, DOI 10.1002/jwmg.785
   Emerson BC, 2008, TRENDS ECOL EVOL, V23, P619, DOI 10.1016/j.tree.2008.07.005
   ESRI, 2014, ARCGIS DESKT
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Fisher JT, 2013, ECOGRAPHY, V36, P240, DOI 10.1111/j.1600-0587.2012.07556.x
   Gilbert Jonathan H., 1997, P135
   HAIRSTON NG, 1960, AM NAT, V94, P421, DOI 10.1086/282146
   Howk F, 2009, J GREAT LAKES RES, V35, P159, DOI 10.1016/j.jglr.2008.11.002
   Jackman, 2015, PSCL CLASSES METHODS
   Judziewicz E.J., 1993, MICHIGAN BOT, V32, P43
   Leibold MA, 2004, ECOL LETT, V7, P601, DOI 10.1111/j.1461-0248.2004.00608.x
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   LEVIN SA, 1992, ECOLOGY, V73, P1943, DOI 10.2307/1941447
   MAC ARTHUR ROBERT H., 1967
   Naing H, 2015, RAFFLES B ZOOL, V63, P376
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Oksanen L, 2000, AM NAT, V155, P703, DOI 10.1086/303354
   POWER ME, 1992, ECOLOGY, V73, P733, DOI 10.2307/1940153
   R Core Team, 2017, R LANG ENV STAT COMP
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   RUGGIERO LF, 1994, CONSERV BIOL, V8, P364, DOI 10.1046/j.1523-1739.1994.08020364.x
   Shnberloff D.S., 1974, Annual Rev Ecol Syst, V5, P161
   Thompson ID., 2012, BIOL CONSERVATION MA, P209
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Whittaker R.J., 2007, ISLAND BIOGEOGRAPHY, V2, P416
   Woodford J.E., 2011, CONSERVATION MANAGEM, P43
   Zeileis A, 2008, J STAT SOFTW, V27, P1, DOI 10.18637/jss.v027.i08
   Zielinski WJ, 2008, J WILDLIFE MANAGE, V72, P1558, DOI 10.2193/2007-397
   Zielinski WJ, 2004, J MAMMAL, V85, P470, DOI 10.1644/1545-1542(2004)085<0470:DOSPOA>2.0.CO;2
NR 36
TC 8
Z9 8
U1 0
U2 9
PU WALTER DE GRUYTER GMBH
PI BERLIN
PA GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY
SN 0025-1461
EI 1864-1547
J9 MAMMALIA
JI Mammalia
PD NOV
PY 2019
VL 83
IS 6
BP 552
EP 561
DI 10.1515/mammalia-2017-0162
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA JL9SZ
UT WOS:000495866900004
DA 2022-02-10
ER

PT J
AU Harrison, D
   De Leo, FC
   Gallin, WJ
   Mir, F
   Marini, S
   Leys, SP
AF Harrison, Dominica
   De Leo, Fabio Cabrera
   Gallin, Warren J.
   Mir, Farin
   Marini, Simone
   Leys, Sally P.
TI Machine Learning Applications of Convolutional Neural Networks and Unet
   Architecture to Predict and Classify Demosponge Behavior
SO WATER
LA English
DT Article
DE convolutional neural networks (CNN); unet; machine learning; semantic
   segmentation; demosponge behavior; classification; time series; deep
   learning; image analysis
ID FOREST MAMMALS; CAMERA-TRAP; CONTRACTIONS
AB Biological data sets are increasingly becoming information-dense, making it effective to use a computer science-based analysis. We used convolution neural networks (CNN) and the specific CNN architecture Unet to study sponge behavior over time. We analyzed a large time series of hourly high-resolution still images of a marine sponge, Suberites concinnus (Demospongiae, Suberitidae) captured between 2012 and 2015 using the NEPTUNE seafloor cabled observatory, off the west coast of Vancouver Island, Canada. We applied semantic segmentation with the Unet architecture with some modifications, including adapting parts of the architecture to be more applicable to three-channel images (RGB). Some alterations that made this model successful were the use of a dice-loss coefficient, Adam optimizer and a dropout function after each convolutional layer which provided losses, accuracies and dice scores of up to 0.03, 0.98 and 0.97, respectively. The model was tested with five-fold cross-validation. This study is a first step towards analyzing trends in the behavior of a demosponge in an environment that experiences severe seasonal and inter-annual changes in climate. The end objective is to correlate changes in sponge size (activity) over seasons and years with environmental variables collected from the same observatory platform. Our work provides a roadmap for others who seek to cross the interdisciplinary boundaries between biology and computer science.
C1 [Harrison, Dominica; Gallin, Warren J.; Mir, Farin; Leys, Sally P.] Univ Alberta, Dept Biol Sci, Edmonton, AB T6H 3C4, Canada.
   [Harrison, Dominica; De Leo, Fabio Cabrera] Univ Victoria, Dept Biol, Victoria, BC V8W 2Y2, Canada.
   [De Leo, Fabio Cabrera] Univ Victoria, Ocean Networks Canada, Victoria, BC V8N IV8, Canada.
   [Marini, Simone] Natl Res Council Italy, Inst Marine Sci, Forte Santa Teresa, I-19032 La Spezia, Italy.
   [Marini, Simone] Stazione Zool Anton Dohrn SZN, I-80122 Naples, Italy.
RP Leys, SP (corresponding author), Univ Alberta, Dept Biol Sci, Edmonton, AB T6H 3C4, Canada.
EM dominica@ualberta.ca; fdeleo@uvic.ca; wgallin@ualberta.ca;
   farin@ualberta.ca; simone.marini@sp.ismar.cnr.it; sleys@ualberta.ca
RI Marini, Simone/C-3872-2012; Marini, Simone/AAA-3513-2022
OI Marini, Simone/0000-0003-0665-7815; Marini, Simone/0000-0003-0665-7815
FU Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC)
FX Natural Sciences and Engineering Research Council of Canada (NSERC)
   Discovery Grant awarded to S.P.L.
CR Aguzzi J, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-85240-3
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Allen RM, 2018, ANNU REV MAR SCI, V10, P19, DOI 10.1146/annurev-marine-121916-063134
   Alonso I, 2017, IEEE INT CONF COMP V, P2874, DOI 10.1109/ICCVW.2017.339
   [Anonymous], 2010, MATL COMP TOOL BOX I
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Chenard C, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52648-x
   Chu JWF, 2011, MAR ECOL PROG SER, V441, P1, DOI 10.3354/meps09381
   Doya C, 2014, J MARINE SYST, V130, P69, DOI 10.1016/j.jmarsys.2013.04.003
   Elliott GRD, 2007, J EXP BIOL, V210, P3736, DOI 10.1242/jeb.003392
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   FINCH CE, 1995, Q REV BIOL, V70, P1, DOI 10.1086/418864
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Guidi L., 2020, FUTURE SCI BRIEF BIG, V6th
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   He P., 2015, FISHERIES BYCATCH GL, DOI [10.4027/FBGICS.2015.07, DOI 10.4027/FBGICS.2015.07, 10.4027/fbgics.2015.07]
   Hughes TP, 2018, SCIENCE, V359, P80, DOI 10.1126/science.aan8048
   Ing N, 2018, PROC SPIE, V10581, DOI 10.1117/12.2293000
   Jadon S, 2020, P IEEE C COMP INT BI, DOI [10.1109/CIBCB48159.2020.9277638, DOI 10.1109/CIBCB48159.2020.9277638]
   Kahn AS, 2020, DEEP-SEA RES PT II, V173, DOI 10.1016/j.dsr2.2019.104729
   Kohavi R, 1995, AM J ORTHOD DENTOFAC, V118, P456, DOI [10.1067/mod.2000.109032, DOI 10.1067/MOD.2000.109032]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Langenkamper D, 2017, FRONT MAR SCI, V4, DOI 10.3389/fmars.2017.00083
   LAUR DR, 1986, MAR BIOL, V93, P209, DOI 10.1007/BF00508258
   Lelievre Y, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2016.2123
   Leys SP, 2019, INTEGR COMP BIOL, V59, P751, DOI 10.1093/icb/icz122
   MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a
   Malde K, 2020, ICES J MAR SCI, V77, P1274, DOI 10.1093/icesjms/fsz057
   Marini S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32089-8
   McIntosh D., 2020, ARXIV201114070
   Mcquaid C.D., 2021, OCEANOGRAPHY MARINE, V58
   Moeslund T. B., 2012, INTRO VIDEO IMAGE PR, V1st
   Nickel M, 2004, J EXP BIOL, V207, P4515, DOI 10.1242/jeb.01289
   Nylin S, 1998, ANNU REV ENTOMOL, V43, P63, DOI 10.1146/annurev.ento.43.1.63
   Pollak Daniel J, 2019, J Undergrad Neurosci Educ, V17, pT12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rovero F, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103300
   Tills O, 2018, PLOS BIOL, V16, DOI 10.1371/journal.pbio.3000074
   Torben M., 2019, PATTERN RECOGN, V1, P45
   Yao R, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3391743
   Yau T.H.Y., 2014, THESIS U ALBERTA EDM
   Zhang AB, 2017, METHODS ECOL EVOL, V8, P627, DOI 10.1111/2041-210X.12682
NR 42
TC 0
Z9 0
U1 5
U2 5
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2073-4441
J9 WATER-SUI
JI Water
PD SEP
PY 2021
VL 13
IS 18
AR 2512
DI 10.3390/w13182512
PG 17
WC Environmental Sciences; Water Resources
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Water Resources
GA UY6HL
UT WOS:000701622600001
OA gold
DA 2022-02-10
ER

PT J
AU Askerov, E
   Trepet, SA
   Eskina, TG
   Bibina, KV
   Narkevich, AI
   Pkhitikov, AB
   Zazanashvili, N
   Akhmadova, K
AF Askerov, E.
   Trepet, S. A.
   Eskina, T. G.
   Bibina, K., V
   Narkevich, A., I
   Pkhitikov, A. B.
   Zazanashvili, N.
   Akhmadova, K.
TI ESTIMATION OF THE POPULATION DENSITIES OF SPECIES PREY OR COMPETITOR TO
   THE LEOPARD (PANTHERA PARDUS) IN THE HYRCAN NATIONAL PARK, AZERBAIJAN
SO ZOOLOGICHESKY ZHURNAL
LA Russian
DT Article
DE Talysh Mountains; camera trap; abundance index; random encounter model
ID CAMERA TRAPS; PHOTOGRAPHIC RATES; PATTERNS; RANGE; PORCUPINES;
   ABUNDANCE; CAUCASUS; MOVEMENT; RESERVE; TIGERS
AB Based on the random encounter model, the population densities of species potentially prey or competitor to the leopard (Panthera pardus) were estimated in the Hyrcan National Park, Azerbaijan. Data obtained from 18 camera traps were processed, 11 of which were installed in the southern part of the park, and 7 in the northern part. The total operating time of the cameras amounted to 3950 traps per day for the period from April 2018 to December 2019. A very high Wild boar population density was revealed in the southern part of the park (32.5 ind./1000 ha). On the contrary, no Roe deer was recorded in the southern part, while it had a rather low density in the northern part (4.4 ind./1000 ha). The population densities of the Golden jackal (0.6 ind./1000 ha), the Grey wolf (0.1 ind./1000 ha), the Jungle cat (6.7 ind./1000 ha), and the Raccoon (46.9 ind./1000 ha) ranged within the normal limits characteristic of similar populations in other parts of their global distribution. The population densities of the Indian porcupine (9.7 ind./1000 ha) and the Brown bear (0.4 ind./1000 ha) seem to have also corresponded to the mountain forest conditions of the study area. From a view point of food supply, the territory of the Hyrcan National Park is quite suitable for permanently supporting 3-4 leopard individuals.
C1 [Askerov, E.; Akhmadova, K.] WWF Azerbaijan Off, Baku 1001, Azerbaijan.
   [Askerov, E.] Azerbaijan Natl Acad Sci, Inst Zool, Baku 1073, Azerbaijan.
   [Askerov, E.; Zazanashvili, N.] Ilia State Univ, GE-0162 Tbilisi, Georgia.
   [Trepet, S. A.; Pkhitikov, A. B.] Russian Acad Sci, Tembotov Inst Ecol Mt Terr, Nalchik 360051, Russia.
   [Trepet, S. A.; Eskina, T. G.; Bibina, K., V; Narkevich, A., I] Shaposhnikov Caucasian State Biosphere Nat Reserv, Soci 354340, Russia.
   [Zazanashvili, N.] WWF Caucasus Programme Off, GE-0193 Tbilisi, Georgia.
RP Trepet, SA (corresponding author), Russian Acad Sci, Tembotov Inst Ecol Mt Terr, Nalchik 360051, Russia.
EM trepetsergey@gmail.com
CR Acosta-Pankov I., 2019, LYNX PRAHA, V50, P113
   Anile S., 2012, Wildlife Biology in Practice, V8, P1
   [Anonymous], 2017, STRATEGY CONSERVATIO
   Askerov E, 2019, ZOOL MIDDLE EAST, V65, P88, DOI 10.1080/09397140.2018.1552349
   Askerov E, 2015, ZOOL MIDDLE EAST, V61, P95, DOI 10.1080/09397140.2015.1035003
   Banea OC, 2012, ACTA ZOOL BULGAR, V64, P353
   Belousova A.V., 1993, Lutreola (Moscow), V2, P16
   Bragg CJ, 2005, J ARID ENVIRON, V61, P261, DOI 10.1016/j.jaridenv.2004.09.007
   Breitenmoser U, 2014, RECOVERY LEOPARD AZE
   Butterfield Robert T., 1944, TRANS NORTH AMER WILDLIFE CONF, V9, P337
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Gray TNE, 2018, J ZOOL, V305, P173, DOI 10.1111/jzo.12547
   Hazaryan, 2004, BEITRAGE JAGD WILDFO, V29, P303
   Jennelle CS, 2002, ANIM CONSERV, V5, P119, DOI 10.1017/S1367943002002160
   Jourdain NOAS, 2020, J AGR BIOL ENVIR ST, V25, P148, DOI 10.1007/s13253-020-00385-4
   Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
   LAWTON JH, 1993, TRENDS ECOL EVOL, V8, P409, DOI 10.1016/0169-5347(93)90043-O
   LOMOLINO MV, 1995, J MAMMAL, V76, P335, DOI 10.2307/1382345
   Lukarevsky Victor, 2007, Cat News, P15
   Maharramova Elmira, 2018, Cat News, V67, P8
   Majumder Aniruddha, 2011, Journal of Threatened Taxa, V3, P2221
   Nickerson B.S., 2019, ESTIMATING POPULATIO, DOI 10.13140/RG.2.2.28655.18083
   Pfeffer SE, 2018, REMOTE SENS ECOL CON, V4, P173, DOI 10.1002/rse2.67
   Romani T, 2018, MAMMAL RES, V63, P477, DOI 10.1007/s13364-018-0386-9
   Rotem G., 2008, EFFECT LANDSCAPE HET
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Sanei Arezoo, 2016, Cat News, P43
   SEVER Z, 1991, MAMMALIA, V55, P187, DOI 10.1515/mamm.1991.55.2.187
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Spassov N., 2020, LYNX N S PRAHA, V50, P113
   Strampelli P, 2020, ORYX, V54, P405, DOI 10.1017/S0030605318000121
   Zazanashvili N., 2007, CAT NEWS, V2, P4
NR 35
TC 0
Z9 0
U1 1
U2 1
PU MAIK NAUKA-INTERPERIODICA PUBL
PI MOSCOW
PA GSP-1, MARONOVSKII PER 26, MOSCOW, 119049, RUSSIA
SN 0044-5134
J9 ZOOL ZH
JI Zool. Zhurnal
PD AUG
PY 2021
VL 100
IS 8
BP 947
EP 955
DI 10.31857/S0044513421080031
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA UX0RA
UT WOS:000700555000009
DA 2022-02-10
ER

PT J
AU Chen, JH
   Little, JJ
AF Chen, Jianhui
   Little, James J.
TI Where should cameras look at soccer games: Improving smoothness using
   the overlapped hidden Markov model
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Camera planning; Camera calibration; Hidden Markov model; Soccer games
ID TRACKING
AB Automatic camera planning for sports has been a long term goal in computer vision and machine learning. In this paper, we study camera planning for soccer games using pan, tilt and zoom (PTZ) cameras. Two important problems have been addressed. First, we propose the Overlapped Hidden Markov Model (OHMM) method which effectively optimizes the camera trajectory in overlapped local windows. The OHMM method significantly improves the smoothness of the camera planning by optimizing the camera trajectory in the temporal space, resulting in much more natural camera movements present in real broadcasts. We also propose CalibMe which is a highly automatic camera calibration method for soccer games. CalibMe enables users to collect large amounts of training data for learning algorithms. The precision of CalibMe is evaluated on a motion blur affected sequence and outperforms several strong existing methods. The performance of the OHMM method is extensively evaluated on both synthetic and real data. It outperforms the state-of-the-art algorithms in terms of smoothness without sacrificing accuracy. (C) 2016 Elsevier Inc. All rights reserved.
C1 [Chen, Jianhui; Little, James J.] Univ British Columbia, Dept Comp Sci, 2366 Main Mall, Vancouver, BC, Canada.
RP Chen, JH (corresponding author), Univ British Columbia, Dept Comp Sci, 2366 Main Mall, Vancouver, BC, Canada.
EM jhchen14@cs.ubc.ca; little@cs.ubc.ca
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)CGIAR; Disney
   Research
FX This work was funded partially by the Natural Sciences and Engineering
   Research Council of Canada and a Research Grant from Disney Research.
CR Ariki Y, 2006, IEEE INT SYM MULTIM, P851
   Bishop C.M.., 2006, PATTERN RECOGN
   Bloit J., 2008, IEEE INT C AC SPEECH
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Bu J, 2011, IEEE INT CON MULTI
   Carr P., 2012, EUR C COMP VIS
   Carr P., 2012, IEEE WORKSH APPL COM
   Chen J., 2015, IEEE WINT C APPL COM
   Chen J., 2014, WORKSH AAAI C ART IN
   Chen J, 2016, IEEE IPCCC
   Dantone M., 2012, IEEE C COMP VIS PATT
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Donoser M., 2006, IEEE C COMP VIS PATT
   Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620
   Fanello S., 2014, IEEE C COMP VIS PATT
   Farin D., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5307, P80, DOI 10.1117/12.526813
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963
   Ghanem B., 2012, IEEE INT C AC SPEECH
   Gupta A., 2011, CAN C COMP ROB VIS
   Hess R., 2007, COMPUTER VISION PATT
   Hilton A, 2011, IEEE T BROADCAST, V57, P462, DOI 10.1109/TBC.2011.2131870
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Homayounfar N., 2016, ARXIV160402715
   Irani R, 2015, IEEE COMPUT SOC CONF
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Liu SM, 2014, IEEE CONF COMPU INTE
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu WL, 2013, IEEE T PATTERN ANAL, V35, P1704, DOI 10.1109/TPAMI.2012.242
   Lu Y., 2015, IEEE INT C COMP VIS
   Maksai A., 2016, IEEE C COMP VIS PATT
   Narasimhan M., 2006, INT C MACH LEARN
   Okuma K., 2004, AS C COMP VIS
   Patraucean V., 2012, EUR C COMP VIS
   Pettersen S.A., 2014, ACM MULT SYST C
   Puwein J., 2012, IEEE WORKSH APPL COM
   Puwein J., 2011, IEEE WORKSH APPL COM
   Ramanathan V., 2016, COMPUTER VISION PATT
   Rosten E., 2005, INT C COMP VIS
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047
   Schaffalitzky F., 2002, EUR C COMP VIS
   Shah R., 2015, IEEE WINT C APPL COM
   Sutskever I., 2014, ADV NEURAL INFORM PR
   Thomas G, 2007, J REAL-TIME IMAGE PR, V2, P117, DOI 10.1007/s11554-007-0041-1
   Tomasi Carlo, 1991, TECH REP
   Vir Singh D., 2010, AUTOMATIC VIRTUAL CA
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang XC, 2014, COMPUT VIS IMAGE UND, V119, P102, DOI 10.1016/j.cviu.2013.11.010
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
NR 51
TC 9
Z9 10
U1 1
U2 10
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD JUN
PY 2017
VL 159
SI SI
BP 59
EP 73
DI 10.1016/j.cviu.2016.10.017
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA EZ0UR
UT WOS:000404422100005
DA 2022-02-10
ER

PT J
AU Allen, ML
   Ward, MP
   Juznic, D
   Krofel, M
AF Allen, Maximilian L.
   Ward, Michael P.
   Juznic, Damjan
   Krofel, Miha
TI SCAVENGING BY OWLS: A GLOBAL REVIEW AND NEW OBSERVATIONS FROM EUROPE AND
   NORTH AMERICA
SO JOURNAL OF RAPTOR RESEARCH
LA English
DT Review
DE Great Horned Owl; Bubo virginianus; Barred Owl; Strix varia; Ural Owl;
   Strix uralensis; camera trap; diet; owl; pellet analysis; scavenging
ID CARRION CONSUMPTION; 1ST RECORD; PREY; ACQUISITION; MORTALITY;
   PREDATION; FOREST; BUBO
AB Scavenging is an important ecological function that increases individual fitness and transfers energy between trophic levels. Scavenging by owls has been documented opportunistically through direct observations, camera trapping, and pellet analyses, but it is unknown how frequent or widespread the behavior is. We documented three new scavenging events front North America and Europe, and also performed a systematic literature review of the reports documenting scavenging by owls. The number of such reports was similar in each decade from the 1970s to the 2000s, but the decade of the 2010s had more reports than all previous decades combined. Owls scavenged primarily on mammals (81%), followed by birds (16%) and reptiles (3%); almost half (47%) of carrion scavenged were Artiodactyla (hoofed mammals) and most of the species scavenged were larger than the feeding owl. Most reports documenting scavenging by owls were from either Europe ( n= 14) or North America (n=11), with few reports from Asia (n=2), South America ( n =2), or Australia (n=1), and none from Africa. The most frequent type of report was direct observations (n= 14), followed by camera trapping (n=9) and pellet analyses (n=6). Our review indicates that scavenging is a widespread behavior among owl species, but most observations were opportunistic, suggesting that additional incidents of scavenging by owls are likely unobserved. Further research is needed to establish the frequency of scavenging by owls and the effects that scavenging may have on owl populations and scavenging communities.
C1 [Allen, Maximilian L.; Ward, Michael P.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Ward, Michael P.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin, Urbana, IL 61801 USA.
   [Juznic, Damjan; Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Vecna Pot 83, SI-1000 Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Illinois Natural History Survey; Slovenian Research AgencySlovenian
   Research Agency - Slovenia [P4-0059]
FX Funding for the study was generously provided by The Illinois Natural
   History Survey and the Slovenian Research Agency (P4-0059).
CR Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Allen ML, 2013, WILSON J ORNITHOL, V125, P417, DOI 10.1676/12-176.1
   Anza J, 2015, REV BRAS ORNITOL, V23, P377
   Bishop CA, 2013, AVIAN CONSERV ECOL, V8, DOI 10.5751/ACE-00604-080202
   Borda-de-Agua L, 2014, ECOL MODEL, V276, P29, DOI 10.1016/j.ecolmodel.2013.12.022
   Boves TJ, 2012, J WILDLIFE MANAGE, V76, P1381, DOI 10.1002/jwmg.378
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   del Hoyo J, 1999, HDB BIRDS WORLD BARN
   Diaz-Ruiz F, 2010, J RAPTOR RES, V44, P78, DOI 10.3356/JRR-09-21.1
   DunningJr J. B., 2007, CRC HDB AVIAN BODY M, VSecond
   Dunsire C., 1978, Scottish Birds, V10, P56
   Graves GR, 2006, J RAPTOR RES, V40, P175, DOI 10.3356/0892-1016(2006)40[175:PPOGGO]2.0.CO;2
   Hebblewhite Mark, 2010, P69
   Hiraldo F., 1975, Doiiana Acta vert, V2, P161
   Jedrzejewska B., 1998, PREDATION VERTEBRATE
   Kapfer JM, 2011, WILSON J ORNITHOL, V123, P646, DOI 10.1676/11-015.1
   Konig C., 1999, OWLS GUIDE OWLS WORL
   KORPIMAKI E, 1987, IBIS, V129, P499, DOI 10.1111/j.1474-919X.1987.tb08237.x
   Krofel M., 2005, ACROCEPHALUS, V26, P49
   Krofel Miha, 2011, Acrocephalus, V32, P45, DOI 10.2478/v10100-011-0003-3
   Leditznig Christoph, 2001, Egretta, V44, P45
   Macdonald D, 1984, ENCY MAMMALS
   MARTI CD, 1979, AUK, V96, P319
   Milchev Boyan, 2017, Ornis Hungarica, V25, P58
   Mori E, 2014, ITAL J ZOOL, V81, P471, DOI 10.1080/11250003.2014.920928
   Mori E, 2015, BIRD STUDY, V62, P257, DOI 10.1080/00063657.2015.1013522
   Mueller MA, 2019, LANDSCAPE URBAN PLAN, V189, P362, DOI 10.1016/j.landurbplan.2019.04.023
   Mueller MA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190971
   Novoa Fernando, 2016, Revista Chilena de Ornitologia, V22, P200
   Patterson J. Micheal, 2007, Northwestern Naturalist, V88, P12, DOI 10.1898/1051-1733(2007)88[12:AAOSOB]2.0.CO;2
   Peers Michael J.L., 2018, Northwestern Naturalist, V99, P232
   Peers MJL, 2017, WILSON J ORNITHOL, V129, P875, DOI 10.1676/16-170.1
   Pleshak T.V., 1998, Russkii Ornitologicheskii Zhurnal Ekspress Vypusk, V44, P12
   Proudfoot GA, 1997, WILSON BULL, V109, P741
   SEEBECK J H, 1976, Emu, V76, P167
   Selva N, 2007, P ROY SOC B-BIOL SCI, V274, P1101, DOI 10.1098/rspb.2006.0232
   Serrano David, 2000, Ardeola, V47, P101
   Smallwood KS, 2010, J WILDLIFE MANAGE, V74, P1089, DOI 10.2193/2009-266
   Smith D.G., 1974, Canadian Field Naturalist, V88, P96
   van de Velde E., 1980, Veldornitologisch Tijdschrift, V3, P137
   Varland DE, 2018, J RAPTOR RES, V52, P291, DOI 10.3356/JRR-17-38.1
   Walker LE, 2018, ECOL EVOL, V8, P11158, DOI 10.1002/ece3.4583
   Wassink Gejo, 2003, Limosa, V76, P1
   Welch Stephen, 2012, Scottish Birds, V32, P300
   Wilmers CC, 2003, J ANIM ECOL, V72, P909, DOI 10.1046/j.1365-2656.2003.00766.x
   Wilson EE, 2011, TRENDS ECOL EVOL, V26, P129, DOI 10.1016/j.tree.2010.12.011
NR 46
TC 3
Z9 3
U1 3
U2 34
PU RAPTOR RESEARCH FOUNDATION INC
PI HASTINGS
PA 14377 117TH STREET SOUTH, HASTINGS, MN 55033 USA
SN 0892-1016
EI 2162-4569
J9 J RAPTOR RES
JI J. Raptor Res.
PD DEC
PY 2019
VL 53
IS 4
BP 410
EP 418
DI 10.3356/0892-1016-53.4.410
PG 9
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA JN2KV
UT WOS:000496730800006
DA 2022-02-10
ER

PT J
AU de Oliveira, ML
   Peres, PHD
   Gatti, A
   Morales-Donoso, JA
   Mangini, PR
   Duarte, JMB
AF de Oliveira, Marcio Leite
   de Faria Peres, Pedro Henrique
   Gatti, Andressa
   Morales-Donoso, Jorge Alfonso
   Mangini, Paulo Rogerio
   Duarte, Jose Mauricio Barbanti
TI Faecal DNA and camera traps detect an evolutionarily significant unit of
   the Amazonian brocket deer in the Brazilian Atlantic Forest
SO EUROPEAN JOURNAL OF WILDLIFE RESEARCH
LA English
DT Article
DE Conservation; Cytochrome b; Detection dogs; Mazama nemorivaga
ID HAIR-TRAP; CYTOGENETIC DESCRIPTION; MITOCHONDRIAL; ARTIODACTYLA;
   CONSERVATION; PHYLOGENY; MAMMALIA; DENSITY
AB The Amazonian grey brocket deer (Mazama nemorivaga) is a large mammal species that until now has been assumed to be limited to the Amazon region and has not been categorized to be threatened. In this study, we provide evidences, obtained by camera traps and faecal DNA, of the existence of two populations of this species in the Brazilian Atlantic Forest, more than a thousand kilometres away from its assumed distribution limit. Furthermore, we employed genetic analysis to identify the collected faecal samples using detection dogs in six protected areas that were within 500 km of the first photographic records. Phylogenetic analysis, performed on hair samples, indicated that these populations were genetically related to the M. nemorivaga population of the western Amazon. The discovery of these populations emphasizes the importance of noninvasive techniques for species detection of elusive or rare populations. It is necessary to re-evaluate the conservation status of this species, with special attention to the detected populations (Linhares-Sooretama forest complex and the Una Biological Reserve). The conservation of these two new populations of evolutionarily significant units is urgent, and we recommend the adoption of measures against highly impacting deer threats, such as hunting and predation by domestic dogs. Finally, before any drastic population management is taken, it is necessary to determine whether there is historical or recent genetic isolation among the M. nemorivaga populations of the Atlantic Forest.
C1 [de Oliveira, Marcio Leite; de Faria Peres, Pedro Henrique; Morales-Donoso, Jorge Alfonso; Duarte, Jose Mauricio Barbanti] Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
   [Gatti, Andressa] Inst Pesquisas Mata Atlantica IPEMA, Vitoria, ES, Brazil.
   [Mangini, Paulo Rogerio] Brazilian Inst Conservat Med TRIADE, Curitiba, Parana, Brazil.
RP de Oliveira, ML (corresponding author), Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
EM oliveiraml1@yahoo.com.br
RI de Oliveira, Márcio L/F-3792-2012; Duarte, José Maurício
   Barbanti/AAG-5149-2019
OI de Oliveira, Márcio L/0000-0002-7705-0626; Duarte, José Maurício
   Barbanti/0000-0002-7805-0265; Peres, Pedro Henrique/0000-0002-3158-0963;
   Morales Donoso, Jorge Alfonso/0000-0002-1684-1512; Mangini, Paulo
   Rogerio/0000-0002-2912-2242
FU FAPESPFundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)
   [15/25742-5, 17/02200-8, 17/07014-8]; CNPqConselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPQ) [302368/2018-3]
FX This study was funded by FAPESP (15/25742-5, 17/02200-8, 17/07014-8) and
   CNPq (302368/2018-3).
CR Abril VV, 2010, CYTOGENET GENOME RES, V128, P177, DOI 10.1159/000298819
   Duarte JMB, 2017, ORYX, V51, P656, DOI 10.1017/S0030605316000405
   Batalha H, 2013, J ORNITHOL, V154, P41, DOI 10.1007/s10336-012-0866-7
   Beja-Pereira A, 2009, MOL ECOL RESOUR, V9, P1279, DOI 10.1111/j.1755-0998.2009.02699.x
   Bickford D, 2007, TRENDS ECOL EVOL, V22, P148, DOI 10.1016/j.tree.2006.11.004
   Biondo Cibele, 2010, Suiform Soundings, V10, P24
   Bollback T, 2004, DNA EXTRACTION PROTO
   Bovendorp Ricardo S, 2017, Ecology, V98, P2226, DOI 10.1002/ecy.1893
   Buso AA, 2013, RADIOCARBON, V55, P1747, DOI 10.1017/S0033822200048669
   Carstens BC, 2013, MOL ECOL, V22, P4369, DOI 10.1111/mec.12413
   Castro-Arellano I, 2008, J WILDLIFE MANAGE, V72, P1405, DOI 10.2193/2007-476
   Cullingham CI, 2010, J WILDLIFE MANAGE, V74, P849, DOI 10.2193/2008-292
   Cursino MS, 2014, BMC EVOL BIOL, V14, DOI 10.1186/1471-2148-14-40
   Darriba D, 2012, NAT METHODS, V9, P772, DOI 10.1038/nmeth.2109
   de Oliveira ML, 2012, ZOOLOGIA-CURITIBA, V29, P183, DOI 10.1590/S1984-46702012000200012
   de Oliveira ML, 2016, MAMM BIOL, V81, P281, DOI 10.1016/j.mambio.2016.01.004
   De Queiroz K, 2007, SYST BIOL, V56, P879, DOI 10.1080/10635150701701083
   de Souza JN, 2013, CONSERV GENET RESOUR, V5, P639, DOI 10.1007/s12686-013-9870-3
   Ledo RMD, 2017, J BIOGEOGR, V44, P2551, DOI 10.1111/jbi.13049
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2003, MAMMALIA, V67, P403, DOI 10.1515/mamm.2003.67.3.403
   Duarte JMB, 2015, THE IUCN RED LIST OF, DOI [10.2305/IUCN.UK.2015-4.RLTS.T29621A22154379.en, DOI 10.2305/IUCN.UK.2015-4.RLTS.T29621A22154379.EN]
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Escobedo-Morales LA, 2016, MAMM BIOL, V81, P303, DOI 10.1016/j.mambio.2016.02.003
   Fiorillo BF, 2013, COMP CYTOGENET, V7, P25, DOI 10.3897/CompCytogen.v7i1.4314
   Gilbert C, 2006, MOL PHYLOGENET EVOL, V40, P101, DOI 10.1016/j.ympev.2006.02.017
   Gonzalez S, 2009, MOL ECOL RESOUR, V9, P754, DOI 10.1111/j.1755-0998.2008.02390.x
   Grotta-Neto F, 2019, J MAMMAL, V100, P454, DOI 10.1093/jmammal/gyz056
   Gutierrez EE, 2017, ZOOKEYS, P87, DOI 10.3897/zookeys.697.15124
   Gutierrez EE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129113
   Hall T.A., 1999, NUCL ACIDS S SERIES, V41, P95, DOI DOI 10.1021/BK-1999-0734.CH008
   Hebert PDN, 2003, P ROY SOC B-BIOL SCI, V270, pS96, DOI 10.1098/rsbl.2003.0025
   Heckeberg NS, 2016, PEERJ, V4, DOI 10.7717/peerj.2307
   Instituto Brasileiro de Geografia e Estatistica (IBGE), 1992, MAP VEG BRAS
   Kamgaing TOW, 2018, AFR J ECOL, V56, P908, DOI 10.1111/aje.12518
   Karanth KU, 2004, ANIM CONSERV, V7, P285, DOI 10.1017/S1367943004001477
   Katoh K., 2017, BRIEF BIOINFORM
   Kumar S, 2018, MOL BIOL EVOL, V35, P1547, DOI 10.1093/molbev/msy096
   Lima Fernando, 2017, Ecology, V98, P2979, DOI 10.1002/ecy.1998
   Luduverio DJ, 2018, THESIS UNESP BRASIL
   Martins GS, 2015, THESIS
   Miranda-Ribeiro A, 1919, REV MUS PAULISTA, V11, P209, DOI [10.5962/bhl.part.9305, DOI 10.5962/BHL.PART.9305]
   Morales-Donoso JA, 2017, THESIS
   Moritz C, 1995, AM FISH S S, V17, P249
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   NEFF DJ, 1968, J WILDLIFE MANAGE, V32, P597, DOI 10.2307/3798941
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Oliveira ML, 2019, EUR J WILDLIFE RES, V65, P21, DOI [10.1007/s10344-019-1258-6, DOI 10.1007/S10344-019-1258-6]
   Pauli JN, 2008, J WILDLIFE MANAGE, V72, P1650, DOI 10.2193/2007-588
   Pfeiffer WT, 2010, GAT COMP ENV WORKSH, DOI [10.1109/GCE.2010.5676129, DOI 10.1109/GCE.2010.5676129]
   Pitra C, 2004, MOL PHYLOGENET EVOL, V33, P880, DOI 10.1016/j.ympev.2004.07.013
   Por FD, 1992, SOORETAMA ATLANTIC R
   Ramos-Robles M, 2013, TROP CONSERV SCI, V6, P70, DOI 10.1177/194008291300600109
   Ratnasingham S, 2007, MOL ECOL NOTES, V7, P355, DOI 10.1111/j.1471-8286.2007.01678.x
   Reiners TE, 2011, EUR J WILDLIFE RES, V57, P991, DOI 10.1007/s10344-011-0543-9
   Ribeiro MC, 2009, BIOL CONSERV, V142, P1141, DOI 10.1016/j.biocon.2009.02.021
   Rodriguez PC, 2010, MEDICC REV, V12, P17, DOI 10.37757/MR2010.V12.N1.4
   Rossi Rogerio Vieira, 2010, P202
   Salviano MB, 2017, BIOL REPROD, V96, P1279, DOI 10.1093/biolre/iox041
   Santos PM, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2663
   Smith DA, 2001, SCIENCE, V291, P435, DOI 10.1126/science.291.5503.435B
   Valeri MP, 2018, CYTOGENET GENOME RES, V154, P147, DOI 10.1159/000488377
   Vogliotti A, 2016, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2016-1.RLTS.T41023A22155086.en, DOI 10.2305/IUCN.UK.2016-1.RLTS.T41023A22155086.EN]
NR 63
TC 6
Z9 6
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1612-4642
EI 1439-0574
J9 EUR J WILDLIFE RES
JI Eur. J. Wildl. Res.
PD FEB 21
PY 2020
VL 66
IS 2
AR 28
DI 10.1007/s10344-020-1367-2
PG 10
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA KO1HS
UT WOS:000515300100001
DA 2022-02-10
ER

PT J
AU Eakin, CJ
   Hunter, ML
   Calhoun, AJK
AF Eakin, Carly J.
   Hunter, Malcolm L.
   Calhoun, Aram J. K.
TI Bird and mammal use of vernal pools along an urban development gradient
SO URBAN ECOSYSTEMS
LA English
DT Article
DE Vernal pool; Camera trap; Urban gradient; Urban wildlife; Subsidized
   wildlife; Urban wetlands
ID SPECIES RICHNESS; LAND-USE; CAMERA TRAPS; URBANIZATION; BIODIVERSITY;
   FORESTS; FRAGMENTATION; ACCUMULATION; COMMUNITIES; MANAGEMENT
AB Vernal pools in the northeastern US are of conservation concern primarily because of their role as habitat for specialized pool-breeding amphibians, but their use by birds and mammals may also be of interest, especially from the perspective of the impact of urbanization. We describe camera-trapped wildlife (CTW) at 38 vernal pools along an urban development gradient in greater Bangor, Maine, USA. We detected 20 mammal and 39 bird taxa (29 contacted pool water; 39 detected at >1 site). Land cover type within 1000m (%), within-pool vegetation (%), and amphibian egg mass numbers explained a substantial portion of the variance (40.8%) in CTW assemblage composition. Submerged vegetation within pools and cover by water and impervious surfaces within 1000m of pools were key site characteristics defining assemblages. We scored the urban-affiliation of taxa and modeled the relationship between weighted assemblage scores for each site and impervious cover. Impervious cover within 1000m of pools was positively (p<0.01) related to site urban-affiliation scores. Use probability for red fox increased and snowshoe hare decreased with impervious cover at 1000m. These results indicate that within-pool vegetation and land cover types at 1000m influenced bird and mammal assemblages that used study pools and greater impervious cover at 100 and 1000m was correlated with a shift in assemblages from being dominated by urban-avoider to urban-adapted species. We encourage land use planners and managers to consider the influence of land use practices within 1000m of vernal pools on birds and mammals, especially near amphibian breeding pools.
C1 [Eakin, Carly J.; Hunter, Malcolm L.; Calhoun, Aram J. K.] Univ Maine, Dept Wildlife Fisheries & Conservat Biol, 5755 Nutting Hall, Orono, ME 04469 USA.
RP Eakin, CJ (corresponding author), Univ Maine, Dept Wildlife Fisheries & Conservat Biol, 5755 Nutting Hall, Orono, ME 04469 USA.
EM carly.eakin@gmail.com; mhunter@maine.edu; calhoun@maine.edu
FU McIntire-Stennis; Hatch ActUnited States Department of Agriculture
   (USDA); National Science FoundationNational Science Foundation (NSF)
   [313627]; USDA National Institute of Food and Agriculture, Hatch project
   through the Maine Agricultural & Forest Experiment Station [ME021705];
   Division Of Behavioral and Cognitive SciNational Science Foundation
   (NSF)NSF - Directorate for Social, Behavioral & Economic Sciences (SBE)
   [1313627] Funding Source: National Science Foundation
FX We are grateful for support for this study provided by McIntire-Stennis,
   the Hatch Act, and the National Science Foundation under grant no.
   313627. We thank A. Mortelliti for assistance with occupancy modeling,
   H. Greig, R. Holberton, and M. Kinnison for help with study and analysis
   design, and D. Dunham for hundreds of hours of visually scanning trail
   camera photos for animals. This is a Maine Agricultural and Forest
   Experiment Station Publication Number 3611. This project was supported
   by the USDA National Institute of Food and Agriculture, Hatch project
   number #ME021705 through the Maine Agricultural & Forest Experiment
   Station.
CR Anderson K. A., 2002, MODEL SELECTION MULT
   Aronson MFJ, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2013.3330
   BEISSINGER SR, 1982, CONDOR, V84, P75, DOI 10.2307/1367825
   BERVEN KA, 1990, ECOLOGY, V71, P1599, DOI 10.2307/1938295
   Blair RB, 1996, ECOL APPL, V6, P506, DOI 10.2307/2269387
   Blair RB, 2001, BIOTIC HOMOGENIZATION, P33
   Borcard D, 2011, USE R, P1, DOI 10.1007/978-1-4419-7976-6
   Boren JC, 1999, J RANGE MANAGE, V52, P420, DOI 10.2307/4003767
   Calhoun AJK, 2017, BIOL CONSERV, V211, P3, DOI 10.1016/j.biocon.2016.11.024
   Chace JF, 2006, LANDSCAPE URBAN PLAN, V74, P46, DOI 10.1016/j.landurbplan.2004.08.007
   Chamberlain MJ, 2002, AM MIDL NAT, V147, P102, DOI 10.1674/0003-0031(2002)147[0102:SHSBRP]2.0.CO;2
   CHILDS HE, 1953, EVOLUTION, V7, P228, DOI 10.1111/j.1558-5646.1953.tb00084.x
   Chupp AD, 2013, NORTHEAST NAT, V20, P631, DOI 10.1656/045.020.0415
   Clergeau P, 1998, CONDOR, V100, P413, DOI 10.2307/1369707
   Colburn EA., 2004, VERNAL POOLS NATURAL
   Cox RR, 1998, J WILDLIFE MANAGE, V62, P124, DOI 10.2307/3802270
   CROONQUIST MJ, 1991, ENVIRON MANAGE, V15, P701, DOI 10.1007/BF02589628
   Crouch WB, 2000, WILDLIFE SOC B, V28, P895
   Dorazio RM, 2006, ECOLOGY, V87, P842, DOI 10.1890/0012-9658(2006)87[842:ESRAAB]2.0.CO;2
   Faulkner Stephen, 2004, Urban Ecosystems, V7, P89, DOI 10.1023/B:UECO.0000036269.56249.66
   Fischer JD, 2012, BIOSCIENCE, V62, P809, DOI 10.1525/bio.2012.62.9.6
   Fischer J, 2007, GLOBAL ECOL BIOGEOGR, V16, P265, DOI 10.1111/j.1466-8238.2007.00287.x
   Fiske I, 2017, PACKAGE BUNMARKED, P116
   Friesen LE, 1995, CONSERV BIOL, V9, P1408, DOI 10.1046/j.1523-1739.1995.09061408.x
   Fuller TK, 2003, FOREST ECOL MANAG, V185, P75, DOI 10.1016/S0378-1127(03)00247-0
   Gotelli NJ, 2001, ECOL LETT, V4, P379, DOI 10.1046/j.1461-0248.2001.00230.x
   Gray MJ, 2009, DIS AQUAT ORGAN, V87, P243, DOI 10.3354/dao02138
   Hanowski J, 2006, FOREST ECOL MANAG, V229, P63, DOI 10.1016/j.foreco.2006.03.011
   Hansen AJ, 2005, ECOL APPL, V15, P1893, DOI 10.1890/05-5221
   Homan RN, 2004, ECOL APPL, V14, P1547, DOI 10.1890/03-5125
   Hunter M, 2016, ECOL INDIC, V63, P121, DOI 10.1016/j.ecolind.2015.11.049
   JOHNSON DH, 1980, ECOLOGY, V61, P65, DOI 10.2307/1937156
   Lawler JJ, 2014, P NATL ACAD SCI USA, V111, P7492, DOI 10.1073/pnas.1405557111
   Legendre P, 2001, OECOLOGIA, V129, P271, DOI 10.1007/s004420100716
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   MacKenzie DI, 2006, J WILDLIFE MANAGE, V70, P367, DOI 10.2193/0022-541X(2006)70[367:MTPORU]2.0.CO;2
   MacKenzie DI, 2004, J AGR BIOL ENVIR ST, V9, P300, DOI 10.1198/108571104X3361
   McKinney M. L., 2008, Urban Ecosystems, V11, P161, DOI 10.1007/s11252-007-0045-4
   McKinney ML, 2006, BIOL CONSERV, V127, P247, DOI 10.1016/j.biocon.2005.09.005
   McKinney ML, 2002, BIOSCIENCE, V52, P883, DOI 10.1641/0006-3568(2002)052[0883:UBAC]2.0.CO;2
   McLoughlin PD, 2000, ECOSCIENCE, V7, P123, DOI 10.1080/11956860.2000.11682580
   Minor Emily, 2010, Urban Ecosystems, V13, P51, DOI 10.1007/s11252-009-0103-1
   Mitchell JC, 2008, SCI CONSERVATION VER, P169
   Murray MP, 2005, CAN FIELD NAT, V119, P291, DOI 10.22621/cfn.v119i2.116
   NAGELKERKE NJD, 1991, BIOMETRIKA, V78, P691, DOI 10.1093/biomet/78.3.691
   Nilon CH, 1986, P NAT S URB WILDL 19, P53
   Oksanen J, 2017, vegan: Community Ecology Package. Vegan. 2.5-7
   R Core Team, 2017, R LANG ENV STAT COMP
   RACEY GD, 1982, CAN J ZOOL, V60, P865, DOI 10.1139/z82-119
   Rittenhouse TAG, 2007, J HERPETOL, V41, P645, DOI 10.1670/07-015.1
   Rodewald AD, 2003, WILDLIFE SOC B, V31, P586
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Royle J.A., 2008, HIERARCHICAL MODELIN
   Shurin JB, 2006, P ROY SOC B-BIOL SCI, V273, P1, DOI 10.1098/rspb.2005.3377
   Silveira Joseph G., 1998, P92
   Thompson GG, 2003, AUSTRAL ECOL, V28, P355, DOI 10.1046/j.1442-9993.2003.01294.x
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Trzcinski MK, 1999, ECOL APPL, V9, P586, DOI 10.1890/1051-0761(1999)009[0586:IEOFCA]2.0.CO;2
   Wake DB, 2008, P NATL ACAD SCI USA, V105, P11466, DOI 10.1073/pnas.0801921105
   Welsh AH, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0052015
   Zedler P. H., 1987, ECOLOGY SO CALIFORNI
   ZEDLER PH, 1992, AM MIDL NAT, V128, P1, DOI 10.2307/2426407
   [No title captured]
NR 63
TC 4
Z9 4
U1 7
U2 54
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1083-8155
EI 1573-1642
J9 URBAN ECOSYST
JI Urban Ecosyst.
PD DEC
PY 2018
VL 21
IS 6
BP 1029
EP 1041
DI 10.1007/s11252-018-0782-6
PG 13
WC Biodiversity Conservation; Ecology; Environmental Sciences; Urban
   Studies
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology; Urban
   Studies
GA HD2ZX
UT WOS:000452382200002
DA 2022-02-10
ER

PT J
AU de Oliveira, ML
   Grotta-Netto, F
   Peres, PHD
   Vogliotti, A
   Brocardo, CR
   Cherem, JJ
   Landis, M
   Paolino, RM
   Fusco-Costa, R
   Gatti, A
   Moreira, DO
   Ferreira, PM
   Mendes, SL
   Huguenin, J
   Zanin, M
   Nodari, JZ
   Leite, YLR
   Lyrio, GS
   Ferraz, KMPMD
   Passos, FC
   Duarte, JMB
AF de Oliveira, Marcio Leite
   Grotta-Netto, Francisco
   de Faria Peres, Pedro Henrique
   Vogliotti, Alexandre
   Brocardo, Carlos Rodrigo
   Cherem, Jorge Jose
   Landis, Mariana
   Paolino, Roberta Montanheiro
   Fusco-Costa, Roberto
   Gatti, Andressa
   Moreira, Danielle Oliveira
   Ferreira, Paula Modenesi
   Mendes, Sergio Lucena
   Huguenin, Jade
   Zanin, Marina
   Nodari, Joana Zorzal
   Reis Leite, Yuri Luiz
   Lyrio, Georgea Silva
   Micchi de Barros Ferraz, Katia Maria Paschoaletto
   Passos, Fernando C.
   Barbanti Duarte, Jose Mauricio
TI Elusive deer occurrences at the Atlantic Forest: 20 years of surveys
SO MAMMAL RESEARCH
LA English
DT Article
DE Atlantic Forest; Elusive species; Mammals survey; Mazama; Tropical
   forest
ID EVOLUTIONARY HISTORY; GENUS MAZAMA; JAGUARS
AB The Atlantic Forest, a hotspot for biodiversity conservation, harbours five forest deer species (Mazama spp.). Due to their elusiveness, there is a severe scarcity of occurrence data to support ecological studies and conservation planning. Thus, we assembled an occurrence dataset of Atlantic Forest deer with reliable taxonomic information aggregating data from scat and camera traps surveys, and opportunistic data collection over the last 20 years. From 2002 to 2019, we surveyed 77 protected areas using scats detection dogs and genetically identifying the faecal samples. We successfully identified 1,147 out of 1,450 collected samples. From 2000 to 2020, we sampled six protected areas in 92 sampling points with 13,328 camera trap days of sampling effort. In addition, we established an active search for potential contributors within the scientific community and environmental consultants since 2010, offering a taxonomic identification service for camera traps images, and biological field-collected samples. With our efforts, we assembled a dataset with 1,456 records of forest deer occurrence at the Atlantic Forest. Of these records, 494 are from M. americana, 350 from M. bororo, 309 from M. gouazoubira, 268 from M. nana and 35 from M. nemorivaga. The faecal sampling was the most predominant method in these records (n = 1043) followed by photographs from camera traps (n = 388); both methods represent 98.2% of our dataset records. Most of the records (79.5%) in the dataset are inside protected areas (n = 1,130). Our dataset is the most comprehensive source of information on Neotropical forest deer occurrence to date.
C1 [de Oliveira, Marcio Leite; Grotta-Netto, Francisco; de Faria Peres, Pedro Henrique; Barbanti Duarte, Jose Mauricio] Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
   [Vogliotti, Alexandre] Fed Univ Latin Amer Integrat, Foz Do Iguacu, PR, Brazil.
   [Brocardo, Carlos Rodrigo] Neotrop Inst Res & Conservat, Curitiba, Parana, Brazil.
   [Brocardo, Carlos Rodrigo] Univ Fed Oeste Para Santarem, Programa Posgrad Biodiversidade, Santarem, PA, Brazil.
   [Cherem, Jorge Jose] Caipora Cooperat, Florianopolis, SC, Brazil.
   [Landis, Mariana; Paolino, Roberta Montanheiro; Micchi de Barros Ferraz, Katia Maria Paschoaletto] Univ Sao Paulo, Luiz de Queiroz Coll Agr, Forest Sci Dept, Wildlife Ecol Management & Conservat Lab LEMaC, Piracicaba, SP, Brazil.
   [Landis, Mariana] Manaca Inst, Sao Miguel Arcanjo, SP, Brazil.
   [Fusco-Costa, Roberto] Univ Fed Parana, Programa Posgrad Ecol & Conservacao, Curitiba, Parana, Brazil.
   [Fusco-Costa, Roberto] Inst Pesquisas Cananeia, Cananeia, SP, Brazil.
   [Gatti, Andressa; Moreira, Danielle Oliveira; Ferreira, Paula Modenesi; Huguenin, Jade; Nodari, Joana Zorzal; Lyrio, Georgea Silva] Protapir Inst Biodivers, Vila Velha, ES, Brazil.
   [Moreira, Danielle Oliveira; Mendes, Sergio Lucena] Inst Nacl Mata Atlantica, Santa Teresa, ES, Brazil.
   [Mendes, Sergio Lucena; Nodari, Joana Zorzal; Reis Leite, Yuri Luiz] Univ Fed Espirito Santo, Vitoria, ES, Brazil.
   [Zanin, Marina] Univ Fed Maranhao, Sao Luis, Maranhao, Brazil.
   [Passos, Fernando C.] Univ Fed Parana UFPR, Dept Zool, Lab Biodiversidade Conservacao & Ecol Anim Silves, Programa Posgrad & Ecol & Conservacao, Curitiba, Parana, Brazil.
RP de Oliveira, ML (corresponding author), Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
EM oliveiraml1@yahoo.com.br
OI Grotta Neto, Francisco/0000-0002-2390-936X; Leite de Oliveira,
   Marcio/0000-0002-7705-0626; Montanheiro Paolino,
   Roberta/0000-0002-6948-3333; Cherem, Jorge/0000-0001-9718-9628
FU Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)Fundacao de
   Amparo a Pesquisa do Estado de Sao Paulo (FAPESP) [2014/09300-0,
   2015/25742-5, 2017/00331-8, 2017/02200-8]; National Council for
   Scientific and Technological Development (CNPq)Conselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPQ) [302368/2018-3,
   307303/2017-9, 308503/2014-7]; Araucaria Foundation to Support
   Scientific Development of ParanaFundacao Araucaria de Apoio ao
   Desenvolvimento Cientifico e Tecnologico do Estado do Parana FA
   [008/2017]; Fundacao Grupo Boticario [1001_ 20141, 1097_ 20171]; Rufford
   Foundation; Legado das Aguas-Reserva Votorantim; Fundacao de Amparo a
   Pesquisa do Estado do Espirito Santo (FAPES) [511187434/2010, 148/2019];
   Vale S.A. [529/2016]; Coordination for the Improvement of Higher
   Education Personnel (CAPES)Coordenacao de Aperfeicoamento de Pessoal de
   Nivel Superior (CAPES) [88887.478136/2020-00]
FX This study was funded by the Fundacao de Amparo a Pesquisa do Estado de
   Sao Paulo (FAPESP) (#2014/09300-0, #2015/25742-5, #2017/00331-8,
   #2017/02200-8), the National Council for Scientific and Technological
   Development (CNPq) (#302368/2018-3, #307303/2017-9, #308503/2014-7), the
   Araucaria Foundation to Support Scientific Development of Parana
   (008/2017), Fundacao Grupo Boticario (#1001_ 20141 and #1097_ 20171),
   Rufford Foundation, Legado das Aguas-Reserva Votorantim, Fundacao de
   Amparo a Pesquisa do Estado do Espirito Santo (FAPES) (#511187434/2010,
   #148/2019), Vale S.A. (#529/2016) and Coordination for the Improvement
   of Higher Education Personnel (CAPES) (#88887.478136/2020-00).
CR Angeli T, 2014, STUD NEOTROP FAUNA E, V49, P199, DOI 10.1080/01650521.2014.958898
   Asfora Paulo Henrique, 2009, Biota Neotrop., V9
   Aubry KB, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179152
   Azevedo, 2009, ANALISE COMP PERIODO
   Duarte JMB, 2017, ORYX, V51, P656, DOI 10.1017/S0030605316000405
   Beck H, 2006, J MAMMAL, V87, P519, DOI 10.1644/05-MAMM-A-174R1.1
   Beck H, 2013, BIOL CONSERV, V163, P115, DOI 10.1016/j.biocon.2013.03.012
   Beier P, 2017, CONSERV LETT, V10, P288, DOI 10.1111/conl.12300
   Bollback T, 2004, DNA EXTRACTION PROTO
   Mantellatto AMB, 2020, GENET MOL BIOL, V43, DOI [10.1590/1678-4685-GMB-2019-0008, 10.1590/1678-4685-gmb-2019-0008]
   Bonney R, 2014, SCIENCE, V343, P1436, DOI 10.1126/science.1251554
   Bovendorp Ricardo S, 2017, Ecology, V98, P2226, DOI 10.1002/ecy.1893
   Cavalcanti SMC, 2010, J MAMMAL, V91, P722, DOI 10.1644/09-MAMM-A-171.1
   Cifuentes-Rincon A, 2020, ZOOKEYS, P143, DOI 10.3897/zookeys.958.50300
   Costa LP, 2000, BIOTROPICA, V32, P872
   da Silva BFS, 2020, IHERINGIA SER ZOOL, V110, DOI 10.1590/1678-4766e2020029
   de Oliveira ML, 2012, ZOOLOGIA-CURITIBA, V29, P183, DOI 10.1590/S1984-46702012000200012
   de Oliveira ML, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-1367-2
   de Oliveira ML, 2019, EUR J WILDLIFE RES, V65, DOI 10.1007/s10344-019-1258-6
   de Oliveira ML, 2016, MAMM BIOL, V81, P281, DOI 10.1016/j.mambio.2016.01.004
   de Souza JN, 2013, CONSERV GENET RESOUR, V5, P639, DOI 10.1007/s12686-013-9870-3
   DUARTE J.M.B, 2016, IUCN RED LIST THREAT, V2016, DOI [10.2305/IUCN.UK.2016-1.RLTS.T29619A22154827.en, DOI 10.2305/IUCN.UK.2016-1.RLTS.T29619A22154827.EN]
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Ferraz KMPMD, 2021, CONSERV SCI PRACT, V3, DOI 10.1111/csp2.330
   Ferreguetti AC, 2015, J MAMMAL, V96, P1245, DOI 10.1093/jmammal/gyv132
   Foster RJ, 2010, J ZOOL, V280, P309, DOI 10.1111/j.1469-7998.2009.00663.x
   Frey Jennifer K., 2013, Animals, V3, P327, DOI 10.3390/ani3020327
   Gascon, 2011, BIODIVERSITY HOTSPOT, P3, DOI DOI 10.1007/978-3-642-20992-5_1
   Gayot M, 2004, J TROP ECOL, V20, P31, DOI 10.1017/S0266467404006157
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Gonzalez Susana, 2020, Mastozoologia Neotropical, V27, P37, DOI 10.31687/saremMN_SI.20.27.1.05
   Gonzalez S, 2009, MOL ECOL RESOUR, V9, P754, DOI 10.1111/j.1755-0998.2008.02390.x
   Grotta-Neto F., 2020, ECOLOGIA CERVIDEOS F
   Grotta-Neto F, 2020, WILDLIFE SOC B, V44, P640, DOI 10.1002/wsb.1121
   Grotta-Neto F, 2019, J MAMMAL, V100, P454, DOI 10.1093/jmammal/gyz056
   Gutierrez EE, 2017, ZOOKEYS, P87, DOI 10.3897/zookeys.697.15124
   Hurt A., 2009, CANINE ERGONOMICS, P175
   Instituto Brasileiro de Geografia e Estatistica (IBGE), 1992, MAP VEG BRAS
   IUCN, 2001, IUCN RED LIST CATEGO
   IUCN, 2012, IUCN RED LIST CAT CR
   IUCN Standards and Petitions Committee, 2019, GUIDELINES USING IUC
   Lima Fernando, 2017, Ecology, V98, P2979, DOI 10.1002/ecy.1998
   McShea WJ., 2012, DEER ANIMAL ANSWER G
   Morales-Donoso JA, 2017, CARACTERIZACAO MORFO
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   Nagy-Reis M, 2020, ECOLOGY, V101, DOI 10.1002/ecy.3128
   Pearson RG, 2007, SYNTH AM MUSEUM NAT, V50, P54
   Peres, 2020, FILOGENIA DELIMITACA
   Peres, 2015, USO ESPACO PELO VEAD
   Rezende CL, 2018, PERSPECT ECOL CONSER, V16, P208, DOI 10.1016/j.pecon.2018.10.002
   Ribeiro MC, 2009, BIOL CONSERV, V142, P1141, DOI 10.1016/j.biocon.2009.02.021
   Rivero K, 2005, MAMMALIA, V69, P169, DOI 10.1515/mamm.2005.015
   Rodrigues TF, 2017, J MAMMAL, V98, P1301, DOI 10.1093/jmammal/gyx099
   Rosa CAD, 2020, ECOLOGY, V101, DOI 10.1002/ecy.3115
   Sales LP, 2020, GLOBAL CHANGE BIOL, V26, P7036, DOI 10.1111/gcb.15374
   Sandoval EDP, 2019, CARACTERIZACAO MORFO
   Santos PM, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2663
   Silva TR, 2018, PRIM PLANO REDUCAO I
   Smith DA, 2001, SCIENCE, V291, P435, DOI 10.1126/science.291.5503.435B
   Souza Y, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2785
   Costa EBV, 2017, STUD NEOTROP FAUNA E, V52, P37, DOI 10.1080/01650521.2016.1263418
   Vogliotti, 2003, HIST NATURAL MAZAMA
   Vogliotti, 2009, PARTICAO HABITATS EN
   Vogliotti PA, 2016, IUCN RED LIST THREAT, P8235
   Williams PH, 2002, J BIOSCIENCES, V27, P327, DOI 10.1007/BF02704963
NR 66
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2199-2401
EI 2199-241X
J9 MAMMAL RES
JI Mammal Res.
PD JAN
PY 2022
VL 67
IS 1
BP 51
EP 59
DI 10.1007/s13364-021-00604-4
EA OCT 2021
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA XZ9MD
UT WOS:000704522300001
OA Bronze
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Stephens, C
   Dieguez, P
   Samuni, L
   Despres-Einspenner, ML
   Harder, B
   Landsmann, A
   Lynn, LK
   Maldonado, N
   Rockaiova, Z
   Widness, J
   Wittig, RM
   Boesch, C
   Kuhl, HS
   Arandjelovic, M
AF McCarthy, Maureen S.
   Stephens, Colleen
   Dieguez, Paula
   Samuni, Liran
   Despres-Einspenner, Marie-Lyne
   Harder, Briana
   Landsmann, Anja
   Lynn, Laura K.
   Maldonado, Nuria
   Rockaiova, Zuzana
   Widness, Jane
   Wittig, Roman M.
   Boesch, Christophe
   Kuehl, Hjalmar S.
   Arandjelovic, Mimi
TI Chimpanzee identification and social network construction through an
   online citizen science platform
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera trap; chimpanzee; citizen science; Pan troglodytes; social
   network analysis
ID CAMERA TRAPS; DATA QUALITY; CONSERVATION; VARIABILITY; POPULATION;
   VALIDATION; AGREEMENT
AB Citizen science has grown rapidly in popularity in recent years due to its potential to educate and engage the public while providing a means to address a myriad of scientific questions. However, the rise in popularity of citizen science has also been accompanied by concerns about the quality of data emerging from citizen science research projects. We assessed data quality in the online citizen scientist platform Chimp&See, which hosts camera trap videos of chimpanzees (Pan troglodytes) and other species across Equatorial Africa. In particular, we compared detection and identification of individual chimpanzees by citizen scientists with that of experts with years of experience studying those chimpanzees. We found that citizen scientists typically detected the same number of individual chimpanzees as experts, but assigned far fewer identifications (IDs) to those individuals. Those IDs assigned, however, were nearly always in agreement with the IDs provided by experts. We applied the data sets of citizen scientists and experts by constructing social networks from each. We found that both social networks were relatively robust and shared a similar structure, as well as having positively correlated individual network positions. Our findings demonstrate that, although citizen scientists produced a smaller data set based on fewer confirmed IDs, the data strongly reflect expert classifications and can be used for meaningful assessments of group structure and dynamics. This approach expands opportunities for social research and conservation monitoring in great apes and many other individually identifiable species.
C1 [McCarthy, Maureen S.; Stephens, Colleen; Dieguez, Paula; Maldonado, Nuria; Wittig, Roman M.; Boesch, Christophe; Kuehl, Hjalmar S.; Arandjelovic, Mimi] Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
   [Samuni, Liran] Harvard Univ, Dept Human Evolutionary Biol, Cambridge, MA 02138 USA.
   [Samuni, Liran; Wittig, Roman M.] Ctr Suisse Rech Sci, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Despres-Einspenner, Marie-Lyne] Ecocorridors Laurentiens, Saint Jerome, PQ, Canada.
   [Harder, Briana; Landsmann, Anja; Lynn, Laura K.; Rockaiova, Zuzana; Widness, Jane] Max Planck Inst Evolutionary Anthropol, Zooniverse Citizen Scientist, Leipzig, Germany.
   [Landsmann, Anja] Univ Leipzig, Fac Med, Inst Drug Discovery, Leipzig, Germany.
   [Maldonado, Nuria] iScapes, Valencia, Spain.
   [Widness, Jane] Yale Univ, Dept Anthropol, New Haven, CT 06520 USA.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Halle, Germany.
RP McCarthy, MS (corresponding author), Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
EM maureen_mc@eva.mpg.de
FU Centre for Forest Research-Fonds de Recherche Quebec Nature et
   Technologies International; Robert Bosch Foundation; Max Planck
   SocietyMax Planck SocietyFoundation CELLEX; Heinz L. Krekeler
   Foundation; Max Planck Society Innovation Fund; Centre Suisse de
   Recherches Scientifiques; Centre for Forest ResearchFonds de Recherche
   Quebec Nature et Technologies International internship program
FX Centre for Forest Research-Fonds de Recherche Quebec Nature et
   Technologies International; Robert Bosch Foundation; Max Planck Society;
   Heinz L. Krekeler Foundation; Max Planck Society Innovation Fund; Max
   Planck Society, the Robert Bosch Foundation; Centre Suisse de Recherches
   Scientifiques; Centre for Forest ResearchFonds de Recherche Quebec
   Nature et Technologies International internship program
CR Aplin LM, 2012, P ROY SOC B-BIOL SCI, V279, P4199, DOI 10.1098/rspb.2012.1591
   Bird TJ, 2014, BIOL CONSERV, V173, P144, DOI 10.1016/j.biocon.2013.07.037
   Boesch C, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22613
   Bonney R, 2014, SCIENCE, V343, P1436, DOI 10.1126/science.1251554
   Ontl KB, 2020, INT J PRIMATOL, V41, P916, DOI 10.1007/s10764-020-00165-4
   Brassine E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0142508
   Brossard D, 2005, INT J SCI EDUC, V27, P1099, DOI 10.1080/09500690500069483
   CAIRNS SJ, 1987, ANIM BEHAV, V35, P1454, DOI 10.1016/S0003-3472(87)80018-0
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Cox TE, 2012, ECOL APPL, V22, P1201, DOI 10.1890/11-1614.1
   Crall AW, 2011, CONSERV LETT, V4, P433, DOI 10.1111/j.1755-263X.2011.00196.x
   Croft DP, 2008, EXPLORING ANIMAL SOCIAL NETWORKS, P1
   Csardi G, 2006, INTERJOURNAL, P1695
   Danielsen F, 2014, BIOSCIENCE, V64, P236, DOI 10.1093/biosci/biu001
   Davis GH, 2018, ANIM BEHAV, V141, P29, DOI 10.1016/j.anbehav.2018.04.012
   Dekker D, 2007, PSYCHOMETRIKA, V72, P563, DOI 10.1007/s11336-007-9016-1
   Delaney DG, 2008, BIOL INVASIONS, V10, P117, DOI 10.1007/s10530-007-9114-0
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Edgar GJ, 2009, MAR ECOL PROG SER, V388, P51, DOI 10.3354/meps08149
   Farine DR, 2015, CURR BIOL, V25, P2184, DOI 10.1016/j.cub.2015.06.071
   Farine DR, 2013, METHODS ECOL EVOL, V4, P1187, DOI 10.1111/2041-210X.12121
   Foster-Smith J, 2003, BIOL CONSERV, V113, P199, DOI 10.1016/S0006-3207(02)00373-7
   Frigerio D, 2018, ETHOLOGY, V124, P365, DOI 10.1111/eth.12746
   Galloway AWE, 2006, WILDLIFE SOC B, V34, P1425, DOI 10.2193/0091-7648(2006)34[1425:TROCSA]2.0.CO;2
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Gardiner MM, 2012, FRONT ECOL ENVIRON, V10, P471, DOI 10.1890/110185
   Goldenberg SZ, 2016, CURR BIOL, V26, P75, DOI 10.1016/j.cub.2015.11.005
   Gollan J, 2012, ENVIRON MANAGE, V50, P969, DOI 10.1007/s00267-012-9924-4
   Goodall J., 1986, CHIMPANZEES GOMBE PA
   Green SE, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010132
   Gura Trisha, 2013, Nature, V496, P259
   He Y., 2015, COMMUNITY SERVICE DA, P1
   Hobaiter C, 2014, PLOS BIOL, V12, DOI 10.1371/journal.pbio.1001960
   Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
   Johansson O, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63367-z
   Johnson KVA, 2017, ANIM BEHAV, V128, P21, DOI 10.1016/j.anbehav.2017.04.001
   Johnson MF, 2014, GLOBAL ENVIRON CHANG, V29, P235, DOI 10.1016/j.gloenvcha.2014.10.006
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Kelling S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139600
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Kremen C, 2011, CONSERV BIOL, V25, P607, DOI 10.1111/j.1523-1739.2011.01657.x
   Kuhl HS, 2016, SCI REP-UK, V6, DOI 10.1038/srep22219
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kullenberg C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0147152
   McCarthy MS, 2019, ANIM BEHAV, V157, P227, DOI 10.1016/j.anbehav.2019.08.008
   McCarthy MS, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22904
   Moyer-Horner L, 2012, J WILDLIFE MANAGE, V76, P1472, DOI 10.1002/jwmg.373
   Nagy C, 2012, NORTHEAST NAT, V19, P143, DOI 10.1656/045.019.s611
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Patton F, 2008, PACHYDERM, P35
   Pebsworth PA, 2014, INT J PRIMATOL, V35, P825, DOI 10.1007/s10764-014-9802-4
   R Core Team, 2018, STATS PACK LANG ENV
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Siegel S., 1988, NONPARAMETRIC STAT B, V2nd ed
   Silk MJ, 2017, BIOSCIENCE, V67, P245, DOI 10.1093/biosci/biw175
   Silk MJ, 2015, ANIM BEHAV, V104, P1, DOI 10.1016/j.anbehav.2015.03.005
   Silvertown J, 2015, ZOOKEYS, P125, DOI 10.3897/zookeys.480.8803
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Stevick PT, 2001, CAN J FISH AQUAT SCI, V58, P1861, DOI 10.1139/cjfas-58-9-1861
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Toomey AH, 2013, HUM ECOL REV, V20, P50
   Van Horn RC, 2014, WILDLIFE BIOL, V20, P291, DOI 10.2981/wlb.00023
   VanderWaal KL, 2009, ANIM BEHAV, V77, P949, DOI 10.1016/j.anbehav.2008.12.028
   Viera AJ, 2005, FAM MED, V37, P360
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wittig R, 2019, CHIMPANZEES OF THE TAI FOREST: 40 YEARS OF RESEARCH, P44
   Wursig B., 1990, Reports of the International Whaling Commission Special Issue, P43
NR 70
TC 1
Z9 1
U1 1
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD FEB
PY 2021
VL 11
IS 4
BP 1598
EP 1608
DI 10.1002/ece3.7128
EA DEC 2020
PG 11
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA QG9IV
UT WOS:000599096200001
PM 33613992
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU de Oliveira, ML
   Peres, PHD
   Vogliotti, A
   Grotta-Neto, F
   de Azevedo, ADK
   Cerveira, JF
   do Nascimento, GB
   Peruzzi, NJ
   Carranza, J
   Duarte, JMB
AF de Oliveira, Marcio Leite
   de Faria Peres, Pedro Henrique
   Vogliotti, Alexandre
   Grotta-Neto, Francisco
   Koester de Azevedo, Allyson Diaz
   Cerveira, Josi Fernanda
   do Nascimento, Guilherme Batista
   Peruzzi, Nelson Jose
   Carranza, Juan
   Barbanti Duarte, Jose Mauricio
TI Phylogenetic signal in the circadian rhythm of morphologically
   convergent species of Neotropical deer
SO MAMMALIAN BIOLOGY
LA English
DT Article
DE Activity pattern; Mazama; Phylogenetic constrain; Camera trap; Brocket
   deer
ID DAILY ACTIVITY PATTERNS; NICHE CONSERVATISM; ACTIVITY PERIODS;
   PLASTICITY; SPECIATION; AMERICANA; ECOLOGY; TIME
AB Deer species included in the genus Mazama descend from two different clades that experienced a strong evolutionary convergence in morphology and behaviour when they adapted to Neotropical forests. We would expect that circadian activity rhythms also converged according to habitat features or responded to temporal niche segregation in sympatric species. We used camera trapping in four study areas, representing three main biomes in Brazil, together with data taken from the literature, to analyse activity patterns of five Mazama species in four biomes in South America. Our results show that Glade assignment was the main predictor of diurnal versus nocturnal activity, thus suggesting a phylogenetic constraint rather than any other ecological influence on circadian activity. We discuss how the evolutionary history of both lineages may have influenced their activity patterns. (C) 2016 Deutsche Gesellschaft fur Saugetierkunde. Published by Elsevier GmbH. All rights reserved.
C1 [de Oliveira, Marcio Leite; de Faria Peres, Pedro Henrique; Grotta-Neto, Francisco; Koester de Azevedo, Allyson Diaz; Cerveira, Josi Fernanda; Carranza, Juan; Barbanti Duarte, Jose Mauricio] Univ Estadual Paulista UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, Jaboticabal, SP, Brazil.
   [do Nascimento, Guilherme Batista] Univ Estadual Paulista UNESP, Dept Ciencias Exatas, Lab Estat Aplicada Genet & Melhoramento Anim, Jaboticabal, SP, Brazil.
   [Peruzzi, Nelson Jose] Univ Estadual Paulista UNESP, Dept Ciencias Exatas, Jaboticabal, SP, Brazil.
   [Carranza, Juan] Univ Cordoba, Ungulate Res Unit, Catedra Recursos Cineget & Piscicolas CRCP, E-14071 Cordoba, Spain.
   [Barbanti Duarte, Jose Mauricio] Univ Estadual Paulista UNESP, Dept Zootecnia, Jaboticabal, SP, Brazil.
   [Vogliotti, Alexandre] Univ Fed Integracao Latinoamer UNILA, Inst Latinoamer Ciencias Vida & Nat, Foz Do Iguacu, PR, Brazil.
   [Grotta-Neto, Francisco] Univ Fed Parana UFPR, Lab Biodiversidade Conservacao & Ecol Anim Silves, Curitiba, Parana, Brazil.
   [Carranza, Juan] Univ Agr Ecuador, Fac Med Vet & Zootecnia, Guayaquil, Ecuador.
RP de Oliveira, ML (corresponding author), Univ Estadual Paulista UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, Jaboticabal, SP, Brazil.
EM oliveiraml1@yahoo.com.br
RI Nascimento, Guilherme/D-1926-2014; Carranza, Juan/E-4472-2010; de
   Oliveira, Márcio L/F-3792-2012; Duarte, José Maurício
   Barbanti/AAG-5149-2019
OI Nascimento, Guilherme/0000-0003-2370-322X; de Oliveira, Márcio
   L/0000-0002-7705-0626; Duarte, José Maurício
   Barbanti/0000-0002-7805-0265; Peres, Pedro Henrique/0000-0002-3158-0963;
   Grotta Neto, Francisco/0000-0002-2390-936X
FU Sao Paulo Research Foundation (FAPESP)Fundacao de Amparo a Pesquisa do
   Estado de Sao Paulo (FAPESP); National Council for Scientific and
   Technological Development (CNPq)Conselho Nacional de Desenvolvimento
   Cientifico e Tecnologico (CNPQ); Coordination for the Improvement of
   Higher Education Personnel (CAPES)Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior (CAPES); Project for the Conservation and
   Sustainable Use of Brazilian Biological Diversity (PROBIO)
FX This research was supported by the Sao Paulo Research Foundation
   (FAPESP), the National Council for Scientific and Technological
   Development (CNPq), the Coordination for the Improvement of Higher
   Education Personnel (CAPES) and the Project for the Conservation and
   Sustainable Use of Brazilian Biological Diversity (PROBIO). Collection
   licenses were provided by the Chico Mendes Institute for Biodiversity
   Conservation (ICMBio) and the Forestry Institute of Sao Paulo (IFSP). We
   thank Concha Mateos for advice on statistical procedures. Two anonymous
   reviewers made constructive comments that improved the manuscript.
CR Abril Vanessa Veltrini, 2010, P160
   Andersen R, 1998, EUROPEAN ROE DEER: THE BIOLOGY OF SUCCESS, P285
   Ayres M, 2007, BIOESTAT APLICACOES
   BEIER P, 1990, Wildlife Monographs, P1
   Black-Decima Patricia, 2000, Mastozoologia Neotropical, V7, P5
   Black-Decima Patricia, 2010, P190
   Boczko R., 1988, CONCEITOS ASTRONOMIA
   de Vivo M, 2004, J BIOGEOGR, V31, P943, DOI 10.1111/j.1365-2699.2004.01068.x
   Di Bitetti MS, 2008, BIOTROPICA, V40, P636, DOI 10.1111/j.1744-7429.2008.00413.x
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Foster SA, 2013, ANIM BEHAV, V85, P1003, DOI 10.1016/j.anbehav.2013.04.006
   Fox RJ, 2011, FUNCT ECOL, V25, P1096, DOI 10.1111/j.1365-2435.2011.01874.x
   Gallina Sonia, 2010, P101
   Geist, 1998, DEER WORLD THEIR EVO
   Gomez H, 2005, STUD NEOTROP FAUNA E, V40, P91, DOI 10.1080/01650520500129638
   Gonzalez Susana, 2010, P119
   HAFFER J, 1969, SCIENCE, V165, P131, DOI 10.1126/science.165.3889.131
   HARDIN G, 1960, SCIENCE, V131, P1292, DOI 10.1126/science.131.3409.1292
   JACOBS GH, 1993, BIOL REV, V68, P413, DOI 10.1111/j.1469-185X.1993.tb00738.x
   JARMAN PJ, 1974, BEHAVIOUR, V48, P215, DOI 10.1163/156853974X00345
   KAMMERMEYER KE, 1977, J WILDLIFE MANAGE, V41, P315, DOI 10.2307/3800612
   Kronfeld-Schor N, 2003, ANNU REV ECOL EVOL S, V34, P153, DOI 10.1146/annurev.ecolsys.34.011802.132435
   Kronfeld-Schor N, 2001, AM NAT, V158, P451, DOI 10.1086/321991
   Kronfeld-Schor N, 2008, BIOL RHYTHM RES, V39, P193, DOI 10.1080/09291010701683268
   Losos JB, 2008, ECOL LETT, V11, P995, DOI 10.1111/j.1461-0248.2008.01229.x
   Maffei L., 2002, REV B ECOL, V11, P55
   Merino Mariano L., 2010, P2
   Monterroso P, 2014, BEHAV ECOL SOCIOBIOL, V68, P1403, DOI 10.1007/s00265-014-1748-1
   Nowak R.M., 1999, WALKERS MAMMALS WORL
   Oliveira-Santos LGR, 2013, ANIM BEHAV, V85, P269, DOI 10.1016/j.anbehav.2012.09.033
   Owen-Smith N, 2014, J ZOOL, V293, P181, DOI 10.1111/jzo.12132
   Perez-Barberia FJ, 2002, EVOLUTION, V56, P1276
   Piersma T, 2003, TRENDS ECOL EVOL, V18, P228, DOI 10.1016/S0169-5347(03)00036-3
   Piovezan Ubiratan, 2010, P66
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rivero K, 2005, MAMMALIA, V69, P169, DOI 10.1515/mamm.2005.015
   Rivero K, 2004, EUR J WILDLIFE RES, V50, P161, DOI 10.1007/s10344-004-0064-x
   Roll U, 2006, EVOL ECOL, V20, P479, DOI 10.1007/s10682-006-0015-y
   Rossi Rogerio Vieira, 2010, P202
   Tobler MW, 2009, J TROP ECOL, V25, P261, DOI 10.1017/S0266467409005896
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Varela Diego Martin, 2010, P151
   Vila Alejandro R., 2010, P89
   Vogliotti A, 2010, NEOTROPICAL CERVIDOL, P218
   Webb Stephen L., 2010, International Journal of Ecology, V2010, P1
   Weber M, 2003, ECOSCIENCE, V10, P443, DOI 10.1080/11956860.2003.11682792
   Wiens JJ, 2004, EVOLUTION, V58, P193, DOI 10.1111/j.0014-3820.2004.tb01586.x
NR 48
TC 9
Z9 11
U1 1
U2 25
PU ELSEVIER GMBH
PI MUNICH
PA HACKERBRUCKE 6, 80335 MUNICH, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
PD MAY
PY 2016
VL 81
IS 3
BP 281
EP 289
DI 10.1016/j.mambio.2016.01.004
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA DN1CV
UT WOS:000376804800007
OA Green Published
DA 2022-02-10
ER

PT J
AU Gonzalez, LF
   Montes, GA
   Puig, E
   Johnson, S
   Mengersen, K
   Gaston, KJ
AF Gonzalez, Luis F.
   Montes, Glen A.
   Puig, Eduard
   Johnson, Sandra
   Mengersen, Kerrie
   Gaston, Kevin J.
TI Unmanned Aerial Vehicles (UAVs) and Artificial Intelligence
   Revolutionizing Wildlife Monitoring and Conservation
SO SENSORS
LA English
DT Article
DE Unmanned Aerial Vehicle (UAV); wildlife monitoring; artificial
   intelligence; thermal imaging; robotics; conservation; automatic
   classification; koala; deer; wild pigs; dingo; conservation
AB Surveying threatened and invasive species to obtain accurate population estimates is an important but challenging task that requires a considerable investment in time and resources. Estimates using existing ground-based monitoring techniques, such as camera traps and surveys performed on foot, are known to be resource intensive, potentially inaccurate and imprecise, and difficult to validate. Recent developments in unmanned aerial vehicles (UAV), artificial intelligence and miniaturized thermal imaging systems represent a new opportunity for wildlife experts to inexpensively survey relatively large areas. The system presented in this paper includes thermal image acquisition as well as a video processing pipeline to perform object detection, classification and tracking of wildlife in forest or open areas. The system is tested on thermal video data from ground based and test flight footage, and is found to be able to detect all the target wildlife located in the surveyed area. The system is flexible in that the user can readily define the types of objects to classify and the object characteristics that should be considered during classification.
C1 [Gonzalez, Luis F.; Montes, Glen A.; Puig, Eduard] Queensland Univ Technol, ARCAA, 2 George St, Brisbane, Qld 4000, Australia.
   [Johnson, Sandra; Mengersen, Kerrie] Queensland Univ Technol, ARC Ctr Excellence Math & Stat Frontiers ACEMS, 2 George St, Brisbane, Qld 4000, Australia.
   [Gaston, Kevin J.] Univ Exeter, Environm & Sustainabil Inst, Penryn TR10 9EZ, Cornwall, England.
RP Gonzalez, LF (corresponding author), Queensland Univ Technol, ARCAA, 2 George St, Brisbane, Qld 4000, Australia.
EM felipe.gonzalez@qut.edu.au; glen.montes@mail.escuelaing.edu.co;
   eduard.puiggarcia@qut.edu.au; sandra.johnson@qut.edu.au;
   k.mengersen@qut.edu.au; k.j.gaston@exeter.ac.uk
RI Gonzalez, Felipe/G-7661-2011
OI Gonzalez, Felipe/0000-0002-4342-3682; Mengersen,
   Kerrie/0000-0001-8625-9168; Gaston, Kevin J./0000-0002-7235-7928
CR Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150
   [Anonymous], IM WATCH WILDL VID F
   [Anonymous], KANG BOX INFR VID FI
   Bevan Elizabeth, 2015, Marine Turtle Newsletter, P19
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chabot D, 2012, WATERBIRDS, V35, P170, DOI 10.1675/063.035.0119
   Christiansen P, 2014, SENSORS-BASEL, V14, P13778, DOI 10.3390/s140813778
   Cork Lennon, 2007, IEEE Aerospace and Electronic Systems Magazine, V22, P29, DOI 10.1109/MAES.2007.4408524
   Cristescu R. H., 2015, SCI REP, V5, P1
   Ditmer MA, 2015, CURR BIOL, V25, P2278, DOI 10.1016/j.cub.2015.07.024
   DJI, S800 EVO
   Dos Santos Gilberto Antonio Marcon, 2014, 2014 IEEE 11th International Conference on Mobile Ad-Hoc and Sensor Systems (MASS), P761, DOI 10.1109/MASS.2014.48
   Gaston KJ, 2009, J APPL ECOL, V46, P1, DOI 10.1111/j.1365-2664.2008.01596.x
   Hodgson A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079556
   Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
   Korner F., 2010, P 2010 IEEE RSJ INT
   Leonardo M., 2013, P 2013 ASME IEEE INT
   Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
   Mulero-Pazmany M., 2015, ECOL EVOL
   Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
   Murray JV, 2008, BIOL CONSERV, V141, P7, DOI 10.1016/j.biocon.2007.07.020
   Schaub M, 2007, CONSERV BIOL, V21, P945, DOI 10.1111/j.1523-1739.2007.00743.x
   Senate Environment and Communications References Committee, COMPL INQ 2010 2013
   Shumway N, 2015, ENVIRON SCI POLICY, V54, P297, DOI 10.1016/j.envsci.2015.07.024
   Soriano P., 2009, INT S ROBOT, V40, P239
   van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
   Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
   Vincent JB, 2015, FRONT ECOL ENVIRON, V13, P74, DOI 10.1890/15.WB.002
   Williams B.P., 2014, P 14 AIAA AV TECHN I
   Witmer GW, 2005, WILDLIFE RES, V32, P259, DOI 10.1071/WR04003
NR 31
TC 181
Z9 191
U1 8
U2 174
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JAN
PY 2016
VL 16
IS 1
AR 97
DI 10.3390/s16010097
PG 18
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA DE5OC
UT WOS:000370679800078
PM 26784196
OA Green Published, Green Submitted, gold
DA 2022-02-10
ER

PT J
AU Weinstein, BG
AF Weinstein, Ben G.
TI A computer vision for animal ecology
SO JOURNAL OF ANIMAL ECOLOGY
LA English
DT Review
DE automation; camera traps; ecological monitoring; images; unmanned aerial
   vehicles
ID SATELLITE IMAGERY; AERIAL IMAGES; IDENTIFICATION; CLASSIFICATION;
   RECOGNITION; APPEARANCE; CAMOUFLAGE; ABUNDANCE; BEHAVIOR; SYSTEM
AB 1. A central goal of animal ecology is to observe species in the natural world. The cost and challenge of data collection often limit the breadth and scope of ecological study. Ecologists often use image capture to bolster data collection in time and space. However, the ability to process these images remains a bottleneck.
   2. Computer vision can greatly increase the efficiency, repeatability and accuracy of image review. Computer vision uses image features, such as colour, shape and texture to infer image content.
   3.I provide a brief primer on ecological computer vision to outline its goals, tools and applications to animal ecology.
   4.I reviewed 187 existing applications of computer vision and divided articles into ecological description, counting and identity tasks.
   5. I discuss recommendations for enhancing the collaboration between ecologists and computer scientists and highlight areas for future growth of automated image analysis.
C1 [Weinstein, Ben G.] Oregon State Univ, Marine Mammal Inst, Dept Fisheries & Wildlife, Newport, OR 97365 USA.
RP Weinstein, BG (corresponding author), Oregon State Univ, Marine Mammal Inst, Dept Fisheries & Wildlife, Newport, OR 97365 USA.
EM weinsteb@oregonstate.edu
OI Weinstein, Ben/0000-0002-2176-7935
CR Abramoff M.D., 2004, BIOPHOTONICS INT, V11, P36, DOI DOI 10.1117/1.3589100
   Adams Jeffrey D., 2006, Aquatic Mammals, V32, P374, DOI 10.1578/AM.32.3.2006.374
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Arzoumanian Z, 2005, J APPL ECOL, V42, P999, DOI 10.1111/j.1365-2664.2005.01117.x
   Atanbori J, 2016, PATTERN RECOGN LETT, V81, P53, DOI 10.1016/j.patrec.2015.08.015
   Barber-Meyer SM, 2007, POLAR BIOL, V30, P1565, DOI 10.1007/s00300-007-0317-8
   Barnard S, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0158748
   Bartomeus I, 2016, FUNCT ECOL, V30, P1894, DOI 10.1111/1365-2435.12666
   Beekmans Bas W. P. M., 2005, Aquatic Mammals, V31, P243, DOI 10.1578/AM.31.2.2005.243
   Beijbom O, 2016, SCI REP-UK, V6, DOI 10.1038/srep23166
   Belongie S, 2016, PATTERN RECOGN LETT, V72, P15, DOI 10.1016/j.patrec.2015.11.023
   Berg TL, 2010, P IEEE, V98, P1434, DOI 10.1109/JPROC.2009.2032355
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Blanc K., 2014, P 3 ACM INT WORKSH M, P1, DOI [10.1145/2661821.2661827, DOI 10.1145/2661821.2661827]
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906
   Bradski G, 2000, DR DOBBS J, V25, P120
   Branson S., 2010, LECT NOTES COMPUTER, V6314
   Branson S., 2014, BRIT MACH VIS C BMVC
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
   Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
   Cross Matthew D., 2014, Herpetological Review, V45, P584
   Crutsinger GM, 2016, J UNMANNED VEH SYST, V4, P161, DOI 10.1139/juvs-2016-0008
   Dala-Corte RB, 2016, NEOTROP ICHTHYOL, V14, DOI 10.1590/1982-0224-20150074
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   Descamps S, 2011, BIRD STUDY, V58, P302, DOI 10.1080/00063657.2011.588195
   Desell T, 2013, P IEEE INT C E-SCI, P107, DOI 10.1109/eScience.2013.50
   Duyck J, 2015, PATTERN RECOGN, V48, P1059, DOI 10.1016/j.patcog.2014.07.017
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8
   Favret C, 2016, SYST ENTOMOL, V41, P133, DOI 10.1111/syen.12146
   Feng LN, 2016, PATTERN RECOGN, V51, P225, DOI 10.1016/j.patcog.2015.09.012
   Fretwell PT, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088655
   Gilmetdinova A., 2016, INT J BILING EDUC BI, P1, DOI [10.1080/13670050.2016.1231772, DOI 10.1080/13670050.2016.1231772]
   Giraldo-Zuluaga J.-H., 2017, ARXIV170108180
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Groom G, 2011, INT J REMOTE SENS, V32, P4611, DOI 10.1080/01431161.2010.489068
   Haggag H, 2016, IEEE SYS MAN CYBERN, P855, DOI 10.1109/SMC.2016.7844347
   Hernandez-Serna A, 2014, PEERJ, V2, DOI 10.7717/peerj.563
   Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
   Howland J. C., 2012, OCEANS 2012 MTS IEEE, P1, DOI [10. 1109/OCEANS. 2012. 6404859, DOI 10.1109/0CEANS.2012.6404859]
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006
   Jones AM, 2008, CORAL REEFS, V27, P521, DOI 10.1007/s00338-008-0354-y
   Kosmala M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090726
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kvilekval K, 2010, BIOINFORMATICS, V26, P544, DOI 10.1093/bioinformatics/btp699
   LaRue MA, 2017, CONSERV BIOL, V31, P213, DOI 10.1111/cobi.12809
   LaRue MA, 2015, WILDLIFE SOC B, V39, P772, DOI 10.1002/wsb.596
   Lavy A, 2015, METHODS ECOL EVOL, V6, P521, DOI 10.1111/2041-210X.12331
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levy K, 2014, BIOL OPEN, V3, P1245, DOI 10.1242/bio.20149175
   Liu CC, 2015, ECOL INFORM, V30, P170, DOI 10.1016/j.ecoinf.2015.10.008
   Lynch HJ, 2012, POLAR BIOL, V35, P963, DOI 10.1007/s00300-011-1138-3
   Lytle DA, 2010, J N AM BENTHOL SOC, V29, P867, DOI 10.1899/09-080.1
   Marburg A., 2016, OCEANS 2016 MTS IEEE, P1
   Matuska S, 2014, AASRI PROC, V9, P25, DOI 10.1016/j.aasri.2014.09.006
   McDowall P, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0166773
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Naumann MS, 2009, CORAL REEFS, V28, P109, DOI 10.1007/s00338-008-0459-3
   Olsen AM, 2015, METHODS ECOL EVOL, V6, P351, DOI 10.1111/2041-210X.12326
   Pennekamp F, 2013, METHODS ECOL EVOL, V4, P483, DOI 10.1111/2041-210X.12036
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Reda K, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-48
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Robie AA, 2017, J EXP BIOL, V220, P25, DOI 10.1242/jeb.142281
   Seymour AC, 2017, SCI REP-UK, V7, DOI 10.1038/srep45127
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Steen R, 2014, MAR FRESHWATER RES, V65, P1094, DOI 10.1071/MF13139
   Stoddard MC, 2016, SCI REP-UK, V6, DOI 10.1038/srep32059
   Stoddard MC, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5117
   Sun X, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P471, DOI 10.1109/CISP-BMEI.2016.7852757
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Tankus A, 2009, PHILOS T R SOC B, V364, P529, DOI 10.1098/rstb.2008.0211
   Torney CJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156342
   Town C, 2013, ECOL EVOL, V3, P1902, DOI 10.1002/ece3.587
   Troscianko J, 2017, BMC EVOL BIOL, V17, DOI 10.1186/s12862-016-0854-2
   Van Andel AC, 2015, AM J PRIMATOL, V77, P1122, DOI 10.1002/ajp.22446
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Van Horn Grant, 2017, ARXIV170706642
   Villon S., 2016, LECT NOTES COMPUTER, V10016
   Weinstein B. G., 2017, DRYAD DIGITAL REPOSI, DOI [10. 5061/dryad. b700h, DOI 10.5061/DRYAD.B700H]
   Weinstein BG, 2017, ECOL LETT, V20, P326, DOI 10.1111/ele.12730
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Witharana C, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8050375
   Yang CC, 2016, BIOL J LINN SOC, V117, P422, DOI 10.1111/bij.12690
   Yang Zhixian, 2014, ScientificWorldJournal, V2014, P140863, DOI 10.1155/2014/140863
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
   Zhang H., 2012, P 21 INT C WORLD WID, P749, DOI [DOI 10.1145/2187836.2187938, 10.1145/2187836.2187938]
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 96
TC 111
Z9 113
U1 18
U2 128
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0021-8790
EI 1365-2656
J9 J ANIM ECOL
JI J. Anim. Ecol.
PD MAY
PY 2018
VL 87
IS 3
BP 533
EP 545
DI 10.1111/1365-2656.12780
PG 13
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA GC8QE
UT WOS:000430059900003
PM 29111567
OA Bronze
DA 2022-02-10
ER

PT J
AU Durbach, I
   Borchers, D
   Sutherland, C
   Sharma, K
AF Durbach, Ian
   Borchers, David
   Sutherland, Chris
   Sharma, Koustubh
TI Fast, flexible alternatives to regular grid designs for spatial
   capture-recapture
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera trap; population ecology; sampling; spatial capture&#8208;
   recapture; surveys
AB Spatial capture-recapture (SCR) methods use the location of detectors (camera traps, hair snares and live-capture traps) and the locations at which animals were detected (their spatial capture histories) to estimate animal density. Despite the often large expense and effort involved in placing detectors in a landscape, there has been relatively little work on how detectors should be located. A natural criterion is to place traps so as to maximize the precision of density estimators, but the lack of a closed-form expression for precision has made optimizing this criterion computationally demanding.
   Recent results by Efford and Boulanger (2019) show that precision can be well approximated by a function of the expected number of detected individuals and expected number of recapture events, both of which can be evaluated at low computational cost. We use these results to develop a method for obtaining survey designs that optimize this approximate precision for SCR studies using count or binary proximity detectors, or multi-catch traps.
   We show how the basic design protocol can be extended to incorporate spatially varying distributions of activity centres and animal detectability. We illustrate our approach by simulating from a camera trap study of snow leopards in Mongolia and comparing estimates from our designs to those generated by regular or optimized grid designs. Optimizing detector placement increased the number of detected individuals and recaptures, but this did not always lead to more precise density estimators due to less precise estimation of the effective sampling area. In most cases, the precision of density estimators was comparable to that obtained with grid designs, with improvement in some scenarios where approximate CV(D) < 20% and density varied spatially.
   Designs generated using our approach are transparent and statistically grounded. They can be produced for survey regions of any shape, adapt to known information about animal density and detectability, and are potentially easier and less costly to implement. We recommend their use as good, flexible candidate designs for SCR surveys when reasonable knowledge of model parameters exists. We provide software for researchers to construct their own designs, in the form of updates to design functions in the r package oSCR.
C1 [Durbach, Ian; Borchers, David; Sutherland, Chris] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
   [Durbach, Ian] Univ Cape Town, Ctr Stat Ecol Environm & Conservat, Cape Town, South Africa.
   [Sutherland, Chris] Univ Massachusetts, Dept Environm Conservat, Amherst, MA 01003 USA.
   [Sharma, Koustubh] Snow Leopard Trust, Seattle, WA USA.
RP Durbach, I (corresponding author), Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.; Durbach, I (corresponding author), Univ Cape Town, Ctr Stat Ecol Environm & Conservat, Cape Town, South Africa.
EM id52@st-andrews.ac.uk
RI Sutherland, Chris/H-6624-2019
OI Sutherland, Chris/0000-0003-2073-1751; Durbach, Ian/0000-0003-0769-2153;
   Sharma, Koustubh/0000-0001-7301-441X; Borchers,
   David/0000-0002-3944-0754
FU National Research Foundation of South AfricaNational Research Foundation
   - South Africa [105782, 90782]; Global Snow Leopard & Ecosystem
   Protection Program
FX National Research Foundation of South Africa, Grant/Award Number: 105782
   and 90782; Global Snow Leopard & Ecosystem Protection Program
CR Alexander JS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134815
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Clark JD, 2019, POPUL ECOL, V61, P93, DOI 10.1002/1438-390X.1011
   Dupont G., 2020, ECOLOGY, DOI DOI 10.1101/2020.04.16.045740V3
   Efford MG, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2580
   Efford MG, 2019, METHODS ECOL EVOL, V10, P1529, DOI 10.1111/2041-210X.13239
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Johansson O, 2016, BIODIVER WORL CONS, P355, DOI 10.1016/B978-0-12-802213-9.00026-2
   Kristensen TV, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2217
   McCarthy KP, 2008, J WILDLIFE MANAGE, V72, P1826, DOI 10.2193/2008-040
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Seber G.A.F., 1982, ESTIMATION ANIMAL AB, V8
   Sharma K, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0101319
   Sharma RK, 2015, BIOL CONSERV, V190, P8, DOI 10.1016/j.biocon.2015.04.026
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Sutherland C, 2015, METHODS ECOL EVOL, V6, P169, DOI 10.1111/2041-210X.12316
   Wolters, 2015, J STAT SOFTW CODE SN, V68, P1455, DOI [10.18637/jss.v068.c01, DOI 10.18637/JSS.V068.C01]
NR 18
TC 3
Z9 3
U1 0
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD FEB
PY 2021
VL 12
IS 2
BP 298
EP 310
DI 10.1111/2041-210X.13517
EA NOV 2020
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QB7SQ
UT WOS:000587573800001
OA Green Accepted
DA 2022-02-10
ER

PT J
AU Pereira, AD
   Bogoni, JA
   Bazilio, S
   Orsi, ML
AF Pereira, Alan Deivid
   Bogoni, Juliano A.
   Bazilio, Sergio
   Orsi, Mario Luis
TI Mammalian defaunation across the Devonian kniferidges and meridional
   plateaus of the Brazilian Atlantic Forest
SO BIODIVERSITY AND CONSERVATION
LA English
DT Article
DE Deforestation; Forest fragmentation; Land use; Mammal assemblages;
   Protected areas; Tropical forest
ID HABITAT STRUCTURE; CAMERA TRAPS; EXTRAPOLATION; BIODIVERSITY;
   RAREFACTION; DISPERSAL; DIVERSITY; POPULATIONS; DISTURBANCE; EXTINCTION
AB The vast majority of empirical studies on regional mammal defaunation across the Atlantic Forest biome of South America are concentrated in Southeastern and Northeastern regions. Thus, the lack of empirical information on the medium- to large-bodied mammal population declines across the subtropical region of this tropical biome and its major causes are paramount to conservation efforts. We investigate the influence of land use on mammal defaunation across the subtropical Atlantic Forest by sampling 91 points using 65 camera-traps installed for five consecutive years-totaling an effort of 30,189 camera-trap-days. We observed that the average defaunation across the Devonian Kniferidges Environmental Protection Area meta-region (spanning over subtropical Atlantic Forest plateaus) was 42% (D = 0.42; +/- 0.17) when compared to presumed assemblages of historical times going back to the Pre-Columbian era (ca. 500 years ago). The main landscape predictors of regional defaunation were silviculture and agriculture, once the highest defaunation indexes were concentered in sites intensely human-modified by these features. Protected areas had significantly lower defaunation indexes in comparison with the unprotected areas. Although our results of defaunation were 29.2% lesser than the average for the entire biome, the negative consequences of regional defaunation on ecosystem services are already occurring, once two in each five local species presumably are locally extinct or present low abundances (i.e. functionally extinct). Given that habitat conversion is the primary cause of global biodiversity decline, our results reinforce that biodiversity conservation is still strongly dependent on natural protected areas networks.
C1 [Pereira, Alan Deivid; Bogoni, Juliano A.] Univ Estadual Londrina, Ctr Ciencias Biol, Dept Biol Anim & Vegetal, Programa Posgrad Ciencias Biol, Rodovia Celso Garcia Cid,PR 445,Km 380, BR-86057970 Londrina, Parana, Brazil.
   [Bogoni, Juliano A.] Univ Sao Paulo, Escola Super Agr Luiz de Queiroz, Lab Ecol, Manejo & Conservacao Fauna Silvestre LEMaC, Sao Paulo, Brazil.
   [Bogoni, Juliano A.] Univ East Anglia, Sch Environm Sci, Norwich NR4 7TJ, Norfolk, England.
   [Bazilio, Sergio] Univ Estadual Parana UNESPAR, Campus Uniao da Vitoria,Caixa Postal 241, BR-84600970 Paranavai, Parana, Brazil.
   [Orsi, Mario Luis] Univ Estadual Londrina, Ctr Ciencias Biol, Dept Biol Anim & Vegetal, Lab Ecol Peixes & Invasoes Biol, Rodovia Celso Garcia Cid,PR 445,Km 380, BR-86057970 Londrina, Parana, Brazil.
RP Pereira, AD (corresponding author), Univ Estadual Londrina, Ctr Ciencias Biol, Dept Biol Anim & Vegetal, Programa Posgrad Ciencias Biol, Rodovia Celso Garcia Cid,PR 445,Km 380, BR-86057970 Londrina, Parana, Brazil.
EM alandeivid_bio@live.com
OI Pereira, Alan/0000-0002-3182-2344
FU Coordination for the Upgrading of Higher Education Personnel
   (CAPES)Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior
   (CAPES); Sao Paulo Research Foundation (FAPESP)Fundacao de Amparo a
   Pesquisa do Estado de Sao Paulo (FAPESP) [2018-05970-1, 2019-11901-5]
FX Coordination for the Upgrading of Higher Education Personnel (CAPES).
   JAB is supported by the Sao Paulo Research Foundation (FAPESP)
   postdoctoral fellowship grants 2018-05970-1 and 2019-11901-5.
CR Altrichter M, 2012, ORYX, V46, P87, DOI 10.1017/S0030605311000421
   Arroyo-Rodriguez V, 2020, ECOL LETT, V23, P1404, DOI 10.1111/ele.13535
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Beck H, 2006, J MAMMAL, V87, P519, DOI 10.1644/05-MAMM-A-174R1.1
   Beisiegel Beatriz de Mello, 2012, Cat News, P14
   Bello C, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1501105
   Bogoni JA, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-72010-w
   Bogoni JA, 2020, ECOSYST SERV, V45, DOI 10.1016/j.ecoser.2020.101173
   Bogoni JA, 2019, ECOGRAPHY, V42, P1803, DOI 10.1111/ecog.04670
   Bogoni JA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204515
   Bogoni JA, 2016, BIODIVERS CONSERV, V25, P1661, DOI 10.1007/s10531-016-1147-1
   Brown JH, 2004, ECOLOGY, V85, P1771, DOI 10.1890/03-9000
   Canale GR, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041671
   Cardillo M, 2005, SCIENCE, V309, P1239, DOI 10.1126/science.1116030
   Chao A, 2014, ECOL MONOGR, V84, P45, DOI 10.1890/13-0133.1
   Chao A, 2012, ECOLOGY, V93, P2533, DOI 10.1890/11-1952.1
   Chiarello A. G., 2000, Revista Brasileira de Biologia, V60, P237, DOI 10.1590/S0034-71082000000200007
   Chiarello AG, 2000, CONSERV BIOL, V14, P1649, DOI 10.1046/j.1523-1739.2000.99071.x
   Conte C.E., 2017, REVISOES ZOOLOGIA MA, P391
   Culot L, 2013, BIOL CONSERV, V163, P79, DOI 10.1016/j.biocon.2013.04.004
   Da Fonseca G.A.B., 2004, HOTSPOTS REVISITED
   Dean W, 1996, FERRO FOGO HIST DEV
   Devictor V, 2008, OIKOS, V117, P507, DOI 10.1111/j.2008.0030-1299.16215.x
   DIRZO R, 1990, CONSERV BIOL, V4, P444, DOI 10.1111/j.1523-1739.1990.tb00320.x
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Dobbins M, 2020, J APPL ECOL, V57, P2100, DOI 10.1111/1365-2664.13750
   Elschot K, 2015, MAR ECOL PROG SER, V537, P9, DOI 10.3354/meps11447
   Fahrig L, 2003, ANNU REV ECOL EVOL S, V34, P487, DOI 10.1146/annurev.ecolsys.34.011802.132419
   Farago PV., 2001, PUBLICATIO UEPG PONT, V7, P57, DOI [10.5212/publicatio, DOI 10.5212/PUBLICATIO]
   Ferraz, 2021, MAMMAL REV
   Galetti M, 2006, BOT J LINN SOC, V151, P141, DOI 10.1111/j.1095-8339.2006.00529.x
   Galetti M, 2017, ANIM CONSERV, V20, P270, DOI 10.1111/acv.12311
   Galetti M, 2015, GLOB ECOL CONSERV, V3, P824, DOI 10.1016/j.gecco.2015.04.008
   Galetti M, 2015, BIOL CONSERV, V190, P2, DOI 10.1016/j.biocon.2015.04.032
   Galetti M, 2013, BIOL CONSERV, V163, P1, DOI 10.1016/j.biocon.2013.04.020
   Galetti M, 2009, BIOL CONSERV, V142, P1229, DOI 10.1016/j.biocon.2009.01.023
   Gardner TA, 2009, ECOL LETT, V12, P561, DOI 10.1111/j.1461-0248.2009.01294.x
   Gayot M, 2004, J TROP ECOL, V20, P31, DOI 10.1017/S0266467404006157
   Giacomini HC, 2013, BIOL CONSERV, V163, P33, DOI 10.1016/j.biocon.2013.04.007
   Gonzalez-Maya JF, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175931
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Herkt KMB, 2017, GLOBAL ECOL BIOGEOGR, V26, P930, DOI 10.1111/geb.12601
   Hines NJ, 2017, OCCUPANCY ESTIMATION
   Hoskins AJ, 2016, ECOL EVOL, V6, P3040, DOI 10.1002/ece3.2104
   Hsieh TC, 2016, METHODS ECOL EVOL, V7, P1451, DOI 10.1111/2041-210X.12613
   IBGE, 2019, I BRAS GEOGR EST CON
   IUCN, 2019, IUCN RED LIST THREAT
   Jorge MLSP, 2013, BIOL CONSERV, V163, P49, DOI 10.1016/j.biocon.2013.04.018
   Kurten EL, 2015, ECOLOGY, V96, P1923, DOI 10.1890/14-1735.1
   Kurten EL, 2013, BIOL CONSERV, V163, P22, DOI 10.1016/j.biocon.2013.04.025
   Lanz B, 2018, ECOL ECON, V144, P260, DOI 10.1016/j.ecolecon.2017.07.018
   Lyra-Jorge MC, 2008, EUR J WILDLIFE RES, V54, P739, DOI 10.1007/s10344-008-0205-8
   Maack R, 2017, GEOGRAFA FISICA ESTA
   Magioli M, 2016, EUR J WILDLIFE RES, V62, P431, DOI 10.1007/s10344-016-1017-x
   Markl JS, 2012, CONSERV BIOL, V26, P1072, DOI 10.1111/j.1523-1739.2012.01927.x
   Melo FPL, 2013, TRENDS ECOL EVOL, V28, P462, DOI 10.1016/j.tree.2013.01.001
   Moro RS, 2007, TERR PLURAL, V1, P115
   Newbold T, 2015, NATURE, V520, P45, DOI 10.1038/nature14324
   Nja, 2012, J MATH RES, V4, P148, DOI [10.5539/jmr.v4n2p148, DOI 10.5539/JMR.V4N2P148]
   Nolte S, 2013, ESTUAR COAST SHELF S, V135, P296, DOI 10.1016/j.ecss.2013.10.026
   Nunez-Regueiro MM, 2015, BIOL CONSERV, V187, P19, DOI 10.1016/j.biocon.2015.04.001
   O'Farrill G, 2013, INTEGR ZOOL, V8, P4, DOI 10.1111/j.1749-4877.2012.00316.x
   Paglia A. P., 2012, CONS BIOL, V6, P1
   PARANA, 2010, LIST ESP MAM PERT FA
   Paviolo A, 2016, SCI REP-UK, V6, DOI 10.1038/srep37147
   Peel MC, 2007, HYDROL EARTH SYST SC, V11, P1633, DOI 10.5194/hess-11-1633-2007
   Pereira AD, 2020, STUD NEOTROP FAUNA E, V55, P139, DOI 10.1080/01650521.2019.1707419
   PERES CA, 1990, BIOL CONSERV, V54, P47, DOI 10.1016/0006-3207(90)90041-M
   Peres CA, 2016, P NATL ACAD SCI USA, V113, P892, DOI 10.1073/pnas.1516525113
   Polishchuk LV, 2010, EVOL ECOL RES, V12, P1
   Puttker T, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108368
   R Core Team, 2020, LANGUAGE ENV STAT CO
   REDFORD KH, 1992, BIOSCIENCE, V42, P412, DOI 10.2307/1311860
   Ribeiro MC, 2009, BIOL CONSERV, V142, P1141, DOI 10.1016/j.biocon.2009.02.021
   Rondinini C, 2011, PHILOS T R SOC B, V366, P2633, DOI 10.1098/rstb.2011.0113
   Schmidt BR, 2005, AQUAT CONSERV, V15, P681, DOI 10.1002/aqc.740
   Schrama M, 2013, OECOLOGIA, V172, P231, DOI 10.1007/s00442-012-2484-8
   Srbek-Araujo AC, 2017, ORYX, V51, P246, DOI 10.1017/S0030605315001222
   Srbek-Araujo AC, 2013, BIOTA NEOTROP, V13, P51, DOI 10.1590/S1676-06032013000200005
   Stoner C, 2007, AFR J ECOL, V45, P202, DOI 10.1111/j.1365-2028.2006.00705.x
   Taylor RA, 2016, ECOLOGY, V97, P951, DOI 10.1890/15-0885.1
   TERBORGH J, 1988, CONSERV BIOL, V2, P402, DOI 10.1111/j.1523-1739.1988.tb00207.x
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Trigo TC, 2013, CURR BIOL, V23, P2528, DOI 10.1016/j.cub.2013.10.046
   Venables W.N., 2002, MODERN APPL STAT S P, VFourth, DOI [10.1007/978-0-387-21706-2, DOI 10.1007/978-0-387-21706-2]
   Weckel M, 2006, J ZOOL, V270, P25, DOI 10.1111/j.1469-7998.2006.00106.x
   WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269
   Wilkie DS, 2011, ANN NY ACAD SCI, V1223, P120, DOI 10.1111/j.1749-6632.2010.05908.x
   Wilson MC, 2016, LANDSCAPE ECOL, V31, P219, DOI 10.1007/s10980-015-0312-3
   Zimbres B, 2017, BIOL CONSERV, V206, P283, DOI 10.1016/j.biocon.2016.11.033
   Zuur Alain F., 2009, P1
NR 91
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0960-3115
EI 1572-9710
J9 BIODIVERS CONSERV
JI Biodivers. Conserv.
PD NOV
PY 2021
VL 30
IS 13
BP 4005
EP 4022
DI 10.1007/s10531-021-02288-3
EA SEP 2021
PG 18
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA WH5KB
UT WOS:000697616800002
DA 2022-02-10
ER

PT J
AU Whitworth, A
   Whittaker, L
   Huarcaya, RP
   Flatt, E
   Morales, ML
   Connor, D
   Priego, MG
   Forsyth, A
   Beirne, C
AF Whitworth, Andrew
   Whittaker, Lawrence
   Pillco Huarcaya, Ruthmery
   Flatt, Eleanor
   Morales, Marvin Lopez
   Connor, Danielle
   Priego, Marina Garrido
   Forsyth, Adrian
   Beirne, Chris
TI Spider Monkeys Rule the Roost: Ateline Sleeping Sites Influence
   Rainforest Heterogeneity
SO ANIMALS
LA English
DT Article
DE rainforest; wildlife; camera traps; Ateline; primates; biodiversity;
   seed dispersal; seed predation; trophic interactions
ID ATELES-GEOFFROYI-YUCATANENSIS; SEED DISPERSAL; CAMERA TRAPS; ACTIVITY
   RHYTHM; HOWLER MONKEYS; DUNG; PRIMATES; PATTERNS; MAMMALS; IMPACT
AB Simple Summary Spider monkeys are important dispersers of many hardwood trees that contribute greatly to the carbon sequestration of tropical forests. One way in which Spider monkeys influence tropical ecosystem structure and function is through the creation of visible terrestrial latrines beneath their "sleeping sites"-trees in which they frequently return to sleep. Spider monkey latrines are thought to create high quality resource patches for rainforest plants and other wildlife to exploit. We investigate this using camera traps placed in both the canopy and on the rainforest floor to determine which rainforest wildlife are attracted to the latrines beneath the sleeping sites of spider monkeys. We also assess the tree species and dung beetles found within the latrines compared with other areas of the forest. Our evidence suggests that spider monkey roosting sites are a hub of activity for other rainforest wildlife, and act as germinating beds for many rainforest trees. If rainforests were to lose spider monkeys, from intensive hunting for example, many other rainforest wildlife species would be affected, and forests would therefore be made up of different tree communities than landscapes where spider monkeys exist.
   Abstract The sleeping site behavior of Ateline primates has been of interest since the 1980s, yet limited focus has been given to their influence upon other rainforest species. Here, we use a combination of arboreal and terrestrial camera traps, and dung beetle pitfall traps, to characterize spider monkey sleeping site use and quantify the impact of their associated latrines on terrestrial vertebrate and dung beetle activity. We also characterize the physical characteristics of the sleeping sites and the floristic and soil composition of latrines beneath them. Spider monkey activity at sleeping sites peaked at dawn and dusk and group composition varied by sex of the adults detected. The habitat-use of terrestrial fauna (vertebrates and dung beetles) differed between latrine sites and non-latrine controls, underpinned by species-specific changes in the relative abundance of several seed-dispersing species (such as paca and great curassow). Seedling density was higher in latrines than in non-latrine controls. Although most soil properties were similar between latrines and controls, potassium and manganese concentrations were different. These results suggest that spider monkey sleeping site fidelity leads to a hotspot of ecological activity in latrines and downstream impacts on rainforest floristic composition and diversity.
C1 [Whitworth, Andrew; Whittaker, Lawrence; Pillco Huarcaya, Ruthmery; Flatt, Eleanor; Morales, Marvin Lopez; Priego, Marina Garrido; Forsyth, Adrian] Osa Conservat, Conservat Sci Team, Washington, DC 20005 USA.
   [Whitworth, Andrew] Univ Glasgow, Inst Biodivers Anim Hlth & Comparat Med, Coll Med Vet & Life Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Whittaker, Lawrence] Imperial Coll London, Div Biol, Silwood Pk Campus, Ascot SL5 7PY, Berks, England.
   [Connor, Danielle; Beirne, Chris] Univ Exeter, Sch Bio Sci, Ctr Ecol & Conservat, Penryn Campus, Penryn TR10 9FE, Cornwall, England.
RP Whitworth, A (corresponding author), Osa Conservat, Conservat Sci Team, Washington, DC 20005 USA.; Whitworth, A (corresponding author), Univ Glasgow, Inst Biodivers Anim Hlth & Comparat Med, Coll Med Vet & Life Sci, Glasgow G12 8QQ, Lanark, Scotland.
EM andy.w.whitworth@gmail.com; LawrenceWhittaker@hotmail.co.uk;
   ruthp@osaconservation.org; eleanorflatt@osaconservation.org;
   marvinlopez@osaconservation.org; daniconnor1995@gmail.com;
   marinagarrido@osaconservation.org; adrianforsyth@gmail.com;
   c.w.beirne@gmail.com
OI Whitworth, Andrew/0000-0001-6197-996X
FU Margot Marsh Biodiversity Fund
FX This field research was funded by the Margot Marsh Biodiversity Fund.
CR Urrea-Galeano LA, 2019, BIOTROPICA, V51, P186, DOI 10.1111/btp.12631
   AHUMADA JA, 1992, INT J PRIMATOL, V13, P33, DOI 10.1007/BF02547726
   Altrichter M, 2001, REV BIOL TROP, V49, P1183
   Anderson JR, 1998, AM J PRIMATOL, V46, P63, DOI 10.1002/(SICI)1098-2345(1998)46:1<63::AID-AJP5>3.3.CO;2-F
   Andresen E, 2004, OECOLOGIA, V139, P45, DOI 10.1007/s00442-003-1480-4
   AQUINO R, 1986, AM J PRIMATOL, V11, P319, DOI 10.1002/ajp.1350110403
   Bates D., 2007, MANUSCR U WIS, V15, P1
   Bello C, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1501105
   Bogoni JA, 2019, ECOGRAPHY, V42, P1803, DOI 10.1111/ecog.04670
   Bowler MT, 2017, REMOTE SENS ECOL CON, V3, P146, DOI 10.1002/rse2.35
   Boza M.A., 1988, COSTA RICA NATL PARK
   Brodie JF, 2016, TRENDS ECOL EVOL, V31, P414, DOI 10.1016/j.tree.2016.03.019
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Busia L, 2017, ETHOLOGY, V123, P405, DOI 10.1111/eth.12609
   BUXTON AP, 1951, J ANIM ECOL, V20, P31, DOI 10.2307/1642
   Camargo-Sanabria AA, 2016, ACTA OECOL, V73, P45, DOI 10.1016/j.actao.2016.02.005
   Ceballos G, 2017, P NATL ACAD SCI USA, V114, pE6089, DOI 10.1073/pnas.1704949114
   Chanthorn W, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-46399-y
   CHAPMAN CA, 1989, AM J PRIMATOL, V18, P53, DOI 10.1002/ajp.1350180106
   CHAPMAN CA, 1989, OECOLOGIA, V79, P506, DOI 10.1007/BF00378668
   Chapman CA, 2013, INT J PRIMATOL, V34, P1, DOI 10.1007/s10764-012-9645-9
   CLARKE KR, 1993, AUST J ECOL, V18, P117, DOI 10.1111/j.1442-9993.1993.tb00438.x
   Culot L, 2018, INT J PRIMATOL, V39, P397, DOI 10.1007/s10764-018-0041-y
   Dew J.L., 2008, SPIDER MONKEYS BEHAV
   Dos Santos Neves N, 2010, AUSTRAL ECOL, V35, P549, DOI 10.1111/j.1442-9993.2009.02066.x
   Droscher I, 2014, BEHAV ECOL SOCIOBIOL, V68, P2043, DOI 10.1007/s00265-014-1810-z
   Fan PF, 2008, AM J PRIMATOL, V70, P153, DOI 10.1002/ajp.20470
   Feeley K, 2005, J TROP ECOL, V21, P99, DOI 10.1017/S0266467404001701
   Fragoso JMV, 2003, ECOLOGY, V84, P1998, DOI 10.1890/01-0621
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Gonzalez-Zamora A, 2009, AM J PRIMATOL, V71, P8, DOI [10.1002/ajp.20625, 10.1002/ajp.20753]
   Gonzalez-Zamora A, 2015, J TROP ECOL, V31, P305, DOI 10.1017/S026646741500022X
   Gonzalez-Zamora A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0089346
   Gonzalez-Zamora A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0046852
   Gregory T, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04112-x
   Griffiths HM, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.1634
   HARTSHORN G S, 1983, P423
   Hartwell KS, 2014, INT J PRIMATOL, V35, P425, DOI 10.1007/s10764-013-9746-0
   Herwitz S.R., 1981, REGENERATION SELECTE, V24
   Holdridge L. R., 1967, Life zone ecology.
   HOWDEN H F, 1975, Biotropica, V7, P77, DOI 10.2307/2989750
   Irwin MT, 2004, J MAMMAL, V85, P420, DOI 10.1644/1545-1542(2004)085<0420:LLOOLB>2.0.CO;2
   Lambert Joanna E., 2005, P137, DOI 10.1079/9780851998060.0137
   Larsen TH, 2005, BIOTROPICA, V37, P322, DOI 10.1111/j.1744-7429.2005.00042.x
   Lawson CR, 2012, BIOTROPICA, V44, P271, DOI 10.1111/j.1744-7429.2012.00871.x
   LAWTON JH, 1995, LINKING SPECIES & ECOSYSTEMS, P141
   Levi T, 2009, J APPL ECOL, V46, P804, DOI 10.1111/j.1365-2664.2009.01661.x
   Link A, 2006, J TROP ECOL, V22, P235, DOI 10.1017/S0266467405003081
   Jose-Dominguez JM, 2015, INT J PRIMATOL, V36, P948, DOI 10.1007/s10764-015-9865-x
   Marsh CJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073147
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Munoz-Delgado J, 2004, PHYSIOL BEHAV, V83, P107, DOI 10.1016/j.physbeh.2004.07.015
   Munoz-Delgado J, 2014, CHRONOBIOL INT, V31, P983, DOI 10.3109/07420528.2014.938813
   Nichols E, 2008, BIOL CONSERV, V141, P1461, DOI 10.1016/j.biocon.2008.04.011
   Oksanen J., 2007, VEGAN PACK COMMUN EC, V10, P631
   Oliveira-Santos LGR, 2008, J TROP ECOL, V24, P563, DOI 10.1017/S0266467408005324
   Bravo SP, 2012, ECOL RES, V27, P311, DOI 10.1007/s11284-011-0904-6
   Peres CA, 2016, P NATL ACAD SCI USA, V113, P892, DOI 10.1073/pnas.1516525113
   Huarcaya RP, 2020, ORYX, V54, P901, DOI 10.1017/S0030605318001096
   Pouvelle S, 2009, J TROP ECOL, V25, P239, DOI 10.1017/S0266467409005987
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Russo SE, 2004, ECOL LETT, V7, P1058, DOI 10.1111/j.1461-0248.2004.00668.x
   Scherbaum C, 2013, CURR ZOOL, V59, P125, DOI 10.1093/czoolo/59.1.125
   Spaan D, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3020034
   Tan CL, 2013, PRIMATES, V54, P1, DOI 10.1007/s10329-012-0318-2
   Velazquez-Vazquez G, 2015, INT J PRIMATOL, V36, P1154, DOI 10.1007/s10764-015-9883-8
   Wallace R.B., 2010, SPIDER MONKEYS, P138
   Wallace Robert B., 2001, Neotropical Primates, V9, P101
   Watmough SA, 2007, APPL GEOCHEM, V22, P1241, DOI 10.1016/j.apgeochem.2007.03.039
   Weghorst JA, 2007, PRIMATES, V48, P108, DOI 10.1007/s10329-006-0025-y
   Whitworth A, 2019, DIVERS DISTRIB, V25, P1166, DOI 10.1111/ddi.12930
   Whitworth A, 2018, CONDOR, V120, P852, DOI 10.1650/CONDOR-18-57.1
   Whitworth A, 2016, TROP CONSERV SCI, V9, P675, DOI 10.1177/194008291600900208
   Wright SJ, 2011, ECOLOGY, V92, P1616, DOI 10.1890/10-1558.1
NR 75
TC 1
Z9 2
U1 5
U2 11
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD DEC
PY 2019
VL 9
IS 12
AR 1052
DI 10.3390/ani9121052
PG 16
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Veterinary Sciences; Zoology
GA KB6XV
UT WOS:000506636400051
PM 31805694
OA gold, Green Published, Green Accepted
DA 2022-02-10
ER

PT J
AU Radig, B
   Bodesheim, P
   Korsch, D
   Denzler, J
   Haucke, T
   Klasen, M
   Steinhage, V
AF Radig, Bernd
   Bodesheim, Paul
   Korsch, Dimitri
   Denzler, Joachim
   Haucke, Timm
   Klasen, Morris
   Steinhage, Volker
TI Automated Visual Large Scale Monitoring of Faunal Biodiversity
SO PATTERN RECOGNITION AND IMAGE ANALYSIS
LA English
DT Article
DE biodiversity monitoring; species classification; animal detection;
   sensor networks; camera trapping with color and depth sensors; motion
   estimation in color and depth videos; fine-grained recognition;
   multi-object tracking; depth estimation; deep learning; novelty
   detection; anomaly detection; incremental learning; lifelong learning;
   active learning
AB To observe biodiversity, the variety of plant and animal life in the world or in a particular habitat, human observers make the most common examinations, often assisted by technical equipment. Measuring objectively the number of different species of animals, plants, fungi, and microbes that make up the ecosystem can be difficult. In order to monitor changes in biodiversity, data have to be compared across space and time. Cameras are an essential sensor to determine the species range, abundance, and behavior of animals. The millions of recordings from camera traps set up in natural environments can no longer be analyzed by biologists. We started research on doing this analysis automatically without human interaction. The focus of our present sensor is on image capture of wildlife and moths. Special hardware elements for the detection of different species are designed, implemented, tested, and improved, as well as the algorithms for classification and counting of samples from images and image sequences, e.g., to calculate presence, absence, and abundance values or the duration of characteristic activities related to the spatial mobilities. For this purpose, we are developing stereo camera traps that allow spatial reconstruction of the observed animals. This allows three-dimensional coordinates to be recorded and the shape to be characterized. With this additional feature data, species identification and movement detection are facilitated. To classify and count moths, they are attracted to an illuminated screen, which is then photographed at intervals by a high-resolution color camera. To greatly reduce the volume of data, redundant elements and elements that are consistent from image to image are eliminated. All design decisions take into account that at remote sites and in fully autonomous operation, power supply on the one hand and possibilities for data exchange with central servers on the other hand are limited. Installation at hard-to-reach locations requires a sophisticated and demanding system design with an optimal balance between power requirements, bandwidth for data transmission, required service and operation in all environmental conditions for at least ten years.
C1 [Radig, Bernd] Tech Univ Munich, Fac Informat, D-80290 Munich, Germany.
   [Bodesheim, Paul; Korsch, Dimitri; Denzler, Joachim] Friedrich Schiller Univ Jena, Comp Vis Grp, D-07737 Jena, Germany.
   [Haucke, Timm; Klasen, Morris; Steinhage, Volker] Univ Bonn, Inst Informat, D-53115 Bonn, Germany.
RP Radig, B (corresponding author), Tech Univ Munich, Fac Informat, D-80290 Munich, Germany.
EM radig@in.tum.de; paul.bodesheim@uni-jena.de; dimitri.korsch@uni-jena.de;
   joachim.denzler@uni-jena.de; haucke@cs.uni-bonn.de;
   klasen@cs.uni-bonn.de; steinhag@cs.uni-bonn.de
FU German Federal Ministry of Education and Research (Bundesministerium fur
   Bildung und Forschung (BMBF), Bonn, Germany)Federal Ministry of
   Education & Research (BMBF) [FKZ 01LC1903B]
FX This work was partially done within the project "Automated Multisensor
   Station for Monitoring of Species Diversity" (AMMOD), which is funded by
   the German Federal Ministry of Education and Research (Bundesministerium
   fur Bildung und Forschung (BMBF), Bonn, Germany (FKZ 01LC1903B).
CR Apps PJ, 2018, AFR J ECOL, V56, P702, DOI 10.1111/aje.12563
   Bodesheim P, 2015, IEEE WINT CONF APPL, P813, DOI 10.1109/WACV.2015.113
   Bodesheim P, 2013, PROC CVPR IEEE, P3374, DOI 10.1109/CVPR.2013.433
   Bohlke J, 2021, VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS - VOL. 5: VISAPP, P466, DOI 10.5220/0010244704660477
   Brehm G, 2021, INSECT CONSERV DIVER, V14, P188, DOI 10.1111/icad.12476
   Brehm G, 2017, NOTA LEPIDOPTEROLOGI, V40, P87, DOI 10.3897/nl.40.11887
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Chambert T, 2015, ECOLOGY, V96, P332, DOI 10.1890/14-1507.1
   Chandler RB, 2014, METHODS ECOL EVOL, V5, P1351, DOI 10.1111/2041-210X.12153
   Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Follmann P., 2018, Pattern Recognition and Image Analysis, V28, P605, DOI 10.1134/S1054661818040107
   Haase D, 2014, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2014.185
   Haucke T., ARXIV210205607
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He K, 2017, P IEEE C COMPUTER VI, P2961, DOI DOI 10.1109/ICCV.2017.322
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Kalan AK, 2015, ECOL INDIC, V54, P217, DOI 10.1016/j.ecolind.2015.02.023
   Keselman Leonid, 2017, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPRW.2017.167.
   Korsch D, 2019, LECT NOTES COMPUT SC, V11824, P62, DOI 10.1007/978-3-030-33676-9_5
   Korsch D, 2018, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE (ICPRAI 2018), P627, DOI 10.1134/S105466181804020X
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lin T. Y., 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.106
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Radig B., 2018, INT C PATTERN RECOGN, P591
   Rodner E., 2015, P JAHR DTSCH GES MED
   Rodner E, 2011, PATTERN RECOGN LETT, V32, P244, DOI 10.1016/j.patrec.2010.08.009
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Simon M, 2015, LECT NOTES COMPUT SC, V9004, P162, DOI 10.1007/978-3-319-16808-1_12
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Yang GS, 2020, PROC CVPR IEEE, P1331, DOI 10.1109/CVPR42600.2020.00141
   Zhou K, 2016, DESTECH TRANS COMP
NR 34
TC 0
Z9 0
U1 2
U2 2
PU SPRINGERNATURE
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1054-6618
EI 1555-6212
J9 PATTERN RECOGN IMAGE
JI Pattern Recogn. Image Anal.
PD JUL
PY 2021
VL 31
IS 3
BP 477
EP 488
DI 10.1134/S1054661821030214
PG 12
WC Computer Science, Interdisciplinary Applications
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA US8JJ
UT WOS:000697671200012
DA 2022-02-10
ER

PT J
AU Lucherini, M
   Reppucci, JI
   Walker, RS
   Villalba, ML
   Wurstten, A
   Gallardo, G
   Iriarte, A
   Villalobos, R
   Perovic, P
AF Lucherini, Mauro
   Reppucci, Juan I.
   Walker, R. Susan
   Villalba, M. Lilian
   Wurstten, Alvaro
   Gallardo, Giovana
   Iriarte, Agustin
   Villalobos, Rodrigo
   Perovic, Pablo
TI ACTIVITY PATTERN SEGREGATION OF CARNIVORES IN THE HIGH ANDES
SO JOURNAL OF MAMMALOGY
LA English
DT Article
DE camera trapping; intraguild competition; Lagidium viscacia; Lycalopex
   culpaeus; Leopardus colocolo; Leopardus jacobita; Puma concolor; South
   America
ID FOREST; CULPAEUS; DENSITY; CAT
AB Intraguild competition may be reduced if ecologically similar species segregate temporally. Using data from 1,596 camera-trap photos, we present the 1st quantitative analyses of the activity patterns of Andean cats (Leopardus jacobita), Pampas cats (Leopardus colocolo), culpeos (Lycalopex culpaeus), and pumas (Pinna concolor) in high-altitude deserts of the Andes. We compared daily activity patterns for these carnivores with those of mountain vizcachas (Lagidium viscacia), the main prey of Andean cats. Activity patterns of all species were positively skewed toward night. Pampas cats displayed the greatest proportion of nocturnal activity, whereas Andean cats were the most diurnal. Activity of Andean cats differed significantly only from that of Pampas cats; Pampas cats also differed from pumas. Activity of Andean cats was generally similar to that of mountain vizcachas. The dissimilar activity patterns of Andean and Pampas cats support the hypothesis of temporal niche segregation of these felids.
C1 [Lucherini, Mauro; Reppucci, Juan I.] Univ Nacl Invest Cient & Tecn, CONICET, Dept Biol Bioquim & Farm, GECM Catedra Fisiol Anim, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   [Walker, R. Susan; Wurstten, Alvaro] Wildlife Conservat Soc, Neuquen, Argentina.
   [Villalba, M. Lilian] Colecc Boliviana Fauna, La Paz, Bolivia.
   [Gallardo, Giovana] BIOTA, Ctr Estudios Biol Teor & Aplicada, La Paz, Bolivia.
   [Iriarte, Agustin; Villalobos, Rodrigo] Pontificia Univ Catolica Chile, CASEB, Fdn Biodiversitas, Santiago, Chile.
RP Lucherini, M (corresponding author), Univ Nacl Invest Cient & Tecn, CONICET, Dept Biol Bioquim & Farm, GECM Catedra Fisiol Anim, San Juan 670, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
EM lucherinima@yahoo.com
RI Lucherini, Mauro/D-9668-2016
OI Lucherini, Mauro/0000-0002-9695-3943
FU Wildlife Conservation Network; Wild About Cats; Cat Action Treasury;
   Cleveland Zoological Society; Rufford Foundation; International Society
   for Endangered Cats Canada; La Torbiera Zoological Society;
   CONICETConsejo Nacional de Investigaciones Cientificas y Tecnicas
   (CONICET)
FX We are grateful to E. Delgado, P. Cuello, E. Luengos, C. Tellaeche, and
   all the volunteers and villagers who contributed to data collection, as
   well as to E. Casanave (Universidad Nacional del Sur), WildCRU`
   (University of Oxford), and the other members of the Andean Cat Alliance
   for providing advice. Two anonymous referees and E. Lacey greatly
   improved a previous version of this paper. Our project received support
   from Wildlife Conservation Network, Darwin Initiative, BP Conservation
   Programme, Whitley Fund for Nature, Wild About Cats, Cat Action
   Treasury, Cleveland Zoological Society, Rufford Foundation,
   International Society for Endangered Cats Canada, and La Torbiera
   Zoological Society. We acknowledge all the governmental institutions
   that endorsed and authorized our project, and Administracion de Parques
   Nacionales Argentinos, Reserva Nacional de Fauna Andina Eduardo Avaroa,
   Coleccion Boliviana de Fauna for their support. JIR was supported by
   doctoral scholarships from CONICET.
CR Bunnell F.L., 1990, Current Mammalogy, V2, P245
   CAJAL JL, 1998, BASES CONSERVACION M, P175
   Cuellar E, 2006, STUD NEOTROP FAUNA E, V41, P169, DOI 10.1080/01650520600840001
   Di Bitetti MS, 2006, J ZOOL, V270, P153, DOI 10.1111/j.1469-7998.2006.00102.x
   DIBITETTI MS, 2009, J ZOOLOGY LONDON
   Donadio E, 2006, AM NAT, V167, P524, DOI 10.1086/501033
   Fedriani JM, 1999, OECOLOGIA, V121, P138, DOI 10.1007/s004420050915
   Grassman LI, 2006, MAMMALIA, V70, P306, DOI 10.1515/MAMM.2006.048
   Hwang MH, 2007, J ZOOL, V271, P203, DOI 10.1111/j.1469-7998.2006.00203.x
   Jimenez JE, 1996, REV CHIL HIST NAT, V69, P113
   JOHNSON WE, 1994, CAN J ZOOL, V72, P1788, DOI 10.1139/z94-242
   Karanth K. Ullas, 2004, P229
   Kauhala K, 2007, MAMM BIOL, V72, P342, DOI 10.1016/j.mambio.2006.10.006
   Krebs CJ., 1998, ECOLOGICAL METHODOLO
   Kronfeld-Schor N, 2003, ANNU REV ECOL EVOL S, V34, P153, DOI 10.1146/annurev.ecolsys.34.011802.132435
   KUCERA TE, 1993, WILDLIFE SOC B, V21, P505
   Linnell John D. C., 2000, Diversity and Distributions, V6, P169, DOI 10.1046/j.1472-4642.2000.00069.x
   LUCHERINI M, 2003, ENDANGERED SPECIES U, V2, P211
   Lucherini M, 2008, MT RES DEV, V28, P81, DOI 10.1659/mrd.0903
   Lucherini M, 2008, MAMMALIA, V72, P95, DOI 10.1515/MAMM.2008.018
   Lucherini Mauro, 2008, Cat News, V49, P29
   Maffei L, 2004, J ZOOL, V262, P295, DOI 10.1017/S0952836903004655
   Napolitano C, 2008, MOL ECOL, V17, P678, DOI 10.1111/j.1365-294X.2007.03606.x
   NIELSEN ET, 1984, BEHAVIOUR, V89, P147, DOI 10.1163/156853984X00083
   Nowell K., 1996, WILD CATS STATUS SUR
   O'Donoghue M, 1998, OIKOS, V82, P169, DOI 10.2307/3546927
   ODUM EP, 1966, ECOLOGIA
   OFARRELL MJ, 1974, J MAMMAL, V55, P809, DOI 10.2307/1379409
   Palomares F, 1999, AM NAT, V153, P492, DOI 10.1086/303189
   Patterson BR, 1999, CAN FIELD NAT, V113, P251
   Perovic P, 2003, ORYX, V37, P374, DOI 10.1017/S0030605303000644
   PEROVIC P, 1998, BASES CONSERVACION M, P182
   Redford KH, 1992, MAMMALS NEOTROPICS C
   Rogowitz GL, 1997, J MAMMAL, V78, P1172, DOI 10.2307/1383060
   Salvatori V, 1999, J MAMMAL, V80, P980, DOI 10.2307/1383268
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Silveira L, 2005, PAMPAS CAT ECOLOGY C
   SINANI MEV, 2008, THESIS U MAYOR SAN A
   Theuerkauf J, 2003, J MAMMAL, V84, P243, DOI 10.1644/1545-1542(2003)084<0243:DPADOW>2.0.CO;2
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Villalba L, 2004, ANDEAN CAT CONSERVAT
   Walker RS, 2007, LANDSCAPE ECOL, V22, P1303, DOI 10.1007/s10980-007-9118-2
   Walker RS, 2007, J MAMMAL, V88, P519, DOI 10.1644/06-MAMM-A-172R.1
NR 43
TC 88
Z9 100
U1 5
U2 79
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 0022-2372
EI 1545-1542
J9 J MAMMAL
JI J. Mammal.
PD DEC
PY 2009
VL 90
IS 6
BP 1404
EP 1409
DI 10.1644/09-MAMM-A-002R.1
PG 6
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA 536YT
UT WOS:000273080600015
OA Green Published
DA 2022-02-10
ER

PT J
AU Allen, ML
   Wojcik, B
   Evans, BE
   Iehl, EE
   Barker, RE
   Wheeler, ME
   Peterson, BE
   Dohm, RI
   Mueller, MA
   Olson, LO
   Ederer, B
   Stewart, M
   Crimmins, S
   Pemble, K
   Van Stappen, J
   Olson, E
   Van Deelen, TR
AF Allen, Maximilian L.
   Wojcik, Beth
   Evans, Bryn E.
   Iehl, Emily E.
   Barker, Rachel E.
   Wheeler, Michael E.
   Peterson, Brittany E.
   Dohm, Regan I.
   Mueller, Marcus A.
   Olson, Lucas O.
   Ederer, Brittany
   Stewart, Margaret
   Crimmins, Shawn
   Pemble, Ken
   Van Stappen, Julie
   Olson, Erik
   Van Deelen, Timothy R.
TI Detection of Endangered American Martens (Martes americana) in Apostle
   Islands National Lakeshore, Wisconsin
SO AMERICAN MIDLAND NATURALIST
LA English
DT Article
ID CARNIVORES; MARKING
AB American martens (Martes americana) warrant concern in Wisconsin, U.S.A., for multiple reasons, including being the state's only endangered mammal and a clan animal of the Ojibwe tribes. American martens were once present throughout much of the state but were extirpated in the early 20th century through habitat loss and unregulated trapping. In the 1950s two reintroductions of martens to Stockton island of the Apostle Islands archipelago were considered failures, with the last confirmed sighting in the archipelago in 1969. In the decades since the Stockton Island reintroduction efforts; anecdotal reports of martens have surfaced throughout the archipelago. In 2014-2016 we deployed 91 camera traps on 13 of the 21 Apostle Islands to survey the archipelago's extant carnivore species. We detected American martens at 28 of 87 functioning camera trap sites on 5 of 13 monitored islands and documented the existence of American martens in APIS in Wisconsin for the first time its over 50 y. We suggest continued research to evaluate the status of the APIS population and its potential origins to guide future conservation efforts.
C1 [Allen, Maximilian L.; Wojcik, Beth; Evans, Bryn E.; Iehl, Emily E.; Barker, Rachel E.; Wheeler, Michael E.; Peterson, Brittany E.; Dohm, Regan I.; Mueller, Marcus A.; Olson, Lucas O.; Ederer, Brittany; Stewart, Margaret; Van Deelen, Timothy R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Crimmins, Shawn] Univ Wisconsin, Coll Nat Resources, 800 Reserve St, Stevens Point, WI 54481 USA.
   [Pemble, Ken; Van Stappen, Julie] Apostle Isl Natl Lakeshore, Planning & Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
   [Olson, Erik] Northland Coll, Nat Resources, 1411 Ellis Ave S, Ashland, WI 54806 USA.
RP Allen, ML (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM maximilian.allen@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU United States Department of the Interior (USDI) [P14AC01180]; Apostle
   Islands National Lakeshore [P14AC01180]; Great Lakes Northern Forest
   CESU [P14AC01180]; Northland College (Department of Natural Resources,
   Sigurd Olson Professorship in the Natural Sciences, Morris O. Ristvedt
   Professorship in the Natural Sciences); University of Wisconsin
   (Schorger fund); University of Wisconsin (Department of Forest and
   Wildlife Ecology); University of Wisconsin (Beers-Bascom Professorship
   in Conservation, College of Agriculture and Life Sciences)
FX This project was supported by the United States Department of the
   Interior (USDI), Apostle Islands National Lakeshore and the Great Lakes
   Northern Forest CESU (Cooperative Agreement P14AC01180), Northland
   College (Department of Natural Resources, Sigurd Olson Professorship in
   the Natural Sciences, Morris O. Ristvedt Professorship in the Natural
   Sciences) and the University of Wisconsin (Schorger fund, Department of
   Forest and Wildlife Ecology; Beers-Bascom Professorship in Conservation,
   College of Agriculture and Life Sciences). We thank the personnel from
   each group that contributed to this project, especially the numerous
   technicians from the National Park Service, and students from Northland
   College that provided assistance over the course of the project.
CR ALLEN M. L., 2017, SURVEY TECHNIQUES CO
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   BECANT J. L, 2002, NAT AREA J, V22, P180
   BiEscit J. C., 2008, PEOPLE PLACES HUMAN
   BRANDER R. B., 1978, APOSTLE ISLANDS NATL
   Buskirk S. W., 1994, SCI BASIS CONSERVING, P7
   Carlson JE, 2014, J WILDLIFE MANAGE, V78, P1499, DOI 10.1002/jwmg.785
   CRAVEN S R, 1987, Colonial Waterbirds, V10, P64, DOI 10.2307/1521232
   Harmsen BJ, 2010, J MAMMAL, V91, P1225, DOI 10.1644/09-MAMM-A-416.1
   Jackson H. H. T., 1961, MAMMALS OF WISCONSIN
   JoiwirAt H. C., 1956, MARTEN RELEASE STOCK
   Kohn B.E., 1987, 143 WISC DEP NAT RES
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   MAcARTHUR R. H, 1967, THEORY ISLAND BIOGEO, P206
   Manlick PJ, 2017, CONSERV LETT, V10, P178, DOI 10.1111/conl.12257
   Naing H, 2015, RAFFLES B ZOOL, V63, P376
   O'COSISI A. F., 2011, CAMERA TRAPS ANIMAL
   SNIFF U, 2015, GLKN200503
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilson EO, 2010, THEORY OF ISLAND BIOGEOGRAPHY REVISITED, P1
   WOODEOFD J. E., 2011, CONSERVATION MANAGEM
   Woodford JE, 2013, WILDLIFE SOC B, V37, P616, DOI 10.1002/wsb.291
NR 23
TC 2
Z9 2
U1 1
U2 24
PU AMER MIDLAND NATURALIST
PI NOTRE DAME
PA UNIV NOTRE DAME, BOX 369, ROOM 295 GLSC, NOTRE DAME, IN 46556 USA
SN 0003-0031
EI 1938-4238
J9 AM MIDL NAT
JI Am. Midl. Nat.
PD APR
PY 2018
VL 179
IS 2
BP 294
EP 298
PG 5
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA GB5WA
UT WOS:000429137400010
DA 2022-02-10
ER

PT J
AU Wang, BX
   Rocha, DG
   Abrahams, MI
   Antunes, AP
   Costa, HCM
   Goncalves, ALS
   Spironello, WR
   de Paula, MJ
   Peres, CA
   Pezzuti, J
   Ramalho, E
   Reis, ML
   Carvalho, E
   Rohe, F
   Macdonald, DW
   Tan, CKW
AF Wang, Bingxin
   Rocha, Daniel G.
   Abrahams, Mark I.
   Antunes, Andre P.
   Costa, Hugo C. M.
   Sousa Goncalves, Andre Luis
   Spironello, Wilson Roberto
   de Paula, Milton Jose
   Peres, Carlos A.
   Pezzuti, Juarez
   Ramalho, Emiliano
   Reis, Marcelo Lima
   Carvalho, Elildo, Jr.
   Rohe, Fabio
   Macdonald, David W.
   Tan, Cedric Kai Wei
TI Habitat use of the ocelot (Leopardus pardalis) in Brazilian Amazon
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE Brazilian Amazon; camera traps; mesopredator; occupancy; ocelot;
   restricted spatial regression
ID ATLANTIC FOREST; NEOFELIS-NEBULOSA; ACTIVITY PATTERNS; TEMPORAL
   ACTIVITY; TROPICAL FORESTS; PROTECTED AREAS; FELIS-PARDALIS;
   RAIN-FOREST; OCCUPANCY; CONSERVATION
AB Amazonia forest plays a major role in providing ecosystem services for human and sanctuaries for wildlife. However, ongoing deforestation and habitat fragmentation in the Brazilian Amazon has threatened both. The ocelot is an ecologically important mesopredator and a potential conservation ambassador species, yet there are no previous studies on its habitat preference and spatial patterns in this biome. From 2010 to 2017, twelve sites were surveyed, totaling 899 camera trap stations, the largest known dataset for this species. Using occupancy modeling incorporating spatial autocorrelation, we assessed habitat use for ocelot populations across the Brazilian Amazon. Our results revealed a positive sigmoidal correlation between remote-sensing derived metrics of forest cover, disjunct core area density, elevation, distance to roads, distance to settlements and habitat use, and that habitat use by ocelots was negatively associated with slope and distance to river/lake. These findings shed light on the regional scale habitat use of ocelots and indicate important species-habitat relationships, thus providing valuable information for conservation management and land-use planning.
C1 [Wang, Bingxin; Macdonald, David W.; Tan, Cedric Kai Wei] Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney, Oxon, England.
   [Wang, Bingxin] Chinese Acad Sci, Inst Bot, State Key Lab Vegetat & Environm Change, Beijing, Peoples R China.
   [Wang, Bingxin] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Rocha, Daniel G.] Univ Calif Davis, Dept Wildlife Fish & Conservat Biol, Grad Grp Ecol, Davis, CA 95616 USA.
   [Rocha, Daniel G.] Inst Dev Sustentavel Mamiraua, Grp Ecol & Conservacao Felinos Amazonia, Tefe, Brazil.
   [Abrahams, Mark I.] Bristol Zool Soc, Field Conservat & Sci Dept, Bristol, Avon, England.
   [Antunes, Andre P.] Redefauna Rede Pesquisa Biodiversidade Conservaca, Manaus, Amazonas, Brazil.
   [Costa, Hugo C. M.] Univ Estadual Santa Cruz, Programa Posgrad Ecol & Conservacao Biodiversidad, Ilheus, Brazil.
   [Sousa Goncalves, Andre Luis; Spironello, Wilson Roberto] Inst Nacl de Pesquisas da Amazonia, Grp Pesquisa Mamiferos Amazon, Manaus, Amazonas, Brazil.
   [de Paula, Milton Jose; Pezzuti, Juarez] Univ Para, Ctr Adv Amazon Studies, Altamira, Brazil.
   [de Paula, Milton Jose] Univ Fed Para, Programa Posgrad Ecol, Belem, Para, Brazil.
   [de Paula, Milton Jose] EMBRAPA Amazonia Oriental, Belem, Para, Brazil.
   [Peres, Carlos A.] Univ East Anglia, Cetre Ecol Evolut & Conservat, Sch Environm Sci, Norwich, Norfolk, England.
   [Ramalho, Emiliano] Inst Dev Sustentavel Mamiraua, Tefe, Brazil.
   [Reis, Marcelo Lima] ICMBio CNPq, Brasilia, DF, Brazil.
   [Carvalho, Elildo, Jr.] Inst Chico Mendes Conservacao Biodiversidade, Ctr Nacl Pesquisa & Conservacao Mamiferos Carnivo, Atibaia, Brazil.
   [Carvalho, Elildo, Jr.] Norwegian Univ Life Sci, Fac Ecol & Nat Resource Management, As, Norway.
   [Rohe, Fabio] INPA, Programa Posgrad Genet Conservacao & Biol Evolut, Manaus, Amazonas, Brazil.
   [Rohe, Fabio] Wildlife Conservat Soc Brazil, Amazon Program, Manaus, Amazonas, Brazil.
RP Tan, CKW (corresponding author), Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney, Oxon, England.
EM cedric.tan@zoo.ox.ac.uk
RI Peres, Carlos A./ABE-8361-2020; Carvalho, Elildo/R-8556-2019; Peres,
   Carlos A./B-1276-2013; Peres, Carlos Augusto/N-8275-2019; Pezzuti,
   Juarez/X-4061-2019; Antunes, Andre/Q-3236-2018
OI Carvalho, Elildo/0000-0003-4356-2954; Peres, Carlos
   A./0000-0002-1588-8765; Peres, Carlos Augusto/0000-0002-1588-8765;
   abrahams, mark/0000-0002-6424-187X; Tan, Cedric/0000-0001-6505-2467;
   Wang, Bingxin/0000-0002-7021-8625; Gomes da Rocha,
   Daniel/0000-0002-0100-3102; Antunes, Andre/0000-0003-4404-3486
FU CAPES/Doutorado Pleno no exterior [88881.128140/2016-01];
   Recanati-Kaplan Foundation; Tang Family Foundation; Gordon & Betty Moore
   FoundationGordon and Betty Moore Foundation; Instituto Nacional de
   Pesquisas da Amazonia; Programa Areas Protegidas da Amazonia
FX CAPES/Doutorado Pleno no exterior, Grant/Award Number:
   88881.128140/2016-01; Recanati-Kaplan Foundation; Tang Family
   Foundation; Gordon & Betty Moore Foundation; Instituto Nacional de
   Pesquisas da Amazonia; Programa Areas Protegidas da Amazonia
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Antunes AP, 2016, SCI ADV, V2, DOI 10.1126/sciadv.1600936
   Barbieri MM, 2004, ANN STAT, V32, P870, DOI 10.1214/009053604000000238
   Barto K, 2013, MUMIN MULTIMODEL INF
   Blake JG, 2016, J MAMMAL, V97, P455, DOI 10.1093/jmammal/gyv190
   Burnham KP, 2004, SOCIOL METHOD RES, V33, P261, DOI 10.1177/0049124104268644
   Costa HCM, 2018, PEERJ, V6, DOI 10.7717/peerj.5058
   Cove M. V., 2014, HYSTRIX, V25, P5049
   da Rocha DG, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154624
   de Oliveira Tadeu G., 2010, P559
   Di Bitetti MS, 2006, J ZOOL, V270, P153, DOI 10.1111/j.1469-7998.2006.00102.x
   Di Bitetti MS, 2008, J TROP ECOL, V24, P189, DOI 10.1017/S0266467408004847
   Di Bitetti MS, 2013, MAMM BIOL, V78, P21, DOI 10.1016/j.mambio.2012.08.006
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   DiMiceli C. M., 2011, ANN GLOBAL AUTOMATED, V65, P2000
   Droz M, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.051909
   Emmons L. H., 1998, ENVIRON CONSERV, V25, P175
   EMMONS LH, 1988, REV ECOL-TERRE VIE, V43, P133
   EMMONS LH, 1987, BEHAV ECOL SOCIOBIOL, V20, P271, DOI 10.1007/BF00292180
   Cuyckens GAE, 2014, ENDANGER SPECIES RES, V26, P167, DOI 10.3354/esr00640
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Geary WL, 2018, J APPL ECOL, V55, P1594, DOI 10.1111/1365-2664.13125
   Gibson L, 2011, NATURE, V478, P378, DOI 10.1038/nature10425
   Goncalves A. L. S., 2013, COMPOSITION OCCURREN
   Gonzalez--Borrajo N, 2017, MAMMAL REV, V47, P62, DOI 10.1111/mam.12081
   Grace JB, 2012, ECOSPHERE, V3, DOI 10.1890/ES12-00048.1
   Grassman LI, 2005, J MAMMAL, V86, P29, DOI 10.1644/1545-1542(2005)086&lt;0029:EOTSFI&gt;2.0.CO;2
   Haddad NM, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1500052
   Haidir Iding A., 2013, Cat News, V59, P7
   Haines AM, 2006, EUR J WILDLIFE RES, V52, P216, DOI 10.1007/s10344-006-0043-5
   Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1215904, 10.1126/science.1244693]
   Hearn AJ, 2019, ORYX, V53, P643, DOI 10.1017/S0030605317001065
   Hines JE, 2010, ECOL APPL, V20, P1456, DOI 10.1890/09-0321.1
   Hines JE., 2006, PRESENCE SOFTWARE ES
   Hodges JS, 2010, AM STAT, V64, P325, DOI 10.1198/tast.2010.10052
   Houghton RA, 2001, GLOBAL CHANGE BIOL, V7, P731, DOI 10.1046/j.1365-2486.2001.00426.x
   Hughes J, 2013, J R STAT SOC B, V75, P139, DOI 10.1111/j.1467-9868.2012.01041.x
   IBGE IBDEGEE, 2011, SIN CENS DEM 2010, P261
   Johnson D. S., 2015, STOCC R PACKAGE FITT
   Johnson DS, 2013, ECOLOGY, V94, P801, DOI 10.1890/12-0564.1
   Kalies EL, 2012, ECOL APPL, V22, P204, DOI 10.1890/11-0758.1
   Kolowski JM, 2010, BIOL CONSERV, V143, P917, DOI 10.1016/j.biocon.2009.12.039
   Kottek M, 2006, METEOROL Z, V15, P259, DOI 10.1127/0941-2948/2006/0130
   Laurance WF, 1998, ECOLOGY, V79, P2032, DOI [10.1890/0012-9658(1998)079[2032:RFFATD]2.0.CO;2, 10.1890/05-0064]
   Macdonald EA, 2017, GLOB ECOL CONSERV, V12, P204, DOI 10.1016/j.gecco.2017.11.006
   MacKenzie DI, 2004, J AGR BIOL ENVIR ST, V9, P300, DOI 10.1198/108571104X3361
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   Malhi Y, 2014, ANNU REV ENV RESOUR, V39, P125, DOI 10.1146/annurev-environ-030713-155141
   Massara RL, 2018, BIOTROPICA, V50, P125, DOI 10.1111/btp.12481
   Massara RL, 2016, J MAMMAL, V97, P1634, DOI 10.1093/jmammal/gyw129
   Massara RL, 2018, MAMM BIOL, V92, P86, DOI 10.1016/j.mambio.2018.04.009
   Massara RL, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141333
   Mazerolle M.J., 2017, R PACKAGE VERS, V2, P1, DOI DOI 10.1515/pacres-2017-0001
   McGarigal K., 2002, FRAGSTATS V3 SPATIAL
   Murray Julie L., 1997, Mammalian Species, V548, P1
   Newbold T, 2015, NATURE, V520, P45, DOI 10.1038/nature14324
   Noss RF, 1996, CONSERV BIOL, V10, P949, DOI 10.1046/j.1523-1739.1996.10040949.x
   Nowell K., 1996, WILD CATS STATUS SUR
   Otis D. L., 1978, WILDLIFE MONOGR, V62, P3, DOI DOI 10.2307/3830650
   Paviolo A., 2015, LEOPARDUS PARDALIS E
   Penido G, 2016, BIOTA NEOTROP, V16, DOI 10.1590/1676-0611-BN-2016-0168
   Penjor U, 2018, ECOL EVOL, V8, P4278, DOI 10.1002/ece3.3970
   PERES CA, 1994, BIOTROPICA, V26, P285, DOI 10.2307/2388849
   Poley LG, 2014, J BIOGEOGR, V41, P122, DOI 10.1111/jbi.12200
   Prange S, 2004, CAN J ZOOL, V82, P1804, DOI 10.1139/Z04-179
   Pratas-Santiago LP, 2016, J ZOOL, V299, P275, DOI 10.1111/jzo.12359
   Prentice RC, 2016, WETLAND BOOK 2 DISTR, P1
   Prugh LR, 2009, BIOSCIENCE, V59, P779, DOI 10.1525/bio.2009.59.9.9
   QGIS Development Team, 2017, QGIS GEOGR INF SYST
   R Core Team, 2019, R LANGUAGE ENV STAT
   Razzaghi M, 2013, J MOD APPL STAT METH, V12, P164, DOI 10.22237/jmasm/1367381880
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Robertson AL, 2010, GLOBAL CHANGE BIOL, V16, P3193, DOI 10.1111/j.1365-2486.2010.02314.x
   Rota CT, 2016, METHODS ECOL EVOL, V7, P1164, DOI 10.1111/2041-210X.12587
   Rota CT, 2009, J APPL ECOL, V46, P1173, DOI 10.1111/j.1365-2664.2009.01734.x
   SHUKLA J, 1990, SCIENCE, V247, P1322, DOI 10.1126/science.247.4948.1322
   Smith N.J.H., 1976, Oryx, V13, P362
   Tan CKW, 2017, BIOL CONSERV, V206, P65, DOI 10.1016/j.biocon.2016.12.012
   USGS, 2003, STRM DOC
   Vargas LEP, 2016, BIODIVERS CONSERV, V25, P739, DOI 10.1007/s10531-016-1089-7
   Vasconcelos P. G. de A., 2017, African Journal of Agricultural Research, V12, P169
   Vetter D, 2011, ECOGRAPHY, V34, P1, DOI 10.1111/j.1600-0587.2010.06453.x
   Wang E, 2002, STUD NEOTROP FAUNA E, V37, P207, DOI 10.1076/snfe.37.3.207.8564
NR 84
TC 7
Z9 7
U1 7
U2 25
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD MAY
PY 2019
VL 9
IS 9
BP 5049
EP 5062
DI 10.1002/ece3.5005
PG 14
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA HX7KR
UT WOS:000467584200002
PM 31110661
OA Green Accepted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Reilly, ML
   Tobler, MW
   Sonderegger, DL
   Beier, P
AF Reilly, M. L.
   Tobler, M. W.
   Sonderegger, D. L.
   Beier, P.
TI Spatial and temporal response of wildlife to recreational activities in
   the San Francisco Bay ecoregion
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Outdoor recreation; Camera traps; Mammal habitat use; Diel activity
   patterns; Occupancy model; Bayesian analysis
ID ESTIMATING SITE OCCUPANCY; ACTIVITY PATTERNS; BEHAVIORAL-RESPONSES;
   STRESS-RESPONSE; CAMERA-TRAP; DISTURBANCE; CARNIVORES; MANAGEMENT;
   EXPOSURE; COYOTES
AB Non-motorized human recreation may displace animals from otherwise suitable habitat; in addition, animals may alter their activity patterns to reduce (or increase) interactions with recreationists. We investigated how hiking, mountain biking, equestrians, and recreationists with domestic dogs affected habitat use and diel activity patterns of ten species of medium and large-sized mammals in the San Francisco Bay ecoregion. We Used camera traps to quantify habitat use and activity patterns of wild mammals and human recreationists at 241 locations in 87 protected areas. We modeled habitat use with a multi-species occupancy model. Species habitat Use was most closely associated with environmental covariates such as landcover, precipitation, and elevation. Although recreation had less influence on habitat use, the presence of domestic dogs was negatively associated with habitat use of mountain lions and Virginia opossum. We also compared diel activity patterns of species at sites with no observed recreation to the activity patterns of species at sites with high (>= eight per day) levels of non-motorized recreation. Coyotes were more active at night and less active during the day in areas with high levels of recreation. Striped skunks were slightly more active later into the morning in areas that allowed human recreation. Smaller carnivores with nocturnal activity patterns may not be directly affected by recreational activities that are limited to daylight hours. We suggest that by maintaining habitat free of domestic dogs, and creating trail free buffers, land managers can manage recreation in a way that minimizes impacts to wildlife habitat and preserves the value of protected areas to people and wildlife. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Reilly, M. L.; Beier, P.] No Arizona Univ, Sch Forestry, Flagstaff, AZ USA.
   [Tobler, M. W.] San Diego Zoo Global, Inst Conserv Res Escondido, San Diego, CA USA.
   [Sonderegger, D. L.] No Arizona Univ, Dept Math & Stat, Flagstaff, AZ USA.
   [Reilly, M. L.] New Mexico Highlands Univ, Dept Biol & Chem, Las Vegas, NM 87701 USA.
RP Reilly, ML (corresponding author), New Mexico Highlands Univ, Dept Biol & Chem, Las Vegas, NM 87701 USA.
EM m1r326@nau.edu
OI Sonderegger, Derek/0000-0001-5151-8588; Tobler,
   Mathias/0000-0002-8587-0560
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation
   [2718]
FX E. Holldorf, C. Griffin, T. Batter, M. Grey, L. Lucore, N. Gengler, K.
   Lauger, S. Espinosa, A. Nickles, T. Volk, C. Miller, G. Pfau, A.
   Coconis, M. Sutton, K. Galbreath, B. Halliwell, C. Cooper assisted ably
   in the field work; Jeff Jenness developed GIS tools for the project. The
   Gordon and Betty Moore Foundation (Grant 2718) funded this work. We
   thank the agencies that allowed us to perform research on their lands
   and the biologists that helped coordinate permits for fieldwork.
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   BAOSC, 2011, CONS LANDS NETW SAN
   Barja I, 2007, J STEROID BIOCHEM, V104, P136, DOI 10.1016/j.jsbmb.2007.03.008
   Beyer HL, 2010, PHILOS T R SOC B, V365, P2245, DOI 10.1098/rstb.2010.0083
   Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
   Brown CL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040505
   Busch R. H., 2004, COUGAR ALMANAC COMPL, V24
   CASSIRER EF, 1992, WILDLIFE SOC B, V20, P375
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   Dickson BG, 2005, J WILDLIFE MANAGE, V69, P264, DOI 10.2193/0022-541X(2005)069&lt;0264:IOVTAR&gt;2.0.CO;2
   Dickson BG, 2002, J WILDLIFE MANAGE, V66, P1235, DOI 10.2307/3802956
   Dolton-Thorton N., 2015, BAY NATURE      0820
   Ellenberg U, 2007, GEN COMP ENDOCR, V152, P54, DOI 10.1016/j.ygcen.2007.02.022
   ESRI, 2010, ARCGLS REL 10 0
   Freeman C., 2015, COMMUNICATION
   Frid A, 2002, CONSERV ECOL, V6
   Gehring TM, 2003, BIOL CONSERV, V109, P283, DOI 10.1016/S0006-3207(02)00156-8
   George SL, 2006, BIOL CONSERV, V133, P107, DOI 10.1016/j.biocon.2006.05.024
   Gillogly Michael, 2015, COMMUNICATION
   Gu WD, 2004, BIOL CONSERV, V116, P195, DOI 10.1016/S0006-3207(03)00190-3
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Harmsen BJ, 2009, J MAMMAL, V90, P612, DOI 10.1644/08-MAMM-A-140R.1
   Jalkosky M. G., 1997, 1354 ARC WILDL SERV
   Karanth KK, 2009, J APPL ECOL, V46, P1189, DOI 10.1111/j.1365-2664.2009.01710.x
   Kays R., 2016, J APPL ECOL
   Kitchen AM, 2000, CAN J ZOOL, V78, P853, DOI 10.1139/cjz-78-5-853
   Krausman P. R., 2008, 281 FWP
   Krausman P. R., 1995, RMGTR264 USDA
   Krausman P. R., 1995, RMGTR264
   LaPoint S. D., 2014, ANIM CONSERV, V18
   Lenth BE, 2008, NAT AREA J, V28, P218, DOI 10.3375/0885-8608(2008)28[218:TEODOW]2.0.CO;2
   Linkie M., 2011, J ZOOL, P1
   Linkie M, 2007, BIOL CONSERV, V137, P20, DOI 10.1016/j.biocon.2007.01.016
   MACARTHUR RA, 1982, J WILDLIFE MANAGE, V46, P351, DOI 10.2307/3808646
   Mackay P., 2008, NONINVASIVE SURVEY M
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Markovchick-Nicholls L, 2008, CONSERV BIOL, V22, P99, DOI 10.1111/j.1523-1739.2007.00846.x
   McClennen N, 2001, AM MIDL NAT, V146, P27, DOI 10.1674/0003-0031(2001)146[0027:TEOSAA]2.0.CO;2
   Meredith M., 2014, OVERLAP ESTIMATES CO
   Mordecai RS, 2011, J APPL ECOL, V48, P56, DOI 10.1111/j.1365-2664.2010.01921.x
   Mullner A, 2004, BIOL CONSERV, V118, P549, DOI 10.1016/j.biocon.2003.10.003
   National Oceanic and Atmospheric Association, 1995, WR126 NWS
   O'Connell AF, 2006, J WILDLIFE MANAGE, V70, P1625, DOI 10.2193/0022-541X(2006)70[1625:ESOADP]2.0.CO;2
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Papouchis CM, 2001, J WILDLIFE MANAGE, V65, P573, DOI 10.2307/3803110
   Plummer M., 2017, JAGS PROGRAM ANAL BA
   Powell BF, 2006, G WRIGHT FORUM, V23, P50
   Rasmussen GSA, 2012, J ZOOL, V286, P232, DOI 10.1111/j.1469-7998.2011.00874.x
   Ray J., 2000, 15 WILDL CONS SOC
   Reed S. E., 2014, P1182112 WILDL CONS
   Reed SE, 2011, CONSERV BIOL, V25, P504, DOI 10.1111/j.1523-1739.2010.01641.x
   Reed SE, 2008, CONSERV LETT, V1, P146, DOI 10.1111/j.1755-263X.2008.00019.x
   Reilly M. L., 2016, ASSESSING PRED UNPUB
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Robert K., 2015, P ROYAL ACAD B, V282
   Royle A. J., 2008, HIERARCHICAL MODELIN, P101
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Savidge M., 2015, COMMUNICATION
   Sime C. A., 1999, WILDL SOC B, V8
   Su YS., 2015, R2JAGS USING R RUN J
   Taylor AR, 2003, ECOL APPL, V13, P951, DOI 10.1890/1051-0761(2003)13[951:WRTRAA]2.0.CO;2
   Tigas LA, 2002, BIOL CONSERV, V108, P299, DOI 10.1016/S0006-3207(02)00120-9
   Tobler M. T., 2012, CAMERABASE 1 6 ATRIU
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Tobler MW, 2009, J TROP ECOL, V25, P261, DOI 10.1017/S0266467409005896
   Tyre AJ, 2003, ECOL APPL, V13, P1790, DOI 10.1890/02-5078
   United States Census Bureau, AM FACT FIND
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilmers CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060590
   Yamaura Y, 2012, BIODIVERS CONSERV, V21, P1365, DOI 10.1007/s10531-012-0244-z
   Zaradic PA, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007367
   Zipkin EF, 2010, BIOL CONSERV, V143, P479, DOI 10.1016/j.biocon.2009.11.016
NR 74
TC 39
Z9 43
U1 8
U2 120
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD MAR
PY 2017
VL 207
BP 117
EP 126
DI 10.1016/j.biocon.2016.11.003
PG 10
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA EM8ZU
UT WOS:000395601400014
DA 2022-02-10
ER

PT J
AU Shen, J
   Yan, WJ
   Li, P
   Xiong, X
AF Shen, Jie
   Yan, Wenjie
   Li, Peng
   Xiong, Xin
TI Deep learning-based object identification with instance segmentation and
   pseudo-LiDAR point cloud for work zone safety
SO COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING
LA English
DT Article
ID DEPTH ESTIMATION; MODEL; VISION
AB Automated object identification in three-dimensional (3D) space is crucial for work zone safety, such as compliance with construction rules and preventing workplace injuries and deaths. However, it is greatly challenged by some factors like high-quality detection, high-quality instance segmentation, few engineering object datasets with masks, and accurate 3D object understanding due to scale variations and limited cues in the 3D world. Traditional hand-crafted methods suffer from these challenges. Our key insight is to use 2D object detection, instance segmentation and camera vision to compute pseudo-light detection and ranging (LiDAR) point cloud for 3D object identification. On the one hand, an enhanced feature pyramid network is proposed to extract more fine-grained object features, and an improved cascade mask R-CNN is applied to detect bounding boxes and masks for all 2D objects efficiently. Moreover, the AIM dataset for heavy equipment detection is augmented, and a new object class with the bounding box and mask is added. On the other hand, pseudo-LiDAR point clouds of objects based on bounding boxes and masks are recovered from a monocular image by deep learning, automatic camera parameter estimation, vision-based method, and space filter. Extensive experiments and analyses show that the new methodology can identify 3D objects and automatically analyze work zone safety. The proposed object detection model has achieved state-of-the-art results on the AIM dataset and 97.2% in mean average precision for the augmented dataset. The collision detection model using pseudo-LiDAR point cloud has obtained 95.99% in accuracy. The new model will serve as a baseline to support 3D object identification research for other 3D tasks.
C1 [Shen, Jie; Yan, Wenjie; Li, Peng; Xiong, Xin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
RP Shen, J (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM sjie@uestc.edu.cn
FU National Science andTechnologyMajor Project Foundation ofChina
   [ZX03002002]
FX National Science andTechnologyMajor Project Foundation ofChina,
   Grant/Award Number: ZX03002002
CR Arabi S, 2020, COMPUT-AIDED CIV INF, V35, P753, DOI 10.1111/mice.12530
   Arnold E, 2019, IEEE T INTELL TRANSP, V20, P3782, DOI 10.1109/TITS.2019.2892405
   Awolusi I, 2019, FRONT BUILT ENVIRON, V5, DOI 10.3389/fbuil.2019.00021
   Benito-Picazo J, 2020, INTEGR COMPUT-AID E, V27, P373, DOI 10.3233/ICA-200632
   Bhoi A, 2019, MONOCULAR DEPTH ESTI
   Bochkovskiy A., 2020, ARXIV200410934
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Cao ZR, 2021, IEEE T SYST MAN CY-S, V51, P1523, DOI 10.1109/TSMC.2019.2898428
   Chang CT, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019641
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Cho, 2018, CONSTR RES C NEW ORL
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong, 2017, CTR CONSTRUCTION RES
   Fang WL, 2018, ADV ENG INFORM, V37, P139, DOI 10.1016/j.aei.2018.05.003
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Garcia-Gonzalez J, 2020, INTEGR COMPUT-AID E, V27, P253, DOI 10.3233/ICA-200621
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171
   Guo JJ, 2021, COMPUT-AIDED CIV INF, V36, P302, DOI 10.1111/mice.12632
   Hahner S, 2020, LECT NOTES COMPUT SC, V12396, P284, DOI 10.1007/978-3-030-61609-0_23
   He T, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P8409
   Jiang JM, 2019, IEEE ACCESS, V7, P134718, DOI 10.1109/ACCESS.2019.2940755
   Jiang Xiaomo, 2004, Int J Neural Syst, V14, P147, DOI 10.1142/S0129065704001954
   Karim A, 2003, J TRANSP ENG, V129, P134, DOI 10.1061/(ASCE)0733-947X(2003)129:2(134)
   Kim H, 2018, J COMPUT CIVIL ENG, V32, DOI 10.1061/(ASCE)CP.1943-5487.0000731
   Lara-Benitez P, 2020, INTEGR COMPUT-AID E, V27, P101, DOI 10.3233/ICA-200617
   Li S, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0086-1
   Lin T. Y., 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.106
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu P, 2020, IEEE ACCESS, V8, P184437, DOI 10.1109/ACCESS.2020.3030097
   Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217
   Mishra P, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500604
   Nnaji C., 2018, PROF SAF, V63, P36
   Oh BK, 2017, APPL SOFT COMPUT, V58, P576, DOI 10.1016/j.asoc.2017.05.029
   Park HS, 2007, COMPUT-AIDED CIV INF, V22, P19, DOI 10.1111/j.1467-8667.2006.00466.x
   Park J., 2015, J CONSTR ENG MANAGE, DOI 10. 1061/(ASCE)CO. 1943-7862. 0001031
   Park MW, 2015, J CONSTR ENG M, V141, DOI 10.1061/(ASCE)CO.1943-7862.0000974
   Park MW, 2012, AUTOMAT CONSTR, V28, P15, DOI 10.1016/j.autcon.2012.06.001
   Park SW, 2015, MEASUREMENT, V59, P352, DOI 10.1016/j.measurement.2014.09.063
   Qian R, 2020, PROC CVPR IEEE, P5880, DOI 10.1109/CVPR42600.2020.00592
   Qin ZY, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P8851
   Raddaoui O, 2020, IATSS RES, V44, P230, DOI 10.1016/j.iatssr.2020.01.001
   Renyin D, 2020, DIVERSEDEPTH AFFINE
   Reyes O, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S012906571950014X
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shalimova EA, 2020, COMPUT OPT, V44, P385, DOI 10.18287/2412-6179-CO-600
   Shen J, 2021, COMPUT-AIDED CIV INF, V36, P180, DOI 10.1111/mice.12579
   Shen J, 2019, COMPUT-AIDED CIV INF, V34, P897, DOI 10.1111/mice.12454
   [施敏敏 Shi Minmin], 2019, [高分子通报, Polymer Bulletin], P1
   Son H, 2019, J COMPUT CIVIL ENG, V33, DOI 10.1061/(ASCE)CP.1943-5487.0000845
   Son H, 2019, AUTOMAT CONSTR, V99, P27, DOI 10.1016/j.autcon.2018.11.033
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vera-Olmos FJ, 2019, INTEGR COMPUT-AID E, V26, P85, DOI 10.3233/ICA-180584
   Weng XS, 2019, IEEE INT CONF COMP V, P857, DOI 10.1109/ICCVW.2019.00114
   Wong KYK, 2003, IEEE T PATTERN ANAL, V25, P147, DOI 10.1109/TPAMI.2003.1177148
   Wu JH, 2020, J PHYS CONF SER, V1518, DOI 10.1088/1742-6596/1518/1/012049
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xinzhu Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P311, DOI 10.1007/978-3-030-58601-0_19
   Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249
   Xue F, 2020, IEEE INT C INT ROBOT, P2330, DOI 10.1109/IROS45743.2020.9340802
   Yang T, 2019, INTEGR COMPUT-AID E, V26, P273, DOI 10.3233/ICA-180596
   You Yurong, 2020, ICLR
   Zhang R., 2019, J PHYS C SERIES, V1213, P042079, DOI 10.1088/1742-6596/1213/4/042079
   Zhou W, 2019, WIDER FACE PEDESTRIA
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 66
TC 1
Z9 1
U1 21
U2 21
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1093-9687
EI 1467-8667
J9 COMPUT-AIDED CIV INF
JI Comput.-Aided Civil Infrastruct. Eng.
PD DEC
PY 2021
VL 36
IS 12
BP 1549
EP 1567
DI 10.1111/mice.12749
EA AUG 2021
PG 19
WC Computer Science, Interdisciplinary Applications; Construction &
   Building Technology; Engineering, Civil; Transportation Science &
   Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Construction & Building Technology; Engineering;
   Transportation
GA WX0RO
UT WOS:000681247500001
DA 2022-02-10
ER

PT J
AU Satter, CB
   Augustine, BC
   Harmsen, BJ
   Foster, RJ
   Sanchez, EE
   Wultsch, C
   Davis, ML
   Kelly, MJ
AF Satter, Christopher B.
   Augustine, Ben C.
   Harmsen, Bart J.
   Foster, Rebecca J.
   Sanchez, Emma E.
   Wultsch, Claudia
   Davis, Miranda L.
   Kelly, Marcella J.
TI Long-term monitoring of ocelot densities in Belize
SO JOURNAL OF WILDLIFE MANAGEMENT
LA English
DT Article
DE Belize; camera-trapping; density; multi-session models; ocelots;
   sex-specific models; spatially explicit capture-recapture
ID JAGUAR PANTHERA-ONCA; LEOPARDUS-PARDALIS; CAMERA-TRAPS; HABITAT USE;
   ACTIVITY PATTERNS; FOREST; POPULATION; DEFORESTATION; CONSERVATION;
   ABUNDANCE
AB Ocelots (Leopardus pardalis) are listed as least concern on the International Union for Conservation of Nature (IUCN) Red list of Threatened Species, yet we lack knowledge on basic demographic parameters across much of the ocelot's geographic range, including population density. We used camera-trapping methodology and spatially explicit capture-recapture (SECR) models with sex-specific detection function parameters to estimate ocelot densities across 7 field sites over 1 to 12 years (from data collected during 2002-2015) in Belize, Central America. Ocelot densities in the broadleaf rainforest sites ranged between 7.2 and 22.7 ocelots/100 km(2), whereas density in the pine (Pinus spp.) forest site was 0.9 ocelots/100 km(2). Applying an inverse-variance weighted average over all years for each broadleaf site increased precision and resulted in average density ranging from 8.5 to 13.0 ocelots/100 km(2). Males often had larger movement parameter estimates and higher detection probabilities at their activity centers than females. In most years, the sex ratio was not significantly different from 50:50, but the pooled sex ratio estimated using an inverse weighted average over all years indicated a female bias in 1 site, and a male bias in another. We did not detect any population trends as density estimates remained relatively constant over time; however, the power to detect such trends was generally low. Our SECR density estimates were lower but more precise than previous estimates and indicated population stability for ocelots in Belize. (c) 2018 The Authors. Journal of Wildlife Management published by Wiley Periodicals, Inc. on behalf of The Wildlife Society.
C1 [Satter, Christopher B.; Kelly, Marcella J.] Virginia Tech, Dept Fish & Wildlife Conservat, 310 W Campus Dr,Cheatham Hall, Blacksburg, VA 24061 USA.
   [Augustine, Ben C.] Cornell Univ, Atkinson Ctr Sustainable Future, G02 Fernow Hall, Ithaca, NY 14850 USA.
   [Augustine, Ben C.] Cornell Univ, Dept Nat Resources, G02 Fernow Hall, Ithaca, NY 14850 USA.
   [Harmsen, Bart J.; Foster, Rebecca J.; Sanchez, Emma E.] Panthera, 8 W 40th St,18th Floor, New York, NY 10018 USA.
   [Wultsch, Claudia] Amer Museum Nat Hist, Sackler Inst Comparat Genom, Cent Pk West & 79th St, New York, NY 10024 USA.
   [Davis, Miranda L.] Univ Connecticut, Ecol & Evolutionary Biol, 75 N Eagleville Rd,Unit 3043, Storrs, CT 06269 USA.
RP Satter, CB (corresponding author), Virginia Tech, Dept Fish & Wildlife Conservat, 310 W Campus Dr,Cheatham Hall, Blacksburg, VA 24061 USA.
EM chrissatter@vt.edu
RI Satter, Christopher/AAF-9152-2019; Kelly, Marcella J/B-4891-2011
OI FOSTER, REBECCA/0000-0002-6055-422X
FU National Science FoundationNational Science Foundation (NSF);
   Philadelphia Zoo; Virginia Tech; Wildlife Conservation Society; Panthera
FX We thank the Belize Forest Department, Program for Belize, Gallon Jug
   Estate, Yalbac Ranch and Cattle Company, Las Cuevas Research Station,
   Friends for Conservation and Development, Wildtracks, Belize Audubon
   Society, and Bull Run Farm (G. W. and M. Y. Headley) for permission to
   conduct this research. We thank local personnel such as N. "Chapal" Bol,
   and M. Aguilar for their local knowledge and expertise, and numerous
   volunteers and assistants (especially A. Dillon and T. Mc Namara) over
   the years who conducted field and lab work, from multiple universities,
   especially the University of Belize and Virginia Tech. We thank the
   National Science Foundation, the Philadelphia Zoo, Virginia Tech,
   Wildlife Conservation Society, and Panthera for funding over the years.
CR Abadi F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062636
   Aide TM, 2013, BIOTROPICA, V45, P262, DOI 10.1111/j.1744-7429.2012.00908.x
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155
   Gomez-Ramirez MA, 2017, ENDANGER SPECIES RES, V32, P471, DOI 10.3354/esr00828
   Bashir T, 2014, ACTA THERIOL, V59, P35, DOI 10.1007/s13364-013-0145-x
   BELETSKY L., 1999, BELIZE NO GUATEMALA
   Borenstein M, 2010, RES SYNTH METHODS, V1, P97, DOI 10.1002/jrsm.12
   Boulanger J, 2002, URSUS, V13, P137
   Carrillo S, 2002, DISTRIBUTION ECOLOGY
   Chandler RB, 2014, METHODS ECOL EVOL, V5, P1351, DOI 10.1111/2041-210X.12153
   Cherrington EA, 2010, FOREST COVER DEFORES
   Clabaugh J., 2017, BIODIVERSITY ENV RES
   da Rocha DG, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154624
   Davis M., 2008, THESIS
   Davis ML, 2011, ANIM CONSERV, V14, P56, DOI 10.1111/j.1469-1795.2010.00389.x
   de Oliveira Tadeu G., 2010, P559
   Di Bitetti MS, 2006, J ZOOL, V270, P153, DOI 10.1111/j.1469-7998.2006.00102.x
   Dillon A, 2008, J ZOOL, V275, P391, DOI 10.1111/j.1469-7998.2008.00452.x
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   Efford, 2011, SECR SPATIALLY EXPLI
   Efford M., 2018, SECR 3 1 SPATIALLY E
   Efford M. G., 2004, Animal Biodiversity and Conservation, V27, P217
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Emmons L., 1988, FIELD STUDY OCELOTS
   Foster RJ, 2016, ORYX, V50, P63, DOI 10.1017/S003060531400060X
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Gardner B, 2010, J WILDLIFE MANAGE, V74, P318, DOI 10.2193/2009-101
   Gerber BD, 2012, POPUL ECOL, V54, P43, DOI 10.1007/s10144-011-0276-3
   Grassman LI, 2005, J ZOOL, V266, P45, DOI 10.1017/S095283690500659X
   Haines AM, 2006, EUR J WILDLIFE RES, V52, P216, DOI 10.1007/s10344-006-0043-5
   Haines AM, 2006, ORYX, V40, P90, DOI 10.1017/S0030605306000044
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Harveson PM, 2004, WILDLIFE SOC B, V32, P948, DOI 10.2193/0091-7648(2004)032[0948:HUBOIS]2.0.CO;2
   Horne JS, 2009, SOUTHWEST NAT, V54, P119, DOI 10.1894/PS-49.1
   Hunter L., 2011, CARNIVORES WORLD
   Kellman M, 1997, J BIOGEOGR, V24, P23, DOI 10.1111/j.1365-2699.1997.tb00047.x
   Kelly M.J., 2014, ANAL 5 YEARS DATA RI
   Kolowski JM, 2010, BIOL CONSERV, V143, P917, DOI 10.1016/j.biocon.2009.12.039
   LEBRETON JD, 1992, ECOL MONOGR, V62, P67, DOI 10.2307/2937171
   LUDLOW ME, 1987, NATL GEOGR RES, V3, P447
   MacKenzie DI, 2005, ECOLOGY, V86, P1101, DOI 10.1890/04-1060
   Martinez-Hernandez A, 2015, ORYX, V49, P619, DOI 10.1017/S0030605313001452
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Miller CM, 2006, JAGUAR DENSITY FIREB
   Murray Julie L., 1997, Mammalian Species, V548, P1
   Nogueira SSC, 2011, BIODIVERS CONSERV, V20, P1385, DOI 10.1007/s10531-011-0047-7
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   Paviolo A, 2009, J MAMMAL, V90, P926, DOI 10.1644/08-MAMM-A-128.1
   Programme for Belize, 2008, FOR TIMB EXTR
   Reed JM, 2002, CONSERV BIOL, V16, P7, DOI 10.1046/j.1523-1739.2002.99419.x
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Salvador J, 2016, MAMMALIA, V80, P395, DOI 10.1515/mammalia-2014-0172
   Sandell M., 1989, P164
   Satter C. B., 2013, WILD FELID MONITOR, V9, P22
   Satter C. B., 2017, THESIS
   Schmid-Holmes S, 2001, BIOL CONSERV, V99, P293, DOI 10.1016/S0006-3207(00)00195-6
   Silveira L, 2010, ORYX, V44, P104, DOI 10.1017/S0030605309990433
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Soisalo MK, 2006, BIOL CONSERV, V129, P487, DOI 10.1016/j.biocon.2005.11.023
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Sunquist M., 2002, WILD CATS WORLD, DOI 10.1644/1545-1542(2004)0852.0.co;2
   Tobler MW, 2013, BIOL CONSERV, V159, P109, DOI 10.1016/j.biocon.2012.12.009
   White GC, 2005, WILDLIFE RES, V32, P211, DOI 10.1071/WR03123
   Wright A. C. S., 1959, VOLUME COLONIAL RES, V24
   Young CA, 2008, TROP CONSERV SCI, V1, P18, DOI 10.1177/194008290800100102
NR 67
TC 11
Z9 12
U1 1
U2 26
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0022-541X
EI 1937-2817
J9 J WILDLIFE MANAGE
JI J. Wildl. Manage.
PD FEB
PY 2019
VL 83
IS 2
BP 283
EP 294
DI 10.1002/jwmg.21598
PG 12
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA HH1NM
UT WOS:000455486900005
OA Green Published, hybrid
DA 2022-02-10
ER

PT J
AU Jones, FM
   Allen, C
   Arteta, C
   Arthur, J
   Black, C
   Emmerson, LM
   Freeman, R
   Hines, G
   Lintott, CJ
   Machackova, Z
   Miller, G
   Simpson, R
   Southwell, C
   Torsey, HR
   Zisserman, A
   Hart, T
AF Jones, Fiona M.
   Allen, Campbell
   Arteta, Carlos
   Arthur, Joan
   Black, Caitlin
   Emmerson, Louise M.
   Freeman, Robin
   Hines, Greg
   Lintott, Chris J.
   Machackova, Zuzana
   Miller, Grant
   Simpson, Rob
   Southwell, Colin
   Torsey, Holly R.
   Zisserman, Andrew
   Hart, Tom
TI Time-lapse imagery and volunteer classifications from the Zooniverse
   Penguin Watch project
SO SCIENTIFIC DATA
LA English
DT Article; Data Paper
ID CITIZEN SCIENCE; CAMERA TRAPS; BIODIVERSITY
AB Automated time-lapse cameras can facilitate reliable and consistent monitoring of wild animal populations. In this report, data from 73,802 images taken by 15 different Penguin Watch cameras are presented, capturing the dynamics of penguin (Spheniscidae; Pygoscelis spp.) breeding colonies across the Antarctic Peninsula, South Shetland Islands and South Georgia (03/2012 to 01/2014). Citizen science provides a means by which large and otherwise intractable photographic data sets can be processed, and here we describe the methodology associated with the Zooniverse project Penguin Watch, and provide validation of the method. We present anonymised volunteer classifications for the 73,802 images, alongside the associated metadata (including date/time and temperature information). In addition to the benefits for ecological monitoring, such as easy detection of animal attendance patterns, this type of annotated timelapse imagery can be employed as a training tool for machine learning algorithms to automate data extraction, and we encourage the use of this data set for computer vision development.
C1 [Jones, Fiona M.; Black, Caitlin; Hart, Tom] Univ Oxford, Dept Zool, South Parks Rd, Oxford OX1 3PS, England.
   [Allen, Campbell; Arthur, Joan; Hines, Greg; Lintott, Chris J.; Machackova, Zuzana; Miller, Grant; Torsey, Holly R.] Univ Oxford, Dept Phys, Zooniverse, Denys Wilkinson Bldg,Keble Rd, Oxford OX1 3RH, England.
   [Arteta, Carlos; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
   [Emmerson, Louise M.; Southwell, Colin] Australian Antarctic Div, Dept Environm & Energy, 203 Channel Highway, Kingston, Tas 7050, Australia.
   [Freeman, Robin] Inst Zool, Wellcome Bldg,Regents Pk, London NW1 4RY, England.
   [Simpson, Rob] Google UK, Belgrave House,76 Buckingham Palace Rd, London SW1W 9TQ, England.
RP Jones, FM; Hart, T (corresponding author), Univ Oxford, Dept Zool, South Parks Rd, Oxford OX1 3PS, England.
EM fiona.jones@zoo.ox.ac.uk; tom.hart@zoo.ox.ac.uk
OI Black, Caitlin/0000-0001-9591-3571
FU Alfred P. Sloan FoundationAlfred P. Sloan Foundation; Google Global
   Impact AwardGoogle Incorporated; NERCUK Research & Innovation
   (UKRI)Natural Environment Research Council (NERC); Darwin Initiative;
   Citizen Science Alliance; AAS [2722, 4088]; EPSRCUK Research &
   Innovation (UKRI)Engineering & Physical Sciences Research Council
   (EPSRC) [EP/M013774/1] Funding Source: UKRI
FX The authors express their gratitude to the Zooniverse team members, the
   Zooniverse partners (see https://www.penguinwatch.org/#/ team) and the
   47,961 registered Penguin Watch volunteers. The authors also wish to
   thank Quark Expeditions, Cheeseman's Ecology Safaris and Oceanwide
   Expeditions for their assistance with fieldwork logistics, and Brooke
   Simmons for helpful comments on the manuscript. Zooniverse and Penguin
   Watch were supported by grants from the Alfred P. Sloan Foundation and a
   Google Global Impact Award. This work was supported by NERC, the Darwin
   Initiative and the Citizen Science Alliance, as well as individual
   contributors to Penguin Watch, particularly via donations during Quark
   Expeditions' voyages. Work by Australian Antarctic Division researchers
   to establish a camera network in East Antarctica was carried out under
   AAS projects 2722 and 4088. Fiona Jones and Tom Hart had full access to
   all the data in the study and take responsibility for the integrity of
   the data and the accuracy of the data analysis.
CR Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30
   Beaumont CN, 2014, ASTROPHYS J SUPPL S, V214, DOI 10.1088/0067-0049/214/1/3
   Black C, 2017, AUK, V134, P520, DOI 10.1642/AUK-16-69.1
   Black C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0145676
   Bolton M, 2007, J FIELD ORNITHOL, V78, P213, DOI 10.1111/j.1557-9263.2007.00104.x
   Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
   Freeman R, 2017, PENGUIN WATCH IMAGE, DOI [10.6084/m9.figshare.5432974.v2, DOI 10.6084/M9.FIGSHARE.5432974.V2]
   Hines G, 2017, PENGUIN WATCH AGGREG, DOI [10.6084/m9.figshare.5472544.v1, DOI 10.6084/M9.FIGSHARE.5472544.V1]
   Holden J, 2003, ORYX, V37, P34, DOI 10.1017/S0030605303000097
   Homsy V, 2014, THESIS
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Lynch HJ, 2012, ECOLOGY, V93, P1367
   Newbery KB, 2009, COLD REG SCI TECHNOL, V55, P47, DOI 10.1016/j.coldregions.2008.06.001
   Pettorelli N, 2010, ANIM CONSERV, V13, P131, DOI 10.1111/j.1469-1795.2009.00309.x
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Team R. C, 2016, R LANG ENV STAT COMP
   Turner W, 2014, SCIENCE, V346, P301, DOI 10.1126/science.1256014
NR 20
TC 11
Z9 14
U1 1
U2 29
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
EI 2052-4463
J9 SCI DATA
JI Sci. Data
PD JUN 26
PY 2018
VL 5
AR 180124
DI 10.1038/sdata.2018.124
PG 13
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA GK5ST
UT WOS:000436238100002
PM 29944146
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Kazi, S
   Aurisch, Y
   Coulson, G
   Eldridge, MDB
   Irving, M
   Miller, KA
   Parrott, ML
AF Kazi, Sakib
   Aurisch, Yohanna
   Coulson, Graeme
   Eldridge, Mark D. B.
   Irving, Matt
   Miller, Kimberly A.
   Parrott, Marissa L.
TI Range extension of eastern wallaroo (Osphranter robustus robustus) in
   Victoria
SO AUSTRALIAN MAMMALOGY
LA English
DT Article; Early Access
DE alpine zone; boulderfield; camera; distribution; ecology; Macropodidae;
   marsupial; Osphranter robustus robustus
ID GREY-KANGAROO
AB The eastern wallaroo (Osphranter robustus robustus) is a large macropodid commonly found in New South Wales and Queensland, but rare in Victoria. Previously only known in north-east Victoria from a resident population near Suggan Buggan, and isolated records <50 km from the NSW border, we report camera trap observations of O. r. robustus from Mt Loch, near Hotham Heights. This represents the highest altitude observation of the species (similar to 1720 m above sea level), the furthest Victorian record from NSW, and a south-westerly range extension of 73 km.
C1 [Kazi, Sakib; Parrott, Marissa L.] Zoos Victoria, Wildlife Conservat & Sci, Parkville, Vic 3052, Australia.
   [Aurisch, Yohanna] Pk Victoria, Tawonga South, Vic 3698, Australia.
   [Coulson, Graeme] Univ Melbourne, Sch BioSci, Melbourne, Vic 3010, Australia.
   [Eldridge, Mark D. B.] Australian Museum, Australian Museum Res Inst, 1 William St, Sydney, NSW 2010, Australia.
   [Irving, Matt] Pk Victoria, Mt Buffalo, Vic 3740, Australia.
   [Miller, Kimberly A.] Zoos Victoria, Healesville Sanctuary, Badger Creek, Vic 3777, Australia.
RP Kazi, S (corresponding author), Zoos Victoria, Wildlife Conservat & Sci, Parkville, Vic 3052, Australia.
EM skazi@zoo.org.au
FU Dyson Bequest; Zoos Victoria Bushfire Emergency Wildlife Fund; WildArk;
   Aussie Ark; Global Wildlife Conservation
FX Funding was generously provided by the Dyson Bequest, Zoos Victoria
   Bushfire Emergency Wildlife Fund, WildArk, Aussie Ark and Global
   Wildlife Conservation, for Zoos Victoria's Mountain Pygmy-possum
   research, which led to this discovery.
CR Balland J, 2020, WILDLIFE RES, V47, P381, DOI 10.1071/WR19234
   DELWP, 2021, FLOR FAUN GUAR ACT 1
   JARMAN PJ, 1983, AUST WILDLIFE RES, V10, P33
   Menkhorst P. W., 1995, MAMMALS VICTORIA DIS, P142
   Mittermeier, 2015, HDB MAMMALS WORLD, P630
   Richardson BJ, 2019, AUST MAMMAL, V41, P65, DOI 10.1071/AM17032
   Taylor R.J., 1982, Australian Mammalogy, V5, P221
   TAYLOR RJ, 1983, AUST WILDLIFE RES, V10, P203
   TAYLOR RJ, 1984, J ANIM ECOL, V53, P65, DOI 10.2307/4342
   TAYLOR RJ, 1981, THESIS U NEW ENGLAND
NR 10
TC 0
Z9 0
U1 0
U2 0
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 0310-0049
EI 1836-7402
J9 AUST MAMMAL
JI Aust. Mammal.
DI 10.1071/AM21029
EA OCT 2021
PG 3
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA WO2HX
UT WOS:000712281800001
DA 2022-02-10
ER

PT J
AU Harper, LR
   Handley, LL
   Carpenter, AI
   Ghazali, M
   Di Muri, C
   Macgregor, CJ
   Logan, TW
   Law, A
   Breithaupt, T
   Read, DS
   McDevitt, AD
   Hanfling, B
AF Harper, Lynsey R.
   Handley, Lori Lawson
   Carpenter, Angus I.
   Ghazali, Muhammad
   Di Muri, Cristina
   Macgregor, Callum J.
   Logan, Thomas W.
   Law, Alan
   Breithaupt, Thomas
   Read, Daniel S.
   McDevitt, Allan D.
   Hanfling, Bernd
TI Environmental DNA (eDNA) metabarcoding of pond water as a tool to survey
   conservation and management priority mammals
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Camera traps; Field signs; Lentic; Monitoring; Semi-aquatic mammals;
   Terrestrial mammals
ID TERRESTRIAL MAMMALS; OTTER; SNOW
AB Environmental DNA (eDNA) metabarcoding can identify terrestrial taxa utilising aquatic habitats alongside aquatic communities, but terrestrial species' eDNA dynamics are understudied. We evaluated eDNA metabarcoding for monitoring semi-aquatic and terrestrial mammals, specifically nine species of conservation or management concern, and examined spatiotemporal variation in mammal eDNA signals. We hypothesised eDNA signals would be stronger for semi-aquatic than terrestrial mammals, and at sites where individuals exhibited behaviours. In captivity, we sampled waterbodies at points where behaviours were observed ('directed' sampling) and at equidistant intervals along the shoreline ('stratified' sampling). We surveyed natural ponds (N = 6) where focal species were present using stratified water sampling, camera traps, and field signs. eDNA samples were metabarcoded using vertebrate-specific primers. All focal species were detected in captivity. eDNA signal strength did not differ between directed and stratified samples across or within species, between semi-aquatic or terrestrial species, or according to behaviours. eDNA was evenly distributed in artificial waterbodies, but unevenly distributed in natural ponds. Survey methods deployed at natural ponds shared three species detections. Metabarcoding missed badger and red fox recorded by cameras and field signs, but detected small mammals these tools overlooked, e.g. water vole. Terrestrial mammal eDNA signals were weaker and detected less frequently than semi-aquatic mammal eDNA signals. eDNA metabarcoding could enhance mammal monitoring through large-scale, multi-species distribution assessment for priority and difficult to survey species, and provide early indication of range expansions or contractions. However, eDNA surveys need high spatiotemporal resolution and metabarcoding biases require further investigation before routine implementation.
C1 [Harper, Lynsey R.; Handley, Lori Lawson; Di Muri, Cristina; Logan, Thomas W.; Breithaupt, Thomas; Hanfling, Bernd] Univ Hull, Dept Biol & Marine Sci, Kingston Upon Hull HU6 7RX, N Humberside, England.
   [Harper, Lynsey R.] Univ Illinois, Prairie Res Inst, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
   [Carpenter, Angus I.] Wildwood Trust, Canterbury Rd, Herne Common CT6 7LQ, Herne Bay, England.
   [Ghazali, Muhammad] Edinburgh Zoo, Royal Zool Soc Scotland, 134 Corstorphine Rd, Edinburgh EH12 6TS, Midlothian, Scotland.
   [Macgregor, Callum J.] Univ York, Dept Biol, Wentworth Way, York YO10 5DD, N Yorkshire, England.
   [Law, Alan] Univ Stirling, Biol & Environm Sci, Stirling FK9 4LA, Scotland.
   [Read, Daniel S.] CEH, Benson Lane, Wallingford OX10 8BB, Oxon, England.
   [McDevitt, Allan D.] Univ Salford, Sch Sci Engn & Environm, Ecosyst & Environm res Ctr, Salford M5 4WT, Lancs, England.
RP Harper, LR (corresponding author), Univ Illinois, Prairie Res Inst, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
EM lynsey.harper2@gmail.com
RI Macgregor, Callum/Y-2563-2018; Read, Daniel/B-5446-2008; Harper, Lynsey
   R./AAQ-2705-2020; Carpenter, Angus/AAM-4652-2020; Di Muri,
   Cristina/ABF-5993-2021; Logan, Thomas W./AAV-3319-2021
OI Macgregor, Callum/0000-0001-8281-8284; Read, Daniel/0000-0001-8546-5154;
   Harper, Lynsey R./0000-0003-0923-1801; Carpenter,
   Angus/0000-0002-0262-9895; Di Muri, Cristina/0000-0003-4072-0662; Logan,
   Thomas W./0000-0001-6640-1489; Breithaupt, Thomas/0000-0003-4586-0998;
   Hanfling, Bernd/0000-0001-7630-9360; Law, Alan/0000-0001-5971-3214
FU University of Hull; Natural Environment Research Council as part of the
   UK-SCAPE programme [NE/R016429/1]
FX This work was funded by the University of Hull, and D.S.R was supported
   by the Natural Environment Research Council award number NE/R016429/1 as
   part of the UK-SCAPE programme delivering National Capability. We would
   like to thank Gill Murray-Dickson (National Museums Scotland) and Helen
   Semi (RZSS Edinburgh Zoo) for feedback on the study design, and Richard
   Griffiths (DICE, University of Kent) for support with filtration of
   water samples from Wildwood Trust. We are grateful to staff at Wildwood
   Trust and RZSS Highland Wildlife Park for assisting with eDNA sampling
   from animal enclosures. We thank Richard Hampshire and volunteers at
   Tophill Low Nature Reserve for assisting with camera trap deployment.
   Tim Kohler (Natural England) aPaul Ramsay kindly gave permission to
   sample at Thorne Moors and the Bamff Estate respectively.
CR Balint M, 2018, MOL ECOL RESOUR, V18, P1415, DOI 10.1111/1755-0998.12934
   Bland LM, 2015, CONSERV BIOL, V29, P250, DOI 10.1111/cobi.12372
   Bonesi L, 2004, OIKOS, V106, P509, DOI 10.1111/j.0030-1299.2004.13034.x
   Bronner I.F., 2009, CURRENT PROTOCOLS HU, V18
   Brooks ME, 2017, R J, V9, P378, DOI 10.32614/RJ-2017-066
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2018, PEERJ, V6, DOI 10.7717/peerj.5827
   Deiner K, 2017, MOL ECOL, V26, P5872, DOI 10.1111/mec.14350
   Evans NT, 2017, CAN J FISH AQUAT SCI, V74, P1362, DOI 10.1139/cjfas-2016-0306
   Franklin TW, 2019, BIOL CONSERV, V229, P50, DOI 10.1016/j.biocon.2018.11.006
   Gaughran A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191818
   Hanfling B., 2017, REV RECENT ADV GENET
   Hanfling B, 2016, MOL ECOL, V25, P3101, DOI 10.1111/mec.13660
   Harper LR, 2018, TESTING ECOLOGICAL H, DOI [10.1101/278309, DOI 10.1101/278309]
   Harper LR, 2018, ECOL EVOL, V8, P6330, DOI 10.1002/ece3.4013
   Harris S, 2004, MAMMAL REV, V34, P157, DOI 10.1046/j.0305-1838.2003.00030.x
   Ishige T, 2017, BIOL CONSERV, V210, P281, DOI 10.1016/j.biocon.2017.04.023
   Johnson H., 2019, ENV DNA, V1, P26, DOI [10.1002/edn3.5, DOI 10.1002/EDN3.5]
   Joint Nature Conservation Committee, 2018, UK BAP PRIOR TERR MA
   Kelly RP, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086175
   Kinoshita G, 2019, ZOOL SCI, V36, P198, DOI 10.2108/zs180172
   Kitson JJN, 2019, MOL ECOL, V28, P471, DOI 10.1111/mec.14518
   Klymus KE, 2017, DIVERSITY-BASEL, V9, DOI 10.3390/d9040054
   Lacoursiere-Roussel A, 2016, GENOME, V59, P991, DOI 10.1139/gen-2015-0218
   Leempoel K., 2019, BIORXIV, DOI [10.1101/634022, DOI 10.1101/634022]
   Lugg WH, 2018, METHODS ECOL EVOL, V9, P1049, DOI 10.1111/2041-210X.12951
   Massimino D, 2018, BIOL CONSERV, V226, P153, DOI 10.1016/j.biocon.2018.07.026
   Mathews F., 2018, REV POPULATION CONSE
   McDevitt, 2019, FISHING MAMMALS LAND, DOI [10.1101/629758, DOI 10.1101/629758]
   McKnight D.T., 2019, ENV DNA, V1, P14, DOI [10.1002/edn3.11, DOI 10.1002/EDN3.11]
   R Core Team, 2017, R LANG ENV STAT COMP
   Riaz T, 2011, NUCLEIC ACIDS RES, V39, DOI 10.1093/nar/gkr732
   Rodgers TW, 2015, CONSERV GENET RESOUR, V7, P693, DOI 10.1007/s12686-015-0478-7
   RuizOlmo J, 1997, ACTA THERIOL, V42, P259, DOI 10.4098/AT.arch.97-28
   Sadlier LMJ, 2004, MAMMAL REV, V34, P75, DOI 10.1046/j.0305-1838.2003.00029.x
   Sellers Graham S., 2018, Metabarcoding and Metagenomics, V2, pe24556, DOI 10.3897/mbmg.2.24556
   Sheehy E, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2017.2603
   Staley ZR, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32680-z
   Tessler M, 2018, SYST BIODIVERS, V16, P488, DOI 10.1080/14772000.2018.1433729
   Thomsen PF, 2015, BIOL CONSERV, V183, P4, DOI 10.1016/j.biocon.2014.11.019
   Thomsen PF, 2012, MOL ECOL, V21, P2565, DOI 10.1111/j.1365-294X.2011.05418.x
   Ushio M, 2017, MOL ECOL RESOUR, V17, pe63, DOI 10.1111/1755-0998.12690
   Valentini A, 2016, MOL ECOL, V25, P929, DOI 10.1111/mec.13428
   Visconti P, 2011, PHILOS T R SOC B, V366, P2693, DOI 10.1098/rstb.2011.0105
   Wickham H., 2016, GGPLOT2 ELEGANT GRAP, DOI [10.1007/978-3-319-24277-4, DOI 10.1007/978-3-319-24277-4_9]
   Williams KE, 2018, ECOL EVOL, V8, P688, DOI 10.1002/ece3.3698
   Zuur Alain F., 2009, P1
NR 47
TC 21
Z9 26
U1 10
U2 51
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD OCT
PY 2019
VL 238
AR 108225
DI 10.1016/j.biocon.2019.108225
PG 11
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA JO0BM
UT WOS:000497252700027
OA Green Submitted, Green Accepted
DA 2022-02-10
ER

PT J
AU Farmer, MJ
   Allen, ML
   Olson, ER
   Van Stappen, J
   Van Deelen, TR
AF Farmer, M. J.
   Allen, M. L.
   Olson, E. R.
   Van Stappen, J.
   Van Deelen, T. R.
TI Agonistic interactions and island biogeography as drivers of carnivore
   spatial and temporal activity at multiple scales
SO CANADIAN JOURNAL OF ZOOLOGY
LA English
DT Article
DE Apostle Islands; camera traps; carnivores; island biogeography; spatial
   segregation; temporal segregation
ID INTRAGUILD PREDATION; RED FOXES; COMPETITION; COYOTES; EXAMPLE; PREY;
   COLONIZATION; COEXISTENCE; OCCUPANCY; FISHERS
AB Carnivore communities can be diverse and complex, and lack of knowledge regarding intraguild interactions and alternative drivers of carnivore distributions can preclude effective conservation of co-occurring species. As such, our objectives were to evaluate the relative importance of intraguild interactions and island biogeography to carnivore community spatiotemporal activity at multiple spatial scales. We monitored the carnivore community of the Apostle Islands National Lakeshore (Wisconsin, USA) using a grid of camera traps from 2014 to 2018. We used generalized linear mixed-effects models and information-theoretic model selection to evaluate whether subordinate carnivore presence was related to dominant carnivore relative abundance (interactions) or to island biogeography at the island level and camera site level, and we calculated temporal overlap between each pair of species to determine whether subordinate carnivores were using temporal segregation. At the island level, the relative importance of interactions and island biogeography was species dependent. At the site level, relative abundance of dominant carnivores was not a significant predictor of subordinate carnivore presence, and all pairs exhibited high or neutral temporal overlap. At the island level, island biogeography and interactions may both impact species distributions; however, at finer spatial scales, the carnivore community may be using alternative segregation strategies, or the island system may preclude segregation.
C1 [Farmer, M. J.; Van Deelen, T. R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Allen, M. L.] Univ Illinois, Illinois Nat Hist Survey, 1816 South Oak St, Champaign, IL 61820 USA.
   [Olson, E. R.] Northland Coll, Nat Resources, 1411 Ellis Ave South, Ashland, WI 54806 USA.
   [Van Stappen, J.] Apostle Isl Natl Lakeshore, Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
RP Farmer, MJ (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM mjmorales@wisc.edu
FU Apostle Islands National Lakeshore, Bayfield, Wisconsin (GLNF CESU)
   [P14AC01180]; Northland College, Ashland, Wisconsin (Department of
   Natural Resources); NASA Earth and Space Science Fellowship
   [NNX16AO61H]; University of Wisconsin-Madison, Madison (Schorger fund,
   Department of Forest and Wildlife Ecology, Beers-Bascom Professorship in
   Conservation); Northland College, Ashland, Wisconsin (Morris O. Ristvedt
   Professorship in the Natural Sciences); Northland College, Ashland,
   Wisconsin (Sigurd Olson Professorship in the Natural Sciences)
FX We thank the students, employees, and volunteers from each group who
   contributed to this project by deploying and checking camera traps,
   tagging the camera photographs, and providing boat transport. We also
   acknowledge and thank K.M. Gaynor for providing R code to create the
   temporal overlap figures. This work was supported by the Apostle Islands
   National Lakeshore, Bayfield, Wisconsin (GLNF CESU Agreement
   P14AC01180); Northland College, Ashland, Wisconsin (Department of
   Natural Resources, Sigurd Olson Professorship in the Natural Sciences,
   Morris O. Ristvedt Professorship in the Natural Sciences); NASA Earth
   and Space Science Fellowship (grant number NNX16AO61H); and the
   University of Wisconsin-Madison, Madison (Schorger fund, Department of
   Forest and Wildlife Ecology, Beers-Bascom Professorship in
   Conservation).
CR Adams JR, 2011, P ROY SOC B-BIOL SCI, V278, P3336, DOI 10.1098/rspb.2011.0261
   Allen ML, 2020, ANIM BIODIV CONSERV, V43, P97, DOI 10.32800/abc.2020.43.0097
   Allen ML, 2018, COMMUNITY ECOL, V19, P272, DOI 10.1556/168.2018.19.3.8
   Allen ML, 2019, MAMMALIA, V83, P552, DOI 10.1515/mammalia-2017-0162
   Allen ML, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102257
   Anderson D.R, 2002, TECHNOMETRICS
   Arim M, 2004, ECOL LETT, V7, P557, DOI 10.1111/j.1461-0248.2004.00613.x
   Bates D, 2013, J STAT SOFTW, V52, P1, DOI 10.18637/jss.v052.i05
   Berger J, 2001, ECOL APPL, V11, P947, DOI 10.1890/1051-0761(2001)011[0947:AMPPIG]2.0.CO;2
   COLE BJ, 1981, AM NAT, V117, P629, DOI 10.1086/283749
   CRAVEN S R, 1987, Colonial Waterbirds, V10, P64, DOI 10.2307/1521232
   DIAMOND JM, 1974, SCIENCE, V184, P803, DOI 10.1126/science.184.4138.803
   Durant SM, 1998, J ANIM ECOL, V67, P370, DOI 10.1046/j.1365-2656.1998.00202.x
   Esri Inc., 2017, ARCMAP VERS 10 5 1
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Farmer, 2019, URBAN NAT, V29, P1
   Farmer M.J, 2020, THESIS U WISCONSIN M
   Fedriani JM, 2000, OECOLOGIA, V125, P258, DOI 10.1007/s004420000448
   Fisher JT, 2013, ECOGRAPHY, V36, P240, DOI 10.1111/j.1600-0587.2012.07556.x
   Gosselink TE, 2003, J WILDLIFE MANAGE, V67, P90, DOI 10.2307/3803065
   HARRISON DJ, 1989, J WILDLIFE MANAGE, V53, P181, DOI 10.2307/3801327
   Henke SE, 1999, J WILDLIFE MANAGE, V63, P1066, DOI 10.2307/3802826
   Jenks KE, 2011, TROP CONSERV SCI, V4, P113, DOI 10.1177/194008291100400203
   Judziewicz E.J., 1993, MICHIGAN BOT, V32, P43
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Lanszki J, 2011, ACTA ZOOL ACAD SCI H, V57, P291
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   LEVIN SA, 1992, ECOLOGY, V73, P1943, DOI 10.2307/1941447
   Linnell John D. C., 2000, Diversity and Distributions, V6, P169, DOI 10.1046/j.1472-4642.2000.00069.x
   Lomolino M.V., 1988, P185
   Lomolino MV, 2000, GLOBAL ECOL BIOGEOGR, V9, P1, DOI 10.1046/j.1365-2699.2000.00185.x
   MacArthur R.H., 1967, THEORY ISLAND BIOGEO
   Manlick PJ, 2017, J MAMMAL, V98, P690, DOI 10.1093/jmammal/gyx030
   Mcdonald RA, 2002, J ANIM ECOL, V71, P185, DOI 10.1046/j.1365-2656.2002.00588.x
   Meredith M., 2017, OVERVIEW OVERLAP PAC
   Millien-Parra V, 1999, J BIOGEOGR, V26, P959, DOI 10.1046/j.1365-2699.1999.00346.x
   Monterroso P, 2016, J MAMMAL, V97, P928, DOI 10.1093/jmammal/gyw016
   Mueller MA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190971
   NCEI, 2019, CLIM DAT ONL
   Neale JCC, 2001, CAN J ZOOL, V79, P1794, DOI 10.1139/cjz-79-10-1794
   Nouvellet P, 2012, J ZOOL, V286, P179, DOI 10.1111/j.1469-7998.2011.00864.x
   Olson ER, 2019, CAN J ZOOL, V97, P1030, DOI 10.1139/cjz-2018-0254
   Palomares F, 1999, AM NAT, V153, P492, DOI 10.1086/303189
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Preisser EL, 2005, ECOLOGY, V86, P501, DOI 10.1890/04-0719
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Radloff FGT, 2004, J ANIM ECOL, V73, P410, DOI 10.1111/j.0021-8790.2004.00817.x
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Robin X, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-77
   Sax D.F., 2011, THEORY ECOLOGY, P219
   Schmitz OJ, 2010, ECOL LETT, V13, P1199, DOI 10.1111/j.1461-0248.2010.01511.x
   SIMBERLOFF D, 1978, AM NAT, V112, P713, DOI 10.1086/283313
   Sollmann R, 2018, AFR J ECOL, V56, P740, DOI 10.1111/aje.12557
   Swanson A, 2014, J ANIM ECOL, V83, P1418, DOI 10.1111/1365-2656.12231
   Vanak AT, 2013, ECOLOGY, V94, P2619, DOI 10.1890/13-0217.1
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
NR 57
TC 0
Z9 0
U1 3
U2 5
PU CANADIAN SCIENCE PUBLISHING
PI OTTAWA
PA 65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA
SN 0008-4301
EI 1480-3283
J9 CAN J ZOOL
JI Can. J. Zool.
PD APR
PY 2021
VL 99
IS 4
BP 309
EP 317
DI 10.1139/cjz-2020-0195
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA RN6HJ
UT WOS:000640452000008
DA 2022-02-10
ER

PT J
AU Nickel, BA
   Suraci, JP
   Allen, ML
   Wilmers, CC
AF Nickel, Barry A.
   Suraci, Justin P.
   Allen, Maximilian L.
   Wilmers, Christopher C.
TI Human presence and human footprint have non-equivalent effects on
   wildlife spatiotemporal habitat use
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Camera trap; Community ecology; Occupancy model; Wildlife nocturnality;
   Anthropogenic disturbance; Recreation ecology
ID ESTIMATING SITE OCCUPANCY; LARGE CARNIVORES; MODEL SELECTION; PROTECTED
   AREAS; RECREATION; URBAN; DISTURBANCE; LANDSCAPE; ECOLOGY; FEAR
AB Human impacts on wildlife stem from both our footprint on the landscape and the presence of people in wildlife habitat. Each may influence wildlife at very different spatial and temporal scales, yet efforts to disentangle these two classes of anthropogenic disturbance in their effects on wildlife have remained limited, as have efforts to predict the spatial extent of human presence and its impacts independently of human footprint. We used camera trap data from a 1400-km(2) grid spanning wildlands and residential development in central California to compare the effects of human presence (human detections on camera) and footprint (building density) on mammalian predators. We then developed a model predicting the spatial extent of human presence and its impacts across the broader landscape. Occupancy modeling and temporal activity analyses showed that human presence and footprint had non-equivalent and often opposing effects on wildlife. Larger predators (pumas Puma concolor, bobcats Lynx rufus, coyotes Canis latrans) were less active where human footprint was high but avoided high human presence temporally rather than spatially. Smaller predators (striped skunks Mephitis mephitis, Virginia opossums Didelphis virginiana) preferred developed areas but exhibited reduced activity where human presence was high. A spatial model, based on readily available landscape covariates (parking lots, trails, topography), performed well in predicting human activity outside of developed areas, and revealed high human presence even in remote protected areas that provide otherwise intact wildlife habitat. This work highlights the need to integrate multiple disturbance types when evaluating the impacts of anthropogenic activity on wildlife.
C1 [Nickel, Barry A.; Suraci, Justin P.; Wilmers, Christopher C.] Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Environm Studies Dept, Santa Cruz, CA 95064 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, Prairie Res Inst, Champaign, IL 61820 USA.
RP Suraci, JP (corresponding author), Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Environm Studies Dept, Santa Cruz, CA 95064 USA.
EM justin.suraci@gmail.com
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation;
   National Science FoundationNational Science Foundation (NSF) [1255913,
   0963022]; Blue Foundation; Peninsula Open Space Trust; Resources Legacy
   Fund
FX We thank P. Houghtaling, R. King, and K. Briner for help in the field,
   A. Nisi for data management assistances, and multiple undergraduate
   volunteers for help scoring camera trap images. We are grateful to the
   many Santa Cruz Mountains property owners who provided access to their
   land. Funding was provided by the Gordon and Betty Moore Foundation,
   National Science Foundation (grants 1255913 and 0963022 to CCW), the
   Blue Foundation, Peninsula Open Space Trust and Resources Legacy Fund.
CR Balmford A, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002074
   Bateman PW, 2012, J ZOOL, V287, P1, DOI 10.1111/j.1469-7998.2011.00887.x
   Beckmann JP, 2003, J ZOOL, V261, P207, DOI 10.1017/S0952836903004126
   Boydston EE, 2003, ANIM CONSERV, V6, P207, DOI 10.1017/S1367943003003263
   Broms KM, 2016, ECOLOGY, V97, P1759, DOI 10.1890/15-1471.1
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Burton AC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038007
   Carter NH, 2012, P NATL ACAD SCI USA, V109, P15360, DOI 10.1073/pnas.1210490109
   Clinchy M, 2016, BEHAV ECOL, V27, P1826, DOI 10.1093/beheco/arw117
   Cordell H.K., 2008, INT J WILDERNESS, V14, P7
   Creel S, 2008, ANIM BEHAV, V76, P1139, DOI 10.1016/j.anbehav.2008.07.006
   Darimont CT, 2015, SCIENCE, V349, P858, DOI 10.1126/science.aac4249
   Diaz-Ruiz F, 2016, J ZOOL, V298, P128, DOI 10.1111/jzo.12294
   Dickie M, 2017, J APPL ECOL, V54, P253, DOI 10.1111/1365-2664.12732
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Droge E, 2017, NAT ECOL EVOL, V1, P1123, DOI 10.1038/s41559-017-0220-9
   Efford MG, 2012, ECOSPHERE, V3, DOI 10.1890/ES11-00308.1
   Fischer J, 2007, GLOBAL ECOL BIOGEOGR, V16, P265, DOI 10.1111/j.1466-8238.2007.00287.x
   Frid A, 2002, CONSERV ECOL, V6
   Gaynor KM, 2018, SCIENCE, V360, P1232, DOI 10.1126/science.aar7121
   Gelman A, 2008, STAT MED, V27, P2865, DOI 10.1002/sim.3107
   Gutzwiller KJ, 2017, FRONT ECOL ENVIRON, V15, P517, DOI 10.1002/fee.1631
   Hansen AJ, 2005, ECOL APPL, V15, P1893, DOI 10.1890/05-5221
   Hobbs N. T., 2015, BAYESIAN MODELS STAT
   Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
   Kohl MT, 2018, ECOL MONOGR, V88, P638, DOI 10.1002/ecm.1313
   Ladle A, 2017, METHODS ECOL EVOL, V8, P329, DOI 10.1111/2041-210X.12660
   Larson CL, 2018, LANDSCAPE URBAN PLAN, V175, P62, DOI 10.1016/j.landurbplan.2018.03.009
   Larson CL, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167259
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Martin J, 2016, J STAT COMPUT SIM, V86, P3777, DOI 10.1080/00949655.2016.1186166
   Martinuzzi S, 2015, NRS8 USDA FOR SERV
   Monz CA, 2013, FRONT ECOL ENVIRON, V11, P441, DOI 10.1890/120358
   Muhly TB, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017050
   Neilson EW, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2092
   Nix JH, 2018, BEHAV PROCESS, V146, P16, DOI 10.1016/j.beproc.2017.11.002
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Ordiz A, 2011, OECOLOGIA, V166, P59, DOI 10.1007/s00442-011-1920-5
   Patten MA, 2018, BIOL CONSERV, V218, P233, DOI 10.1016/j.biocon.2017.12.033
   R Core Team, 2019, R LANG ENV STAT COMP
   Radeloff VC, 2005, ECOL APPL, V15, P799, DOI 10.1890/04-1413
   Radeloff VC, 2010, P NATL ACAD SCI USA, V107, P940, DOI 10.1073/pnas.0911131107
   Reed SE, 2011, CONSERV BIOL, V25, P504, DOI 10.1111/j.1523-1739.2010.01641.x
   Reed SE, 2008, CONSERV LETT, V1, P146, DOI 10.1111/j.1755-263X.2008.00019.x
   Reilly ML, 2017, BIOL CONSERV, V207, P117, DOI 10.1016/j.biocon.2016.11.003
   Riley SPD, 2006, J WILDLIFE MANAGE, V70, P1425, DOI 10.2193/0022-541X(2006)70[1425:SEOBAG]2.0.CO;2
   Rossi SD, 2015, APPL GEOGR, V63, P77, DOI 10.1016/j.apgeog.2015.06.008
   Royle J.A., 2008, HEIRARCHICAL MODELS
   Smith JA, 2019, LANDSCAPE URBAN PLAN, V183, P50, DOI 10.1016/j.landurbplan.2018.11.003
   Smith JA, 2018, OIKOS, V127, P890, DOI 10.1111/oik.04592
   Smith JA, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2017.0433
   Suraci JP, 2019, ECOL LETT, V22, P1578, DOI 10.1111/ele.13344
   Suraci JP, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2644
   Tablado Z, 2017, BIOL REV, V92, P216, DOI 10.1111/brv.12224
   Tucker MA, 2018, SCIENCE, V359, P466, DOI 10.1126/science.aam9712
   Venter O, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.67
   VUONG QH, 1989, ECONOMETRICA, V57, P307, DOI 10.2307/1912557
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilmers CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060590
NR 61
TC 27
Z9 30
U1 14
U2 45
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD JAN
PY 2020
VL 241
AR 108383
DI 10.1016/j.biocon.2019.108383
PG 11
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KT0IV
UT WOS:000518695100021
DA 2022-02-10
ER

PT J
AU Kitzes, J
   Schricker, L
AF Kitzes, Justin
   Schricker, Lauren
TI The Necessity, Promise and Challenge of Automated Biodiversity Surveys
   Comment
SO ENVIRONMENTAL CONSERVATION
LA English
DT Editorial Material
DE acoustic; machine learning; camera trap; citizen science
AB We are in the midst of a transformation in the way that biodiversity is observed on the planet. The approach of direct human observation, combining efforts of both professional and citizen scientists, has recently generated unprecedented amounts of data on species distributions and populations. Within just a few years, however, we believe that these data will be swamped by indirect biodiversity observations that are generated by autonomous sensors and machine learning classification models. In this commentary, we discuss three important elements of this shift towards indirect, technology-driven observations. First, we note that the biodiversity data sets available today cover a very small fraction of all places and times that could potentially be observed, which suggests the necessity of developing new approaches that can gather such data at even larger scales, with lower costs. Second, we highlight existing tools and efforts that are already available today to demonstrate the promise of automated methods to radically increase biodiversity data collection. Finally, we discuss one specific outstanding challenge in automated biodiversity survey methods, which is how to extract useful knowledge from observations that are uncertain in nature. Throughout, we focus on one particular type of biodiversity data - point occurrence records - that are frequently produced by citizen science projects, museum records and systematic biodiversity surveys. As indirect observation methods increase the spatiotemporal scope of these point occurrence records, ecologists and conservation biologists will be better able to predict shifting species distributions, track changes to populations over time and understand the drivers of biodiversity occurrence.
C1 [Kitzes, Justin; Schricker, Lauren] Univ Pittsburgh, Dept Biol Sci, Fifth & Ruskin Ave, Pittsburgh, PA 15260 USA.
RP Kitzes, J (corresponding author), Univ Pittsburgh, Dept Biol Sci, Fifth & Ruskin Ave, Pittsburgh, PA 15260 USA.
EM justin.kitzes@pitt.edu
FU Department of Biological Sciences; Mascaro Center for Sustainable
   Innovation at the University of Pittsburgh; Microsoft; National
   GeographicNational Geographic Society [NGS-55651T-18]
FX This work was supported by the Department of Biological Sciences and the
   Mascaro Center for Sustainable Innovation at the University of
   Pittsburgh, as well as Microsoft and National Geographic under grant
   NGS-55651T-18.
CR Bravo CJC, 2017, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.113
   Buxton RT, 2018, GLOB ECOL CONSERV, V16, DOI 10.1016/j.gecco.2018.e00493
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   iNaturalist, 2019, INATURALIST COMP VIS
   LifeCLEF, 2019, BIRDCLEF 2018 IMAGEC
   Marconi S, 2019, PEERJ, V7, DOI 10.7717/peerj.5843
   Microsoft, 2019, AI EARTH APIS APPL S
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Towsey M, 2014, ECOL INFORM, V21, P1, DOI 10.1016/j.ecoinf.2014.02.002
   USFWS, 2019, USFWS IND BAT SUMM S
   Zuur Alain F., 2009, P1
NR 15
TC 6
Z9 6
U1 1
U2 12
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0376-8929
EI 1469-4387
J9 ENVIRON CONSERV
JI Environ. Conserv.
PD DEC
PY 2019
VL 46
IS 4
BP 247
EP 250
DI 10.1017/S0376892919000146
PG 4
WC Biodiversity Conservation; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KI5OM
UT WOS:000511399300001
DA 2022-02-10
ER

PT J
AU Haryono, M
   Rahmat, UM
   Daryan, M
   Raharja, AS
   Muhtarom, A
   Firdaus, AY
   Rohaeti, A
   Subchiyatin, I
   Nugraheni, A
   Khairani, KO
   Kartina
AF Haryono, Mohamad
   Rahmat, Ujang Mamat
   Daryan, Muhiban
   Raharja, Agung Suci
   Muhtarom, Aom
   Firdaus, Asep Yayus
   Rohaeti, Ai
   Subchiyatin, Irma
   Nugraheni, Amila
   Khairani, Kurnia Oktalina
   Kartina
TI Monitoring of the Javan rhino population in Ujung Kulon National Park,
   Java
SO PACHYDERM
LA English
DT Article
ID SEX-RATIO
AB A monitoring project of the Javan rhino was conducted so as to understand the extent to which the growth of this population has succeeded. Monitoring was conducted by making use of camera traps, which were strategically placed by using a stratified sampling method based on the area of concentration of Javan rhino. The population size of Javan rhino in 20:13 was a minimal 58 individuals consisting of 8 calves and 50 sub adults or adults with a sex ratio of 35 males: 23 females. The birth rate was recorded at 13.79% while the mortality rate was 3.45%. We also recorded 4 new calves in 2013.
C1 [Haryono, Mohamad; Daryan, Muhiban; Raharja, Agung Suci; Muhtarom, Aom; Firdaus, Asep Yayus; Rohaeti, Ai; Subchiyatin, Irma; Nugraheni, Amila] Ujung Kulon Natl Pk Author, Pandeglang 42264, Indonesia.
   [Rahmat, Ujang Mamat] Minist Forestry, Directorate Gen Forest Protect & Nat Conservat, Directorate Biodivers Conservat, Jakarta, Indonesia.
   [Khairani, Kurnia Oktalina] Cornell Univ, Ithaca, NY 14850 USA.
   [Kartina] Tirtayasa Univ, Fac Agr, Serang 42121, Indonesia.
RP Nugraheni, A (corresponding author), Ujung Kulon Natl Pk Author, Jl P Kemerdekaan 51 Labuan, Pandeglang 42264, Indonesia.
EM amilan_tnuk@yahoo.com
RI Dr. Abid Muhtarom, S.Pd/AAL-7677-2021
OI Dr. Abid Muhtarom, S.Pd/0000-0002-2781-8086
CR Amman H., 1985, THESIS
   Colles A, 2009, ECOL LETT, V12, P849, DOI 10.1111/j.1461-0248.2009.01336.x
   Ewen JG, 2011, J ANIM ECOL, V80, P448, DOI 10.1111/j.1365-2656.2010.01774.x
   Griffiths M., 1993, JAVAN RHINO UJUNG KU
   Hariyadi ARS, 2011, PACHYDERM, P90
   Hoogenyerf A, 1970, UDJUNG KULON LAND LA
   Kemenhut (KementerianKehutanan), 1999, PER PEM NOM 7 TENT P
   Krebs CJ, 2006, MAMMALS ECOLOGICAL C
   Kusuma IH, 2008, MEDIA KONSERVASI, V13, P59
   Lyons JE, 2008, J WILDLIFE MANAGE, V72, P1683, DOI 10.2193/2008-141
   Muntasib E. K. S. H., 2002, THESIS
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P233, DOI 10.1007/978-4-431-99495-4_13
   Rahmat U. M., 2007, THESIS
   Rosenfeld CS, 2004, BIOL REPROD, V71, P1063, DOI 10.1095/biolreprod.104.030890
   Schenkel R, 1969, ACTA TROPICA SPARATU, V26
   Stokes EJ, 2011, MONITORING WILDLIFE
   Trolliet F, 2014, AGRON SOC ENV, V18, P446
   Ujung Kulon National Park, 2010, LAP SENS BAD JAW RHI
   Van Strien N. J., 2008, RHINOCEROS SONDAICUS
   Wedekind C., 2012, TOPICS CONSERVATION
NR 20
TC 1
Z9 1
U1 2
U2 30
PU IUCN-SSC ASIAN ELEPHANT SPECIALIST GROUP
PI RAJAGIRIYA
PA C/O JAYANTHA JAYEWARDENE, BIODIVERSITY & ELEPHANT CONSERVATION TRUST,
   RAJAGIRIYA GARDENS, NAWALA RD, RAJAGIRIYA, 615-32, SRI LANKA
SN 1026-2881
J9 PACHYDERM
JI Pachyderm
PD JUN-JUL
PY 2014
IS 56
BP 82
EP 86
PG 5
WC Biodiversity Conservation; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Zoology
GA CY4SY
UT WOS:000366399800008
DA 2022-02-10
ER

PT J
AU Krofel, M
   Juznic, D
   Allen, ML
AF Krofel, Miha
   Juznic, Damjan
   Allen, Maximilian L.
TI Scavenging and carcass caching behavior by European wildcat (Felis
   silvestris)
SO ECOLOGICAL RESEARCH
LA English
DT Article
DE camera traps; felids; food caching; interference competition; scavengers
AB While scavenging has been repeatedly reported for several felid species, surprisingly little information is available on scavenging behavior of the European wildcat (Felis silvestris). To fill this knowledge gap, we used camera traps to document scavenging behavior at the 48 experimentally set deer carcasses at random locations throughout the year. We recorded European wildcats scavenging on 38% of the eight carcasses set in winter and on none set in the other parts of the year. Wildcats fed on two carcasses for extended periods (up to 22 days) with an average of 3.3 visits per day and 7.8-h interval between the visits. We recorded scavenging throughout the day, but analysis indicated a crepuscular pattern. We also recorded caching behavior on 7% of the visits (n = 105), when wildcats used leaves or snow to partly or completely cover the carcasses. Beside wildcats, 12 other vertebrate species of scavengers were recorded at the carcasses. We recorded agonistic interaction with European badger (Meles meles) and despite its smaller size, the wildcat managed to defend the carcass. The extensive feeding, frequent caching behavior and active defense from scavengers indicate that the wildcats recognized the ungulate carcasses as an important food source in winter and that scavenging could be a neglected aspect of the European wildcat ecology. We also suggest that caching behavior could be regularly used by the European wildcat when feeding on larger carcasses, but was likely previously missed due to limited research effort to record scavenging and caching behavior.
C1 [Krofel, Miha; Juznic, Damjan] Univ Ljubljana, Biotech Fac, Dept Forestry, Ljubljana, Slovenia.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
RP Krofel, M (corresponding author), Univ Ljubljana, Biotech Fac, Dept Forestry, Ljubljana, Slovenia.
EM miha.krofel@bf.uni-lj.si
OI Krofel, Miha/0000-0002-2010-5219
FU Javna Agencija za Raziskovalno Dejavnost RSSlovenian Research Agency -
   Slovenia [N1-0163, P4-0059]
FX Javna Agencija za Raziskovalno Dejavnost RS, Grant/Award Numbers:
   N1-0163, P4-0059
CR Allen ML, 2016, ECOLOGY, V97, P1905, DOI 10.1002/ecy.1462
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Apostolico F, 2016, MAMMAL RES, V61, P109, DOI 10.1007/s13364-015-0255-8
   Balme GA, 2017, J ANIM ECOL, V86, P634, DOI 10.1111/1365-2656.12654
   Bang P, 2001, ANIMAL TRACKS SIGNS
   Bischoff-Mattson Z, 2009, WEST N AM NATURALIST, V69, P343, DOI 10.3398/064.069.0308
   Brighten Alex Leigh, 2019, Journal of Threatened Taxa, V11, P13492, DOI 10.11609/jott.4445.11.4.13492-13496
   Careau V, 2007, BEHAV ECOL SOCIOBIOL, V62, P87, DOI 10.1007/s00265-007-0441-z
   Cirovic D, 2016, BIOL CONSERV, V199, P51, DOI 10.1016/j.biocon.2016.04.027
   de Ruiter DJ, 2001, AFR J ECOL, V39, P396, DOI 10.1046/j.1365-2028.2001.00320.x
   Herbst M., 2009, THESIS U PRETORIA PR
   Hunter Luke, 2015, P1
   Inagaki A, 2020, ECOL EVOL, V10, P1223, DOI 10.1002/ece3.5976
   Kitchener A.C., 2017, Cat News, P3
   Koike, 2020, URSUS
   Krofel, 2016, STOPINJE SLEDOVI ZIV
   Krofel M, 2021, ECOL RES, V36, P556, DOI 10.1111/1440-1703.12211
   Krofel M, 2019, FOLIA ZOOL, V68, P274, DOI 10.25225/fozo.037.2019
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Krofel Miha, 2011, Acrocephalus, V32, P45, DOI 10.2478/v10100-011-0003-3
   MACDONALD DW, 1976, Z TIERPSYCHOL, V42, P170
   Meredith M., 2017, OVERVIEW OVERLAP PAC
   Molinari J.A., 2000, RAUBTIERE WERK HDB B
   Peers Michael J.L., 2018, Northwestern Naturalist, V99, P232
   Renard Aurelie, 2015, Mammalian Species, P78, DOI 10.1093/mspecies/sev008
   Ruiz-Villar H, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-01413-x
   Sebastian-Gonzalez E, 2019, GLOBAL CHANGE BIOL, V25, P3005, DOI 10.1111/gcb.14708
   Sebastian-Gonzalez E, 2020, ECOGRAPHY, V43, P1143, DOI 10.1111/ecog.05083
   Selva N, 2005, CAN J ZOOL, V83, P1590, DOI 10.1139/Z05-158
   Selva N, 2003, ECOSCIENCE, V10, P303, DOI 10.1080/11956860.2003.11682778
   Shackelford, 2017, ENCY ANIMAL COGNITIO, P1, DOI DOI 10.1007/978-3-319-47829-6_444-1
   Sliwa Alexander, 1994, Zoologische Garten, V64, P83
   SONERUD GA, 1986, HOLARCTIC ECOL, V9, P33
   Van Valkenburgh B, 1989, CARNIVORE BEHAV ECOL, V1, P410, DOI DOI 10.1007/978-1-4757-4716-4
   Wilmers CC, 2003, ECOL LETT, V6, P996, DOI 10.1046/j.1461-0248.2003.00522.x
   Wilson EE, 2011, TRENDS ECOL EVOL, V26, P129, DOI 10.1016/j.tree.2010.12.011
NR 36
TC 2
Z9 2
U1 1
U2 4
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0912-3814
EI 1440-1703
J9 ECOL RES
JI Ecol. Res.
PD MAY
PY 2021
VL 36
IS 3
BP 556
EP 561
DI 10.1111/1440-1703.12211
EA FEB 2021
PG 6
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SD0QN
UT WOS:000623346000001
DA 2022-02-10
ER

PT J
AU Santoro, S
   Perez, I
   Gegundez-Arias, ME
   Calzada, J
AF Santoro, Simone
   Perez, Isaac
   Gegundez-Arias, Manuel Emilio
   Calzada, Javier
TI Camera traps and artificial intelligence for monitoring invasive species
   and emerging diseases
SO ECOLOGICAL INFORMATICS
LA English
DT Editorial Material
DE Camera trapping; Artificial intelligence; Biological invasions
C1 [Santoro, Simone; Gegundez-Arias, Manuel Emilio; Calzada, Javier] Univ Huelva, Fac Ciencias Experiment, Dept Ciencias Integradas, Huelva 21007, Spain.
   [Perez, Isaac] Univ Huelva, Dept Tecnol Informaci prime, Escuela Tecnica Super Ingn, Huelva 21007, Spain.
   [Perez, Isaac; Gegundez-Arias, Manuel Emilio] Univ Huelva, Dept Sistemas Vision Predicci Optimizac Control, Centro Cientifico Tecnol Huelva, Huelva 21007, Spain.
RP Santoro, S (corresponding author), Univ Huelva, Fac Ciencias Experiment, Dept Ciencias Integradas, Huelva 21007, Spain.
EM simone.santoro@dci.uhu.es
RI SANTORO, SIMONE/B-2162-2015
OI SANTORO, SIMONE/0000-0003-0986-3278
FU The Fundacion Biodiversidad del Ministerio para la Transicion Ecologica
   y el Reto Demografico (MITECO)
FX The Fundacion Biodiversidad del Ministerio para la Transicion Ecologica
   y el Reto Demografico (MITECO) funded the research reported in this
   manuscript (Project AI-CENSUS) .
CR Crowl TA, 2008, FRONT ECOL ENVIRON, V6, P238, DOI 10.1890/070151
   Dunn AM, 2015, TRENDS PARASITOL, V31, P189, DOI 10.1016/j.pt.2014.12.003
   Hughes J, 2013, BIOL CONSERV, V157, P341, DOI 10.1016/j.biocon.2012.07.005
   Lowe S., 2000, 100 WORLDS WORST INV
   Marchetti M.P., 2013, INVASION ECOLOGY
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Sandino J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18040944
   Scott DM, 2020, URBAN ECOSYST, V23, P1127, DOI 10.1007/s11252-020-00985-5
NR 8
TC 0
Z9 0
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAR
PY 2022
VL 67
AR 101491
DI 10.1016/j.ecoinf.2021.101491
PG 2
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA XM8PN
UT WOS:000729081800008
OA Bronze
DA 2022-02-10
ER

PT C
AU Vaquero, D
   Turk, M
AF Vaquero, Daniel
   Turk, Matthew
GP IEEE
TI Composition Context Photography
SO 2015 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
SE IEEE Winter Conference on Applications of Computer Vision
LA English
DT Proceedings Paper
CT IEEE Winter Conference on Applications of Computer Vision (WACV 2015)
CY JAN 06-09, 2015
CL Waikoloa, HI
SP IEEE, IEEE Comp Soc, IEEE Biometrics Council, Amazon
AB Cameras are becoming increasingly aware of the picture-taking context, collecting extra information around the act of photographing. This contextual information enables the computational generation of a wide range of enhanced photographic outputs, effectively expanding the imaging experience provided by consumer cameras. Computer vision and computational photography techniques can be applied to provide image composites, such as panoramas, high dynamic range images, and stroboscopic images, as well as automatically selecting individual alternative frames. Our technology can be integrated into point-and-shoot cameras, and it effectively expands the photographic possibilities for casual and amateur users, who often rely on automatic camera modes.
C1 [Vaquero, Daniel] Nokia Technol, Sunnyvale, CA 94086 USA.
   [Vaquero, Daniel; Turk, Matthew] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
RP Vaquero, D (corresponding author), Nokia Technol, Sunnyvale, CA 94086 USA.
EM daniel.vaquero@nokia.com; mturk@cs.ucsb.edu
CR Adams A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778766
   Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bourke Steven, 2011, P 16 INT C INT US IN, P13, DOI DOI 10.1145/1943403.1943408
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Cohen MF, 2006, COMPUTER, V39, P40, DOI 10.1109/MC.2006.281
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369
   Duchowski A., 2007, EYE TRACKING METHODO
   Eisemann E., 2004, ACM T GRAPHIC, V23, P673
   Fiss J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024162
   Hakansson Maria, 2006, P 4 NORD C HUM COMP, P262, DOI 10.1145/1182475.1182503
   Holleis P, 2005, 25th IEEE International Conference on Distributed Computing Systems Workshops, Proceedings, P536, DOI 10.1109/ICDCSW.2005.33
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Mihal A., ENBLEND ENFUSE 4 0
   NOMURA Y., 2007, P 18 EUR C REND TECH, P127
   Peterson B. F., 2008, UNDERSTANDING SHUTTE
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251
   Telleen J, 2007, COMPUT GRAPH FORUM, V26, P591, DOI 10.1111/j.1467-8659.2007.01082.x
   Vaquero D. A., 2012, THESIS
   Wagner D, 2010, P IEEE VIRT REAL ANN, P211, DOI 10.1109/VR.2010.5444786
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
NR 24
TC 0
Z9 1
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2472-6737
BN 978-1-4799-6683-7
J9 IEEE WINT CONF APPL
PY 2015
BP 649
EP 656
DI 10.1109/WACV.2015.92
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BF3EN
UT WOS:000380532600085
DA 2022-02-10
ER

PT J
AU Allen, ML
   Wang, SD
   Olson, LO
   Li, Q
   Krofel, M
AF Allen, Maximilian L.
   Wang, Shaodong
   Olson, Lucas O.
   Li, Qing
   Krofel, Miha
TI Counting cats for conservation: seasonal estimates of leopard density
   and drivers of distribution in the Serengeti
SO BIODIVERSITY AND CONSERVATION
LA English
DT Article
DE Competition; Leopard; Panthera pardus; Population density; Predator-prey
   dynamics; Seasonal variation; Spatially explicit capture recapture
ID PANTHERA-PARDUS; CAPTURE-RECAPTURE; LARGE CARNIVORES; ACTIVITY PATTERNS;
   LUANGWA VALLEY; NATIONAL-PARK; SOUTH-AFRICA; MANAGEMENT; MODELS; LIONS
AB Large carnivore conservation is important for ecosystem integrity and understanding drivers of their abundance is essential to guide conservation efforts. Leopard (Panthera pardus) populations are in a general state of decline, although local studies demonstrated large variation in their population trends and density estimates vary widely across their range. We used spatially-explicit capture-recapture models for unmarked populations with camera trap data from a citizen science project to estimate previously-unknown leopard population densities in Serengeti National Park, Tanzania, and determine potential biological drivers of their abundance and distribution. We estimated leopard densities, at 5.41 (95% CrI = 2.23-9.26) and 5.72 (95% CrI = 2.44-9.55) individuals/100 km(2), in the dry and wet season, respectively, which confirmed Serengeti National Park as one of the strongholds of this species in Africa. In contrast to abundance estimates, we found that drivers of leopard abundance and distribution varied among the dry and wet seasons, and were primarily affected by interactions with other larger carnivores and cover. The underlying driver of leopard distribution may be the dynamic prey availability which shifts between seasons, leading to an avoidance of dominant carnivores when prey availability is low in the dry season but an association with dominant carnivores when prey availability is high in the wet season. As efforts to conserve large carnivore populations increase worldwide, our results highlight the benefits of using data from citizen science projects, including large camera-trapping surveys, to estimate local carnivore abundances. Using a Bayesian framework allows of estimation of population density, but it is also important to understand the factors that dictate their distribution across the year to inform conservation efforts.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Wang, Shaodong; Li, Qing] Iowa State Univ, Dept Ind & Mfg Syst Engn, 3004 Black Engn Bldg,2529 Union Dr, Ames, IA 50011 USA.
   [Olson, Lucas O.] Univ Wisconsin, 1630 Linden Dr, Madison, WI 53706 USA.
   [Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Vecna Pot 83, Ljubljana 1000, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X; Li, Qing/0000-0002-3069-3878
FU Slovenian Research AgencySlovenian Research Agency - Slovenia [P4-0059]
FX We thank the Snapshot Serengeti team (www.snapshotserengeti.org) for
   generously providing the data for this study, and Ali Swanson
   specifically for comments on earlier drafts that greatly improved the
   manuscript. We thank the Illinois Natural History Survey, University of
   Illinois and Slovenian Research Agency (Grant P4-0059) for funding.
CR Allen ML, 2019, MAMMALIA, V83, P552, DOI 10.1515/mammalia-2017-0162
   Allen ML, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30988-4
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Anderson D.R., 2003, MODEL SELECTION MULT
   Athreya V, 2011, CONSERV BIOL, V25, P133, DOI 10.1111/j.1523-1739.2010.01599.x
   Balme G, 2019, POPUL ECOL, V61, P256, DOI 10.1002/1438-390X.1023
   Balme GA, 2017, BEHAV ECOL, V28, P1348, DOI 10.1093/beheco/arx098
   Balme GA, 2017, J ANIM ECOL, V86, P634, DOI 10.1111/1365-2656.12654
   Balme GA, 2009, BIOL CONSERV, V142, P2681, DOI 10.1016/j.biocon.2009.06.020
   BORNER M, 1987, OECOLOGIA, V73, P32, DOI 10.1007/BF00376974
   Broekhuis F, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0153875
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chapman S, 2010, S AFR J WILDL RES, V40, P114, DOI 10.3957/056.040.0202
   Chapron G, 2014, SCIENCE, V346, P1517, DOI 10.1126/science.1257553
   Dalerum F, 2008, BIODIVERS CONSERV, V17, P2939, DOI 10.1007/s10531-008-9406-4
   Datta A, 2008, BIOL CONSERV, V141, P1429, DOI 10.1016/j.biocon.2008.02.022
   Davis CL, 2018, ECOL LETT, V21, P1401, DOI 10.1111/ele.13124
   Devens C, 2018, AFR J ECOL, V56, P850, DOI 10.1111/aje.12512
   du Preez B, 2015, ANIM BEHAV, V100, P22, DOI 10.1016/j.anbehav.2014.10.025
   Durant SM, 2017, P NATL ACAD SCI USA, V114, P528, DOI 10.1073/pnas.1611122114
   Durant SM, 1998, J ANIM ECOL, V67, P370, DOI 10.1046/j.1365-2656.1998.00202.x
   Gray TNE, 2012, J WILDLIFE MANAGE, V76, P163, DOI 10.1002/jwmg.230
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Gelman A., 1992, STAT SCI, V7, P457, DOI [10.1214/ss/1177011136., 10.1214/ss/1177011136, DOI 10.1214/SS/1177011136]
   Gese EM, 2001, CONSERV BIOL SER, V5, P372
   Grey JNC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082832
   Hamilton P. H., 1981, LEOPARD PANTHERA PAR
   Harihar A, 2011, J APPL ECOL, V48, P806, DOI 10.1111/j.1365-2664.2011.01981.x
   Harmsen BJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179505
   Hayward MW, 2007, ORYX, V41, P205, DOI 10.1017/S0030605307001767
   Hayward MW, 2006, J ZOOL, V270, P298, DOI 10.1111/j.1469-7998.2006.00139.x
   Hayward MW, 2007, BIOL CONSERV, V139, P219, DOI 10.1016/j.biocon.2007.06.018
   Hopcraft JGC, 2005, J ANIM ECOL, V74, P559, DOI 10.1111/j.1365-2656.2005.00955.x
   Jacobson AP, 2016, PEERJ, V4, DOI 10.7717/peerj.1974
   Kissui BM, 2008, ANIM CONSERV, V11, P422, DOI 10.1111/j.1469-1795.2008.00199.x
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   KRUUK H., 1967, MAMMALIA, V31, P1, DOI 10.1515/mamm.1967.31.1.1
   Lamichhane BR, 2019, BIODIVERS CONSERV, V28, P1473, DOI 10.1007/s10531-019-01737-4
   Lassen KG, 2005, RETROVIROLOGY, V2, DOI 10.1186/1742-4690-2-S1-S74
   Marker LL, 2005, S AFR J WILDL RES, V35, P105
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   Mduma SAR, 1999, J ANIM ECOL, V68, P1101, DOI 10.1046/j.1365-2656.1999.00352.x
   Mizutani F, 1998, J ZOOL, V244, P269, DOI 10.1111/j.1469-7998.1998.tb00031.x
   Mkonyi FJ, 2018, AFR J ECOL, V56, P972, DOI 10.1111/aje.12528
   Mysterud A, 2010, J APPL ECOL, V47, P920, DOI 10.1111/j.1365-2664.2010.01836.x
   NORTON-GRIFFITHS M, 1975, East African Wildlife Journal, V13, P347, DOI 10.1111/j.1365-2028.1975.tb00144.x
   O'Brien TG, 2011, ECOL APPL, V21, P2908, DOI 10.1890/10-2284.1
   Packer C, 2011, CONSERV BIOL, V25, P142, DOI 10.1111/j.1523-1739.2010.01576.x
   Packer C, 2003, ECOL LETT, V6, P797, DOI 10.1046/j.1461-0248.2003.00500.x
   Packer C, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005941
   Power R. J., 2002, Koedoe, V45, P67
   Qi JZ, 2015, BIOL CONSERV, V191, P258, DOI 10.1016/j.biocon.2015.06.034
   R Core Team, 2018, R R PROJ STAT COMP
   Ray-Brambach RR, 2018, MAMM BIOL, V92, P102, DOI 10.1016/j.mambio.2017.11.002
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Rich LN, 2019, BIOL CONSERV, V233, P12, DOI 10.1016/j.biocon.2019.02.018
   Rich LN, 2014, J MAMMAL, V95, P382, DOI 10.1644/13-MAMM-A-126
   Rich LN, 2013, J WILDLIFE MANAGE, V77, P1280, DOI 10.1002/jwmg.562
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Rosenblatt E, 2016, ECOL EVOL, V6, P3772, DOI 10.1002/ece3.2155
   Rovero F., 2016, CAMERA TRAPPING WILD, P1
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Schaller G. B., 1976, SERENGETI LION STUDY
   Sinclair ARE, 2007, CONSERV BIOL, V21, P580, DOI 10.1111/j.1523-1739.2007.00699.x
   Sinclair ARE, 1995, SERENGETI
   Stein AB, 2020, IUCN RED LIST THREAT
   Stein AB, 2015, AFR J WILDL RES, V45, P247, DOI 10.3957/056.045.0247
   Steinmetz R, 2013, BIOL CONSERV, V163, P68, DOI 10.1016/j.biocon.2012.12.016
   Strampelli P, 2020, ORYX, V54, P405, DOI 10.1017/S0030605318000121
   Strampelli P, 2018, MAMM BIOL, V89, P14, DOI 10.1016/j.mambio.2017.12.003
   Swanepoel LH, 2015, WILDLIFE BIOL, V21, P263, DOI 10.2981/wlb.00108
   Swanson A, 2016, ECOL EVOL, V6, P8534, DOI 10.1002/ece3.2569
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swanson A, 2014, J ANIM ECOL, V83, P1418, DOI 10.1111/1365-2656.12231
   Vanak AT, 2013, ECOLOGY, V94, P2619, DOI 10.1890/13-0217.1
   Veldhuis MP, 2019, SCIENCE, V363, P1424, DOI 10.1126/science.aav0564
   Zeileis A, 2008, J STAT SOFTW, V27, P1, DOI 10.18637/jss.v027.i08
NR 79
TC 3
Z9 3
U1 4
U2 19
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0960-3115
EI 1572-9710
J9 BIODIVERS CONSERV
JI Biodivers. Conserv.
PD NOV
PY 2020
VL 29
IS 13
BP 3591
EP 3608
DI 10.1007/s10531-020-02039-w
EA AUG 2020
PG 18
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA OC3UE
UT WOS:000561289700001
OA Green Published
DA 2022-02-10
ER

PT J
AU Richardson, ML
AF Richardson, Matthew L.
TI DAILY AND MONTHLY ACTIVITY OF BROWN BEARS (URSUS ARCTOS) NEAR A PROPOSED
   INDUSTRIAL PROJECT IN COASTAL BRITISH COLUMBIA
SO WESTERN NORTH AMERICAN NATURALIST
LA English
DT Article
ID ACTIVITY PATTERNS; GRIZZLY BEARS; SALMON; HABITUATION; BEHAVIOR; HUMANS;
   BLACK
AB The Kitimat Liquefied Natural Gas (KLNG) Plant is proposed for construction adjacent to Bish Creek (Kitimat, British Columbia, Canada). Bish Creek is a corridor for brown bears (Ursus arctos), and 8 camera traps were deployed along the creek for 1442 trapping days in 2014 to determine baseline activity of brown bears. Brown bear activity varied across weeks, peaking particularly in July and September. Within a 24-h day, bears were commonly photographed during hours 5, 6, and 21 and uncommonly photographed during the 3 hours preceding noon and a 4-h period in the afternoon. However, the time of day that bears were photographed varied across seasons; bears were more commonly photographed during the day in July and at night in September. Understanding this change in activity across seasons will inform management of bear resources and human activities on-site to avoid human-bear interactions.
C1 [Richardson, Matthew L.] Smithsonian Conservat Biol Inst, Ctr Conservat & Sustainabil, Washington, DC 20560 USA.
   [Richardson, Matthew L.] Univ Dist Columbia, Urban Sustainabil & Environm Sci, Coll Agr, Washington, DC 20008 USA.
RP Richardson, ML (corresponding author), Smithsonian Conservat Biol Inst, Ctr Conservat & Sustainabil, Washington, DC 20560 USA.
EM matthew.richardson@udc.edu
FU Kitimat LNG Operating General Partnership; Smithsonian
   InstitutionSmithsonian Institution
FX I thank Jonquil Crosby, Mike Stekelenburg, and Al Hummel for field
   assistance, Danny Aiuto for database management, Sulema Castro for
   logistical support, Rob Heibein, Sheryl Maruca, and Maria Hartley for
   constructive discussions and site access, Ashley Myers for help creating
   Figure 1, and Alfonso Alonso and Francisco Dallmeier for help
   administering the research program. Funding was provided by the Kitimat
   LNG Operating General Partnership through an independent research
   agreement with the Smithsonian Institution.
CR Fortin JK, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0141983
   Fortin JK, 2013, J MAMMAL, V94, P833, DOI 10.1644/12-MAMM-A-238.1
   GARD R, 1971, J WILDLIFE MANAGE, V35, P193, DOI 10.2307/3799591
   Gende SM, 2001, OECOLOGIA, V127, P372, DOI 10.1007/s004420000590
   HERRERO S., 2002, BEAR ATTACKS THEIR C
   JOPE KL, 1985, WILDLIFE SOC B, V13, P32
   Kaczensky P, 2006, J ZOOL, V269, P474, DOI 10.1111/j.1469-7998.2006.00114.x
   Klinka DR, 2002, CAN J ZOOL, V80, P1317, DOI 10.1139/Z02-123
   Machutchon AG, 1998, URSUS-SERIES, V10, P539
   MCLELLAN BN, 1988, J APPL ECOL, V25, P451, DOI 10.2307/2403836
   McLellan ML, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0117734
   Munro RHM, 2006, J MAMMAL, V87, P1112, DOI 10.1644/05-MAMM-A-410R3.1
   Nevin OT, 2005, BIOL CONSERV, V121, P611, DOI 10.1016/j.biocon.2004.06.011
   Olson TL, 1998, URSUS-SERIES, V10, P547
   Ordiz A, 2013, J APPL ECOL, V50, P306, DOI 10.1111/1365-2664.12047
   SAS Institute, 2008, SAS STAT US GUID PER
   Schwartz CC, 2010, J WILDLIFE MANAGE, V74, P1628, DOI 10.2193/2009-571
   Smith TS, 2005, URSUS, V16, P1, DOI 10.2192/1537-6176(2005)016[0001:ABBHAH]2.0.CO;2
   Wheat RE, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1408
NR 19
TC 2
Z9 2
U1 1
U2 9
PU BRIGHAM YOUNG UNIV
PI PROVO
PA 290 LIFE SCIENCE MUSEUM, PROVO, UT 84602 USA
SN 1527-0904
EI 1944-8341
J9 WEST N AM NATURALIST
JI West. North Am. Naturalist
PD MAR
PY 2017
VL 77
IS 1
BP 118
EP 123
DI 10.3398/064.077.0113
PG 6
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA EX9KV
UT WOS:000403577800012
OA Green Published
DA 2022-02-10
ER

PT J
AU Shi, CM
   Liu, D
   Cui, YL
   Xie, JJ
   Roberts, NJ
   Jiang, GS
AF Shi, Chunmei
   Liu, Dan
   Cui, Yonglu
   Xie, Jiajun
   Roberts, Nathan James
   Jiang, Guangshun
TI Amur tiger stripes: individual identification based on deep
   convolutional neural network
SO INTEGRATIVE ZOOLOGY
LA English
DT Article
DE Amur tiger; deep convolutional neural network; individual
   identification; stripe feature
ID PANTHERA-TIGRIS; CONSERVATION; DOGS
AB The automatic individual identification of Amur tigers (Panthera tigris altaica) is important for population monitoring and making effective conservation strategies. Most existing research primarily relies on manual identification, which does not scale well to large datasets. In this paper, the deep convolution neural networks algorithm is constructed to implement the automatic individual identification for large numbers of Amur tiger images. The experimental data were obtained from 40 Amur tigers in Tieling Guaipo Tiger Park, China. The number of images collected from each tiger was approximately 200, and a total of 8277 images were obtained. The experiments were carried out on both the left and right side of body. Our results suggested that the recognition accuracy rate of left and right sides are 90.48% and 93.5%, respectively. The accuracy of our network has achieved the similar level compared to other state of the art networks like LeNet, ResNet34, and ZF_Net. The running time is much shorter than that of other networks. Consequently, this study can provide a new approach on automatic individual identification technology in the case of the Amur tiger.
C1 [Shi, Chunmei; Xie, Jiajun] Northeast Forestry Univ, Sch Sci, Dept Math, Harbin, Peoples R China.
   [Shi, Chunmei; Cui, Yonglu; Roberts, Nathan James; Jiang, Guangshun] Northeast Forestry Univ, Coll Wildlife & Protected Areas, Feline Res Ctr, Natl Forestry & Grassland Adm, Harbin 150040, Peoples R China.
   [Liu, Dan] Siberian Tiger Pk, Harbin, Heilongjiang, Peoples R China.
RP Jiang, GS (corresponding author), Northeast Forestry Univ, Coll Wildlife & Protected Areas, Feline Res Ctr, Natl Forestry & Grassland Adm, Harbin 150040, Peoples R China.
EM jgshun@126.com
OI Jiang, Guangshun/0000-0001-6321-9489
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [2572018BC07, 2572017PZ14];
   Heilongjiang postdoctoral project fund project [LBH-Z18003];
   Biodiversity Survey, Monitoring and Assessment Project of Ministry of
   Ecology and Environment, China [2019HB2096001006]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   (NSFC) [NSFC 31872241, 31572285]; Individual Identification
   Technological Research on Camera-trapping images of Amur tigers (NFGA
   2017)
FX This study was funded by the Fundamental Research Funds for the Central
   Universities (2572018BC07; 2572017PZ14), the Heilongjiang postdoctoral
   project fund project (LBH-Z18003), Biodiversity Survey, Monitoring and
   Assessment Project of Ministry of Ecology and Environment, China
   (2019HB2096001006), the National Natural Science Foundation of China
   (NSFC 31872241; 31572285), the Individual Identification Technological
   Research on Camera-trapping images of Amur tigers (NFGA 2017).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Alibhai Sky K., 2008, Endangered Species Research, V4, P205, DOI 10.3354/esr00067
   Caelen O, 2017, ANN MATH ARTIF INTEL, V81, P429, DOI 10.1007/s10472-017-9564-8
   Caragiulo A, 2015, CONSERV GENET RESOUR, V7, P681, DOI 10.1007/s12686-015-0476-9
   Cheema GS, 2017, JOINT EUR C MACH LEA, P27
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Cho YS, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3433
   Courville A, 2016, DEEP LEARNING
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gibb R, 2019, METHODS ECOL EVOL, V10, P169, DOI 10.1111/2041-210X.13101
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Gulli A., 2017, DEEP LEARNING KERAS
   Kalafi EY, 2018, FOLIA MORPHOL, V77, P179, DOI 10.5603/FM.a2017.0079
   Karanth KU, 2010, TIGERS OF THE WORLD: THE SCIENCE, POLITICS, AND CONSERVATION OF PANTHERA TIGRIS, 2ND EDITION, P241
   Kerley LL, 2007, J WILDLIFE MANAGE, V71, P1349, DOI 10.2193/2006-361
   Kerley LL, 2010, INTEGR ZOOL, V5, P390, DOI 10.1111/j.1749-4877.2010.00217.x
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Marsh DM, 2008, CONSERV BIOL, V22, P647, DOI 10.1111/j.1523-1739.2008.00927.x
   Mason Aaron D, 2012, P 6 IEEE INT C DIG E, P1, DOI DOI 10.1109/DEST.2012.6227943
   Norouzzadeh M. S., 2018, PNAS, V115, P5716
   Riordan P, 1998, ANIM CONSERV, V1, P253, DOI 10.1111/j.1469-1795.1998.tb00036.x
   Sharma S, 2005, J ZOOL, V267, P9, DOI 10.1017/S0952836905007119
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xu X, 2017, CELL RES, V27, P954, DOI 10.1038/cr.2017.32
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 30
TC 2
Z9 3
U1 4
U2 16
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1749-4877
EI 1749-4869
J9 INTEGR ZOOL
JI Integr. Zool.
PD NOV
PY 2020
VL 15
IS 6
BP 461
EP 470
DI 10.1111/1749-4877.12453
EA JUN 2020
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA OI5IR
UT WOS:000543140800001
PM 32329957
DA 2022-02-10
ER

PT J
AU Xie, JJ
   Li, AQ
   Zhang, JG
   Cheng, ZA
AF Xie, Jiangjian
   Li, Anqi
   Zhang, Junguo
   Cheng, Zhean
TI An Integrated Wildlife Recognition Model Based on Multi-Branch
   Aggregation and Squeeze-And-Excitation Network
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE wildlife recognition; SE-ResNeXt; deep convolutional neural network
AB Infrared camera trapping, which helps capture large volumes of wildlife images, is a widely-used, non-intrusive monitoring method in wildlife surveillance. This method can greatly reduce the workload of zoologists through automatic image identification. To achieve higher accuracy in wildlife recognition, the integrated model based on multi-branch aggregation and Squeeze-and-Excitation network is introduced. This model adopts multi-branch aggregation transformation to extract features, and uses Squeeze-and-Excitation block to adaptively recalibrate channel-wise feature responses based on explicit self-mapped interdependencies between channels. The efficacy of the integrated model is tested on two datasets: the Snapshot Serengeti dataset and our own dataset. From experimental results on the Snapshot Serengeti dataset, the integrated model applies to the recognition of 26 wildlife species, with the highest accuracies in Top-1 (when the correct class is the most probable class) and Top-5 (when the correct class is within the five most probable classes) at 95.3% and 98.8%, respectively. Compared with the ROI-CNN algorithm and ResNet (Deep Residual Network), on our own dataset, the integrated model, shows a maximum improvement of 4.4% in recognition accuracy.
C1 [Xie, Jiangjian; Li, Anqi; Zhang, Junguo; Cheng, Zhean] Beijing Forestry Univ, Sch Technol, Beijing 100083, Peoples R China.
   [Xie, Jiangjian; Li, Anqi; Zhang, Junguo; Cheng, Zhean] Adm Forestry Equipment & Automat, Key Lab State Forestry & Grassland, Beijing 100083, Peoples R China.
RP Zhang, JG (corresponding author), Beijing Forestry Univ, Sch Technol, Beijing 100083, Peoples R China.; Zhang, JG (corresponding author), Adm Forestry Equipment & Automat, Key Lab State Forestry & Grassland, Beijing 100083, Peoples R China.
EM zhangjunguo@bjfu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [31670553]; Natural Science Foundation of
   Beijing MunicipalityBeijing Natural Science Foundation [6192019];
   Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [2016ZCQ08]; Baidu
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 31670553, the Natural Science Foundation of
   Beijing Municipality under Grant No. 6192019. and Fundamental Research
   Funds for the Central Universities under Grant No. 2016ZCQ08. The
   authors also thank Baidu for its financial support for the
   "Paddlepaddle-basedWildlife Identification and Classification System"
   project.
CR Chen S., 2017, MOD MANUF TECHNOL EQ, V3, P64
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang J, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9091829
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kamencay P, 2016, 2016 ELEKTRO 11TH INTERNATIONAL CONFERENCE, P62, DOI 10.1109/ELEKTRO.2016.7512036
   Kone I, 2018, LECT NOTES COMPUT SC, V10882, P796, DOI 10.1007/978-3-319-93000-8_90
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LIU W, 2018, J BEIJING FOR UNIV, V40, P124, DOI DOI 10.3969/J.ISSN.1674-0858.2018.06.1
   Manousakis Nikolaos M., 2016, 2016 Power Systems Computation Conference (PSCC), P1, DOI 10.1109/PSCC.2016.7540813
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   McAllister P, 2018, COMPUT BIOL MED, V95, P217, DOI 10.1016/j.compbiomed.2018.02.008
   Na L., 2011, NATURE MONITORING WI
   Nair V., 2010, P 27 INT C MACH LEAR, P807
   Nigati K., 2019, J AGR MACH, V50, P217
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Owoeye K., 2018, P WORKSH MOD DEC MAK
   Pawara P., 2016, P 2016 IEEE S SER CO, P1, DOI 10.1109/ SSCI.2016.7850111
   Peng M, 2018, IEEE INT CONF AUTOMA, P790, DOI 10.1109/FG.2018.00127
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7298594
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tang YX, 2016, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2016.233
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yunlong Y., 2018, COMPUT INTEL NEUROSC, V2018
   Zhang K, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/8214975
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zisserman A, 2015, INT C LEARN REPR ICL
NR 28
TC 5
Z9 5
U1 1
U2 8
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD JUL 2
PY 2019
VL 9
IS 14
AR 2794
DI 10.3390/app9142794
PG 14
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Materials Science; Physics
GA IN9VN
UT WOS:000479026900022
OA gold
DA 2022-02-10
ER

PT J
AU Zitzmann, F
   Reich, M
   Schaarschmidt, F
AF Zitzmann, Felix
   Reich, Michael
   Schaarschmidt, Frank
TI Potential of small-scale and structurally diverse short-rotation coppice
   as habitat for large and medium-sized mammals
SO BIOLOGIA
LA English
DT Article
DE Perennial woody biomass crops; Bioenergy; Biodiversity; Wildlife; Game;
   Camera trapping
AB We surveyed occurrence and activity of large and medium-sized mammals on three experimental short-rotation coppice (SRC) and three afforestations by camera trapping. Both habitat types were surveyed simultaneously in spring. Additional wintertime surveys were performed on the SRC to consider seasonal aspects of habitat utilisation. In spring, SRC and afforestations were predominantly used by the same species. European hare (Lepus europaeus) and roe deer (Capreolus capreolus) were the most active species across all sites. Additionally, the European rabbit (Oryctolagus cuniculus) showed intense activity on one SRC site. Activity of carnivorous and omnivorous species was comparatively low in both habitat types, but even lower on the SRC. The only forest-associated species (European badger Meles meles), detected on all afforestations, was absent from the SRC. In winter, the surveyed SRC were used by the same species as in spring. Most species showed similar activity on the SRC in both seasons. We conclude that small-scale and structurally diverse SRC provide suitable habitat, in different seasons, especially for herbivorous mammals associated with farmland and forest-ecotones rather than forest species. The extent to which our results can be generalised to large-scale commercial SRC is unclear. However, the results indicate that SRC can be managed in a manner compatible with wildlife and may then have a habitat function for mammals comparable to that of young afforestations. Creation of within-plantation heterogeneity can be a suitable measure to improve habitat quality and should, therefore, be considered in the design and management of SRC.
C1 [Zitzmann, Felix; Reich, Michael] Leibniz Univ Hannover, Inst Environm Planning, Herrenhauser Str 2, D-30419 Hannover, Germany.
   [Schaarschmidt, Frank] Leibniz Univ Hannover, Inst Cell Biol & Biophys, Biostat Dept, Herrenhauser Str 2, D-30419 Hannover, Germany.
RP Zitzmann, F (corresponding author), Leibniz Univ Hannover, Inst Environm Planning, Herrenhauser Str 2, D-30419 Hannover, Germany.
EM zitzmann@umwelt.uni-hannover.de
OI Zitzmann, Felix/0000-0002-7732-1860; Reich, Michael/0000-0001-5401-1993
FU Lower Saxony Ministry of Food, Agriculture and Consumer Protection
   (Niedersachsisches Ministerium fur Ernahrung, Landwirtschaft und
   Verbraucherschutz) [105.2-3234/1-13-4]
FX Open Access funding enabled and organized by Projekt DEAL. This work was
   supported by the Lower Saxony Ministry of Food, Agriculture and Consumer
   Protection (Niedersachsisches Ministerium fur Ernahrung, Landwirtschaft
   und Verbraucherschutz, ML), Grant No. 105.2-3234/1-13-4.
CR Armenteros JA, 2021, INTEGR ZOOL, V16, P226, DOI 10.1111/1749-4877.12496
   Baum S, 2009, LANDBAUFORSCH VOLK, V59, P163
   Bergstrom R, 2002, BIOMASS BIOENERG, V23, P27, DOI 10.1016/S0961-9534(02)00027-2
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campbell SP, 2012, BIOMASS BIOENERG, V47, P342, DOI 10.1016/j.biombioe.2012.09.026
   CHRISTIAN DP, 1994, BIOMASS BIOENERG, V6, P31, DOI 10.1016/0961-9534(94)90082-5
   Christian DP, 1997, BIOMASS BIOENERG, V12, P35, DOI 10.1016/S0961-9534(96)00062-1
   Christian DP, 1998, BIOMASS BIOENERG, V14, P395, DOI 10.1016/S0961-9534(97)10076-9
   Christian DP, 1997, J WILDLIFE MANAGE, V61, P171, DOI 10.2307/3802426
   Dauber J, 2010, GCB BIOENERGY, V2, P289, DOI 10.1111/j.1757-1707.2010.01058.x
   Desiree JI, 2014, GCB BIOENERGY, V6, P183, DOI 10.1111/gcbb.12067
   Dimitriou I., 2015, SUSTAINABLE SHORT RO
   Don A, 2012, GCB BIOENERGY, V4, P372, DOI 10.1111/j.1757-1707.2011.01116.x
   Eggers J, 2009, GCB BIOENERGY, V1, P18, DOI 10.1111/j.1757-1707.2009.01002.x
   Englund O, 2020, WIRES ENERGY ENVIRON, V9, DOI 10.1002/wene.375
   Everaars J, 2014, GCB BIOENERGY, V6, P252, DOI 10.1111/gcbb.12135
   Fletcher RJ, 2011, FRONT ECOL ENVIRON, V9, P161, DOI 10.1890/090091
   Gepp N, 2015, NATURSCHUTZ LANDSCHA, V48, P287
   Giordano M, 2009, HYSTRIX, V20, P127
   GORANSSON G, 1994, BIOMASS BIOENERG, V6, P49, DOI 10.1016/0961-9534(94)90084-1
   Gruss Holger, 2011, Naturschutz und Landschaftsplanung, V43, P197
   GUSTAFSSON L, 1987, FOREST ECOL MANAG, V21, P141, DOI 10.1016/0378-1127(87)90078-8
   Halekoh U, 2006, J STAT SOFTW, V15, P1, DOI 10.18637/jss.v015.i02
   Hanowski JM, 1997, CONSERV BIOL, V11, P936, DOI 10.1046/j.1523-1739.1997.96173.x
   Hilbe J., 2013, GEN ESTIMATING EQUAT, Vsecond
   Huston MA, 2003, J ENVIRON MANAGE, V67, P77, DOI 10.1016/S0301-4797(02)00190-1
   Keuling O, 2011, EUR J WILDLIFE RES, V57, P95, DOI 10.1007/s10344-010-0403-z
   Meehan TD, 2010, P NATL ACAD SCI USA, V107, P18533, DOI 10.1073/pnas.1008475107
   Moser BW, 2002, NORTHWEST SCI, V76, P158
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Petrovan SO, 2017, EUR J WILDLIFE RES, V63, DOI 10.1007/s10344-017-1106-5
   R Core Team, 2019, R VERS 361 R LANG EN
   Robertson GP, 2008, SCIENCE, V322, P49, DOI 10.1126/science.1161525
   Rowe RL, 2011, BIOMASS BIOENERG, V35, P325, DOI 10.1016/j.biombioe.2010.08.046
   Rowe RL, 2009, RENEW SUST ENERG REV, V13, P271, DOI 10.1016/j.rser.2007.07.008
   SAGE RB, 1994, BIOMASS BIOENERG, V6, P41, DOI 10.1016/0961-9534(94)90083-3
   Sage RB, 1998, BIOMASS BIOENERG, V15, P39, DOI 10.1016/S0961-9534(97)10055-1
   Sage R, 2006, IBIS, V148, P184, DOI 10.1111/j.1474-919X.2006.00522.x
   Sauerbrei R, 2014, GCB BIOENERGY, V6, P265, DOI 10.1111/gcbb.12146
   Schulz U., 2008, COTTBUSER SCHRIFTEN, V6, P167
   Schulz U, 2009, LANDBAUFORSCH VOLK, V59, P171
   Smith RK, 2004, J APPL ECOL, V41, P1092, DOI 10.1111/j.0021-8901.2004.00976.x
   Taylor G., 2016, PERENNIAL BIOMASS CR, V319, P3, DOI [10.1007/978-3-319-44530-4_1, DOI 10.1007/978-3-319-44530-4_1]
   Vanbeveren SPP, 2019, RENEW SUST ENERG REV, V111, P34, DOI 10.1016/j.rser.2019.05.012
   Vaughan N, 2003, J APPL ECOL, V40, P163, DOI 10.1046/j.1365-2664.2003.00784.x
   Weih M, 2003, BASIC APPL ECOL, V4, P149, DOI 10.1078/1439-1791-00157
NR 46
TC 3
Z9 3
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0006-3088
EI 1336-9563
J9 BIOLOGIA
JI Biologia
PD AUG
PY 2021
VL 76
IS 8
BP 2195
EP 2206
DI 10.1007/s11756-021-00686-0
EA MAR 2021
PG 12
WC Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Life Sciences & Biomedicine - Other Topics
GA UC8NN
UT WOS:000623914400002
OA hybrid
DA 2022-02-10
ER

PT J
AU Wysong, ML
   Gregory, P
   Watson, AWT
   Woolley, LA
   Parker, CW
AF Wysong, Michael L.
   Gregory, Pius
   Watson, Alexander W. T.
   Woolley, Leigh-Ann
   Parker, Christopher W.
CA Yawuru Country Managers
   Yawuru Country Managers
   Karajarri Rangers
   Karajarri Rangers
   Nyikina Mangala Rangers
TI Cross-cultural collaboration leads to greater understanding of the rare
   Spectacled Hare-wallaby in the west Kimberley, Western Australia
SO ECOLOGICAL MANAGEMENT & RESTORATION
LA English
DT Article
DE Camera traps; cross-cultural partnerships; Dampier Peninsula; Indigenous
   knowledge; Indigenous Protected Area; Indigenous rangers; Yawuru
ID KNOWLEDGE; OPPORTUNITIES; CONSERVATION
AB Cross-cultural collaboration between Yawuru Country Managers (Rangers) and WWF-Australia ecologists led to new detections of the Spectacled Hare-wallaby (SHW), (Lagorchestes conspicillatus) in the west Kimberley region of Western Australia where it was presumed to be locally extirpated. This collaboration relied on the expertise of the Yawuru Country Managers to select specific locations for targeted field surveys and resulted in the confirmation of SHW on the Yawuru IPA for the first time in a decade. Subsequent remote camera trap surveys over a larger area included collaboration with two additional neighbouring Indigenous ranger groups, Karrajarri and Nyikina Mangala. These surveys investigated the spatial and temporal relationship between SHW and other mammals which may threaten (e.g., feral Cat [Felis catus], Dingo [Canis familiaris dingo]) or compete (e.g., Agile Wallaby [Macropus agilis]; Cattle [Bos taurus]) with them. We found a negative relationship between SHW and cat activity, suggesting that cats may limit the activity or abundance of SHW. Temporal portioning was evident between SHW and both Cattle and Agile Wallaby suggesting that SHW may avoid times when these species are most active. Further, we found a negative relationship between SHW occurrence and distance to fire scar edge burnt in current or previous fire season. This edge habitat is likely important to SHW because they may require recently burnt areas to forage and dense unburnt areas to shelter. This project highlights the benefits of cross-cultural research and monitoring partnerships with Indigenous rangers as active observers and managers of their traditional lands.
C1 [Wysong, Michael L.; Gregory, Pius; Yawuru Country Managers; Yawuru Country Managers] Nyamba Buru Yawuru, Environm Serv Unit, 55 Reid Rd, Cable Beach, WA 6726, Australia.
   [Wysong, Michael L.] Charles Darwin Univ, Res Inst Environm & Livelihoods, Ellengowan Dr, Casurina, NT 0810, Australia.
   [Watson, Alexander W. T.; Woolley, Leigh-Ann] World Wide Fund Nat Australia, Sydney, NSW, Australia.
   [Parker, Christopher W.] Nyamba Buru Yawuru, Cable Beach, WA, Australia.
   [Karajarri Rangers; Karajarri Rangers] Karajarri Tradit Lands Assoc, Off 2,Broome Lotteries House,Lot 642 Cable Beach, Broome, WA 6725, Australia.
   [Nyikina Mangala Rangers] Walalakoo Aboriginal Corp, 70A Stanley St, Derby, WA 6728, Australia.
RP Wysong, ML (corresponding author), Nyamba Buru Yawuru, Environm Serv Unit, 55 Reid Rd, Cable Beach, WA 6726, Australia.
EM mlwysong@gmail.com; alexander.watson@australianwildlife.org;
   LWoolley@wwf.org.au; chris@spectrumecology.com.au
FU Lotterywest [421008692]; Australian Government's National Indigenous
   Australians Agency; Indigenous Land and Sea Corporation
FX We acknowledge and respect the Yawuru, Karajarri and Nykina Traditional
   Owners past, present and future on whose lands this project took place.
   Staff from WWF-Australia (Jessica Koleck, Jessica Chapman, Ellie Boyle,
   Hamsini Bijlani and Tanya Vernes) assisted on field surveys. Support for
   field surveys on Unallocated Crown Lands was provided by the Department
   of Biodiversity, Conservation and Attractions Broome office. We
   specifically thank Nyikina Mangala ranger coordinator Damien Giles and
   Karajarri ranger coordinator Ewan Noakes for assistance with logistics
   and fieldwork. Support for field surveys was also provided by the Nyamba
   Buru Yawuru Environmental Services IPA team including Nathan Kay, and
   former Yawuru Country Managers Johani Mamid and Jacob Corpus. We also
   thank Steve Reynolds of Environs Kimberley for his early assistance on
   the project. We gratefully acknowledge funding from Lotterywest to
   WWF-Australia (grant number 421008692). The project was also made
   possible by funding for the Yawuru IPA and Country Manager programs from
   the Australian Government's National Indigenous Australians Agency and
   the Indigenous Land and Sea Corporation. The authors declare no conflict
   of interest with respect to the submission of this manuscript.
CR Anonymous, 2014, GUARDIAN 1123
   Atlas of Living Australia, 2021, SPEC PAG, DOI [10.26197/ala.3321258f-c4e0-489c-921c-ff0e5a57c11b, DOI 10.26197/ALA.3321258F-C4E0-489C-921C-FF0E5A57C11B]
   Austin B.J., 2017, GUIDELINES COLLABORA
   Australian Government Department of Agriculture Water and Environment, 2021, IND PROT AR
   Australian Government Department of Sustainability Environment Water Population and Communities, 2011, SURV GUID AUSTR THRE
   Berkes F, 2009, J ENVIRON MANAGE, V90, P1692, DOI 10.1016/j.jenvman.2008.12.001
   Bohensky EL, 2013, ECOL SOC, V18, DOI 10.5751/ES-05846-180320
   Bohensky EL, 2011, ECOL SOC, V16, DOI 10.5751/ES-04342-160406
   Brook LA, 2012, J APPL ECOL, V49, P1278, DOI 10.1111/j.1365-2664.2012.02207.x
   Burbidge A., 1995, MAMMALS AUSTR, P313
   BURBIDGE AA, 1988, AUST WILDLIFE RES, V15, P9
   Dawson SJ, 2018, AUSTRAL ECOL, V43, P159, DOI 10.1111/aec.12553
   Eldridge DJ, 2016, ECOL APPL, V26, P1273, DOI 10.1890/15-1234
   Ens E., 2012, PEOPLE COUNTRY VITAL, P45
   Ens EJ, 2015, BIOL CONSERV, V181, P133, DOI 10.1016/j.biocon.2014.11.008
   Ens Emilie J., 2012, Ecological Management & Restoration, V13, P100, DOI 10.1111/j.1442-8903.2011.00634.x
   Godden L, 2016, RESTOR ECOL, V24, P692, DOI 10.1111/rec.12394
   INGLEBY S, 1992, WILDLIFE RES, V19, P721, DOI 10.1071/WR9920721
   INGLEBY S, 1991, WILDLIFE RES, V18, P501, DOI 10.1071/WR9910501
   Kenneally K.F., 1996, BROOME
   Legge S, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.52
   Legge Sarah, 2011, Ecological Management & Restoration, V12, P84, DOI 10.1111/j.1442-8903.2011.00595.x
   Letnic M, 2010, BIOL REV, V85, P501, DOI 10.1111/j.1469-185X.2009.00113.x
   McGregor H, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133915
   McGregor HW, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0109097
   McKemey M, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12030995
   Mistry J, 2016, SCIENCE, V352, P1274, DOI 10.1126/science.aaf1160
   Paltridge R, 2020, WILDLIFE RES, V47, P709, DOI 10.1071/WR20035
   Parkins K, 2019, FOREST ECOL MANAG, V451, DOI 10.1016/j.foreco.2019.05.013
   Prober SM, 2011, ECOL SOC, V16
   Radford IJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130721
   Radford IJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092341
   Reed G, 2021, CONSERV BIOL, V35, P179, DOI 10.1111/cobi.13532
   Reid A.M., 2019, GRASS FIRE KANGAROOS
   Reid AM, 2020, AUSTRAL ECOL, V45, P529, DOI 10.1111/aec.12860
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Russell-Smith J, 2003, INT J WILDLAND FIRE, V12, P283, DOI 10.1071/WF03015
   Sheil D, 2004, TRENDS ECOL EVOL, V19, P634, DOI 10.1016/j.tree.2004.09.019
   Smyth D., 2015, PARKS, V21, P73, DOI DOI 10.2305/IUCN.CH.2014.PARKS-21-2DS.EN
   Tengo M, 2014, AMBIO, V43, P579, DOI 10.1007/s13280-014-0501-3
   Vigilante T, 2017, LAND-BASEL, V6, DOI 10.3390/land6040068
   Ward-Fear G, 2019, CONSERV LETT, V12, DOI 10.1111/conl.12643
   Winter J., 2016, IUCN RED LIST THREAT
   Wiseman ND, 2016, GEOGR RES-AUST, V54, P52, DOI 10.1111/1745-5871.12150
   Woinarski J. C., 2014, ACTION PLAN AUSTR MA
   WWF-Australia, 2011, STRAT PLAN 2011 2016
   YRMTBC Yawuru Registered Native Title Body Corporate, 2014, WAL NAG BIRR BUR YAW
NR 47
TC 2
Z9 2
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1442-7001
EI 1442-8903
J9 ECOL MANAG RESTOR
JI Ecol. Manag. Restor.
PD JAN
PY 2022
VL 23
SU 1
BP 139
EP 149
DI 10.1111/emr.12524
PG 11
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA YO9YD
UT WOS:000748287400017
DA 2022-02-10
ER

PT J
AU Gonzalez-Gallina, A
   Perez-Garduza, F
   Iglesias-Hernandez, JA
   Oliveras-De Ita, A
   Vazquez-Zuniga, O
   Chacon-Hernandez, A
   Hidalgo-Mihart, MG
AF Gonzalez-Gallina, Alberto
   Perez-Garduza, Freddy
   Iglesias-Hernandez, Jesus A.
   Oliveras-De Ita, Adan
   Vazquez-Zuniga, Octavio
   Chacon-Hernandez, Andres
   Hidalgo-Mihart, Mircea G.
TI A Novel Item, Black Vultures (Coragyps atratus) Used as Food by a Jaguar
   (Panthera onca) in Quintana Roo, Mexico
SO AMERICAN MIDLAND NATURALIST
LA English
DT Article
ID PUMAS; PATTERNS; SITES; DIET
AB Using a GPS satellite collar, we tracked a jaguar close to Playa del Carmen city in Quintana Roo, Mexico from Jan. to Dec. 2013. We observed the jaguar recurrently used a cenote located near the Playa del Carmen city landfill. We searched on two occasions for potential prey items killed by the jaguar and also set two camera traps on the cenote area. We found the carcasses of two black vultures probably eaten by the jaguar, and we also obtained photographic evidence of the jaguar with a black vulture in its mouth. The photo, along with other evidence, reveals the potential significance of vultures as prey for this endangered species in areas where, due to subsistence hunting and urban expansion, jaguar's usual prey species have low abundances.
C1 [Gonzalez-Gallina, Alberto] Inst Ecol AC Red Ambiente & Sustentabilidad, Xalapa 91070, Veracruz, Mexico.
   [Perez-Garduza, Freddy; Hidalgo-Mihart, Mircea G.] Univ Juarez Autonoma Tabasco, Div Acad Ciencias Biol, Km 0-5 Carretera Villahermosa Cardenas, Villahermosa 86039, Tabasco, Mexico.
   [Iglesias-Hernandez, Jesus A.; Oliveras-De Ita, Adan; Vazquez-Zuniga, Octavio; Chacon-Hernandez, Andres] SA DE CV, SEGA, Del Benito Juarez 03230, DF, Mexico.
RP Hidalgo-Mihart, MG (corresponding author), Univ Juarez Autonoma Tabasco, Div Acad Ciencias Biol, Km 0-5 Carretera Villahermosa Cardenas, Villahermosa 86039, Tabasco, Mexico.
EM mhidalgo@yahoo.com
OI Hidalgo Mihart, Mircea G/0000-0002-8779-6886
FU Ingenieros Civiles Asociados; CONACYTConsejo Nacional de Ciencia y
   Tecnologia (CONACyT) [335814/232663]
FX This paper was possible thanks to the financial support granted to
   Sistemas Estrategicos para la Gestion Ambiental SEGA S. A. de C.V. by
   Ingenieros Civiles Asociados from their Infraestructure division (ICAi)
   through Consorcio del Mayab, within the wildlife monitoring for the
   highway Project entitled "Ramales Cedral-Tintal,Tintal-Playa del Carmen
   con una longitud de 54 km en el estado de Quintana Roo, Mexico.'' The
   Division Academica de Ciencias Biologicas de la Universidad Juarez
   Autonoma de Tabasco (DACBiol UJAT), granted logistical support for the
   success of this Project. Capture, management and collaring of the
   jaguars was under the capture permit code SGPA/DGVS/9611/12 Oct. 15th of
   2012 granted to Mircea Gabriel Hidalgo Mihart on behalf of the Direccion
   General de Vida Silvestre-SEMARNAT-Mexico. We thank to Erica Strand for
   revising this manuscript. We thank CONACYT for graduate studies
   scholarship number 335814/232663 awarded to A. Gonzalez Gallina who is
   studying at the Instituto de Ecologia A. C. (INECOL).
CR BEN-SHAHAR R., 1999, TRAIL WILD ENCOUNTER
   Campbell M. ON., 2015, VULTURES THEIR EVOLU
   CAMPOS-CAMARA B.L., 2007, PROCESOS URBANIZACIO
   Carrillo E, 2009, J TROP ECOL, V25, P563, DOI 10.1017/S0266467409990137
   CASO A., 2008, IUCN RED LIST THREAT, V2014
   Cassaigne I, 2016, SOUTHWEST NAT, V61, P125
   Cavalcanti SMC, 2010, J MAMMAL, V91, P722, DOI 10.1644/09-MAMM-A-171.1
   COMISION NACIONAL DE AREAS NATURALES PROTEGIDAS, 2009, PROGR ACC CONS ESP P
   CRAWSHAW PG, 1991, J ZOOL, V223, P357, DOI 10.1111/j.1469-7998.1991.tb04770.x
   CRUZ E. G, 2011, CONSERVACI ESTUDIO J, P81
   Da Silveira R, 2010, J HERPETOL, V44, P418, DOI 10.1670/08-340.1
   De Oliveira T. G., 2002, JAGUAR NUEVO MILENIO, P265
   Dupuy Rada Juan Manuel, 2007, Invest. Geog, P104
   ELBROCH M., 2001, BIRD TRACKS SIGN GUI
   ELLIS E. A., 2015, EVALUATION MAPEO DET
   Ferguson-Lees J., 2006, RAPTORS WORLD
   Foster RJ, 2010, J ZOOL, V280, P309, DOI 10.1111/j.1469-7998.2009.00663.x
   Gese EM, 2016, WILDLIFE RES, V43, P130, DOI 10.1071/WR15196
   Gonzalez CAL, 2002, MAMMALIA, V66, P603
   GONZALEZ-GALLINA A., 2017, HOMERANGE MALE JAGUA
   GRUBE G. E., 1953, WILSON B, V65, P119
   HERNANDEZ-SAN MARTIN A.D, 2015, NAT AREA J, V35, P308
   Howell S.N.G., 1995, GUIDE BIRDS MEXICO N
   HOWES PAUL GRISWOLD, 1926, BIRD LORE, V28, P175
   INIGO E., 1999, BIODIVERSITAS CONABI, V22, P1
   INSTITLTO NACIONAL DE ESTADISTICA Y GEOGRAFIA, 2013, AN EST QUINT ROO 201
   Jackson J.A., 1983, P245
   Logan KA, 1999, WILDLIFE SOC B, V27, P201
   MACKINNON R., 2005, AVES RESERVAS PEN IN
   Newsome TM, 2015, GLOBAL ECOL BIOGEOGR, V24, P1, DOI 10.1111/geb.12236
   Novaes WG, 2013, ZOOLOGIA-CURITIBA, V30, P607, DOI 10.1590/S1984-46702013005000014
   Polisar J, 2003, BIOL CONSERV, V109, P297, DOI 10.1016/S0006-3207(02)00157-X
   REMOLINA-SUAREZ J. F., 2014, COMISION NACL AREAS
   Rodriguez-Soto C, 2011, DIVERS DISTRIB, V17, P350, DOI 10.1111/j.1472-4642.2010.00740.x
   Salom-Perez R, 2007, ORYX, V41, P51, DOI 10.1017/S0030605307001615
   Santos-Fita D, 2012, J ETHNOBIOL ETHNOMED, V8, DOI 10.1186/1746-4269-8-38
   Scott W. E. D., 1892, AUK, V9, P120, DOI [10.2307/4067933, DOI 10.2307/4067933]
   SECRETARIA DEL MEDIO AMBIENTE Y RECURSOS NATURALES, 2010, NORM OFIC MEX NOM 05
   Sikes RS, 2011, J MAMMAL, V92, P235, DOI 10.1644/10-MAMM-F-355.1
NR 39
TC 0
Z9 0
U1 1
U2 27
PU AMER MIDLAND NATURALIST
PI NOTRE DAME
PA UNIV NOTRE DAME, BOX 369, ROOM 295 GLSC, NOTRE DAME, IN 46556 USA
SN 0003-0031
EI 1938-4238
J9 AM MIDL NAT
JI Am. Midl. Nat.
PD JUL
PY 2017
VL 178
IS 1
BP 158
EP 164
DI 10.1674/0003-0031-178.1.158
PG 7
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA FA6RB
UT WOS:000405570500014
DA 2022-02-10
ER

PT C
AU Ilie, A
   Welch, G
AF Ilie, Adrian
   Welch, Greg
GP IEEE
TI On-Line Control of Active Camera Networks for Computer Vision Tasks
SO 2011 FIFTH ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART
   CAMERAS (ICDSC)
LA English
DT Proceedings Paper
CT 13th International Conference on Advanced Concepts for Intelligent
   Vision Systems (ACIVS) / 5th ACM/IEEE International Conference on
   Distributed Smart Cameras (ICDSC)
CY AUG 22-25, 2011
CL Ghent Univ, Ghent, BELGIUM
SP Flemish Fund Sci Res (FWO Vlaanderen), Ghent Univ, Fac Engn & Architecture, Interdisciplinary Inst Broadband Technol (IBBT), Alcatel Lucent Bell Labs, Philips, NICTA, Object Video (OV), ACM, IEEE, VITO, TMC Grp
HO Ghent Univ
DE camera control; active camera networks; computer vision; surveillance;
   motion capture; 3D reconstruction
AB Large networks of cameras have been increasingly employed to capture dynamic events for tasks such as surveillance and training. When using active cameras to capture events distributed throughout a large area, human control becomes impractical and unreliable. This has led to the development of automated approaches for on-line camera control. We introduce a new automated camera control approach that consists of a stochastic performance metric and a constrained optimization method. The metric quantifies the uncertainty in the state of multiple points on each target. It uses state-space methods with stochastic models of the target dynamics and camera measurements. It can account for static and dynamic occlusions, accommodate requirements specific to the algorithm used to process the images, and incorporate other factors that can affect its results. The optimization explores the space of camera configurations over time under constraints associated with the cameras, the predicted target trajectories, and the image processing algorithm. The approach can be applied to conventional surveillance tasks (e. g., tracking or face recognition), as well as tasks employing more complex computer vision methods (e. g., markerless motion capture or 3D reconstruction).
C1 [Ilie, Adrian; Welch, Greg] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
RP Ilie, A (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
EM adyilie@cs.unc.edu; welch@cs.unc.edu
OI Welch, Gregory/0000-0002-8243-646X
CR Allen B. D., 2007, THESIS U N CAROLINA
   Bagdanov A. D., 2005, P ACM INT WORKSH VID, P121
   Bakhtari A, 2006, IEEE T SYST MAN CY C, V36, P668, DOI 10.1109/TSMCC.2005.855525
   Broaddus C., 2009, P SPIE MULTISENSOR M, V7345
   Chen X., 2002, THESIS STANFORD U
   Costello C. J., 2004, P ACM 2 INT WORKSH V, P39
   Davis J., 2002, THESIS STANFORD U
   Davison A. J., 2002, IEEE T PATTERN ANAL
   Del Bimbo A, 2006, PATTERN RECOGN LETT, V27, P1826, DOI 10.1016/j.patrec.2006.02.014
   Denzler J., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P17
   Denzler J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P400
   Deutsch B, 2004, LECT NOTES COMPUT SC, V3175, P359
   Deutsch B., 2005, IEEE INT C IM PROC, V3, P105
   Deutsch B, 2006, LECT NOTES COMPUT SC, V4174, P536
   Ilie A., 2008, WORKSH MULT MULT SEN
   Kanade T., 2000, CMURITR0012
   KRAHNSTOEVER N, 2008, WORKSH MULT MULT SEN
   Lim S.-N., 2007, AS C COMP VIS
   Olague G, 2002, PATTERN RECOGN, V35, P927, DOI 10.1016/S0031-3203(01)00076-0
   Qureshi F. Z., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P177
   Sommerlade E., 2008, WORKSH MULT MULT SEN
   Sommerlade E., 2008, P 5 INT WORKSH ATT C
   Sommerlade E., 2008, P IEEE COMP VIS PATT
   Taylor GR, 2007, PROC CVPR IEEE, P3883
   Welch G., 2001, ANN C COMPUTER GRAPH
   Welch G., 2007, TRENDS ISSUES TRACKI
   Wu JJ, 1998, OPT ENG, V37, P280, DOI 10.1117/1.601615
   Yous Sofiane, 2007, Journal of Multimedia, V2, P10, DOI 10.4304/jmm.2.1.10-19
NR 28
TC 0
Z9 0
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4577-1707-9
PY 2011
PG 6
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Imaging Science & Photographic Technology
GA BGV63
UT WOS:000324309300029
DA 2022-02-10
ER

PT J
AU Overton, CT
   Casazza, ML
   Connelly, D
   Gardner, S
AF Overton, Cory T.
   Casazza, Michael L.
   Connelly, Daniel
   Gardner, Scott
TI Gambel's Quail Survey Variability and Implications for Survey Design in
   the Mojave Desert
SO WILDLIFE SOCIETY BULLETIN
LA English
DT Article
DE acoustic recording; Breeding Bird Survey; California; call-count;
   Callipepla gambelii; Gambel's quail; Mojave; statistical power; survey
ID INDEXES
AB Careful design of a wildlife population monitoring strategy is necessary to obtain accurate and precise results whether the purpose of the survey is development of habitat suitability models, to estimate abundance, or assess site occupancy. Important characteristics to consider in survey design are sources of elevated variability, particularly within-subject variability, which increases the amount of data needed to achieve statistical certainty either in terms of population trend analysis, hypothesis testing, or statistical power. However, alternative objectives, such as associating counts with habitat characteristics, may benefit from increased variation among counts when differences covary with habitat measures. This difference can result in competing needs when developing survey protocols. We investigated the relative precision of differing gamebird monitoring protocols to identify methods with the greatest statistical efficiency. We assessed call-count transects using standard Breeding Bird Survey protocols (Passive call-counts) and modified by including longer survey periods and call playback (Active call-counts), autonomous recording units with supervised call detection (ARU-recorded calls), camera traps, and roadside covey-counts for Gambel's quail (Callipepla gambelii) in the Mojave Desert (CA, USA) during the spring of 2016. Active call-counts had the lowest within-site variation relative to estimated population index values, but Passive call-count transects may be more efficient for some purposes because more survey stations can be completed within a single survey timeframe. The ARU-recorded calls may provide a suitable alternative despite larger sample size needs, especially for occupancy surveys because multiple units can be deployed concurrently. The ultimate sample size required will depend on specific study objectives and scope of interest, but camera traps and breeding-season covey counts are not likely to meet objectives in desert environments. (c) 2020 The Wildlife Society.
C1 [Overton, Cory T.; Casazza, Michael L.] US Geol Survey, Western Ecol Res Ctr, Dixon, CA 95620 USA.
   [Connelly, Daniel] Pheasants Forever, Granite Bay, CA 95746 USA.
   [Gardner, Scott] Calif Dept Fish & Wildlife, Wildlife Branch, Sacramento, CA 95811 USA.
RP Overton, CT (corresponding author), US Geol Survey, Western Ecol Res Ctr, Dixon, CA 95620 USA.
EM coverton@usgs.gov
OI Overton, Cory/0000-0002-5060-7447; casazza, Mike/0000-0002-5636-735X
FU State of California; Pheasants Forever; U.S. Geological SurveyUnited
   States Geological Survey
FX We thank N. Young, H. Pavisich, and A. Merritt for conducting surveys
   and M. Ricca, T. Bui, and L. Parker for administrative assistance during
   the project. We thank J. Weigand for assistance in developing the
   project scope and incorporating design and process features relating to
   the Bureau of Land Management Assessment, Inventory, and Monitoring
   assessment procedures. We are also indebted to T. La Doux and the
   University of California Natural Reserve System, Granite Mountains
   Desert Research Center, and T. Anderson and staff at the Sonny
   Bono-Salton Sea National Wildlife Refuge for housing and logistical
   support during field activities. This research was funded with support
   from the State of California, Pheasants Forever, and the U.S. Geological
   Survey. Research permits for field activities conducted within the
   Mojave National Preserve were provided by the National Park Service and
   we thank N. Darby for his assistance in this regard and in helping us to
   locate wildlife guzzlers. Special thanks to Dr. W. Thogmartin and C.
   Smith for their insightful reviews of this manuscript. Any use of trade,
   firm, or product names is for descriptive purposes only and does not
   imply endorsement by the U.S. Government.
CR Anderson DR, 2003, WILDLIFE SOC B, V31, P288
   Bioacoustics Research Program, 2014, RAV PRO INT SOUND AN
   Casazza ML, 2005, WILDLIFE SOC B, V33, P606, DOI 10.2193/0091-7648(2005)33[606:EOCPIF]2.0.CO;2
   CLARK WAV, 1976, GEOGR ANAL, V8, P428
   Delehanty DJ, 2004, WILDLIFE SOC B, V32, P588, DOI 10.2193/0091-7648(2004)32[588:FTFMQF]2.0.CO;2
   England A. S., 1993, BIRDS N AM, DOI [10.2173/bna.71, DOI 10.2173/BNA.71]
   Jakob C, 2010, EUR J WILDLIFE RES, V56, P907, DOI 10.1007/s10344-010-0388-7
   Johnson DH, 2008, J WILDLIFE MANAGE, V72, P857, DOI 10.2193/2007-294
   Koops HV, 2015, LECT NOTES COMPUT SC, V9283, P261, DOI 10.1007/978-3-319-24027-5_26
   Link WA, 1998, ECOL APPL, V8, P258, DOI 10.2307/2641065
   Lynch James F., 1995, U S Forest Service General Technical Report PSW, V149, P1
   Overton C. T., 2019, COMP GAMBELS QUAIL S, DOI [10.5066/P9SVPK0N, DOI 10.5066/P9SVPK0N]
   RAMSEY FL, 1997, STAT SLEUTH COURSE M
   Rollins D., 2009, GAMEBIRD 2006 QUAIL, P210
   Rollins D., 2005, COUNTING QUAIL B 617
   Rusk JP, 2007, J WILDLIFE MANAGE, V71, P1336, DOI 10.2193/2006-071
   Sauer J. R., 2017, N AM BREEDING BIRD S
   Thompson W., 2004, SAMPLING RARE ELUSIV
   Wagner F. H., 2009, ARID LAND ECOSYSTEMS, V2, P125
NR 19
TC 0
Z9 0
U1 1
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1938-5463
J9 WILDLIFE SOC B
JI Wildl. Soc. Bull.
PD SEP
PY 2020
VL 44
IS 3
BP 493
EP 501
DI 10.1002/wsb.1105
EA JUL 2020
PG 9
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA NV0LS
UT WOS:000544614900001
DA 2022-02-10
ER

PT J
AU Ishige, T
   Miya, M
   Ushio, M
   Sado, T
   Ushioda, M
   Maebashi, K
   Yonechi, R
   Lagan, P
   Matsubayashi, H
AF Ishige, Taichiro
   Miya, Masaki
   Ushio, Masayuki
   Sado, Tetsuya
   Ushioda, Masaharu
   Maebashi, Kaori
   Yonechi, Risako
   Lagan, Peter
   Matsubayashi, Hisashi
TI Tropical-forest mammals as detected by environmental DNA at natural
   saltlicks in Borneo
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Borneo; Endangered species; Environmental DNA; Natural saltlick; NGS
ID CLOUDED LEOPARD; NEOFELIS-NEBULOSA; LICKS; CONSERVATION; ORANGUTANS;
   PYGMAEUS
AB Although tropical forests are among the most species-rich ecosystems on earth, 42% of mammal species in tropical forests are endangered because of overhunting and/or unsustainable exploitation. Camera-trap surveys have shown that natural saltlicks can be used to determine mammalian fauna, especially medium to large endangered species in tropical forests; establishment of camera traps, however, is time and effort intensive. Furthermore, the photographic range and detectable size of species are often restricted. Environmental DNA (eDNA) metabarcoding is a powerful approach that might provide a better way to study terrestrial animals in tropical forests. In this study, we examined whether eDNA from natural saltlicks comprehensively represented species composition in a Bornean tropical forest. We collected 100-150-mL water samples from natural saltlicks in Sabah, Malaysian Borneo. We constructed amplicon libraries for MiSeq sequencing using eDNA extracted from the water samples. Six endangered species were detected using this method, including Bomean orangutan (Pongo pygtnaeus), Bomean banteng (Bos javanicus lowt), Asian elephant (Elephas maximus), Sunda pangolin (Manis javanica), sambar deer (Rosa unicolor) and bearded pig (Sus barbatus). However, most small and minor species were not detected, with low sequence identity (80-96%). Therefore, we propose that more species of tropical forest mammals should have their sequences deposited in DNA databases. This study is the first to report the endangered mammals of a tropical forest detected using eDNA from natural saltlicks.
C1 [Ishige, Taichiro] Tokyo Univ Agr, NODAI Genome Res Ctr, 1-1-1 Sakuragaoka, Tokyo 1568502, Japan.
   [Miya, Masaki; Sado, Tetsuya] Nat Hist Museum & Inst, Dept Ecol & Environm Sci, Chuo Ku, 955-2 Aoba Cho, Chiba 2608682, Japan.
   [Ushio, Masayuki] Kyoto Univ, Ctr Ecol Res, 2-509-3 Hirano, Otsu, Shiga 5202113, Japan.
   [Ushio, Masayuki] Japan Sci & Technol Agcy, PRESTO, 4-1-8 Honcho, Kawaguchi, Saitama 3320012, Japan.
   [Ushioda, Masaharu; Maebashi, Kaori; Yonechi, Risako; Matsubayashi, Hisashi] Tokyo Univ Agr, Fac Agr, 1737 Funako, Atsugi, Kanagawa 2430034, Japan.
   [Lagan, Peter] Sabah Forestry Dept, Locked Bag 68, Sandakan 90009, Sabah, Malaysia.
RP Matsubayashi, H (corresponding author), Tokyo Univ Agr, Fac Agr, 1737 Funako, Atsugi, Kanagawa 2430034, Japan.
EM hm155202@nodai.ac.jp
RI Miya, Masaki/O-2664-2019; Ushio, Masayuki/F-8667-2010
OI Ushio, Masayuki/0000-0003-4831-7181; Miya, Masaki/0000-0002-9791-9886
FU Japan Society for the Promotion of Science (JSPS) Core-to-Core
   ProgramMinistry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT)Japan Society for the Promotion of Science; Advanced
   Research Networks (Wildlife Research Centre of Kyoto University);
   Ministry of Education, Culture, Sports, Science and Technology
   (MEXT)Ministry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT) [S1311017]; Japan Science and Technology Agency (JST) CREST
   Program [JPMJCR13A2]
FX This research conducted a collaborative research with Sabah Forestry
   Department and supported by the Japan Society for the Promotion of
   Science (JSPS) Core-to-Core Program, A. Advanced Research Networks
   (Wildlife Research Centre of Kyoto University), the Ministry of
   Education, Culture, Sports, Science and Technology (MEXT; S1311017) and
   the Japan Science and Technology Agency (JST) CREST Program
   (JPMJCR13A2).
CR Ampeng A, 2016, EUR J WILDLIFE RES, V62, P147, DOI 10.1007/s10344-015-0983-8
   Blake J. G., 2013, ANIMAL CONSERVATION, V16, P420
   Buckley-Beason VA, 2006, CURR BIOL, V16, P2371, DOI 10.1016/j.cub.2006.08.066
   Camacho C, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-421
   Christiansen P, 2008, J MAMMAL, V89, P1435, DOI 10.1644/08-MAMM-A-013.1
   Edgar RC, 2010, BIOINFORMATICS, V26, P2460, DOI 10.1093/bioinformatics/btq461
   IUCN, 2016, IUCN RED LIST THREAT
   Kitchener AC, 2006, CURR BIOL, V16, P2377, DOI 10.1016/j.cub.2006.10.066
   Loken B, 2013, AM J PRIMATOL, V75, P1129, DOI 10.1002/ajp.22174
   Magoc T, 2011, BIOINFORMATICS, V27, P2957, DOI 10.1093/bioinformatics/btr507
   Matsubayashi H, 2007, ECOL RES, V22, P742, DOI 10.1007/s11284-006-0313-4
   Matsubayashi H, 2008, TROPICS, V17, P81
   Matsubayashi H, 2011, RAFFLES B ZOOL, V59, P109
   Matsuda I, 2015, ECOL RES, V30, P191, DOI 10.1007/s11284-014-1219-1
   Miya M, 2015, ROY SOC OPEN SCI, V2, DOI 10.1098/rsos.150088
   Miya M, 2016, JOVE-J VIS EXP, DOI 10.3791/54741
   Thomsen PF, 2015, BIOL CONSERV, V183, P4, DOI 10.1016/j.biocon.2014.11.019
   Timmins R., 2015, IUCN RED LIST THREAT, V2015
   Ushio M., 2016, BIORXIV
   Wilkie DS, 2011, ANN NY ACAD SCI, V1223, P120, DOI 10.1111/j.1749-6632.2010.05908.x
   Wilting A, 2007, FRONT ZOOL, V4, DOI 10.1186/1742-9994-4-15
   Wilting A, 2011, MOL PHYLOGENET EVOL, V58, P317, DOI 10.1016/j.ympev.2010.11.007
NR 22
TC 29
Z9 30
U1 4
U2 92
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD JUN
PY 2017
VL 210
BP 281
EP 285
DI 10.1016/j.biocon.2017.04.023
PN A
PG 5
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA FB1CP
UT WOS:000405881600030
DA 2022-02-10
ER

PT J
AU Allen, ML
   Sibarani, MC
   Utoyo, L
   Krofel, M
AF Allen, M. L.
   Sibarani, M. C.
   Utoyo, L.
   Krofel, M.
TI Terrestrial mammal community richness and temporal overlap between
   tigers and other carnivores in Bukit Barisan Selatan National Park,
   Sumatra
SO ANIMAL BIODIVERSITY AND CONSERVATION
LA English
DT Article
DE Activity patterns; Carnivores; Conservation; Interspecific interactions;
   Panthera tigris sumatrae
ID PANTHERA-TIGRIS-SUMATRAE; ACTIVITY PATTERNS; RAIN-FOREST; TROPICAL
   FOREST; CONSERVATION; BIODIVERSITY; OCCUPANCY; JAGUAR; PUMA; PREY
AB Terrestrial mammal community richness and temporal overlap between tigers and other carnivores in Bukit Barisan Selatan National Park, Sumatra. Rapid and widespread biodiversity losses around the world make it important to survey and monitor endangered species, especially in biodiversity hotspots. Bukit Barisan Selatan National Park (BBSNP) is one of the largest conserved areas on the island of Sumatra, and is important for the conservation of many threatened species. Sumatran tigers (Panthera tigris sumatrae) are critically endangered and serve as an umbrella species for conservation, but may also affect the activity and distribution of other carnivores. We deployed camera traps for 8 years in an area of Bukit Barisan Selatan National Park (BBSNP) with little human activity to document the local terrestrial mammal community and investigate tiger spatial and temporal overlap with other carnivore species. We detected 39 mammal species including Sumatran tiger and several other threatened mammals. Annual species richness averaged 21.5 (range 19-24) mammals, and remained stable over time. The mammal order significantly affected annual detection of species and the number of cameras where a species was detected, while species conservation status did not. Tigers exhibited a diurnal activity pattern, and had the highest temporal overlap with marbled cats (Pardofelis marmorata), dholes (Cuon alpinus), and Malayan sun bears (Helarctos malayanus), but little overlap with other carnivores. These findings suggest that some smaller carnivores might be adjusting temporal activity to avoid tigers or mesocarnivores. The stable trends in richness of terrestrial mammal species show that BBSNP remains an important hotspot for the conservation of biodiversity.
C1 [Allen, M. L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Sibarani, M. C.; Utoyo, L.] Wildlife Conservat Soc Indonesia Program, Jalan Tampomas Ujung 35, Bogor 16151, West Java, Indonesia.
   [Krofel, M.] Univ Ljubljana, Dept Forestry, Biotech Fac, Vecna Pot 83, SI-1000 Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Conservation International; Missouri Botanical Garden; Smithsonian
   InstitutionSmithsonian Institution; Wildlife Conservation Society;
   Gordon and Betty Moore FoundationGordon and Betty Moore Foundation;
   Illinois Natural History Survey
FX All data in this publication are available through the Tropical Ecology
   Assessment and Monitoring (TEAM) Network, a collaboration between
   Conservation International, the Missouri Botanical Garden, the
   Smithsonian Institution, and the Wildlife Conservation Society. The work
   was partially funded by these institutions, the Gordon and Betty Moore
   Foundation, and the Illinois Natural History Survey. Monitoring
   activities were managed by the Wildlife Conservation Society in
   collaboration with the Bukit Barisan Selatan National Park and the
   Ministry of Environment and Forestry, Republic of Indonesia. We thank
   all the field staff and forest rangers involved in the camera trap
   deployment, and W. Marthy and F. R. Affandi for field coordination.
CR Allen M. L., 2019, ORYX, V53, P54
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Allen ML, 2017, J ETHOL, V35, P13, DOI 10.1007/s10164-016-0492-6
   Balme GA, 2017, BEHAV ECOL, V28, P1348, DOI 10.1093/beheco/arx098
   Burton A, 2019, FRONT ECOL ENVIRON, V17, P300, DOI 10.1002/fee.2053
   Chapin FS, 1998, BIOSCIENCE, V48, P45, DOI 10.2307/1313227
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Fitzgerald Christopher S., 2002, Mammalian Species, V696, P1, DOI 10.1644/1545-1410(2002)696<0001:HM>2.0.CO;2
   Foster VC, 2013, BIOTROPICA, V45, P373, DOI 10.1111/btp.12021
   Gopal R, 2010, ORYX, V44, P383, DOI 10.1017/S0030605310000529
   Grassman LI, 2005, J MAMMAL, V86, P29, DOI 10.1644/1545-1542(2005)086&lt;0029:EOTSFI&gt;2.0.CO;2
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   Herrera H, 2018, REV BIOL TROP, V66, P1559, DOI 10.15517/rbt.v66i4.32895
   Hunter Luke, 2015, P1
   Johnson A, 2009, ORYX, V43, P626, DOI 10.1017/S0030605309990238
   Karanth KU, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2016.1860
   Karanth KU, 2010, TIGERS OF THE WORLD: THE SCIENCE, POLITICS, AND CONSERVATION OF PANTHERA TIGRIS, 2ND EDITION, P241
   Kawanishi K, 2004, BIOL CONSERV, V120, P329, DOI 10.1016/j.biocon.2004.03.005
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Linkie M., 2008, IUCN RED LIST THREAT, V8235
   Linkie M, 2008, CONSERV BIOL, V22, P683, DOI 10.1111/j.1523-1739.2008.00906.x
   Linkie M, 2007, BIOL CONSERV, V137, P20, DOI 10.1016/j.biocon.2007.01.016
   Lynam AJ, 2013, RAFFLES B ZOOL, V61, P407
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   Nichols JD, 2007, ECOLOGY, V88, P1395, DOI 10.1890/06-1474
   O'Brien TG, 2008, ANIM CONSERV, V11, P179, DOI 10.1111/j.1469-1795.2008.00178.x
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien Timothy G., 1996, Oryx, V30, P207
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Pattanavibool A, 2015, IUCN RED LIST THREAT
   PIMM SL, 1995, SCIENCE, V269, P347, DOI 10.1126/science.269.5222.347
   Pusparini W, 2018, ORYX, V52, P25, DOI 10.1017/S0030605317001144
   R Core Team, 2016, R LANG ENV STAT COMP
   Rich LN, 2016, J APPL ECOL, V53, P1225, DOI 10.1111/1365-2664.12650
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Romero-Munoz A, 2010, J TROP ECOL, V26, P303, DOI 10.1017/S0266467410000052
   Rovero F., 2016, CAMERA TRAPPING WILD
   Seidensticker J.C. IV., 1973, Wildlife Monogr, VNo. 35, P1
   Seidensticker J, 2010, INTEGR ZOOL, V5, P285, DOI 10.1111/j.1749-4877.2010.00214.x
   Sibarani MC, 2019, J APPL ECOL, V56, P1220, DOI 10.1111/1365-2664.13360
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tobler MW, 2008, ANIM CONSERV, V11, P187, DOI 10.1111/j.1469-1795.2008.00181.x
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Walston J, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000485
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wibisono HT, 2009, ORYX, V43, P634, DOI 10.1017/S003060530999055X
NR 51
TC 7
Z9 7
U1 1
U2 5
PU MUSEU DE CIENCIES NATURALS-ZOOLOGIA
PI BARCELONA
PA PASSEIG PICASSO S-N, PARC CIUTADELLA, BARCELONA, E-08003, SPAIN
SN 1578-665X
EI 2014-928X
J9 ANIM BIODIV CONSERV
JI Anim. Biodivers. Conserv.
PY 2020
VL 43
IS 1
BP 97
EP 107
DI 10.32800/abc.2020.43.0097
PG 11
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA LS2KV
UT WOS:000536219200010
OA gold
DA 2022-02-10
ER

PT C
AU Chawla, H
   Jukola, M
   Marzban, S
   Arani, E
   Zonooz, B
AF Chawla, Hemang
   Jukola, Matti
   Marzban, Shabbir
   Arani, Elahe
   Zonooz, Bahram
BE Farinella, GM
   Radeva, P
   Braz, J
   Bouatouch, K
TI Practical Auto-calibration for Spatial Scene-understanding from
   Crowdsourced Dashcamera Videos
SO VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS -
   VOL. 5: VISAPP
LA English
DT Proceedings Paper
CT 16th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP) / 16th
   International Conference on Computer Vision Theory and Applications
   (VISAPP)
CY FEB 08-10, 2021
CL ELECTR NETWORK
DE Vision for Robotics; Crowdsourced Videos; Auto-calibration; Depth
   Estimation; Ego-motion Estimation
AB Spatial scene-understanding, including dense depth and ego-motion estimation, is an important problem in computer vision for autonomous vehicles and advanced driver assistance systems. Thus, it is beneficial to design perception modules that can utilize crowdsourced videos collected from arbitrary vehicular onboard or dashboard cameras. However, the intrinsic parameters corresponding to such cameras are often unknown or change over time. Typical manual calibration approaches require objects such as a chessboard or additional scene-specific information. On the other hand, automatic camera calibration does not have such requirements. Yet, the automatic calibration of dashboard cameras is challenging as forward and planar navigation results in critical motion sequences with reconstruction ambiguities. Structure reconstruction of complete visualsequences that may contain tens of thousands of images is also computationally untenable. Here, we propose a system for practical monocular onboard camera auto-calibration from crowdsourced videos. We show the effectiveness of our proposed system on the KITTI raw, Oxford RobotCar, and the crowdsourced D-2-City datasets in varying conditions. Finally, we demonstrate its application for accurate monocular dense depth and ego-motion estimation on uncalibrated videos.
C1 [Chawla, Hemang; Jukola, Matti; Marzban, Shabbir; Arani, Elahe; Zonooz, Bahram] Navinfo Europe, Adv Res Lab, Eindhoven, Netherlands.
RP Chawla, H (corresponding author), Navinfo Europe, Adv Res Lab, Eindhoven, Netherlands.
CR Arani E., 2021, P IEEE CVF WINT C AP
   Chawla H., 2020, 2020 IEEE 23 INT C I
   Chawla H., 2020 IEEE RSJ INT C
   Che Z., 2019, ARXIV PREPRINT ARXIV
   Dabeer O, 2017, IEEE INT C INT ROBOT, P634, DOI 10.1109/IROS.2017.8202218
   de Agapito L., 1998, BRIT MACH VIS C BMVC, P1
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gherardi R, 2010, LECT NOTES COMPUT SC, V6311, P790, DOI 10.1007/978-3-642-15549-9_57
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kukelova Z, 2015, IEEE I CONF COMP VIS, P2309, DOI 10.1109/ICCV.2015.266
   Lopez Manuel, 2019, P IEEE C COMP VIS PA, P11817
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Meister S, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P7251
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Santana-Cedres D, 2017, COMPUT VIS IMAGE UND, V161, P1, DOI 10.1016/j.cviu.2017.05.016
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Steger C, 2012, ISPRS J PHOTOGRAMM, V74, P202, DOI 10.1016/j.isprsjprs.2012.09.012
   Tummala GK, 2019, IPSN '19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P157, DOI 10.1145/3302506.3310397
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wildenauer H, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.106
   Wu CC, 2014, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2014.11
   Ying ZQ, 2016, INT CONF ACOUST SPEE, P1921, DOI 10.1109/ICASSP.2016.7472011
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu J, 2019, IEEE I CONF COMP VIS, P3838, DOI 10.1109/ICCV.2019.00394
   Zhuang BB, 2019, IEEE INT C INT ROBOT, P3766, DOI 10.1109/IROS40897.2019.8967912
NR 29
TC 0
Z9 0
U1 1
U2 1
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-488-6
PY 2021
BP 869
EP 880
DI 10.5220/0010255808690880
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BR6JA
UT WOS:000661288200092
OA Green Submitted, hybrid
DA 2022-02-10
ER

PT C
AU Burelli, P
   Yannakakis, GN
AF Burelli, Paolo
   Yannakakis, Georgios N.
BE Dickmann, L
   Volkmann, G
   Malaka, R
   Boll, S
   Kruger, A
   Olivier, P
TI Towards Adaptive Virtual Camera Control in Computer Games
SO SMART GRAPHICS
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 11th International Symposium on Smart Graphics
CY JUL 18-20, 2011
CL Bremen, GERMANY
SP Univ Bremen, TZI Ctr Comp & Commun Technol
AB Automatic camera control aims to define a framework to control virtual camera movements in dynamic and unpredictable virtual environments while ensuring a set of desired visual properties. We investigate the relationship between camera placement and playing behaviour in games and build a user model of the camera behaviour that can be used to control camera movements based on player preferences. For this purpose, we collect eye gaze, camera and game-play data from subjects playing a 3D platform game, we cluster gaze and camera information to identify camera behaviour profiles and we employ machine learning to build predictive models of the virtual camera behaviour. The performance of the models on unseen data reveals accuracies above 70% for all the player behaviour types identified. The characteristics of the generated models, their limits and their use for creating adaptive automatic camera control in games is discussed.
C1 [Burelli, Paolo; Yannakakis, Georgios N.] IT Univ Copenhagen, Ctr Comp Games Res, DK-2300 Copenhagen, Denmark.
RP Burelli, P (corresponding author), IT Univ Copenhagen, Ctr Comp Games Res, Rued Langgaards Vej 7, DK-2300 Copenhagen, Denmark.
EM pabu@itu.dk; yannakakis@itu.dk
RI Burelli, Paolo/AAM-5327-2020; Yannakakis, Georgios N./R-9213-2016
OI Burelli, Paolo/0000-0003-2804-9028; Yannakakis, Georgios
   N./0000-0001-7793-1450
CR Amerson D, 2005, P 2005 ACM SIGCHI IN, P369, DOI DOI 10.1145/1178477.1178552
   Bares W., 2000, Proceedings ACM Multimedia 2000, P177, DOI 10.1145/354384.354463
   Bares W. H., 1998, IUI '98. 1998 International Conference on Intelligent User Interfaces, P81
   Bernhard M, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857897
   Bourne O., 2005, ARTIF INTELL, P3
   Charles F, 2002, INT C INT GAM SIM LO, P1
   Christianson DB, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P148
   Christie M, 2008, COMPUT GRAPH FORUM, V27, P2197, DOI 10.1111/j.1467-8659.2008.01181.x
   DRUCKER SM, 1994, GRAPH INTER, P190
   Jhala Arnav, 2005, P 20 NAT C ART INT A, V5, P307
   Martins H., 2009, IEEE C EM TECHN FACT, P1, DOI DOI 10.1109/ACII.2009.5349592
   Picardi A., 2011, INT C FDN DIG GAM
   Pinelle D, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1453
   Riedmiller M., 1993, DIRECT ADAPTIVE METH
   Sundstedt V, 2008, APGV 2008: PROCEEDINGS OF THE SYMPOSIUM ON APPLIED PERCEPTION IN GRAPHICS AND VISUALIZATION, P43
   Ware C., 1990, Computer Graphics, V24, P175
   Yannakakis G.N., 2010, AFFECTIVE CAMERA CON
   Yarbus A. L., 1967, EYE MOVEMENTS VISION
NR 18
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-642-22570-3
J9 LECT NOTES COMPUT SC
PY 2011
VL 6815
BP 25
EP 36
PG 12
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BAF57
UT WOS:000304024600003
DA 2022-02-10
ER

PT J
AU van Pinxteren, BOCM
   Sirianni, G
   Gratton, P
   Despres-Einspenner, ML
   Egas, M
   Kuhl, H
   Lapuente, J
   Meier, AC
   Janmaat, KRL
AF van Pinxteren, Bryndan O. C. M.
   Sirianni, Giulia
   Gratton, Paolo
   Despres-Einspenner, Marie-Lyne
   Egas, Martijn
   Kuehl, Hjalmar
   Lapuente, Juan
   Meier, Amelia C.
   Janmaat, Karline R. L.
TI Sooty mangabeys scavenge on nuts cracked by chimpanzees and red river
   hogsAn investigation of inter-specific interactions around tropical nut
   trees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE auditory cues; community ecology; interspecific interactions;
   nut-cracking; scavenging; tropics
ID TAI-NATIONAL-PARK; RAIN-FOREST MAMMALS; PREDATION-RISK; TOOL USE;
   HUNTING BEHAVIOR; SPATIAL MEMORY; VIGILANCE; HAMMERS; AGGREGATION;
   MACAQUES
AB Carrion scavenging is a well-studied phenomenon, but virtually nothing is known about scavenging on plant material, especially on remnants of cracked nuts. Just like meat, the insides of hard-shelled nuts are high in energetic value, and both foods are difficult to acquire. In the Tai forest, chimpanzees (Pan troglodytes) and red river hogs (Potamochoerus porcus) crack nuts by using tools or strong jaws, respectively. In this study, previously collected non-invasive camera trap data were used to investigate scavenging by sooty mangabeys (Cercocebus atys), two species of Guinea fowl (Agelestres meleagrides; Guttera verreauxi), and squirrels (Scrunidae spp.) on the nut remnants cracked by chimpanzees and red river hogs. We investigated how scavengers located nut remnants, by analyzing their visiting behavior in relation to known nut-cracking events. Furthermore, since mangabeys are infrequently preyed upon by chimpanzees, we investigated whether they perceive an increase in predation risk when approaching nut remnants. In total, 190 nut-cracking events were observed in four different areas of Tai National Park, Ivory Coast. We could confirm that mangabeys scavenged on the nuts cracked by chimpanzees and hogs and that this enabled them to access food source that would not be accessible otherwise. We furthermore found that mangabeys, but not the other species, were more likely to visit nut-cracking sites after nut-cracking activities than before, and discuss the potential strategies that the monkeys could have used to locate nut remnants. In addition, mangabeys showed elevated levels of vigilance at the chimpanzee nut-cracking sites compared with other foraging sites, suggesting that they perceived elevated danger at these sites. Scavenging on remnants of cracked nuts is a hitherto understudied type of foraging behavior that could be widespread in nature and increases the complexity of community ecology in tropical rainforests.
   By use of camera trap videos it was confirmed that mangabeys scavenge on the nut remnants cracked by chimpanzees and red river hogs. Squirrels and two types of guinea fowl might scavenge on these nut remnants but this could not be seen clearly. Looking at the visitation rate before and after nut cracking events it was found that the possible scavenging species were more present at the nut cracking sites after a nut cracking event took place. It was found that mangabeys have an increase in vigilance behavior at chimpanzee nut cracking sites compared with outside these chimpanzee nut cracking sites, indicating that the mangabeys perceive a higher risk at these nut cracking sites.
C1 [van Pinxteren, Bryndan O. C. M.; Egas, Martijn; Janmaat, Karline R. L.] Univ Amsterdam, Dept Evolutionary & Populat Biol, Inst Biodivers & Ecosyst Dynam, Sci Pk 904, NL-1090 GE Amsterdam, Netherlands.
   [Sirianni, Giulia; Gratton, Paolo; Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar; Lapuente, Juan; Meier, Amelia C.; Janmaat, Karline R. L.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Kuehl, Hjalmar] Halle Jena Leipzig, German Ctr Integrat Biodivers Res, Leipzig, Germany.
   [Meier, Amelia C.] Duke Univ, Nicholas Sch Environm, Durham, NC 27708 USA.
RP van Pinxteren, BOCM (corresponding author), Univ Amsterdam, Dept Evolutionary & Populat Biol, Inst Biodivers & Ecosyst Dynam, Sci Pk 904, NL-1090 GE Amsterdam, Netherlands.
EM bocmvpinxteren@gmail.com
RI Lapuente, Juan/ABG-3912-2021
OI Lapuente, Juan/0000-0002-6783-5585; Sirianni,
   Giulia/0000-0003-4589-0345; Gratton, Paolo/0000-0001-8464-4062
FU Stichting Fonds Doctor Catharine van Tussenbroek; Dobberke Stichting
   voor Vergelijkende Psychology; Stichting Kronendak; Leakey Foundation;
   Lucy Burger Stichting; Koninklijke Nederlandse Akademie van
   Wetenschappen; Wenner-Gren Foundation; Centre for Forest Research
FX Stichting Fonds Doctor Catharine van Tussenbroek; Dobberke Stichting
   voor Vergelijkende Psychology; Stichting Kronendak; The Leakey
   Foundation; Lucy Burger Stichting; Koninklijke Nederlandse Akademie van
   Wetenschappen; Wenner-Gren Foundation; Centre for Forest Research
CR Abernethy K., 1999, WILDLIFE CONSERVATIO, V102, P50
   Atwood TC, 2008, ANIM BEHAV, V75, P753, DOI 10.1016/j.anbehav.2007.08.024
   Ban SD, 2016, ANIM BEHAV, V118, P135, DOI 10.1016/j.anbehav.2016.05.014
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bergmuller R., 1998, THESIS
   Boback SM, 2007, COMP BIOCHEM PHYS A, V148, P651, DOI 10.1016/j.cbpa.2007.08.014
   BOESCH C, 1983, BEHAVIOUR, V83, P265, DOI 10.1163/156853983X00192
   BOESCH C, 1989, AM J PHYS ANTHROPOL, V78, P547, DOI 10.1002/ajpa.1330780410
   BOESCH C, 1990, FOLIA PRIMATOL, V54, P86, DOI 10.1159/000156428
   Boesch C, 2000, CHIMPANZEES TAI FORE
   Bridges A. S., 2011, BEHAV ACTIVITY PATTE
   De Moraes BLC, 2014, AM J PRIMATOL, V76, P967, DOI 10.1002/ajp.22286
   Campos FA, 2014, BEHAV ECOL, V25, P477, DOI 10.1093/beheco/aru005
   Canale GR, 2009, AM J PRIMATOL, V71, P366, DOI 10.1002/ajp.20648
   Cowlishaw G, 1998, BEHAVIOUR, V135, P431, DOI 10.1163/156853998793066203
   Craigie ID, 2010, BIOL CONSERV, V143, P2221, DOI 10.1016/j.biocon.2010.06.007
   del Hoyo J, 2014, HBW BIRDLIFE INT ILL, V1
   Dermody BJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024635
   Desbordes, 2013, PRIMATES WORLD ILLUS
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   DeVault TL, 2003, OIKOS, V102, P225, DOI 10.1034/j.1600-0706.2003.12378.x
   Dobson A.J., 2008, INTRO GEN LINEAR MOD, Vthird
   Dupuch A, 2014, BEHAV ECOL SOCIOBIOL, V68, P299, DOI 10.1007/s00265-013-1645-z
   Estienne V, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22672
   Estrada A, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1600946
   Favreau FR, 2010, P ROY SOC B-BIOL SCI, V277, P2089, DOI 10.1098/rspb.2009.2337
   Forstmeier W, 2011, BEHAV ECOL SOCIOBIOL, V65, P47, DOI 10.1007/s00265-010-1038-5
   Francis I.S., 1992, Bird Conservation International, V2, P25
   Gessner J, 2014, AFR J ECOL, V52, P59, DOI 10.1111/aje.12084
   Gone Bi Z, 2007, THESIS
   Gumert MD, 2009, AM J PRIMATOL, V71, P594, DOI 10.1002/ajp.20694
   Herbinger I, 2001, INT J PRIMATOL, V22, P143, DOI 10.1023/A:1005663212997
   Hoppe-Dominik B, 2011, AFR J ECOL, V49, P450, DOI 10.1111/j.1365-2028.2011.01277.x
   Houston D.C., 1985, Ornithological Monographs, P856
   Hoyo J. D., 1994, NEW WORLD VULTURES G
   Janmaat KRL, 2006, ANIM BEHAV, V72, P797, DOI 10.1016/j.anbehav.2005.12.009
   Janmaat KRL, 2013, ANIM BEHAV, V86, P1183, DOI 10.1016/j.anbehav.2013.09.021
   Janmaat KRL, 2013, ANIM COGN, V16, P851, DOI 10.1007/s10071-013-0617-z
   Janmaat KRL, 2006, THESIS
   Junker J, 2012, DIVERS DISTRIB, V18, P1077, DOI 10.1111/ddi.12005
   Kalan AK, 2015, ANIM BEHAV, V101, P1, DOI 10.1016/j.anbehav.2014.12.011
   Kouakou CY, 2011, J TROP ECOL, V27, P621, DOI 10.1017/S0266467411000423
   Kuhl HS, 2016, SCI REP-UK, V6, DOI 10.1038/srep22219
   Kuhl HS, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0035610
   Laundre J.W., 2010, OPEN ECOL J, V3, P1
   Laurance WF, 2006, CONSERV BIOL, V20, P1251, DOI 10.1111/j.1523-1739.2006.00420.x
   Leslie David M. Jr., 2015, Mammalian Species, P15, DOI 10.1093/mspecies/sev002
   Leus K., 2013, MAMMALS OF AFRICA, VVI, P35
   LIMA SL, 1985, ANIM BEHAV, V33, P155, DOI 10.1016/S0003-3472(85)80129-9
   Luncz LV, 2017, INT J PRIMATOL, V38, P872, DOI 10.1007/s10764-017-9985-6
   Malbrant R., 1949, MAMMALIAN SPECIES, V47, P15
   Martin P., 2007, MEASURING BEHAV INTR, V3rd
   McGraw W. S., 2007, MONKEYS TAI FOREST A, V51
   McGraw WS, 2011, AM J PHYS ANTHROPOL, V144, P140, DOI 10.1002/ajpa.21402
   Mittermeier R. A., 2013, PRIMATES
   Moupela C, 2014, TROP ECOL, V55, P327
   N'Goran P. K., 2013, International Journal of Innovation and Applied Studies, V3, P326
   Nowak K, 2014, BEHAV ECOL, V25, P1199, DOI 10.1093/beheco/aru110
   Oduro W., 1989, THESIS
   Periquet S, 2012, BEHAV ECOL, V23, P970, DOI 10.1093/beheco/ars060
   R Core Development Team, 2013, R LANG ENV STAT COMP
   Range F, 2004, ETHOLOGY, V110, P301, DOI 10.1111/j.1439-0310.2004.00973.x
   Rosevear D. R., 1969, NATURAL HIST
   Rowe N., 2016, ALL WORLDS PRIMATES
   Shettleworth S.J., 2010, COGNITION EVOLUTION, V2nd Edn
   Shultz S, 2002, P ROY SOC B-BIOL SCI, V269, P1797, DOI 10.1098/rspb.2002.2098
   Sirianni G, 2018, ANIM COGN, V21, P109, DOI 10.1007/s10071-017-1144-0
   Sirianni G, 2015, ANIM BEHAV, V100, P152, DOI 10.1016/j.anbehav.2014.11.022
   Smith AC, 2004, BEHAV ECOL SOCIOBIOL, V56, P18, DOI 10.1007/s00265-003-0753-6
   Taylor C. A., 2015, THESIS
   Treves A, 2001, BEHAV ECOL SOCIOBIOL, V50, P90, DOI 10.1007/s002650100328
   Vanthomme H, 2013, CONSERV BIOL, V27, P281, DOI 10.1111/cobi.12017
   Visalberghi E, 2015, PHILOS T R SOC B, V370, DOI 10.1098/rstb.2014.0351
   Watts DP, 2002, INT J PRIMATOL, V23, P1, DOI 10.1023/A:1013270606320
   WHITE LJT, 1994, J ANIM ECOL, V63, P499, DOI 10.2307/5217
   Wikenros C, 2014, J MAMMAL, V95, P862, DOI 10.1644/13-MAMM-A-125
   Wittiger L, 2013, BEHAV ECOL SOCIOBIOL, V67, P1097, DOI 10.1007/s00265-013-1534-5
   Zuberbuehler Klaus, 2010, P64
NR 78
TC 1
Z9 1
U1 0
U2 5
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD AUG
PY 2018
VL 80
IS 8
AR e22895
DI 10.1002/ajp.22895
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Zoology
GA GT7AN
UT WOS:000444672400007
PM 30024029
OA Green Published, hybrid
DA 2022-02-10
ER

PT J
AU Molyneux, J
   Pavey, CR
   James, AI
   Carthew, SM
AF Molyneux, J.
   Pavey, C. R.
   James, A. I.
   Carthew, S. M.
TI The efficacy of monitoring techniques for detecting small mammals and
   reptiles in arid environments
SO WILDLIFE RESEARCH
LA English
DT Article
ID OCCUPANCY-ABUNDANCE RELATIONSHIP; ESTIMATING SITE OCCUPANCY; FIRE-DRIVEN
   SUCCESSION; ULURU-NATIONAL-PARK; CAMERA-TRAPS; POPULATION-DYNAMICS;
   DASYCERCUS-BLYTHI; CENTRAL AUSTRALIA; DIGITAL CAMERAS; RAINFALL
AB Context. Accurate surveying and monitoring of biodiversity provides essential baseline data for developing and implementing effective environmental management strategies. Land managers in arid zones face the challenge of managing vast, remote landscapes that support numerous cryptic species that are difficult to detect and monitor. Although researchers and land managers are using an increasingly wider variety of monitoring techniques to detect and monitor species, little is known of the relative effectiveness and comparative costs of these techniques.
   Aims. The present study simultaneously assessed the efficacy of three popular monitoring techniques utilised in the spinifex sand plains of arid Australia, namely, live trapping, sign surveys and passive infrared (PIR)-camera trapping.
   Methods. We explored variations in capture rates and species richness for each technique and compared initial and on-going costs of the techniques over time.
   Key results. Sign surveys detected the greatest number of species and groups overall. Detectability of small mammals and reptiles, as a target group, was greater using PIR cameras, although the probability of detection by each technique varied among specific species. PIR cameras were initially the most expensive technique; however, the low ongoing costs of maintaining cameras in the field meant that they became the most cost effective after eight survey periods.
   Conclusions. Each of the techniques tested here showed biases towards the detection of specific groups or species in the spinifex sand-plain habitat of Australia. Regardless, PIR cameras performed better at detecting the greatest diversity of target species and financially over time.
   Implications. To accurately survey species across vast areas and climate variations, studies often extend over long time periods. Many long-term studies would be likely to benefit financially from the increased deployment of PIR cameras alongside or in place of live trapping surveys, with little impact on the ability to monitor the presence of most species in the region.
C1 [Molyneux, J.; Carthew, S. M.] Charles Darwin Univ, Res Inst Environm & Livelihoods, Darwin, NT 0909, Australia.
   [Pavey, C. R.] CSIRO, Land & Water, PMB 44, Winnellie, NT 0822, Australia.
   [James, A. I.] Australian Wildlife Conservancy, Mornington, WA 6221, Australia.
RP Molyneux, J (corresponding author), Charles Darwin Univ, Res Inst Environm & Livelihoods, Darwin, NT 0909, Australia.
EM jmolyneux.ecology@gmail.com
RI Pavey, Chris/AAT-6315-2020; Pavey, Chris/D-7209-2011
OI Pavey, Chris/0000-0003-2162-8019; 
FU Margaret Middleton Fund; ANZ Trustees Foundation; Australian Geographic
   Society; Northern Territory Government; Schultz Foundation; Wildlife
   Preservation of Australia; Norman Wettenhall Foundation; Charles Darwin
   University Ethics Committee [A12024]
FX This research was funded by the Margaret Middleton Fund, ANZ Trustees
   Foundation, Australian Geographic Society, Northern Territory
   Government, Schultz Foundation, Wildlife Preservation of Australia and
   the Norman Wettenhall Foundation. The Australian Wildlife Conservancy
   and in particular Newhaven managers, Danae Moore and Josef Schofield, is
   thanked for their invaluable support and guidance in the field. We also
   thank Greg Fyfe, Peter Nunn and Anthony Molyneux for their expertise in
   identifying species in PIR camera photos and the numerous volunteers who
   aided in field surveys. All survey procedures were approved by the
   Charles Darwin University Ethics Committee (approval no. A12024).
CR Alagaili AN, 2014, MAMM BIOL, V79, P195, DOI 10.1016/j.mambio.2013.10.001
   Anderson K. A., 2002, MODEL SELECTION MULT
   Australian Wildlife Conservancy [AWC], 2016, ANN FAUN TRAPP NEWH
   Baillie J.E.M., 2004, IUCN RED LIST THREAT
   Barea-Azcon JM, 2007, BIODIVERS CONSERV, V16, P1213, DOI 10.1007/s10531-006-9114-x
   Bennison K, 2014, AUST MAMMAL, V36, P184, DOI 10.1071/AM13015
   Benshemesh J, 2014, J MAMMAL, V95, P1054, DOI 10.1644/14-MAMM-A-051
   Boitani L, 2000, RES TECHNIQUES ANIMA
   Bolker B, 2012, GLMMADMB GEN LINEAR
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Catling PC, 1997, WILDLIFE RES, V24, P417, DOI 10.1071/WR96073
   Charles Darwin University [CDU], 2014, SAL GUID OFF HUM RES
   Davies MJ, 2014, AUST MAMMAL, V36, P103, DOI 10.1071/AM13017
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Department of National Parks Sport and Racing, 2016, CAMP FEES
   Dickman CR, 2011, J MAMMAL, V92, P1193, DOI 10.1644/10-MAMM-S-329.1
   Dickman CR, 2010, J MAMMAL, V91, P798, DOI 10.1644/09-MAMM-S-205.1
   Dickman CR, 1999, WILDLIFE RES, V26, P389, DOI 10.1071/WR97057
   Dickman CR, 2001, WILDLIFE RES, V28, P493, DOI 10.1071/WR00023
   Fairfax RJ, 2014, CURR ISSUES TOUR, V17, P72, DOI 10.1080/13683500.2012.714749
   Fancourt BA, 2014, AUST MAMMAL, V36, P247, DOI 10.1071/AM14004
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Garden JG, 2007, WILDLIFE RES, V34, P218, DOI 10.1071/WR06111
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Greenville AC, 2014, OECOLOGIA, V175, P1349, DOI 10.1007/s00442-014-2977-8
   Greenville AC, 2013, AUSTRAL ECOL, V38, P754, DOI 10.1111/aec.12033
   Greenville AC, 2012, ECOL EVOL, V2, P2645, DOI 10.1002/ece3.377
   Halls Creek Travel & Tourism, 2016, ACC E KIMB
   Hamel S, 2013, METHODS ECOL EVOL, V4, P105, DOI 10.1111/j.2041-210x.2012.00262.x
   HECK KL, 1975, ECOLOGY, V56, P1459, DOI 10.2307/1934716
   Henderson, 2000, ECOLOGICAL METHODS
   Hothorn T, 2008, BIOMETRICAL J, V50, P346, DOI 10.1002/bimj.200810425
   Hui C, 2009, ECOL APPL, V19, P2038, DOI 10.1890/08-2236.1
   Ieno, 2012, ZERO INFLATED MODELS
   Jones Clyde, 1996, P115
   Karanth KU, 2011, J APPL ECOL, V48, P1048, DOI 10.1111/j.1365-2664.2011.02002.x
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   KENDALL KC, 1992, ECOL APPL, V2, P422, DOI 10.2307/1941877
   Letnic M, 2011, J MAMMAL, V92, P1210, DOI 10.1644/10-MAMM-S-229.1
   Lyra-Jorge MC, 2008, EUR J WILDLIFE RES, V54, P739, DOI 10.1007/s10344-008-0205-8
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   Masters P, 1996, WILDLIFE RES, V23, P39, DOI 10.1071/WR9960039
   MASTERS P, 1993, WILDLIFE RES, V20, P803, DOI 10.1071/WR9930803
   Masters P, 2012, WILDLIFE RES, V39, P419, DOI 10.1071/WR11156
   Masters Pip, 1998, Australian Mammalogy, V20, P403
   McAlpin S, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0019041
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McGrath Tim, 2012, Herpetological Review, V43, P249
   Meek P., 2014, CAMERA TRAPPING WILD
   Milstead WB, 2007, J MAMMAL, V88, P1532, DOI 10.1644/16-MAMM-A-407R.1
   Molyneux J., 2017, FAUNA ASSEMBLAGES SP
   Moseby KE, 2009, TALES SAND GUIDE IDE
   National Health and Medical Research Council, 2015, GUID CAR US AUSTR NA
   NICHOLS JD, 1983, J MAMMAL, V64, P253, DOI 10.2307/1380555
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   Paull DJ, 2011, WILDLIFE RES, V38, P188, DOI 10.1071/WR10203
   Pavey CR, 2008, J MAMMAL, V89, P674, DOI 10.1644/07-MAMM-A-168R.1
   Pavey CR, 2011, AUST J ZOOL, V59, P156, DOI 10.1071/ZO11052
   Perry R. A., 1979, INT BIOL PROGRAMME
   PIANKA ER, 1969, ECOLOGY, V50, P1012, DOI 10.2307/1936893
   Price-Rees SJ, 2013, AUSTRAL ECOL, V38, P493, DOI 10.1111/j.1442-9993.2012.02439.x
   R Core Team, 2015, R LANG ENV STAT COMP
   Scott DM, 2006, BIOL CONSERV, V127, P72, DOI 10.1016/j.biocon.2005.07.014
   Scroggie MP, 2009, J ZOOL, V277, P214, DOI 10.1111/j.1469-7998.2008.00528.x
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Smith MS, 2008, RANGELAND J, V30, P15, DOI 10.1071/RJ07052
   Southgate R, 2005, WILDLIFE RES, V32, P43, DOI 10.1071/WR03087
   Stanley TR, 2005, J WILDLIFE MANAGE, V69, P874, DOI 10.2193/0022-541X(2005)069[0874:ESOAAU]2.0.CO;2
   STEWART AP, 1979, AUST WILDLIFE RES, V6, P165, DOI 10.1071/WR9790165
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Thompson GG, 2007, WILDLIFE RES, V34, P491, DOI 10.1071/WR06081
   Thrifty Car Rentals, 2016, CAR HIR
   Watson M., 2007, SURVEY SO MARSUPIAL
   Webb NF, 2012, WILDLIFE SOC B, V36, P240, DOI 10.1002/wsb.140
   Welbourne DJ, 2015, WILDLIFE RES, V42, P414, DOI 10.1071/WR15054
   Wiewel AS, 2007, J MAMMAL, V88, P250, DOI 10.1644/06-MAMM-A-098R1.1
   Zuur Alain F., 2009, P1
NR 78
TC 8
Z9 9
U1 4
U2 38
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
PY 2017
VL 44
IS 6-7
BP 534
EP 545
DI 10.1071/WR17017
PG 12
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA FQ4SS
UT WOS:000418348700010
DA 2022-02-10
ER

PT J
AU Allen, ML
   Farmer, MJ
   Clare, JDJ
   Olson, ER
   Van Stappen, J
   Van Deelen, TR
AF Allen, M. L.
   Farmer, M. J.
   Clare, J. D. J.
   Olson, E. R.
   Van Stappen, J.
   Van Deelen, T. R.
TI Is there anybody out there? Occupancy of the carnivore guild in a
   temperate archipelago
SO COMMUNITY ECOLOGY
LA English
DT Article
DE Apostle Islands; Camera traps; Carnivore; Community; Distribution;
   Island biogeography; Occupancy; Species richness
ID RANGE CONTRACTIONS; PREDATION RISK; ICE COVER; WOLVES; BEHAVIOR;
   ECOLOGY; MARKING; CATS; LAKE; FEAR
AB Carnivores are important components of ecological communities with wide-ranging effects that vary with carnivore size, natural history, and hunting tactics. Researchers and managers should strive to understand both the presence and distribution of carnivores within their local environment. We studied the carnivore guild in the Apostle Islands, where the distribution and occupancy of carnivores was largely unknown. We monitored 19 islands with 160 functioning camera traps from 2014-2017, from which we collected 203,385 photographs across 49,280 trap nights. We documented 7,291 total wildlife events with 1,970 carnivore events, and detected 10 of the 12 terrestrial carnivores found in Wisconsin. Detection rates for species were generally higher in summer than winter, except for coyotes (Canis latrans) and red foxes (Vulpes vulpes). Finite-sample occupancy estimates for carnivores varied across islands, with mean estimated occupancy across islands varying from a high of 0.73 for black bears to a low of 0.21 for gray wolves. Of the potential island biogeography explanatory variables for carnivore occupancy we considered, island size was the most important, followed by distance to the mainland, and then inter-island distance. We estimated that terrestrial carnivore species varied in the number of islands they were detected on from 1 island for gray wolves to 13 islands for black bears. Estimated carnivore richness across islands (i.e., the number of carnivores occupying an island) also varied substantively from 1 species on Michigan Island to 10 species on Stockton Island. Island size and connectivity between islands appear important for the persistence of the carnivore community in the Apostle Islands.
C1 [Allen, M. L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Farmer, M. J.; Clare, J. D. J.; Van Deelen, T. R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Olson, E. R.] Northland Coll, Nat Resources, 1411 Ellis Ave S, Ashland, WI 54806 USA.
   [Van Stappen, J.] Apostle Isl Natl Lakeshore, Planning & Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
RP Farmer, MJ (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM mjmorales@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Apostle Islands National Lakeshore (GLNF CESU) [P14AC01180]; Northland
   College (Department of Natural Resources); Northland College (Sigurd
   Olson Professorship in the Natural Sciences); Northland College (Morris
   O. Ristvedt Professorship in the Natural Sciences); NASA Earth and Space
   Science Fellowship [NNX16AO61H]; University of Wisconsin (Schorger fund,
   Department of Forest and Wildlife Ecology); University of Wisconsin
   (Beers-Bascom Professorship in Conservation)
FX This project was supported by the Apostle Islands National Lakeshore
   (GLNF CESU Agreement P14AC01180), Northland College (Department of
   Natural Resources; Sigurd Olson Professorship in the Natural Sciences;
   Morris O. Ristvedt Professorship in the Natural Sciences), NASA Earth
   and Space Science Fellowship (Grant number NNX16AO61H), and the
   University of Wisconsin (Schorger fund, Department of Forest and
   Wildlife Ecology; Beers-Bascom Professorship in Conservation). We thank
   the personnel from each group that contributed to this project,
   especially APIS staff and volunteers, graduate students from the Van
   Deelen lab at the University of Wisconsin - Madison, and students from
   Northland College. The support and cooperative spirit between these
   three groups was key to the success of this project. The authors declare
   that they have no conflict of interest.)
CR Adams JR, 2011, P ROY SOC B-BIOL SCI, V278, P3336, DOI 10.1098/rspb.2011.0261
   Allen ML, 2018, AM MIDL NAT, V179, P294
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2016, ECOLOGY, V97, P1905, DOI 10.1002/ecy.1462
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Allen ML, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102257
   Altendorf KB, 2001, J MAMMAL, V82, P430, DOI 10.1644/1545-1542(2001)082&lt;0430:AEOPRO&gt;2.0.CO;2
   Anderson M. K., 2003, Ecological Restoration, V21, P269, DOI 10.3368/er.21.4.269
   [Anonymous], 2018, LIST OF MAMM
   Atwood TC, 2007, J WILDLIFE MANAGE, V71, P1098, DOI 10.2193/2006-102
   Belant JL, 2005, URSUS, V16, P85, DOI 10.2192/1537-6176(2005)016[0085:ABBPSA]2.0.CO;2
   Berger J., 2013, LARGE CARNIVORES CON
   Black KM, 2019, J WILDLIFE MANAGE, V83, P158, DOI 10.1002/jwmg.21556
   Brown JS, 1999, J MAMMAL, V80, P385, DOI 10.2307/1383287
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Busch J. C., 2008, HIST RESOURCE STUDY
   Cederholm CJ, 1999, FISHERIES, V24, P6, DOI 10.1577/1548-8446(1999)024&lt;0006:PSC&gt;2.0.CO;2
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Courchamp F, 1999, J ANIM ECOL, V68, P282, DOI 10.1046/j.1365-2656.1999.00285.x
   CRAVEN S R, 1987, Colonial Waterbirds, V10, P64, DOI 10.2307/1521232
   Efford MG, 2012, ECOSPHERE, V3, DOI 10.1890/ES11-00308.1
   Estes JA, 1996, WILDLIFE SOC B, V24, P390
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Gelman A., 1992, STAT SCI, V7, P457, DOI [10.1214/ss/1177011136., 10.1214/ss/1177011136, DOI 10.1214/SS/1177011136]
   Gelman A, 2008, ANN APPL STAT, V2, P1360, DOI 10.1214/08-AOAS191
   Harmsen BJ, 2010, J MAMMAL, V91, P1225, DOI 10.1644/09-MAMM-A-416.1
   Harrison RL, 2015, WEST N AM NATURALIST, V75, P387, DOI 10.3398/064.075.0409
   Howk F, 2009, J GREAT LAKES RES, V35, P159, DOI 10.1016/j.jglr.2008.11.002
   HUNTER MD, 1992, ECOLOGY, V73, P724
   Jackson H. H. T., 1920, Journal of Mammalogy, V1, DOI 10.2307/1373741
   Judziewicz E.J., 1993, MICHIGAN BOT, V32, P43
   Kellert, 1997, VALUE LIFE BIOL DIVE
   Kellner K., 2015, JAGSUI WRAPPER RJAGS
   Krofel M, 2012, BEHAV ECOL SOCIOBIOL, V66, P1297, DOI 10.1007/s00265-012-1384-6
   Laliberte AS, 2004, BIOSCIENCE, V54, P123, DOI 10.1641/0006-3568(2004)054[0123:RCONAC]2.0.CO;2
   Lariviere Serge, 2001, Mammalian Species, V647, P1, DOI 10.1644/1545-1410(2001)647<0001:UA>2.0.CO;2
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Licht Daniel S., 2015, Canadian Field-Naturalist, V129, P139
   Lomolino M.V., 1988, P185
   MAC ARTHUR ROBERT H., 1967
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   Magnuson JJ, 2000, SCIENCE, V289, P1743, DOI 10.1126/science.289.5485.1743
   Mallick B., 1998, SANKHYA, V60, P65, DOI DOI 10.1186/1471-2105-12-186
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   National Centers for Environmental Information (NCEI), CLIM DAT ONL DAT DIS
   Newsome TM, 2016, MAMMAL REV, V46, P255, DOI 10.1111/mam.12067
   O'Hara RB, 2009, BAYESIAN ANAL, V4, P85, DOI 10.1214/09-BA403
   Okello MM, 2008, TOURISM MANAGE, V29, P751, DOI 10.1016/j.tourman.2007.08.003
   Pauli JN, 2005, NORTHEAST NAT, V12, P245, DOI 10.1656/1092-6194(2005)012[0245:EFLSCI]2.0.CO;2
   Plummer M., 2003, P 3 INT WORKSH DISTR, P125
   Popescu VD, 2014, ECOL EVOL, V4, P933, DOI 10.1002/ece3.997
   Prugh LR, 2009, BIOSCIENCE, V59, P779, DOI 10.1525/bio.2009.59.9.9
   R Core Team, 2018, STATS PACK LANG ENV
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Ripple WJ, 2004, BIOSCIENCE, V54, P755, DOI 10.1641/0006-3568(2004)054[0755:WATEOF]2.0.CO;2
   Sivy KJ, 2017, AM NAT, V190, P663, DOI 10.1086/693996
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilson EO, 2010, THEORY OF ISLAND BIOGEOGRAPHY REVISITED, P1
   Wilton CM, 2015, URSUS, V26, P53, DOI 10.2192/URSUS-D-15-00008.1
   Wolf C, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.170052
NR 61
TC 8
Z9 8
U1 0
U2 21
PU AKADEMIAI KIADO ZRT
PI BUDAPEST
PA BUDAFOKI UT 187-189-A-3, H-1117 BUDAPEST, HUNGARY
SN 1585-8553
EI 1588-2756
J9 COMMUNITY ECOL
JI Community Ecol.
PD DEC
PY 2018
VL 19
IS 3
BP 272
EP 280
DI 10.1556/168.2018.19.3.8
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HF7XL
UT WOS:000454455200008
OA Green Accepted
DA 2022-02-10
ER

PT J
AU Wang, ZJ
   Wang, JN
   Lin, CT
   Han, Y
   Wang, ZS
   Ji, LQ
AF Wang, Zhaojun
   Wang, Jiangning
   Lin, Congtian
   Han, Yan
   Wang, Zhaosheng
   Ji, Liqiang
TI Identifying Habitat Elements from Bird Images Using Deep Convolutional
   Neural Networks
SO ANIMALS
LA English
DT Article
DE bird images; deep convolutional neural networks; habitat elements
ID CAMERA TRAPS; CLASSIFICATION; DISTANCE; DENSITY
AB Simple Summary
   To assist researchers in processing large amounts of bird image data, many algorithms have been proposed, but almost all of them aim at solving the problems of bird identification and counting. We turn our attention to the recognition of habitat elements in bird images, which will help with automatically extracting habitat information from such images. To achieve this goal, we formed a dataset and implemented our proposed method with four kinds of deep convolutional neural networks, and the recognition rate reached a minimum of 89.48% and a maximum of 95.52%. The use of this method will supplement the extraction of bird image information and promote the study of the relationships between birds and habitat elements.
   With the rapid development of digital technology, bird images have become an important part of ornithology research data. However, due to the rapid growth of bird image data, it has become a major challenge to effectively process such a large amount of data. In recent years, deep convolutional neural networks (DCNNs) have shown great potential and effectiveness in a variety of tasks regarding the automatic processing of bird images. However, no research has been conducted on the recognition of habitat elements in bird images, which is of great help when extracting habitat information from bird images. Here, we demonstrate the recognition of habitat elements using four DCNN models trained end-to-end directly based on images. To carry out this research, an image database called Habitat Elements of Bird Images (HEOBs-10) and composed of 10 categories of habitat elements was built, making future benchmarks and evaluations possible. Experiments showed that good results can be obtained by all the tested models. ResNet-152-based models yielded the best test accuracy rate (95.52%); the AlexNet-based model yielded the lowest test accuracy rate (89.48%). We conclude that DCNNs could be efficient and useful for automatically identifying habitat elements from bird images, and we believe that the practical application of this technology will be helpful for studying the relationships between birds and habitat elements.
C1 [Wang, Zhaojun; Wang, Jiangning; Lin, Congtian; Han, Yan; Ji, Liqiang] Chinese Acad Sci, Inst Zool, Key Lab Anim Ecol & Conservat Biol, Beijing 100101, Peoples R China.
   [Wang, Zhaojun; Lin, Congtian] Univ Chinese Acad Sci, Coll Resources & Environm, Beijing 100101, Peoples R China.
   [Wang, Zhaosheng] Chinese Acad Sci, Inst Geog Sci & Nat Resources Res, Key Lab Ecosyst Network Observat & Modeling, Natl Ecosyst Sci Data Ctr, Beijing 100101, Peoples R China.
RP Ji, LQ (corresponding author), Chinese Acad Sci, Inst Zool, Key Lab Anim Ecol & Conservat Biol, Beijing 100101, Peoples R China.
EM wangzhaojun@ioz.ac.cn; wangjn@ioz.ac.cn; linct@ioz.ac.cn;
   hanyan@ioz.ac.cn; wangzs@igsnrr.ac.cn; ji@ioz.ac.cn
RI wang, zhaosheng/AAX-1350-2021
OI wang, zhaosheng/0000-0001-7307-2249; Wang, Zhao jun/0000-0002-8866-7680
FU Strategic Priority Research Program of the Chinese Academy of
   SciencesChinese Academy of Sciences [XDA19050202]; 13th Five-year
   Informatization Plan of the Chinese Academy of Sciences [XXH13503];
   National R&D Infrastructure and Facility Development Program of China
   [DKA2017-12-02-10]
FX This work was supported by grants from the Strategic Priority Research
   Program of the Chinese Academy of Sciences (XDA19050202), the 13th
   Five-year Informatization Plan of the Chinese Academy of Sciences
   (XXH13503), and the National R&D Infrastructure and Facility Development
   Program of China, `Fundamental Science Data Sharing Platform'
   (DKA2017-12-02-10).
CR Adams AA, 2015, SECUR J, V28, P272, DOI 10.1057/sj.2012.48
   Agarap A. F, 2018, ARXIV PREPRINT ARXIV
   Alahmari SS, 2020, IEEE ACCESS, V8, P211860, DOI 10.1109/ACCESS.2020.3039833
   [Anonymous], P IEEE C COMPUTER VI
   Atanbori J, 2018, ECOL INFORM, V48, P12, DOI 10.1016/j.ecoinf.2018.07.005
   Baldi P., 2013, ADV NEURAL INF PROCE, P2814, DOI DOI 10.17744/MEHC.25.2.XHYREGGXDCD0Q4NY
   Belkina AC, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-13055-y
   Ben Boudaoud L, 2019, OCEANS-IEEE
   Bochner AP, 2012, QUAL INQ, V18, P168, DOI 10.1177/1077800411429094
   Booms TL, 2003, J FIELD ORNITHOL, V74, P416, DOI 10.1648/0273-8570-74.4.416
   Brownlee J., 2018, WHAT IS DIFFERENCE B
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Carrascal LM, 2014, ECOL RES, V29, P203, DOI 10.1007/s11284-013-1114-1
   Carvalho T, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P866, DOI 10.1109/ICMLA.2017.00-47
   Cerda P, 2018, MACH LEARN, V107, P1477, DOI 10.1007/s10994-018-5724-2
   Chen, 2019, ARXIV190904126
   Cloyed CS, 2018, J FISH WILDL MANAG, V9, P485, DOI 10.3996/122017-JFWM-103
   Cristani M, 2020, IEEE ACCESS, V8, P126876, DOI 10.1109/ACCESS.2020.3008370
   D'Amico M, 2018, AMBIO, V47, P650, DOI 10.1007/s13280-018-1025-z
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Duda J.J., 2019, ARXIV190707063
   Efros A.A., 2016, ARXIV160808614
   Ellen JS, 2019, LIMNOL OCEANOGR-METH, V17, P439, DOI 10.1002/lom3.10324
   Fan JC, 2020, CYBERNET SYST, V52, P26, DOI 10.1080/01969722.2020.1827799
   Ferreira AC, 2020, METHODS ECOL EVOL, V11, P1072, DOI 10.1111/2041-210X.13436
   Flores CF, 2019, PATTERN RECOGN, V94, P62, DOI 10.1016/j.patcog.2019.05.002
   Galleguillos C, 2010, COMPUT VIS IMAGE UND, V114, P712, DOI 10.1016/j.cviu.2010.02.004
   Gregory RD, 2010, ORNITHOL SCI, V9, P3, DOI 10.2326/osj.9.3
   Guo QH, 2020, SCI CHINA EARTH SCI, V63, P1457, DOI 10.1007/s11430-019-9584-9
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hong SJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071651
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Jin R., P IEEE C COMP VIS PA
   Kim S, 2020, IEEE ACCESS, V8, P155296, DOI 10.1109/ACCESS.2020.3019069
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai DY, 2019, PATTERN RECOGN, V88, P547, DOI 10.1016/j.patcog.2018.12.002
   Lecun Y, P 2015 IEEE HOT CHIP
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li C, 2019, NEURAL PROCESS LETT, V49, P1021, DOI 10.1007/s11063-018-9871-z
   Liu B, 2018, 2018 11TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING (ISCSLP), P11, DOI 10.1109/ISCSLP.2018.8706607
   Muhammed MAE, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES FOR SMART NATION (SMARTTECHCON), P902, DOI 10.1109/SmartTechCon.2017.8358502
   Murphy AJ, 2018, BIRD CONSERV INT, V28, P567, DOI 10.1017/S0959270917000107
   Neary PL, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON COGNITIVE COMPUTING (ICCC), P73, DOI 10.1109/ICCC.2018.00017
   Netrapalli P.J., 2019, ARXIV190412838
   Orekondy T, 2017, IEEE I CONF COMP VIS, P3706, DOI 10.1109/ICCV.2017.398
   Paszke A, 2019, ARXIV191201703
   Pontes FJ, 2016, NEUROCOMPUTING, V186, P22, DOI 10.1016/j.neucom.2015.12.061
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Randler C, 2018, ECOL EVOL, V8, P7151, DOI 10.1002/ece3.4240
   Reif V, 2006, EUR J WILDLIFE RES, V52, P251, DOI 10.1007/s10344-006-0039-1
   Ribeiro-Silva L, 2018, ZOOLOGIA-CURITIBA, V35, DOI 10.3897/zoologia.35.e14678
   Sejnowski TJ, 2020, P NATL ACAD SCI USA, V117, P30033, DOI 10.1073/pnas.1907373117
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Smith L.N., 2018, ARXIV180309820B
   Stein A, 2008, BIODIVERS CONSERV, V17, P3579, DOI 10.1007/s10531-008-9442-0
   Stoddard MC, 2016, SCI REP-UK, V6, DOI 10.1038/srep32059
   Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006
   Suwanrat S, 2015, GLOB ECOL CONSERV, V3, P596, DOI 10.1016/j.gecco.2015.01.010
   Suzuki K, 2017, RADIOL PHYS TECHNOL, V10, P257, DOI 10.1007/s12194-017-0406-5
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tryjanowski P, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10020270
   Wang J., 2017, ARXIV171204621
   Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wu K., P 2018 52 ANN C INF
   Xie GS, 2017, PATTERN RECOGN, V71, P118, DOI 10.1016/j.patcog.2017.06.002
   Xie L, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107205
   Yee CK, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01124-y
   Yosinski J, 2014, ADV NEUR IN, V27
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang S.X., 2015, ARXIV14126651
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhou ZH, 2012, ARTIF INTELL, V176, P2291, DOI 10.1016/j.artint.2011.10.002
NR 78
TC 0
Z9 0
U1 3
U2 3
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD MAY
PY 2021
VL 11
IS 5
AR 1263
DI 10.3390/ani11051263
PG 21
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Veterinary Sciences; Zoology
GA SG3YN
UT WOS:000653377400001
PM 33925654
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Ilie, A
   Welch, G
AF Ilie, Adrian
   Welch, Greg
TI Online Control of Active Camera Networks for Computer Vision Tasks
SO ACM TRANSACTIONS ON SENSOR NETWORKS
LA English
DT Article
DE Algorithms; Experimentation; Measurement; Performance; Camera control;
   active camera networks; computer vision; surveillance; motion capture;
   3D reconstruction
ID SURVEILLANCE
AB Large networks of cameras have been increasingly employed to capture dynamic events for tasks such as surveillance and training. When using active cameras to capture events distributed throughout a large area, human control becomes impractical and unreliable. This has led to the development of automated approaches for online camera control. We introduce a new automated camera control approach that consists of a stochastic performancemetric and a constrained optimizationmethod. The metric quantifies the uncertainty in the state of multiple points on each target. It uses state-space methods with stochastic models of target dynamics and camera measurements. It can account for occlusions, accommodate requirements specific to the algorithms used to process the images, and incorporate other factors that can affect their results. The optimization explores the space of camera configurations over time under constraints associated with the cameras, the predicted target trajectories, and the image processing algorithms. The approach can be applied to conventional surveillance tasks (e. g., tracking or face recognition), as well as tasks employing more complex computer vision methods (e. g., markerless motion capture or 3D reconstruction).
C1 [Ilie, Adrian; Welch, Greg] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
   [Welch, Greg] Univ Cent Florida, Inst Simulat & Training, Orlando, FL 32816 USA.
   [Welch, Greg] Univ Cent Florida, Dept Elect Engn & Comp Sci, Orlando, FL 32816 USA.
RP Ilie, A (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
EM adyillie@cs.unc.edu
OI Welch, Gregory/0000-0002-8243-646X
FU ONROffice of Naval Research [N00014-08-C-0349]
FX This work was supported by ONR grant N00014-08-C-0349 for Behavior
   Analysis and Synthesis for Intelligent Training (BASE-IT), led by Greg
   Welch (PI) at UNC, Amela Sadagic (PI) at the Naval Post-graduate School,
   and Rakesh Kumar (PI) and Hui Cheng (Co-PI) at Sarnoff. Roy Stripling,
   Ph.D., Program Manager.
CR Allen B. D., 2007, THESIS U N CAROLINA
   Allen BD, 2005, VRST 05 P ACM S VIRT, P201, DOI DOI 10.1145/1101616.1101658
   Bagdanov A. D., 2005, P ACM INT WORKSH VID, P121
   Bakhtari A, 2006, IEEE T SYST MAN CY C, V36, P668, DOI 10.1109/TSMCC.2005.855525
   Bodor R, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P552
   Broaddus C., 2009, P SPIE MULTISENSOR M, V7345
   Casper M., 2007, EVENTSCRIPTS PYTHON
   CHANG CB, 1984, IEEE T AUTOMAT CONTR, V29, P98
   Chen X., 2002, THESIS STANFORD U
   Costello C. J., 2004, P ACM 2 INT WORKSH V, P39
   Davis J., 2002, THESIS STANFORD U
   Davison A. J., 2002, IEEE T PATTERN ANAL
   Del Bimbo A, 2006, PATTERN RECOGN LETT, V27, P1826, DOI 10.1016/j.patrec.2006.02.014
   Denzler J., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P17
   Denzler J., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P305
   Denzler J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P400
   Denzler J, 2002, IEEE T PATTERN ANAL, V24, P145, DOI 10.1109/34.982896
   Denzler J., 2001, OPTIMAL OBSERVATION
   Deutsch B, 2004, LECT NOTES COMPUT SC, V3175, P359
   Deutsch B., 2005, IEEE INT C IM PROC, V3, P105
   Deutsch B, 2006, LECT NOTES COMPUT SC, V4174, P536
   Grewal M., 1993, INFORM SYSTEM SCI SE
   Guan L., 2010, THESIS U N CAROLINA
   Ilie A., 2011, P INT C DISTR SMART
   Ilie A., 2008, P WORKSH MULT MULT S
   Ilie D., 2010, THESIS U N CAROLINA
   Kailath T., 2000, LINEAR ESTIMATION IN
   Kanade T., 2000, CMURITR0012
   KRAHNSTOEVER N, 2008, P WORKSH MULT MULT S
   Krahnstoever N., 2001, P IEEE WORKSH DET RE
   Kumar V., 2005, INTRO DATA MINING, V1
   Lim S., 2007, P AS C COMP VIS
   Lim S.-N., 2005, P 3 ACM INT WORKSH V, P141
   Marcenaro L, 2001, P IEEE, V89, P1419, DOI 10.1109/5.959339
   Matsuyama T, 2002, P IEEE, V90, P1136, DOI 10.1109/JPROC.2002.801442
   Mittal A., 2004, P EUR C COMP VIS
   Mittal A, 2008, INT J COMPUT VISION, V76, P31, DOI 10.1007/s11263-007-0057-9
   Naish M. D., 2001, 2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236), P2964, DOI 10.1109/ICSMC.2001.971961
   Naish MD, 2003, ROBOT CIM-INT MANUF, V19, P283, DOI 10.1016/S0736-5845(02)00085-6
   Natarajan P., 2012, P 11 INT C AUT AG MU
   Oberti F, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P415, DOI 10.1109/ICIP.2001.958516
   Olague G, 2002, PATTERN RECOGN, V35, P927, DOI 10.1016/S0031-3203(01)00076-0
   QURESHI F, 2005, P 3 ACM INT WORKSH V, V12, P131
   Qureshi F. Z., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P177
   Qureshi F. Z., 2007, IEEE C COMP VIS PATT, P1
   RAM GSV, 2006, P 4 ACM INT WORKSH V
   Remagnino P, 2004, PATTERN RECOGN, V37, P675, DOI 10.1016/j.patcog.2003.09.017
   Roy-Chowdhury AK, 2004, IEEE T IMAGE PROCESS, V13, P960, DOI 10.1109/TIP.2004.827240
   Sommerlade E., 2010, P IEEE INT C ROB AUT
   Sommerlade E., 2008, P 5 INT WORKSH ATT C
   Sommerlade E., 2008, P IEEE C COMP VIS PA
   Sommerlade E., 2008, P WORKSH MULT MULT S
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940
   Taylor GR, 2007, PROC CVPR IEEE, P3883
   Welch G., 2001, COMPUTER GRAPHICS SI
   Welch G., 2007, P WORKSH TRENDS ISS
   Wu JJ, 1998, OPT ENG, V37, P280, DOI 10.1117/1.601615
   Yous Sofiane, 2007, Journal of Multimedia, V2, P10, DOI 10.4304/jmm.2.1.10-19
   Zhang Z., 1999, P INT C COMP VIS
NR 59
TC 5
Z9 5
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1550-4859
EI 1550-4867
J9 ACM T SENSOR NETWORK
JI ACM Trans. Sens. Netw.
PD JAN
PY 2014
VL 10
IS 2
DI 10.1145/2530283
PG 40
WC Computer Science, Information Systems; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 302SL
UT WOS:000330625100008
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Senocak, A
   Oh, TH
   Kim, J
   Yang, MH
   Kweon, IS
AF Senocak, Arda
   Oh, Tae-Hyun
   Kim, Junsik
   Yang, Ming-Hsuan
   Kweon, In So
TI Learning to Localize Sound Sources in Visual Scenes: Analysis and
   Applications
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Visualization; Videos; Task analysis; Correlation; Deep learning;
   Network architecture; Unsupervised learning; Audio-visual learning;
   sound localization; self-supervision; multi-modal learning; cross-modal
   retrieval
AB Visual events are usually accompanied by sounds in our daily lives. However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans? To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes. In order to achieve this goal, a two-stream network structure which handles each modality with attention mechanism is developed for sound source localization. The network naturally reveals the localized response in the scene without human annotation. In addition, a new sound source dataset is developed for performance evaluation. Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases. Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception. To fix this issue, we extend our network to the supervised and semi-supervised network settings via a simple modification due to the general architecture of our two-stream network. We show that the false conclusions can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup. Furthermore, we present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we extend this proposed algorithm to a new application, sound saliency based automatic camera view panning in 360 degree videos.
C1 [Senocak, Arda; Kim, Junsik; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
   [Oh, Tae-Hyun] POSTECH, Dept Elect Engn, Pohang 37673, South Korea.
   [Yang, Ming-Hsuan] Univ Calif, Dept Elect Engn & Comp Sci, Merced, CA 95343 USA.
RP Kweon, IS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.; Oh, TH (corresponding author), POSTECH, Dept Elect Engn, Pohang 37673, South Korea.
EM arda.senocak@gmail.com; taehyun@csail.mit.edu; mibastro@gmail.com;
   mhyang@ucmerced.edu; iskweon@kaist.ac.kr
RI Yang, Ming-Hsuan/T-9533-2019; Oh, Tae-Hyun/D-7854-2016
OI Yang, Ming-Hsuan/0000-0003-4848-2304; Kim, Junsik/0000-0003-2555-5232;
   Senocak, Arda/0000-0001-9141-3270; Oh, Tae-Hyun/0000-0003-0468-1571
FU National Information Society Agency [2100-2131-305-10719]; NSF
   CAREERNational Science Foundation (NSF)NSF - Office of the Director (OD)
   [1149783]
FX A. Senocak, J. Kim, and I.S. Kweon were supported by the National
   Information Society Agency for construction of training data for
   artificial intelligence (2100-2131-305-10719). M.-H. Yang is supported
   in part by NSF CAREER (No. 1149783).
CR Abadi, 2015, TENSORFLOW LARGE SCA
   Afouras T, 2018, INTERSPEECH, P3244
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Aytar Y., 2017, ABS170600932 CORR
   Aytar Y., 2016, ADV NEURAL INFORM PR, P892, DOI DOI 10.1109/CVPR.2016.18
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bahdanau D., 2014, ARXIV14090473, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047
   Barzelay Zohar, 2007, P IEEE C COMP VIS PA
   Bolia RS, 1999, HUM FACTORS, V41, P664, DOI 10.1518/001872099779656789
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Chou S.-H., 2017, P AAAI, P6748
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   den Oord V., 2013, NIPS, V26, P2643
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fisher III J. W., 2001, P 13 INT C NEUR INF, P742
   Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041
   Gao RH, 2018, LECT NOTES COMPUT SC, V11207, P36, DOI 10.1007/978-3-030-01219-9_3
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Harwath D, 2018, LECT NOTES COMPUT SC, V11210, P659, DOI 10.1007/978-3-030-01231-1_40
   Hershey J. R., 1999, P 12 INT C NEUR INF
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   JONES B, 1975, PERCEPT PSYCHOPHYS, V17, P241, DOI 10.3758/BF03203206
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   Kidron E, 2005, PROC CVPR IEEE, P88
   Kim C., 2018, P AS C COMP VIS, P276
   Kopf J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982405
   Majdak P, 2010, ATTEN PERCEPT PSYCHO, V72, P454, DOI 10.3758/APP.72.2.454
   Morgado P., 2018, P 32 INT C NEUR INF, P360
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Perez A. F., 2019, ARXIV190407933
   Perrott DR, 1996, HUM FACTORS, V38, P702, DOI 10.1518/001872096778827260
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   SKINNER BF, 1948, J EXP PSYCHOL, V38, P168, DOI 10.1037/h0055873
   Stein BE, 2008, NAT REV NEUROSCI, V9, P255, DOI 10.1038/nrn2331
   Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10
   Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Torresani L., 2018, ADV NEURAL INFORM PR, P7774, DOI [10.5555/3327757.3327874, DOI 10.5555/3327757.3327874]
   Van Trees H.L, 2002, OPTIMUM ARRAY PROC 4
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao H, 2018, LECT NOTES COMPUT SC, V11205, P587, DOI 10.1007/978-3-030-01246-5_35
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zunino A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P693, DOI 10.1109/ICCVW.2015.95
NR 55
TC 2
Z9 2
U1 2
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD MAY 1
PY 2021
VL 43
IS 5
BP 1605
EP 1619
DI 10.1109/TPAMI.2019.2952095
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA RJ3YD
UT WOS:000637533800009
PM 31722472
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Lennon, RJ
   Peach, WJ
   Dunn, JC
   Shore, RF
   Pereira, MG
   Sleep, D
   Dodd, S
   Wheatley, CJ
   Arnold, KE
   Brown, CD
AF Lennon, Rosie J.
   Peach, Will J.
   Dunn, Jenny C.
   Shore, Richard F.
   Pereira, M. Gloria
   Sleep, Darren
   Dodd, Steve
   Wheatley, Christopher J.
   Arnold, Kathryn E.
   Brown, Colin D.
TI From seeds to plasma: Confirmed exposure of multiple farmland bird
   species to clothianidin during sowing of winter cereals
SO SCIENCE OF THE TOTAL ENVIRONMENT
LA English
DT Article
DE Agriculture; Insecticide; Clothianidin; Systemic; Sub-lethal effects;
   Ecological monitoring
ID PITUITARY-THYROID AXIS; NEONICOTINOID INSECTICIDES; ENVIRONMENTAL RISKS;
   POTENTIAL EXPOSURE; WILDLIFE BIRD; TREATED SEED; PESTICIDE; RESIDUES;
   DITHIOCARBAMATE; AVAILABILITY
AB Neonicotinoids are the largest group of systemic insecticides worldwide and are most commonly applied as agricultural seed treatments. However, little is known about the extent to which farmland birds are exposed to these compounds during standard agricultural practices. This study uses winter cereal, treated with the neonicotinoid clothianidin, as a test system to examine patterns of exposure in farmland birds during a typical sowing period. The availability of neonicotinoid-treated seed was recorded post-sowing at 39 fields (25 farms), and camera traps were used to monitor seed consumption by wild birds in situ. The concentration of clothianidin in treated seeds and crop seedlings was measured via liquid chromatography-tandem mass spectrometry, and avian blood samples were collected from 11 species of farmland bird from a further six capture sites to quantify the prevalence and level of clothianidin exposure associated with seed treatments. Neonicotinoid-treated seeds were found on the soil surface at all but one of the fields surveyed at an average density of 2.8 seeds/m(2). The concentration of clothianidin in seeds varied around the target application rate, whilst crop seedlings contained on average 5.9% of the clothianidin measured in seeds. Exposure was confirmed in 32% of bird species observed in treated fields and 50% of individual birds post-sowing; the median concentration recorded in positive samples was 12 ng/mL. Results here provide clear evidence that a variety of farmland birds are subject to neonicotinoid exposure following normal agricultural sowing of neonicotinoid-treated cereal seed. Furthermore, the widespread availability of seeds at the soil surface was identified as a primary source of exposure. Overall, these data are likely to have global implications for bird species and current agricultural policies where neonicotinoids are in use, and may be pertinent to any future risk assessments for systemic insecticide seed treatments. Crown Copyright (C) 2020 Published by Elsevier B.V.
C1 [Lennon, Rosie J.; Wheatley, Christopher J.; Arnold, Kathryn E.; Brown, Colin D.] Univ York, Dept Environm & Geog, York, N Yorkshire, England.
   [Peach, Will J.; Dodd, Steve] Royal Soc Protect Birds, Sandy, Beds, England.
   [Dunn, Jenny C.] Univ Lincoln, Sch Life Sci, Joseph Banks Labs, Lincoln, England.
   [Shore, Richard F.; Pereira, M. Gloria; Sleep, Darren] Lancaster Environm Ctr, UK Ctr Ecol & Hydrol, Lancaster, England.
   [Wheatley, Christopher J.] Univ York, Dept Biol, York, N Yorkshire, England.
RP Lennon, RJ (corresponding author), Univ York, Dept Environm & Geog, York, N Yorkshire, England.
EM rjl529@york.ac.uk
OI Wheatley, Christopher/0000-0002-8550-2450; Dunn,
   Jenny/0000-0002-6277-2781
FU Natural Environment Research Council (NERC) as part of the ACCE Doctoral
   Training PartnershipUK Research & Innovation (UKRI)Natural Environment
   Research Council (NERC) [NE/L002450/1]; Royal Society for the Protection
   of Birds (RSPB)
FX The authors thank all landowners who gave permission for data to be
   collected; RSPB staff and Isobel Wright (University of Lincoln) who
   facilitated site access; Kerry Skelhorn, Will Kirby, Andy Bradbury and
   Derek Gruar of the RSPB who collected field samples in East Anglia,
   alongside SD and JCD; Nigel Butcher who helped with camera trap
   development; and Stuart Britton and Vivien Hartwell who facilitated
   sample collection in Lincolnshire. Research was funded by the Natural
   Environment Research Council (NERC; https://nerc.ukri.org/) as part of
   the ACCE (https://acce.shef.ac.uk/) Doctoral Training Partnership (grant
   number NE/L002450/1), and the Royal Society for the Protection of Birds
   (RSPB; https://www.rspb.org.uk/) who acted as a CASE partner for this
   project.
CR Abu Zeid EH, 2019, ECOTOX ENVIRON SAFE, V167, P60, DOI 10.1016/j.ecoenv.2018.09.121
   Addy-Orduna LM, 2018, SCI TOTAL ENVIRON, V10, P1216
   Alford A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173836
   Balfour NJ, 2016, AGR ECOSYST ENVIRON, V215, P85, DOI 10.1016/j.agee.2015.09.020
   Bass C., 2018, CURR BIOL, pR761
   Bayer Crop Science UK, 2019, REDG DET LAB SEED TA
   Bean TG, 2019, ENVIRON SCI TECHNOL, V53, P3888, DOI 10.1021/acs.est.8b07062
   Boatman ND, 2004, IBIS, V146, P131, DOI 10.1111/j.1474-919X.2004.00347.x
   Botias C, 2016, SCI TOTAL ENVIRON, V566, P269, DOI 10.1016/j.scitotenv.2016.05.065
   Bro E, 2016, ENVIRON SCI POLLUT R, V23, P9559, DOI 10.1007/s11356-016-6093-7
   Brooks ME, 2017, R J, V9, P378, DOI 10.32614/RJ-2017-066
   Byholm P, 2018, SCI TOTAL ENVIRON, V639, P929, DOI 10.1016/j.scitotenv.2018.05.185
   Cox C., 2001, Journal of Pesticide Reform, V21, P15
   de Snoo GR, 2004, PEST MANAG SCI, V60, P501, DOI 10.1002/ps.824
   Donald PF, 2001, P ROY SOC B-BIOL SCI, V268, P25, DOI 10.1098/rspb.2000.1325
   Eng ML, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-15446-x
   Ertl HM, 2018, WILDLIFE SOC B, V42, P649, DOI 10.1002/wsb.921
   European Food Safety Authority, 2006, IN RISK ASS PROV RAP, V3, P793
   European Food Safety Authority, 2010, EFSA J, V7
   Goulson D, 2013, J APPL ECOL, V50, P977, DOI 10.1111/1365-2664.12111
   Hao CY, 2018, SCI TOTAL ENVIRON, V644, P1080, DOI 10.1016/j.scitotenv.2018.06.317
   Holland JM, 2006, ANN APPL BIOL, V148, P49, DOI 10.1111/j.1744-7348.2006.00039.x
   Humann-Guilleminot S, 2019, J APPL ECOL, V56, P1502, DOI 10.1111/1365-2664.13392
   Humann-Guilleminot S, 2019, SCI TOTAL ENVIRON, V660, P1091, DOI 10.1016/j.scitotenv.2019.01.068
   Li Y, 2018, ENVIRON SCI POLLUT R, V25, P31318, DOI 10.1007/s11356-018-3121-9
   Li Y, 2018, ECOTOX ENVIRON SAFE, V164, P690, DOI 10.1016/j.ecoenv.2018.08.082
   Lopez-Antia A, 2016, J APPL ECOL, V53, P1373, DOI 10.1111/1365-2664.12668
   Lopez-Antia A, 2013, ECOTOXICOLOGY, V22, P125, DOI 10.1007/s10646-012-1009-x
   Magnusson B., 2012, HDB CALCULATION MEAS
   McGee S, 2018, PEERJ, V6, DOI 10.7717/peerj.5880
   Meier U., 1997, GROWTH STAGES MONO D
   Met Office, 2012, MET OFF INT DAT ARCH
   Millot F, 2017, ENVIRON SCI POLLUT R, V24, P5469, DOI 10.1007/s11356-016-8272-y
   Mineau P., 2013, IMPACT NATIONS MOSTW
   Mohanty B, 2017, REPROD TOXICOL, V71, P32, DOI 10.1016/j.reprotox.2017.04.006
   Newton, 1979, POPULATION ECOLOGY R
   Pandey SP, 2017, NEUROTOXICOLOGY, V60, P16, DOI 10.1016/j.neuro.2017.02.010
   Pandey SP, 2015, CHEMOSPHERE, V122, P227, DOI 10.1016/j.chemosphere.2014.11.061
   Pietravalle, 2013, PESTICIDE USAGE SURV
   Pisa LW, 2015, ENVIRON SCI POLLUT R, V22, P68, DOI 10.1007/s11356-014-3471-x
   Prosser P, 2005, ECOTOXICOLOGY, V14, P679, DOI 10.1007/s10646-005-0018-4
   Prosser P., 2001, PROJECT PN0907 POTEN
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Radolinski J, 2019, CHEMOSPHERE, V222, P445, DOI 10.1016/j.chemosphere.2019.01.150
   Radolinski J, 2018, SCI TOTAL ENVIRON, V618, P561, DOI 10.1016/j.scitotenv.2017.11.031
   Rawi S., 2019, METAB BRAIN DIS, P1
   Redfern CP., 2001, RINGERS MANUAL
   Robinson RA, 2005, BTO RES REPORT 407
   Roy CL, 2019, SCI TOTAL ENVIRON, V682, P271, DOI 10.1016/j.scitotenv.2019.05.010
   Simon-Delso N, 2015, ENVIRON SCI POLLUT R, V22, P5, DOI 10.1007/s11356-014-3470-y
   Siriwardena GM, 2008, IBIS, V150, P585, DOI 10.1111/j.1474-919X.2008.00828.x
   Stanton RL, 2018, AGR ECOSYST ENVIRON, V254, P244, DOI 10.1016/j.agee.2017.11.028
   Taliansky-Chamudis A, 2017, SCI TOTAL ENVIRON, V595, P93, DOI 10.1016/j.scitotenv.2017.03.246
   Tomizawa M, 2005, ANNU REV PHARMACOL, V45, P247, DOI 10.1146/annurev.pharmtox.45.120403.095930
   Tomizawa M, 2000, J AGR FOOD CHEM, V48, P6016, DOI 10.1021/jf000873c
   Turaga U, 2016, ENVIRON TOXICOL CHEM, V35, P1511, DOI 10.1002/etc.3305
   Wood TJ, 2017, ENVIRON SCI POLLUT R, V24, P17285, DOI 10.1007/s11356-017-9240-x
   [No title captured]
NR 58
TC 11
Z9 11
U1 7
U2 21
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0048-9697
EI 1879-1026
J9 SCI TOTAL ENVIRON
JI Sci. Total Environ.
PD JUN 25
PY 2020
VL 723
AR 138056
DI 10.1016/j.scitotenv.2020.138056
PG 11
WC Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA LR7SU
UT WOS:000535897200003
PM 32224397
OA hybrid, Green Accepted
DA 2022-02-10
ER

PT J
AU Silva-Rodriguez, EA
   Verdugo, C
   Aleuy, OA
   Sanderson, JG
   Ortega-Solis, GR
   Osorio-Zuniga, F
   Gonzalez-Acuna, D
AF Silva-Rodriguez, Eduardo A.
   Verdugo, Claudio
   Alejandro Aleuy, O.
   Sanderson, James G.
   Ortega-Solis, Gabriel R.
   Osorio-Zuniga, Felipe
   Gonzalez-Acuna, Daniel
TI Evaluating mortality sources for the Vulnerable pudu Pudu puda in Chile:
   implications for the conservation of a threatened deer
SO ORYX
LA English
DT Article
DE Chile; domestic dog; poaching; Pudu puda; Puma concolor; roadkills;
   South America; temperate forest
ID FOXES PSEUDALOPEX-GRISEUS; ENDANGERED DARWINS FOX; SOUTHERN CHILE;
   WILDLIFE; CONCOLOR; PREDATION; CARNIVORA; FULVIPES; COASTAL; ECOLOGY
AB We assessed the importance of potential Sources of mortality for the Vulnerable southern pudu Pudu puda in southern Chile using the clinical records of wildlife rehabilitation centres, necropsies of animals found in the field and a review of the diet of potential predators. To assess whether the identified mortality sources operate in nominally protected areas, we conducted a camera-trap survey in two areas to determine the presence of pudus and their potential predators. Predation by domestic dogs Canis lupus familiaris and car collisions were the commonest causes of pudu admissions to rehabilitation centres (35 of 44) and of deaths of animals encountered opportunistically in the field (seven of 14). Field data suggest that poaching could also be an important threat to pudus. Pudus were detected in both areas surveyed, accounting for 15.6% of mammal detections. Dogs accounted for 47.8% of all detections of potential predator species, followed by pumas Puma concolor (17.4%), guignas Leopardus guigna (17.4%) and chilla foxes Lycalopex griseus (17.4%). The literature survey implicated only pumas as important pudu predators among native carnivores. Our data suggest that, aside from forest loss, dogs, road kills and probably poaching are important concerns for pudu conservation. Our frequent detections of free-ranging dogs associated with roads Within nominally protected areas suggest that long-term efforts to conserve pudu will require not only the protection of remnant native forest but also substantive environmental education to modify dog management near protected areas.
C1 [Silva-Rodriguez, Eduardo A.] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.
   [Silva-Rodriguez, Eduardo A.] Univ Florida, Sch Nat Resources & Environm, Gainesville, FL 32611 USA.
   [Verdugo, Claudio; Alejandro Aleuy, O.; Ortega-Solis, Gabriel R.] Univ Austral Chile, Fac Ciencias Vet, Ctr Rehabil Fauna Silvestre, Valdivia, Chile.
   [Osorio-Zuniga, Felipe] Univ Austral Chile, Fac Ciencias, Valdivia, Chile.
   [Gonzalez-Acuna, Daniel] Univ Concepcion, Fac Med Vet, Chillan, Chile.
   [Verdugo, Claudio] Univ Florida, Coll Vet Med, Dept Small Anim Clin Sci, Gainesville, FL 32610 USA.
RP Silva-Rodriguez, EA (corresponding author), Univ Florida, Dept Wildlife Ecol & Conservat, 110 Newins Ziegler Hall, Gainesville, FL 32611 USA.
EM eduardosilva@ufl.edu
RI Aleuy, O. Alejandro/AAB-3913-2020; Ortega-Solis, Gabriel/C-4198-2011;
   Silva-Rodriguez, Eduardo A./B-1854-2010
OI Ortega-Solis, Gabriel/0000-0002-0516-5694; Silva-Rodriguez, Eduardo
   A./0000-0001-9416-8653; Aleuy, O. Alejandro/0000-0002-9239-2448
FU Fulbright-Conicyt; MIDEPLAN International Scholarship
FX The authors thank Jaime Jimenez, Kathryn E. Sieving and two anonymous
   reviewers for valuable comments on the manuscript. We thank the US Fish
   & Wildlife Service Latin American & Caribbean Program and the Valdivian
   Coastal Reserve and its park rangers, Parque Oncol, the Darwin
   Initiative, Jose Vistoso, Alfredo Almonacid, Pablo Lepez, Jaime Jimenez,
   Roguet Alba, Pascual Alba and all volunteers of wildlife rehabilitation
   centres at the Universidad de Concepcion and the Universidad Austral de
   Chile for logistical support. EAS was supported by a Fulbright-Conicyt
   fellowship. CV was supported by a MIDEPLAN International Scholarship.
CR ALLEN RE, 1976, J WILDLIFE MANAGE, V40, P317, DOI 10.2307/3800431
   Cavelier J., 2005, HIST BIODIVERSIDAD E, P632
   Ciucci P, 2007, J ZOOL, V273, P125, DOI 10.1111/j.1469-7998.2007.00379.x
   Correa Paola, 2005, Mastozool. neotrop., V12, P57
   DiMaio V., 2001, FORENSIC PATHOLOGY
   Eldridge W.D., 1987, P352
   ELGUETA EI, 2007, MAMM BIOL, V3, P179
   Franklin WL, 1999, BIOL CONSERV, V90, P33, DOI 10.1016/S0006-3207(99)00008-7
   Freer R., 2004, THESIS U DURHAM DURH
   Hershkovitz P., 1982, FIELDIANA ZOOLOGY FI, V11, P1, DOI [10.5962/bhl.title.5080, DOI 10.5962/BHL.TITLE.5080]
   Jimenez J., 2008, IUCN RED LIST THREAT
   Jimenez JE, 2007, J ZOOL, V271, P63, DOI 10.1111/j.1469-7998.2006.00218.x
   JIMENEZ JE, 1991, REV CHIL HIST NAT, V63, P177
   MARTINEZ DR, 1993, REV CHIL HIST NAT, V66, P419
   Mazaris AD, 2008, ORYX, V42, P408, DOI 10.1017/S003060530700066X
   Meier D, 2007, MAMM BIOL, V72, P204, DOI 10.1016/j.mambio.2006.08.007
   Miller S., 1973, Transactions N Am Wildl nat Resour Conf, VNo. 38, P55
   MUNOZPEDREROS A, 2005, HIST BIODIVERSIDAD E, P539
   Parker ID, 2008, J WILDLIFE MANAGE, V72, P354, DOI 10.2193/2007-036
   RAU J, 2002, GESTION AMBIENTAL CH, V8, P57
   RAU JR, 1991, REV CHIL HIST NAT, V64, P139
   Rau JR, 2002, STUD NEOTROP FAUNA E, V37, P201, DOI 10.1076/snfe.37.3.201.8567
   RAU JR, 1995, REV CHIL HIST NAT, V68, P333
   SILVARODRIGUEZ EA, 2006, THESIS U AUSTRAL CHI
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   SPALDING MG, 1993, J ZOO WILDLIFE MED, V24, P271
   TORES N, 2007, THESIS U AUSTRAL CHI
   Vila C, 2004, ANIM CONSERV, V7, P147, DOI 10.1017/s1367943004001271
   Weber M, 2003, ECOSCIENCE, V10, P443, DOI 10.1080/11956860.2003.11682792
   Wemmer C, 1998, DEER STATUS SURVEY C
   Wilson K, 2005, BIOL CONSERV, V122, P9, DOI 10.1016/j.biocon.2004.06.015
   Woodford M. H., 2000, POSTMORTEM PROCEDURE
   Zuniga Alfredo, 2005, Gestion Ambiental, V11, P31
NR 33
TC 44
Z9 46
U1 2
U2 58
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0030-6053
J9 ORYX
JI Oryx
PD JAN
PY 2010
VL 44
IS 1
BP 97
EP 103
DI 10.1017/S0030605309990445
PG 7
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA 558TO
UT WOS:000274770200016
OA Bronze
DA 2022-02-10
ER

PT J
AU Huaranca, JC
   Villalba, ML
   Negroes, N
   Jimenez, JE
   Macdonald, DW
   Pacheco, LF
AF Carlos Huaranca, Juan
   Lilian Villalba, Ma
   Negroes, Nuno
   Jimenez, Jaime E.
   Macdonald, David W.
   Pacheco, Luis F.
TI Density and activity patterns of Andean cat and pampas cat (Leopardus
   jacobita and L. colocolo) in the Bolivian Altiplano
SO WILDLIFE RESEARCH
LA English
DT Article
DE camera-trapping; endangered species; generalist species; overlap;
   spatially explicit capture-recapture; specialist species
ID POPULATION-DENSITY; MICROHABITAT USE; BOBCATS; PREY; CARNIVORES;
   MORTALITY; SURVIVAL; ECOLOGY; FOREST; FOXES
AB ContextUnderstanding the factors that determine the distribution and abundance of species is an important aim of ecology and prerequisite for conservation. The Andean cat (Leopardus jacobita) and the pampas cat (L. colocolo) are two of the least studied felids. Both are threatened, of similar size and live sympatrically in the Andes of Argentina, Bolivia, Chile, and Peru. AimsWe aimed at estimating the population densities of the Andean cat and pampas cat in two continuous areas and to analyse the activity patterns of these two species and that of mountain vizcacha (Lagidium viscacia), the main prey of the Andean cat. MethodsWe used camera traps to evaluate the density of both felid species using the space explicit capture recapture (SECR) framework and the overlap in their activity patterns with that of mountain vizcacha, using the kernel-density estimator in two contiguous areas in the Bolivian Altiplano, at Muro-Amaya and at Micani, both within the Ciudad de Piedra region. Key resultsAndean cat density was estimated at 6.45 individuals per 100km(2) in Muro-Amaya and 6.91 individuals per 100km(2) in Micani, whereas the density of the pampas cat was 5.31 individuals per 100km(2) and 8.99 individuals per 100km(2) respectively. The Andean cat was mainly nocturnal, whereas the pampas cat was cathemeral. The activity of the mountain vizcacha overlapped less with that of its specialised predator, the Andean cat, than with that of the pampas cat. ConclusionsIn line with our predictions, the Andean cat, considered a more specialised nocturnal hunter, particularly of mountain vizcacha, had lower population densities than did the more generalist pampas cat. ImplicationsLow population densities, as compared with theoretical expectations, pose an additional conservation problem for these felids, in an area such as the high Andes.
C1 [Carlos Huaranca, Juan; Jimenez, Jaime E.] Univ Los Lagos, Programa Doctorado Ciencias Menc Conservac & Mane, Ave Fushlocher 1305, Osorno, Chile.
   [Carlos Huaranca, Juan; Lilian Villalba, Ma] Andean Cat Alliance, La Paz, Bolivia.
   [Carlos Huaranca, Juan] Univ Mayor San Simon, Ctr Biodiversidad & Genet, Calle Sucre Frente Parque La Torre, Cochabamba, Bolivia.
   [Negroes, Nuno] Univ Aveiro, CESAM, Campus Univ Santiago, P-3810193 Aveiro, Portugal.
   [Negroes, Nuno] Univ Aveiro, Dept Biol, Campus Univ Santiago, P-3810193 Aveiro, Portugal.
   [Jimenez, Jaime E.] Univ North Texas, Coll Sci, Adv Environm Res Inst, Dept Biol Sci, Denton, TX 76203 USA.
   [Macdonald, David W.] Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney House,Abingdon Rd, Abingdon OX13 5QL, Oxon, England.
   [Pacheco, Luis F.] Univ Mayor San Andres, Carrera Biol, Inst Ecol, Colecc Boliviana Fauna, Calle 27 Cota Cota, La Paz, Bolivia.
   [Pacheco, Luis F.] BIOTA, Ctr Estudios Biol Teor & Aplicada, Ave Las Retamas 15,Entre Calles 34 & 35, La Paz, Bolivia.
RP Huaranca, JC (corresponding author), Univ Los Lagos, Programa Doctorado Ciencias Menc Conservac & Mane, Ave Fushlocher 1305, Osorno, Chile.; Huaranca, JC (corresponding author), Andean Cat Alliance, La Paz, Bolivia.; Huaranca, JC (corresponding author), Univ Mayor San Simon, Ctr Biodiversidad & Genet, Calle Sucre Frente Parque La Torre, Cochabamba, Bolivia.
EM jchuaranca@gmail.com
RI Jimenez, Jaime E./AAO-7531-2020; Huaranca, Juan Carlos/AAH-8050-2020;
   Soares, Nuno/E-4348-2015
OI Huaranca, Juan Carlos/0000-0001-9747-4568; Soares,
   Nuno/0000-0003-1315-6648
FU Wildlife Conservation Network; Small Cats Action Fund (Panthera
   Foundation); Mohamed bin Zayed Species Conservation Fund [11053162];
   Iniciativa de Especies Amenazadas Becas 'Werner Hanagarth' (Fundacion
   Puma); Andean Cat Alliance; ESRI Conservation Program; Idea Wild;
   Instituto de Ecologi'a at Universidad Mayor de San Andres; Wildlife
   Conservation Society of Bolivia
FX We acknowledge the collaboration of indigenous authorities of Ayllu
   Pahaza, and the Municipality of Calacoto. This research was funded by
   Wildlife Conservation Network, Small Cats Action Fund (Panthera
   Foundation), Mohamed bin Zayed Species Conservation Fund (Project:
   11053162), and Iniciativa de Especies Amenazadas Becas 'Werner
   Hanagarth' (Fundacion Puma). We also want to thank the support granted
   by the Andean Cat Alliance, ESRI Conservation Program, Idea Wild,
   Instituto de Ecologi ' a at Universidad Mayor de San Andres, and the
   Wildlife Conservation Society of Bolivia. Our thanks also go to
   Alejandra Torrez for her constant support. Herminio Ticona, Jim
   Sanderson, Miguel Saavedra, Mauricio Penaranda, Yony Quinones, and
   Gregorio Roque helped during fieldwork. We are grateful to Susan Walker
   for helpful comments on a previous version of this manuscript and two
   anonymous reviewers provided useful comments to the previous version of
   this manuscript.
CR Arispe Rosario, 2007, Cat News, V46, P36
   Balme GA, 2009, J WILDLIFE MANAGE, V73, P433, DOI 10.2193/2007-368
   Blankenship TL, 2006, WILDLIFE BIOL, V12, P297, DOI 10.2981/0909-6396(2006)12[297:CSACMB]2.0.CO;2
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Carbone C, 2002, SCIENCE, V295, P2273, DOI 10.1126/science.1067994
   Caruso N, 2012, ANN ZOOL FENN, V49, P181, DOI 10.5735/086.049.0306
   Cossios D., 2007, MANUAL METOLOGIAS RE
   Delibes-Mateos M, 2014, MAMM BIOL, V79, P393, DOI 10.1016/j.mambio.2014.04.006
   Efford MG, 2014, METHODS ECOL EVOL, V5, P599, DOI 10.1111/2041-210X.12169
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Fajardo Ursula, 2014, Rev. peru biol., V21, P061, DOI 10.15381/rpb.v21i1.8248
   Foster VC, 2013, BIOTROPICA, V45, P373, DOI 10.1111/btp.12021
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Gobierno Autonomo Municipal de Calacoto, 2008, PLAN DES MUN MUN CAL
   Haines AM, 2004, ACTA THERIOL, V49, P349, DOI 10.1007/BF03192533
   Huaranca J. C., 2011, OPCIONES ESTABLECIMI
   HUARANCA JC, 2013, CAT NEWS, V58, P4
   Ivan JS, 2013, ECOLOGY, V94, P817, DOI 10.1890/12-0102.1
   Jimenez JE, 1996, REV CHIL HIST NAT, V69, P113
   Jones M, 2000, J CHEM ECOL, V26, P455, DOI 10.1023/A:1005417707588
   Kelt DA, 2001, AM NAT, V157, P637, DOI 10.1086/320621
   Kolowski JM, 2002, J WILDLIFE MANAGE, V66, P822, DOI 10.2307/3803146
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Lopez G, 2014, EUR J WILDLIFE RES, V60, P359, DOI 10.1007/s10344-013-0794-8
   Lucherini M., 2016, IUCN RED LIST THREAT, DOI 10.2305/IUCN.UK.2016-1.RLTS.T6929A85324366.en
   Lucherini M, 2008, MAMMALIA, V72, P95, DOI 10.1515/MAMM.2008.018
   Lucherini M, 2009, J MAMMAL, V90, P1404, DOI 10.1644/09-MAMM-A-002R.1
   Macdonald D.W., 2010, BIOL CONSERVATION WI
   Marino J, 2011, DIVERS DISTRIB, V17, P311, DOI 10.1111/j.1472-4642.2011.00744.x
   Marino Jorgelina, 2010, P581
   MCNAB BK, 1963, AM NAT, V97, P133, DOI 10.1086/282264
   Napolitano C, 2008, MOL ECOL, V17, P678, DOI 10.1111/j.1365-294X.2007.03606.x
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   Nowell K., 1996, WILD CATS STATUS SUR
   Pereira L. J. A, 2009, THESIS
   R Core Team, 2017, R LANG ENV STAT COMP
   Reppucci J, 2011, J MAMMAL, V92, P140, DOI 10.1644/10-MAMM-A-053.1
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Riley SPD, 2006, J WILDLIFE MANAGE, V70, P1425, DOI 10.2193/0022-541X(2006)70[1425:SEOBAG]2.0.CO;2
   Ross J, 2013, J ZOOL, V290, P96, DOI 10.1111/jzo.12018
   Royle JA, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P163, DOI 10.1007/978-4-431-99495-4_10
   Royle JA, 2009, J APPL ECOL, V46, P118, DOI 10.1111/j.1365-2664.2008.01578.x
   Santini L, 2018, GLOBAL ECOL BIOGEOGR, V27, P968, DOI 10.1111/geb.12758
   Santos F, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0213671
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Silva M, 2001, GLOBAL ECOL BIOGEOGR, V10, P469, DOI 10.1046/j.1466-822x.2001.00261.x
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Suraci JP, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170255
   TARIFA T., 2001, REV BOLIVIANA ECOLOG, V9, P29
   Tellaeche C. G., 2015, THESIS
   Teran J., 2012, EVALUACION VEGETACIO
   Theuerkauf J, 2003, J MAMMAL, V84, P243, DOI 10.1644/1545-1542(2003)084<0243:DPADOW>2.0.CO;2
   Torrico J. O, 2009, THESIS
   Villalba L., 2016, IUCN RED LIST THREAT
   Villalba M. L., 2009, LIBRO ROJO FAUNA SIL, P525
   Villalba M. L., 2009, LIBRO ROJO FAUNA SIL, P451
   Villalba M. L., 2009, 10 INT MAMM C, P187
   Villalba M. L., 2009, 10 INT MAMM C, P113
   Viscarra M. E., 2008, THESIS
   Walker RS, 2007, J MAMMAL, V88, P519, DOI 10.1644/06-MAMM-A-172R.1
   Yensen E, 2000, MAMMALIAN SPECIES, V611, P1, DOI [10.1644/1545-1410(2000)644<0001:OJ>2.0.CO;2, DOI 10.1644/1545-1410(2000)644<0001:OJ>2.0.CO;2]
   Zanon-Martinez JI, 2016, WILDLIFE RES, V43, P449, DOI 10.1071/WR16056
NR 62
TC 4
Z9 5
U1 1
U2 21
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
PD FEB
PY 2020
VL 47
IS 1
BP 68
EP 76
DI 10.1071/WR19053
PG 9
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA KL2AI
UT WOS:000513231100007
DA 2022-02-10
ER

PT J
AU Kou, XY
   Wang, Z
   Chen, MZ
   Ye, SH
AF Kou, XY
   Wang, Z
   Chen, MZ
   Ye, SH
TI Fully automatic algorithm for region of interest location in camera
   calibration
SO OPTICAL ENGINEERING
LA English
DT Article
DE fully automatic camera calibration; zoom lenses; Hough transforms; Radon
   transforms; intelligent calibration; computer vision
ID VISION
AB We present an automatic method for region of interest (ROI) location in camera calibration used in computer vision inspection. An intelligent ROI location algorithm based on the Radon transform is developed to automate the calibration process. The algorithm remains robust even if the anchor target has a notable rotation angle in the target plane. This method functions well although the anchor target is not carefully positioned. Several improvement methods are studied to avoid the algorithm's huge time/space consumption problem. The algorithm runs about 100 times faster if these improvement methods are applied. Using this method fully automatic camera calibration is achieved without human interactive ROI specification. Experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically. (C) 2002 society of Photo-Optical Instrumentation Engineers.
C1 Tianjin Univ, State Key Lab Precis Measuring Technol & Instrume, Tianjin 300072, Peoples R China.
RP Kou, XY (corresponding author), Tianjin Univ, State Key Lab Precis Measuring Technol & Instrume, Tianjin 300072, Peoples R China.
RI Chen, Mingzhou/F-3409-2011; Kou, Xinyu/E-2979-2010
OI Chen, Mingzhou/0000-0002-6190-5167; Kou, Xinyu/0000-0002-6641-2492
CR Abdel-Aziz YI., 1971, P S CLOS RANG PHOT U, P1, DOI 10.14358/PERS.81.2.103
   Araabi BN, 2000, REAL-TIME IMAGING, V6, P129, DOI 10.1006/rtim.1999.0181
   Chen YX, 2000, J INFRARED MILLIM W, V19, P43
   *IEEE, 1995, IEEE INT C SYSTEMS M, V4711, P4262
   Kondo H, 1999, IEICE T INF SYST, VE82D, P1200
   NUMAO T, 1998, J I IMAGE INF TV ENG, V52
   TARABANIS K, 1994, CVGIP-IMAG UNDERSTAN, V59, P226, DOI 10.1006/cviu.1994.1017
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   TSAI TH, 2000, J CHIN SOC MECH ENG, V21, P223
   YANG P, 1998, J SHANGHAI JIAOTONG, V32, P61
NR 10
TC 2
Z9 6
U1 0
U2 4
PU SPIE-INT SOCIETY OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA
SN 0091-3286
J9 OPT ENG
JI Opt. Eng.
PD JUN
PY 2002
VL 41
IS 6
BP 1220
EP 1226
DI 10.1117/1.1476327
PG 7
WC Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Optics
GA 562BB
UT WOS:000176176600011
DA 2022-02-10
ER

PT J
AU Simon, JA
   Chancel, E
   Hubert, P
   Aubert, D
   Villena, I
   Gilot-Fromont, E
   Poulle, ML
AF Simon, Julie Alice
   Chancel, Eva
   Hubert, Pauline
   Aubert, Dominique
   Villena, Isabelle
   Gilot-Fromont, Emmanuelle
   Poulle, Marie-Lazarine
TI Pattern of latrine use by domestic cats on dairy farms and the
   implications for Toxoplasma gondii transmission
SO VETERINARY PARASITOLOGY
LA English
DT Article
DE Felis silvestris cams; Video trap; Cat faeces; Toxoplasmosis;
   Intermediate hosts
ID SPATIAL-DISTRIBUTION; PARASITIC ZOONOSES; DEFECATION SITES; CAMERA
   TRAPS; SWINE FARMS; HOME-RANGE; OOCYSTS; SEROPREVALENCE; WILDLIFE; SOIL
AB Toxoplasma gondii is the parasite responsible for toxoplasmosis, a highly prevalent zoonosis that affects humans and warm-blooded animals. Faeces of infected cats can contain millions of T. gondii oocysts, which remain infectious in the environment for months. Sites repeatedly used by cats for defecation (latrines') are recognised as hotspots of T. gondii soil contamination, but this contamination varies from one latrine to another. To understand this spatial heterogeneity, camera traps were deployed in 39 cat latrines on three dairy farms with high density cat populations and programmed to record visits during sixteen 10-day sessions, rotating between three farms over a period of a year. Generalized Linear Mixed Models were used to test the effects of cat sexual maturity, latrine location and season on the number of cat faeces deposited and on the number of cats defecating per latrine, as determined from the analysis of 41,282 video recordings. Sexually immature cats defecated 6.60 fold (95% CI = [2.87-15.25]) more often in latrines located close to a feeding site than in other latrines. This pattern was also observed for mature males (odds ratio [OR] = 9.42, 95% CI = [3.29-26.91]), especially during winter, but not for mature females (OR = 1.77, 95% CI = [0.80-3.94]). The number of defecating cats was also 2.67-fold (95% Cl = [1.66-4.30], P < 0.001) higher in latrines located close to a feeding point than in those located far from it, regardless of cat category and season. Visits by intermediate T. gondii hosts (micromammals, birds and others) were also recorded. Out of the 39 latrines, 30 (76.92%) were visited by at least one intermediate host during the study period, and some latrines were highly frequented (up to 8.74 visits/day on average). These results provide evidence that the location of food resources in dairy farms influences the latrine use pattern by cats. Highly frequented latrines can be of high risk of T. gondii infection for definitive and intermediate hosts.
C1 [Simon, Julie Alice; Aubert, Dominique; Villena, Isabelle; Poulle, Marie-Lazarine] Univ Reims, UFR Med, SFR Cap Sante, Lab Parasitol Mycol EA Escape 7510, 51 Rue Cognacg Jay, F-51095 Reims, France.
   [Simon, Julie Alice; Hubert, Pauline; Poulle, Marie-Lazarine] Univ Reims, CERFE, 5 Rue Heronniere, F-08240 Boult Aux Bois, France.
   [Chancel, Eva; Gilot-Fromont, Emmanuelle] VetAgro Sup, Campus Vet Lyon,1 Ave Bourgelat, F-69280 Marcy Letoile, France.
   [Hubert, Pauline] Faune Act, 6 Rue Jardin Gascon, F-08240 Boult Aux Bois, France.
   [Gilot-Fromont, Emmanuelle] Univ Claude Bernard Lyon 1, UMR CNRS Lab Biometrie & Biol Evolut 5558, 43 Bd 11 Novembre 1918, F-69622 Villeurbanne, France.
RP Simon, JA (corresponding author), Univ Reims, UFR Med, SFR Cap Sante, Lab Parasitol Mycol EA Escape 7510, 51 Rue Cognacg Jay, F-51095 Reims, France.
EM julie.rabeisensimon@gmail.com; chancel_eva@hotmail.fr;
   pauline_hubert@hotmail.fr; daubert@chu-reims.fr; ivillena@chu-reims.fr;
   emmanuelle.gilotfromont@vetagro-sup.fr;
   marie-lazarine.poulle@univ-reims.fr
RI Aubert, dominique/AAH-5854-2019
OI Gilot-Fromont, Emmanuelle/0000-0003-0011-7519
FU Agence de l'Environnement et de la Maitrise de l'Energie (ADEME);
   Ministry of Higher Education, Research and Innovation, France
FX Part of this work was financially supported by the Agence de
   l'Environnement et de la Maitrise de l'Energie (ADEME) in the framework
   of the "L'animal, le Sol, l'Eau: Dynamique de la Contamination
   Environnementale par la toxoplasmose" project (AFSSET-ADEME, 2010-2014).
   Julie Alice Simon was also supported by a grant from the Ministry of
   Higher Education, Research and Innovation, France.
CR Afonso E, 2008, INT J PARASITOL, V38, P1017, DOI 10.1016/j.ijpara.2008.01.004
   Afonso Eve, 2013, International Journal for Parasitology Parasites and Wildlife, V2, P278, DOI 10.1016/j.ijppaw.2013.09.006
   Anderson K. A., 2002, MODEL SELECTION MULT
   Baneth G., 2016, J COMP PATHOL, V155, pS54
   Barratt DG, 1997, ECOGRAPHY, V20, P271, DOI 10.1111/j.1600-0587.1997.tb00371.x
   Bastien M, 2018, FOLIA PARASIT, V65, DOI 10.14411/fp.2018.002
   Boyer K, 2011, CLIN INFECT DIS, V53, P1081, DOI 10.1093/cid/cir667
   Bradshaw JWS, 2012, BEHAV DOMESTIC CAT
   Buesching CD, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1328
   Conrad PA, 2005, INT J PARASITOL, V35, P1155, DOI 10.1016/j.ijpara.2005.07.002
   Courchamp F, 2000, WILDLIFE RES, V27, P603, DOI 10.1071/WR99049
   Dabritz HA, 2010, ZOONOSES PUBLIC HLTH, V57, P34, DOI 10.1111/j.1863-2378.2009.01273.x
   del Hoyo J., 1992, HDB BIRDS WORLD, V1
   Di Cerbo AR, 2008, HELMINTHOLOGIA, V45, P13, DOI 10.2478/s11687-008-0002-7
   Dubey JP, 2006, VET PARASITOL, V140, P69, DOI 10.1016/j.vetpar.2006.03.018
   DUBEY JP, 1995, J PARASITOL, V81, P723, DOI 10.2307/3283961
   Dubey JP., 2010, TOXOPLASMOSIS ANIMAL
   Elizondo EC, 2016, WILDLIFE BIOL, V22, P246, DOI 10.2981/wlb.00237
   Ferreira JP, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025970
   Forin-Wiart MA, 2014, EUR J WILDLIFE RES, V60, P665, DOI 10.1007/s10344-014-0833-0
   Gauss CBL, 2003, J PARASITOL, V89, P1067, DOI 10.1645/GE-114
   Germain E, 2008, J ZOOL, V276, P195, DOI 10.1111/j.1469-7998.2008.00479.x
   Gilot-Fromont E, 2009, VET PARASITOL, V161, P36, DOI 10.1016/j.vetpar.2008.12.004
   Gilot-Fromont E., 2012, TOXOPLASMOSIS RECENT, P1
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Goszczynski J, 2009, FOLIA ZOOL, V58, P363
   Gotteland C, 2014, INT J HEALTH GEOGR, P13
   Gotteland C, 2014, VET PARASITOL, V205, P629, DOI 10.1016/j.vetpar.2014.08.003
   Guislain MH, 2007, PARASITE, V14, P299, DOI 10.1051/parasite/2007144299
   Herrmann DC, 2010, INT J PARASITOL, V40, P285, DOI 10.1016/j.ijpara.2009.08.001
   Hill D, 2002, CLIN MICROBIOL INFEC, V8, P634, DOI 10.1046/j.1469-0691.2002.00485.x
   Horn JA, 2011, J WILDLIFE MANAGE, V75, P1177, DOI 10.1002/jwmg.145
   Ishida Y, 1998, J ETHOL, V16, P15, DOI 10.1007/BF02896349
   IZAWA M, 1982, Japanese Journal of Ecology, V32, P373
   Jordan NR, 2007, ANIM BEHAV, V73, P613, DOI 10.1016/j.anbehav.2006.06.010
   Kitts-Morgan SE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120513
   Knapp J, 2018, INT J PARASITOL, V48, P937, DOI [10.1016/j.ijpara.2018.05.007, 10.1016/j.ijpara.2018.]
   Krauze-Gryz D, 2017, URBAN ECOSYST, V20, P945, DOI 10.1007/s11252-016-0634-1
   Kukielka E, 2013, PREV VET MED, V112, P213, DOI 10.1016/j.prevetmed.2013.08.008
   Lehmann T, 2003, INFECT GENET EVOL, V3, P135, DOI 10.1016/S1567-1348(03)00067-4
   Lelu M, 2012, APPL ENVIRON MICROB, V78, P5127, DOI 10.1128/AEM.00246-12
   LIBERG O, 1982, ACTA THERIOL, V27, P115, DOI 10.4098/AT.arch.82-9
   Liberg Olof, 2000, P119
   Lindsay DS, 2009, J PARASITOL, V95, P1019, DOI 10.1645/GE-1919.1
   Macdonald D.W., 1985, P619
   Macdonald David W., 2000, P95
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Milkovic M, 2009, AREA, V41, P310, DOI 10.1111/j.1475-4762.2008.00865.x
   Munoz-Zanzi CA, 2010, EMERG INFECT DIS, V16, P1591, DOI 10.3201/eid1610.091674
   Page LK, 2009, EMERG INFECT DIS, V15, P1530, DOI 10.3201/eid1509.090128
   Page LK, 1998, AM MIDL NAT, V140, P180, DOI 10.1674/0003-0031(1998)140[0180:RLSAIP]2.0.CO;2
   Polley L, 2005, INT J PARASITOL, V35, P1279, DOI 10.1016/j.ijpara.2005.07.003
   R Core Team, 2014, R LANG ENV STAT COMP
   Raoul F, 2015, VET PARASITOL, V213, P162, DOI 10.1016/j.vetpar.2015.07.034
   Richomme C, 2010, EPIDEMIOL INFECT, V138, P1257, DOI 10.1017/S0950268810000117
   Robertson ID, 2002, MICROBES INFECT, V4, P867, DOI 10.1016/S1286-4579(02)01607-6
   Rodgers TW, 2015, MAMM BIOL, V80, P380, DOI 10.1016/j.mambio.2015.05.004
   Rovero Francesco, 2010, Abc Taxa, V8, P100
   Say L, 1999, P ROY SOC B-BIOL SCI, V266, P2071, DOI 10.1098/rspb.1999.0889
   Schares G, 2016, INT J PARASITOL, V46, P263, DOI 10.1016/j.ijpara.2015.12.006
   Simon JA, 2017, INT J PARASITOL, V47, P357, DOI 10.1016/j.ijpara.2017.01.004
   Simon JA, 2018, PARASITE VECTOR, V11, DOI 10.1186/s13071-018-2834-4
   Soler Lucía, 2009, Mastozool. neotrop., V16, P485
   Tenter AM, 2000, INT J PARASITOL, V30, P1217, DOI 10.1016/S0020-7519(00)00124-7
   Traversa D, 2014, PARASITE VECTOR, V7, DOI 10.1186/1756-3305-7-67
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Turner D.C., 2013, DOMESTIC CAT BIOL IT
   VanWormer E, 2013, COMP IMMUNOL MICROB, V36, P217, DOI 10.1016/j.cimid.2012.10.006
   WEIGEL RM, 1995, J PARASITOL, V81, P736, DOI 10.2307/3283964
   Zuur Alain F., 2009, P1
NR 70
TC 0
Z9 0
U1 2
U2 23
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0304-4017
EI 1873-2550
J9 VET PARASITOL
JI Vet. Parasitol.
PD SEP
PY 2019
VL 273
BP 112
EP 121
DI 10.1016/j.vetpar.2019.08.001
PG 10
WC Parasitology; Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Parasitology; Veterinary Sciences
GA JA1IK
UT WOS:000487570500018
PM 31476666
OA Bronze
DA 2022-02-10
ER

PT J
AU Howe, EJ
   Buckland, ST
   Despres-Einspenner, ML
   Kuhl, HS
AF Howe, Eric J.
   Buckland, Stephen T.
   Despres-Einspenner, Marie-Lyne
   Kuehl, Hjalmar S.
TI Model selection with overdispersed distance sampling data
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal abundance; camera trapping; cue counting; distance sampling;
   model selection; overdispersion; QAIC
ID TRANSECT SURVEYS; SPATIAL MODELS; INFERENCE
AB Distance sampling (DS) is a widely used framework for estimating animal abundance. DS models assume that observations of distances to animals are independent. Non-independent observations introduce overdispersion, causing model selection criteria such as AIC or AIC(c) to favour overly complex models, with adverse effects on accuracy and precision. We describe, and evaluate via simulation and with real data, estimators of an overdispersion factor (c), and associated adjusted model selection criteria (QAIC) for use with overdispersed DS data. In other contexts, a single value of c<^> is calculated from the "global" model, that is the most highly parameterised model in the candidate set, and used to calculate QAIC for all models in the set; the resulting QAIC values, and associated Delta QAIC values and QAIC weights, are comparable across the entire set. Candidate models of the DS detection function include models with different general forms (e.g. half-normal, hazard rate, uniform), so it may not be possible to identify a single global model. We therefore propose a two-step model selection procedure by which QAIC is used to select among models with the same general form, and then a goodness-of-fit statistic is used to select among models with different forms. A drawback of this approach is that QAIC values are not comparable across all models in the candidate set. Relative to AIC, QAIC and the two-step model selection procedure avoided overfitting and improved the accuracy and precision of densities estimated from simulated data. When applied to six real datasets, adjusted criteria and procedures selected either the same model as AIC or a model that yielded a more accurate density estimate in five cases, and a model that yielded a less accurate estimate in one case. Many DS surveys yield overdispersed data, including cue counting surveys of songbirds and cetaceans, surveys of social species including primates, and camera-trapping surveys. Methods that adjust for overdispersion during the model selection stage of DS analyses therefore address a conspicuous gap in the DS analytical framework as applied to species of conservation concern.
C1 [Howe, Eric J.; Buckland, Stephen T.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
   [Howe, Eric J.] Trent Univ, Wildlife Res & Monitoring Sect, Ontario Minist Nat Resources & Forestry, DNA Bldg, Peterborough, ON, Canada.
   [Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP Howe, EJ (corresponding author), Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.; Howe, EJ (corresponding author), Trent Univ, Wildlife Res & Monitoring Sect, Ontario Minist Nat Resources & Forestry, DNA Bldg, Peterborough, ON, Canada.
EM ejh20@st-andrews.ac.uk
RI Buckland, Stephen T/A-1998-2012
OI Buckland, Stephen/0000-0002-9939-709X
FU Robert Bosch Foundation; Max Planck SocietyMax Planck Society;
   University of St AndrewsDeutsche KrebshilfeHelmholtz Association;
   Ministere de l'Enseignement Superieur et de la Recherche Scientifique
FX We thank the Robert Bosch Foundation, the Max Planck Society and the
   University of St Andrews for funding, the Ministere de l'Enseignement
   Superieur et de la Recherche Scientifique and the Ministere de
   l'Environnement et des Eaux et Forets in Cote d'Ivoire for permission to
   conduct field research in Tai National Park, Dr. Roman Wittig for
   permitting data collection in the area of the Tai Chimpanzee Project,
   and Dr. Joeseph Nocera for informal discussions about model selection in
   the presence of overdispersion.
CR Akaikei H., 1973, 2 INT S INFORM THEOR, P267
   Anderson D.R., 2002, MODEL SELECTION MULT
   Borchers DL, 2002, ESTIMATING ANIMAL AB, DOI [DOI 10.1007/978-1-4471-3708-5, 10.1007/978-1-4471-3708-5]
   Buckland S.T., 2001, pi
   BUCKLAND ST, 1984, BIOMETRICS, V40, P811, DOI 10.2307/2530926
   Buckland ST., 2004, ADV DISTANCE SAMPLIN
   Buckland ST, 2006, AUK, V123, P345, DOI 10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Burnham KP, 2001, WILDLIFE RES, V28, P111, DOI 10.1071/WR99107
   Cox D.R., 1989, ANAL BINARY DATA, V2nd ed.
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Hedley SL, 2004, J AGR BIOL ENVIR ST, V9, P181, DOI 10.1198/1085711043578
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Johnson DS, 2010, BIOMETRICS, V66, P310, DOI 10.1111/j.1541-0420.2009.01265.x
   Johnson JB, 2004, TRENDS ECOL EVOL, V19, P101, DOI 10.1016/j.tree.2003.10.013
   LEBRETON JD, 1992, ECOL MONOGR, V62, P67, DOI 10.2307/2937171
   LIANG KY, 1993, BIOMETRICS, V49, P623, DOI 10.2307/2532575
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Miller DL, 2013, METHODS ECOL EVOL, V4, P1001, DOI 10.1111/2041-210X.12105
   R Core Team, 2017, R LANG ENV STAT COMP
NR 20
TC 7
Z9 7
U1 3
U2 18
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JAN
PY 2019
VL 10
IS 1
BP 38
EP 47
DI 10.1111/2041-210X.13082
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HK2NU
UT WOS:000457750600004
OA Green Accepted, Bronze
DA 2022-02-10
ER

PT J
AU Weinstein, BG
AF Weinstein, Ben G.
TI MotionMeerkat: integrating motion video detection and ecological
   monitoring
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera traps; computer vision; hummingbirds; plant-animal interactions
AB Human observation is expensive and limits the breadth of data collection. For this reason, remotely placed video cameras are increasingly used to monitor animals. One drawback of field-based video recordings is extensive review time. Computer vision can mitigate this cost and enhance data collection by extracting biological information from images with minimal time investment. MotionMeerkat is a new standalone program that identifies motion events from a video stream. After running a video, the user reviews a folder of candidate motion frames for the target organism. This tool reduces the time needed to review videos and accommodates a variety of inputs. I tested MotionMeerkat using hummingbird-plant videos recorded in a tropical montane forest. To validate the optimal parameter set for finding motion events, I counted hummingbirds observed from direct video review compared to events found in images returned from MotionMeerkat. To show the generality of the approach, MotionMeerkat was tested on a set of terrestrial and underwater videos. To assess the performance of the background subtraction for further image analysis, I hand counted the number of frames with target organisms and compared them to the MotionMeerkat output. MotionMeerkat was highly successful in finding motion events and often reduced the number of frames needed to capture hummingbird visitation by over 90%. Both background approaches effectively found a variety of organisms in ecological videos. I provide general recommendations for parameter settings and extending this approach in the future.
C1 SUNY Stony Brook, Dept Ecol & Evolut, Stony Brook, NY 11794 USA.
RP Weinstein, BG (corresponding author), SUNY Stony Brook, Dept Ecol & Evolut, Stony Brook, NY 11794 USA.
EM bweinste@life.bio.sunysb.edu
OI Weinstein, Ben/0000-0002-2176-7935
FU National Geographic GrantNational Geographic Society [9382-13]; NZ
   government through Ministry for Primary Industries and Land Information
   New Zealand Ocean Survey
FX I would thank Rebecca Justicia and the families of Santa Lucia for their
   support. Field work was assisted by K. Lohman, H. Beck, A. Shankar and
   N. Munoz. Camera review was assisted by L. Ditmar, and the manuscript
   was improved with help from C. Graham and H. Lynch. This project was
   supported by a National Geographic Grant #9382-13. Thanks to A. Rees, R.
   O'Driscoll and N. Moy for sharing videos. The O'Driscoll fisheries video
   data were collected by the New Zealand National Institute of Water and
   Atmospheric Research (NIWA) with funding from the NZ government through
   Ministry for Primary Industries and Land Information New Zealand Ocean
   Survey 2020. This study was greatly improved by comments from two
   anonymous reviewers. The author declares no conflict of interest in this
   work.
CR Maglianesi MA, 2014, ECOLOGY, V95, P3325, DOI 10.1890/13-2261.1
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Boom BJ, 2014, ECOL INFORM, V23, P83, DOI 10.1016/j.ecoinf.2013.10.006
   Bouwmans T, 2014, MACH VISION APPL, V25, P1101, DOI 10.1007/s00138-013-0578-x
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chunmei Qing, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3577, DOI 10.1109/ICIP.2011.6116491
   Crall J. P., 2013, 2013 IEEE WORKSH APP
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   Kalal Z., 2011, PAMI, P1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Mallet D, 2014, FISH RES, V154, P44, DOI 10.1016/j.fishres.2014.01.019
   O'Driscoll RL, 2012, ICES J MAR SCI, V69, P648, DOI 10.1093/icesjms/fss010
   Pennekamp F, 2013, METHODS ECOL EVOL, V4, P483, DOI 10.1111/2041-210X.12036
   Ribic CA, 2012, STUD AVIAN BIOL, P1, DOI 10.1525/california/9780520273139.001.0001
   Rutz C, 2013, METHODS ECOL EVOL, V4, P114, DOI 10.1111/2041-210x.12003
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 19
TC 52
Z9 52
U1 4
U2 50
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD MAR
PY 2015
VL 6
IS 3
BP 357
EP 362
DI 10.1111/2041-210X.12320
PG 6
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA CE2AF
UT WOS:000351613900014
OA Bronze
DA 2022-02-10
ER

PT J
AU Luna, N
   Varela, AI
   Brokordt, K
   Luna-Jorquera, G
AF Luna, Nicolas
   Varela, Andrea, I
   Brokordt, Katherina
   Luna-Jorquera, Guillermo
TI Assessing Potential Predation Risk by Introduced Predators on Unattended
   Eggs in the Red-Tailed Tropicbird, Phaethon rubricauda, on Rapa Nui
   (Easter Island)
SO TROPICAL CONSERVATION SCIENCE
LA English
DT Article
DE breeding seabirds; new breeders; invasive species; subtropical oceanic
   islands; southeastern Pacific Ocean
ID INVASIVE RATS; BIRD EGGS; COLONIES; SEABIRDS; BIOLOGY; SEA
AB Anthropogenic impact has been heavy in remote oceanic islands, including the introduction of alien species, having negative effects on native seabirds. The isolated and subtropical Rapa Nui (Easter Island) is one of the few known breeding sites of the red-tailed tropicbird, Phaethon rubricauda in Chile (southeastern Pacific Ocean) where is listed as vulnerable. A relatively new breeding colony is found in the Rano Raraku volcano, where human-introduced species are present. We used hen eggs as a proxy for red-tailed tropicbird eggs to assess potential predation risk on unattended eggs. Each experimental egg was monitored by camera traps during 6 days. Three predatory species were identified on the records: the Brown rat Rattus norvegicus, the Polynesian rat Rattus exulans, and the raptor Chimango Caracara Phalcoboenus chimango. The most frequent species were the Rattus spp .A total of 45 predatory visits were recorded with a total time of 1.7 h, accounting for the 0.3% of the experimental time. Within this time of visits, all the potential predators spent time in both interacting activities (trying to prey on) and no-interacting activities with the experimental eggs. Only a Brown rat was able to prey on one of the eggs. Our results suggest that these invasive species are a low threat for unattended red-tailed tropicbird eggs at Rano Raraku, Rapa Nui. However, future research is needed to determine the potential negative effects over unattended red-tailed tropicbird nestlings that are easier for these predators to handle compared with an egg.
C1 [Luna, Nicolas; Varela, Andrea, I; Luna-Jorquera, Guillermo] Univ Catolica Norte, Dept Biol Marina, Millennium Nucleus Ecol & Sustainable Management, Coquimbo, Chile.
   [Luna, Nicolas] Univ Catolica Norte, Fac Ciencias Mar, Programa Magister Ciencias Mar Menc Recursos Cost, Coquimbo, Chile.
   [Brokordt, Katherina] Univ Catolica Norte, Lab Fisiol & Genet Marina, Coquimbo, Chile.
   [Brokordt, Katherina; Luna-Jorquera, Guillermo] Univ Catolica Norte, Ctr Estudios Avanzados Zonas Aridas, Coquimbo, Chile.
RP Varela, AI (corresponding author), Univ Catolica Norte, Sede Coquimbo, Larrondo 1281, Coquimbo 1781421, Chile.
EM and.vrl@gmail.com
OI Guillermo, Luna-Jorquera/0000-0003-4274-7025; Varela, Andrea
   I/0000-0002-9752-0816
FU FONDECYTComision Nacional de Investigacion Cientifica y Tecnologica
   (CONICYT)CONICYT FONDECYT [3160324]; CONICYTComision Nacional de
   Investigacion Cientifica y Tecnologica (CONICYT) [22161894]; Millenium
   Nucleus of Ecology and Sustainable Management of Oceanic Islands
   (ESMOI); Ministry of Economy, Development and Tourism (Chile);
   Universidad Catolica del Norte, Coquimbo, Chile
FX The author(s) disclosed receipt of the following financial support for
   the research, authorship, and/or publication of this article: Funding
   for this project was provided by a postdoctoral research grant awarded
   to A. I. Varela (FONDECYT No 3160324), by a MSc scholarship (CONICYT No
   22161894) awarded to N. Luna, and by the Millenium Nucleus of Ecology
   and Sustainable Management of Oceanic Islands (ESMOI), a Scientific
   Initiative supported by the Ministry of Economy, Development and Tourism
   (Chile). Funding for publication was provided by Universidad Catolica
   del Norte, Coquimbo, Chile.
CR Aguirre J.E., 2009, Boletin Chileno de Ornitologia, V15, P44
   Anderson A, 2002, WORLD ARCHAEOL, V33, P375, DOI 10.1080/00438240120107431
   Biondi LM, 2015, ANIM COGN, V18, P139, DOI 10.1007/s10071-014-0785-5
   BirdLife International, 2017, SPEC FACTSH PHAETH R
   Boland CRJ, 2004, IBIS, V146, P687, DOI 10.1111/j.1474-919x.2004.00310.x
   Bolton M, 2014, POLAR BIOL, V37, P1659, DOI 10.1007/s00300-014-1554-2
   del Hoyo J., 1992, HDB BIRDS WORLD, V1
   Duron Q, 2017, CURR ZOOL, V63, P583, DOI 10.1093/cz/zox009
   Dutson G, 2010, ACTION PLAN AUSTR BI
   FLEET RR, 1972, AUK, V89, P651
   Flores M., 2017, THESIS
   Flores M, 2017, PAC SCI, V71, P149, DOI 10.2984/71.2.4
   Friard O, 2016, METHODS ECOL EVOL, V7, P1325, DOI 10.1111/2041-210X.12584
   Furness RW, 1987, SEABIRD ECOLOGY
   Gaskin C. P., 2011, SEABIRDS KERMADEC RE
   Harper GA, 2015, GLOB ECOL CONSERV, V3, P607, DOI 10.1016/j.gecco.2015.02.010
   Hatfield JS, 2012, CONSERV BIOL, V26, P667, DOI 10.1111/j.1523-1739.2012.01853.x
   Varela AI, 2018, EMU, V118, P381, DOI 10.1080/01584197.2018.1464372
   Jaramillo A., 2008, B CHIL ORNITOL, V14, P8
   Jones HP, 2008, CONSERV BIOL, V22, P16, DOI 10.1111/j.1523-1739.2007.00859.x
   Krajick K, 2005, SCIENCE, V310, P1410, DOI 10.1126/science.310.5753.1410
   Marin Manuel, 2010, Boletin del Museo Nacional de Historia Natural Chile, V59, P75
   Ministry of the Environment Chile, 2017, 14 PROC WILD SPEC CL
   Nice M. M., 1962, T LINNEAN SOC N Y, V13
   Prieto J, 2003, BIODIVERS CONSERV, V12, P2477, DOI 10.1023/A:1025825924678
   QGIS Development Team, 2017, QGIS GEOGR INF SYST
   R Core Team, 2017, R LANG ENV STAT COMP
   Richards C, 2011, WORLD ARCHAEOL, V43, P191, DOI 10.1080/00438243.2011.579483
   SCHLATTER RP, 1987, ISLAS OCEANICAS CHIL, P271
   Schreiber B. A., 2009, BIRDS N AM, DOI [10.2173/bna.43, DOI 10.2173/BNA.43]
   Schreiber EA, 1996, COLON WATERBIRD, V19, P45, DOI 10.2307/1521806
   Simeone A, 2012, J ORNITHOL, V153, P1079, DOI 10.1007/s10336-012-0837-z
   STEADMAN DW, 1995, SCIENCE, V267, P1123, DOI 10.1126/science.267.5201.1123
   Vanderwerf Eric A., 2014, Marine Ornithology, V42, P73
   Zarzoso-Lacoste D, 2011, J ZOOL, V285, P188, DOI 10.1111/j.1469-7998.2011.00828.x
NR 35
TC 7
Z9 7
U1 2
U2 14
PU SAGE PUBLICATIONS INC
PI THOUSAND OAKS
PA 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN 1940-0829
J9 TROP CONSERV SCI
JI Trop. Conserv. Sci.
PD JUL 3
PY 2018
VL 11
DI 10.1177/1940082918785079
PG 8
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA GL8YX
UT WOS:000437516300001
OA gold
DA 2022-02-10
ER

PT J
AU Rocha, M
   Serronha, A
   Rodrigues, M
   Alves, PC
   Monterroso, P
AF Rocha, M.
   Serronha, A.
   Rodrigues, M.
   Alves, P. C.
   Monterroso, P.
TI Comfort over safety: thermoregulation overshadows predation risk effects
   in the activity of a keystone prey
SO JOURNAL OF ZOOLOGY
LA English
DT Article; Early Access
DE activity budgets; activity level; diel activity; European rabbit;
   Oryctolagus cuniculus algirus; predation risk; thermoregulation
ID RABBIT ORYCTOLAGUS-CUNICULUS; EUROPEAN WILD RABBITS; GROUP-SIZE;
   ACTIVITY PATTERNS; PHYSIOLOGICAL-RESPONSES; ANTIDORCAS-MARSUPIALIS;
   TEMPORAL VARIATION; NATIONAL-PARK; VIGILANCE; ABUNDANCE
AB The activity level is a fundamental metric of animal behavior, related to the avoidance of predators, food acquisition, and thermoregulation. Animals need to weigh their activity budget to fulfill their energetic, social and reproductive requirements over the energetic costs of these activities. This task becomes further challenging for prey species, which also need to account for predation risk. To investigate the factors shaping proactive behavioral decisions leading prey species engagement in their diel activities, we implemented a multisite year-round monitoring study on the Iberian rabbit (Oryctolagus cuniculus algirus), a key prey species in Mediterranean ecosystems. We deployed remotely triggered cameras over 15 sites to continuously monitor Iberian rabbits' activity. We estimated activity levels from time-of-detection data from camera traps, and modeled it as a function of climatic, intraspecific, predation, and resource-related covariates. We found that Iberian rabbits exhibit a bimodal activity pattern peaking at sunrise and sunset, with a more pronounced peak occurring at sunrise during the nonbreeding season, and spend 9.15 +/- 3.00 h/day (mean +/- sd) active. Diel activity levels were negatively affected by extreme environmental temperatures and density dependence, demonstrating the privileged importance of social interactions and normothermia maintenance. We found mammalian predator activity and abundance to have near-negligible effects on the activity levels of this key prey, suggesting decreased antipredator behavior when risk is perceived as prolonged. Moreover, we argue that perceived risk may be more important than realized risk in shaping Iberian rabbits' activity level. These results provide valuable insights toward a comprehensive understanding of the factors underlying behavioral decisions made by prey species, relevant for maintaining their energetic and homeostatic balance.
C1 [Rocha, M.] Univ Minho, Escola Ciencias, Braga, Portugal.
   [Serronha, A.; Rodrigues, M.; Alves, P. C.; Monterroso, P.] Univ Porto, Ctr Invest Biodiversidade & Recursos Genet, InBIO Lab Associado, CIBIO, Vairao, Portugal.
   [Serronha, A.; Rodrigues, M.; Alves, P. C.; Monterroso, P.] CIBIO, BIOPOLIS Program Genom Biodivers & Land Planning, Vairao, Portugal.
   [Rodrigues, M.; Alves, P. C.; Monterroso, P.] Estacao Biol Mertola EBM, Mertola, Portugal.
   [Alves, P. C.] Univ Porto, Dept Biol, Fac Ciencias, Porto, Portugal.
   [Alves, P. C.] Univ Montana, Wildlife Biol Program, Dept Ecosyst & Conservat Sci, WA Franke Coll Forestry & Conservat, Missoula, MT 59812 USA.
RP Monterroso, P (corresponding author), Univ Porto, Ctr Invest Biodiversidade & Recursos Genet, CIBIO InBIO, Campus Vairao,Rua Padre Armando Quintas 7, P-4485661 Vairao, Portugal.
EM pcalves@fc.up.pt; pmonterroso@cibio.up.pt
RI Alves, Paulo C/B-5448-2009
OI Alves, Paulo C/0000-0003-4797-0939; rocha, jorge/0000-0001-5460-7615
FU Portuguese Fund for the Conservation of Nature and Biodiversity
   [UID/BIA/50027/2020]; FCT/MCTES through national funds
FX This work was supported by the project "SOS Coelho", funded by the
   Portuguese Fund for the Conservation of Nature and Biodiversity. PM was
   supported by UID/BIA/50027/2020 with funding from FCT/MCTES through
   national funds. We thank Raquel Neves, Teresa Oliveira, Nelson
   Fernandes, Antonio Matos, and Luiz Cesca for their assistance during
   fieldwork and image classification. We acknowledge all the logistic
   support provided by the ANPC - AssociacAo Nacional de Proprietarios
   Rurais GestAo Cinegetica e Biodiversidade, particularly through JoAo
   Carvalho, and by their game estate affiliates without whose support this
   work would not have been possible.
CR Anderson D.R, 2002, TECHNOMETRICS
   Angulo E, 2004, BIOL CONSERV, V115, P291, DOI 10.1016/S0006-3207(03)00148-4
   Barbosa A.M.R., 2019, ATLAS MAMIFEROS PORT, P66
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Borowik T, 2013, EUR J WILDLIFE RES, V59, P675, DOI 10.1007/s10344-013-0720-0
   Bowman F., 1958, INTRO BESSEL FUNCTIO
   Boyers M, 2019, CONSERV PHYSIOL, V7, DOI 10.1093/conphys/coz064
   Brown JS, 1999, J MAMMAL, V80, P385, DOI 10.2307/1383287
   Cabezas-Diaz S, 2011, ANIM BIOL, V61, P319, DOI 10.1163/157075511X584254
   Cain JW, 2006, WILDLIFE SOC B, V34, P570, DOI 10.2193/0091-7648(2006)34[570:MOTAWB]2.0.CO;2
   Calvete C, 2004, LANDSCAPE ECOL, V19, P531, DOI 10.1023/B:LAND.0000036139.04466.06
   Ferreira CC, 2015, BIOL J LINN SOC, V116, P106, DOI 10.1111/bij.12556
   Carpio AJ, 2014, WILDLIFE BIOL, V20, P161, DOI 10.2981/wlb.13113
   Childress MJ, 2003, ANIM BEHAV, V66, P389, DOI 10.1006/anbe.2003.2217
   Cid B, 2020, OIKOS, V129, P668, DOI 10.1111/oik.07022
   Copernicus Global Land Service, 2019, NORMALIZED DIFFERENC
   COWAN DP, 1986, MAMMAL REV, V16, P169, DOI 10.1111/j.1365-2907.1986.tb00039.x
   Creel S, 2018, ECOL LETT, V21, P947, DOI 10.1111/ele.12975
   Curveira-Santos G, 2017, AGR ECOSYST ENVIRON, V237, P280, DOI 10.1016/j.agee.2016.12.037
   Dail D, 2011, BIOMETRICS, V67, P577, DOI 10.1111/j.1541-0420.2010.01465.x
   DEHN MM, 1990, BEHAV ECOL SOCIOBIOL, V26, P337
   Delibes-Mateos, 2019, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2019-3.RLTS.T41291A170619657.en, DOI 10.2305/IUCN.UK.2019-3.RLTS.T41291A170619657.EN]
   Delibes-Mateos M, 2008, CONSERV BIOL, V22, P1106, DOI 10.1111/j.1523-1739.2008.00993.x
   Delibes-Mateos M, 2007, BIOL CONSERV, V137, P149, DOI 10.1016/j.biocon.2007.01.024
   Delibes-Mateos M, 2017, AMBIO, V46, P237, DOI 10.1007/s13280-016-0817-2
   Descalzo E, 2021, WILDLIFE RES, V48, P481, DOI 10.1071/WR20156
   Devillard S, 2008, MAMM BIOL, V73, P128, DOI 10.1016/j.mambio.2007.01.003
   DGT D, 2018, ESP TECN CART US OC, P11
   Diez C, 2013, WORLD RABBIT SCI, V21, P263, DOI 10.4995/wrs.2013.1332
   Diez C., 2005, Wildlife Biology in Practice, V1, P41
   Dinerstein E, 2017, BIOSCIENCE, V67, P534, DOI 10.1093/biosci/bix014
   Direthao-Geral do Territrio, 2018, ESP TECN CART US OC
   DiVincenti L, 2016, J AM ASSOC LAB ANIM, V55, P729
   DYaz, 2004, MAPA BIOCLIMATICO EU
   ELGAR MA, 1989, BIOL REV, V64, P13, DOI 10.1111/j.1469-185X.1989.tb00636.x
   Fairbanks B, 2007, ANIM BEHAV, V73, P115, DOI 10.1016/j.anbehav.2006.07.002
   Fernandez-de-Simon J, 2011, WILDLIFE BIOL, V17, P317, DOI 10.2981/10-001
   Ferrand Nuno, 2008, P47, DOI 10.1007/978-3-540-72446-9_4
   Ferreira C, 2012, EUR J WILDLIFE RES, V58, P885, DOI 10.1007/s10344-012-0664-9
   Ferreira C, 2009, EUR J WILDLIFE RES, V55, P487, DOI 10.1007/s10344-009-0257-4
   Ferrer M, 2004, CONSERV BIOL, V18, P344, DOI 10.1111/j.1523-1739.2004.00096.x
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Fletcher DJ, 1999, WILDLIFE RES, V26, P609, DOI 10.1071/WR97004
   Gaynor KM, 2019, TRENDS ECOL EVOL, V34, P355, DOI 10.1016/j.tree.2019.01.004
   Gaynor KM, 2018, SCIENCE, V360, P1232, DOI 10.1126/science.aar7121
   GIBB JA, 1993, J ZOOL, V229, P581, DOI 10.1111/j.1469-7998.1993.tb02658.x
   Goncalves H, 2002, WILDLIFE RES, V29, P165, DOI 10.1071/WR00048
   Halle S, 2000, ECOL STU AN, V141, P67
   Heurich M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114143
   Hofmann RR, 1995, T ROY SOC S AFR, V50, P125, DOI 10.1080/00359199509520344
   Hostetler JA, 2015, ECOLOGY, V96, P1713, DOI 10.1890/14-1487.1
   Joffre R, 1999, AGROFOREST SYST, V45, P57, DOI 10.1023/A:1006259402496
   Karger DN, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.122
   Kontsiotis VJ, 2019, MAMMALIA, V83, P134, DOI 10.1515/mammalia-2017-0136
   Lashley MA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22638-6
   Laundre JW, 2001, CAN J ZOOL, V79, P1401, DOI 10.1139/cjz-79-8-1401
   Lees AC, 2008, MAMMAL REV, V38, P304, DOI 10.1111/j.1365-2907.2008.00116.x
   Lima SL, 1999, AM NAT, V153, P649, DOI 10.1086/303202
   LIMA SL, 1995, ANIM BEHAV, V49, P11, DOI 10.1016/0003-3472(95)80149-9
   LIMA SL, 1990, CAN J ZOOL, V68, P619, DOI 10.1139/z90-092
   Lingle S, 2001, ETHOLOGY, V107, P295, DOI 10.1046/j.1439-0310.2001.00664.x
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Lombardi L, 2003, J MAMMAL, V84, P26, DOI 10.1644/1545-1542(2003)084<0026:HRDIRO>2.0.CO;2
   Maloney SK, 2005, J COMP PHYSIOL A, V191, P1055, DOI 10.1007/s00359-005-0030-4
   Martin-Diaz P, 2018, MAMM BIOL, V88, P114, DOI 10.1016/j.mambio.2017.10.006
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Milling CR, 2017, BEHAV ECOL, V28, P1236, DOI 10.1093/beheco/arx084
   Monclus R, 2005, ANIM BEHAV, V70, P753, DOI 10.1016/j.anbehav.2004.12.019
   Monclus R, 2008, ETHOLOGY, V114, P287, DOI 10.1111/j.1439-0310.2007.01463.x
   Monclus R, 2009, OECOLOGIA, V158, P615, DOI 10.1007/s00442-008-1201-0
   MONNEROT M, 1994, GENET SEL EVOL, V26, pS167, DOI 10.1051/gse:19940712
   Monterroso P, 2020, ECOLOGY, V101, DOI 10.1002/ecy.3059
   Monterroso P, 2016, SCI REP-UK, V6, DOI 10.1038/srep36072
   Monterroso P, 2016, J MAMMAL, V97, P928, DOI 10.1093/jmammal/gyw016
   Monterroso P, 2014, BEHAV ECOL SOCIOBIOL, V68, P1403, DOI 10.1007/s00265-014-1748-1
   Monterroso P, 2013, ETHOLOGY, V119, P1044, DOI 10.1111/eth.12156
   Moreno S, 1996, CAN J ZOOL, V74, P1656, DOI 10.1139/z96-183
   NAGY KA, 1994, J MAMMAL, V75, P860, DOI 10.2307/1382468
   Navarro-Castilla A, 2019, ECOL INDIC, V97, P175, DOI 10.1016/j.ecolind.2018.10.016
   Neto C, 1998, QUERCETEA, V1, P5
   Palomares F, 2003, MAMM BIOL, V68, P224, DOI 10.1078/1616-5047-00088
   Piorno V, 2020, J NAT CONSERV, V56, DOI 10.1016/j.jnc.2020.125832
   PYKE GH, 1977, Q REV BIOL, V52, P137, DOI 10.1086/409852
   Quantum GIS Development Team, 2016, QUANT GIS GEOGR INF
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Revelle W., 2018, PSYCH PROCEDURES PER
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rodel HG, 2006, PHYSIOL BEHAV, V89, P180, DOI 10.1016/j.physbeh.2006.05.042
   Romanovsky AA, 2002, J APPL PHYSIOL, V92, P2667, DOI 10.1152/japplphysiol.01173.2001
   Rosalino LM, 2009, ACTA OECOL, V35, P507, DOI 10.1016/j.actao.2009.03.006
   Rouco C, 2016, ECOL INDIC, V64, P212, DOI 10.1016/j.ecolind.2015.12.039
   Rouco C, 2019, MAMM BIOL, V95, P35, DOI 10.1016/j.mambio.2019.01.006
   Rouco C, 2011, EUR J WILDLIFE RES, V57, P395, DOI 10.1007/s10344-010-0443-4
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   ROWLEY IAN, 1957, C S I R O WILDLIFE RES, V2, P168
   SHIFFLER RE, 1988, AM STAT, V42, P79, DOI 10.2307/2685269
   Sih A, 2002, ANIM BEHAV, V63, P437, DOI 10.1006/anbe.2001.1921
   Sinervo B., 1997, BEHAV ECOLOGY, P105
   Stone DB, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178477
   Suselbeek L, 2014, ANIM BEHAV, V88, P41, DOI 10.1016/j.anbehav.2013.11.012
   Tablado Z, 2009, ECOGRAPHY, V32, P310, DOI 10.1111/j.1600-0587.2008.05532.x
   Terrien J, 2011, FRONT BIOSCI-LANDMRK, V16, P1428, DOI 10.2741/3797
   Tortosa FS, 2015, BEHAV ECOL SOCIOBIOL, V69, P1649, DOI 10.1007/s00265-015-1976-z
   Treves A, 2000, ANIM BEHAV, V60, P711, DOI 10.1006/anbe.2000.1528
   Vaguerizas PH, 2020, ENDANGER SPECIES RES, V43, P99, DOI 10.3354/esr01058
   Veldhuis MP, 2020, NAT ECOL EVOL, V4, P1069, DOI 10.1038/s41559-020-1218-2
   VILLAFUERTE R, 1993, MAMMALIA, V57, P341, DOI 10.1515/mamm.1993.57.3.341
   Villafuerte R, 1997, ACTA THERIOL, V42, P225, DOI 10.4098/AT.arch.97-23
   Villafuerte R, 1997, REV ECOL-TERRE VIE, V52, P345
   Wilson JC, 2002, AUSTRAL ECOL, V27, P183, DOI 10.1046/j.1442-9993.2002.01169.x
   Zar JH., 1999, BIOSTAT ANAL
   Ziege M, 2016, MAMM BIOL, V81, P534, DOI 10.1016/j.mambio.2016.07.002
NR 113
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0952-8369
EI 1469-7998
J9 J ZOOL
JI J. Zool.
DI 10.1111/jzo.12947
EA DEC 2021
PG 14
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA XQ1OV
UT WOS:000731323200001
DA 2022-02-10
ER

PT J
AU Pardo, LE
   Bombaci, S
   Huebner, SE
   Somers, MJ
   Fritz, H
   Downs, C
   Guthmann, A
   Hetem, RS
   Keith, M
   le Roux, A
   Mgqatsa, N
   Packer, C
   Palmer, MS
   Parker, DM
   Peel, M
   Slotow, R
   Strauss, WM
   Swanepoel, L
   Tambling, C
   Tsie, N
   Vermeulen, M
   Willi, M
   Jachowski, DS
   Venter, JA
AF Pardo, Lain E.
   Bombaci, Sara
   Huebner, Sarah E.
   Somers, Michael J.
   Fritz, Herve
   Downs, Colleen
   Guthmann, Abby
   Hetem, Robyn S.
   Keith, Mark
   le Roux, Aliza
   Mgqatsa, Nokubonga
   Packer, Craig
   Palmer, Meredith S.
   Parker, Daniel M.
   Peel, Mike
   Slotow, Rob
   Strauss, W. Maartin
   Swanepoel, Lourens
   Tambling, Craig
   Tsie, Nairobi
   Vermeulen, Mika
   Willi, Marco
   Jachowski, David S.
   Venter, Jan A.
TI Snapshot Safari: A large-scale collaborative to monitor Africa's
   remarkable biodiversity
SO SOUTH AFRICAN JOURNAL OF SCIENCE
LA English
DT Editorial Material
DE citizen science; camera trap; conservation; machine learning; mammals
ID CAMERA TRAPS; CARNIVORES; LANDSCAPE; FEAR
C1 [Pardo, Lain E.; Fritz, Herve; Vermeulen, Mika; Venter, Jan A.] Nelson Mandela Univ, Sch Nat Resource Management, George, South Africa.
   [Bombaci, Sara] Colorado State Univ, Dept Fish Wildlife & Conservat Biol, Ft Collins, CO 80523 USA.
   [Huebner, Sarah E.; Guthmann, Abby; Packer, Craig] Univ Minnesota, Coll Biol Sci, St Paul, MN 55108 USA.
   [Somers, Michael J.; Keith, Mark; Tsie, Nairobi] Univ Pretoria, Mammal Res Inst, Dept Zool & Entomol, Eugene Marais Chair Wildlife Management, Pretoria, South Africa.
   [Somers, Michael J.] Univ Pretoria, Ctr Invas Biol, Pretoria, South Africa.
   [Pardo, Lain E.; Fritz, Herve; Vermeulen, Mika; Venter, Jan A.] Nelson Mandela Univ, Univ Lyon 1, French Natl Ctr Sci Res CNRS, REHABS,Int Res Lab, George, South Africa.
   [Downs, Colleen; Peel, Mike; Slotow, Rob; Jachowski, David S.] Univ KwaZulu Natal, Sch Life Sci, Durban, South Africa.
   [Hetem, Robyn S.] Univ Witwatersrand, Sch Anim Plant & Environm Sci, Johannesburg, South Africa.
   [le Roux, Aliza; Parker, Daniel M.] Univ Free State, Dept Zool & Entomol, Phuthaditjhaba, South Africa.
   [le Roux, Aliza] Univ Free State, Afromontane Res Unit, Phuthaditjhaba, South Africa.
   [Mgqatsa, Nokubonga] Rhodes Univ, Dept Zool & Entomol, Wildlife & Reserve Management Res Grp, Makhanda, South Africa.
   [Palmer, Meredith S.] Princeton Univ, Dept Ecol & Evolutionary Biol, Princeton, NJ USA.
   [Parker, Daniel M.] Univ Mpumalanga, Sch Biol & Environm Sci, Mbombela, South Africa.
   [Peel, Mike] Agr Res Council, Rangeland Ecol, Anim Prod Inst, Pretoria, South Africa.
   [Peel, Mike] Univ South Africa, Appl Behav Ecol & Ecosystems Res Unit, Johannesburg, South Africa.
   [Strauss, W. Maartin] Univ South Africa, Dept Environm Sci, Johannesburg, South Africa.
   [Swanepoel, Lourens] Univ Venda, Dept Zool, Thohoyandou, South Africa.
   [Swanepoel, Lourens] African Inst Conservat Ecol, Makhado, South Africa.
   [Tambling, Craig] Univ Ft Hare, Dept Zool & Entomol, Alice, South Africa.
   [Willi, Marco] Univ Minnesota, Sch Phys & Astron, Minneapolis, MN 55455 USA.
   [Jachowski, David S.] Clemson Univ, Dept Forestry & Environm Conservat, Clemson, SC USA.
RP Huebner, SE (corresponding author), Univ Minnesota, Coll Biol Sci, St Paul, MN 55108 USA.
EM huebn090@umn.edu
RI Parker, Daniel/J-2649-2019; Downs, Colleen/A-7770-2010; Fritz,
   Herve/D-1729-2014; Keith, Mark/Q-1527-2018; Strauss, W.
   Maartin/F-8579-2014; /A-1523-2008
OI Parker, Daniel/0000-0001-7555-5674; Downs, Colleen/0000-0001-8334-1510;
   Fritz, Herve/0000-0002-7106-3661; Keith, Mark/0000-0001-7179-9989;
   Bombaci, Sara/0000-0001-7734-2381; Strauss, W.
   Maartin/0000-0002-3087-1937; /0000-0002-5836-8823; Pardo Vargas, Lain
   Efren/0000-0002-2533-6577; Huebner, Sarah/0000-0001-5682-6467; Hetem,
   Robyn/0000-0003-1953-3520; le Roux, Aliza/0000-0002-9869-2580
FU South African National Biodiversity Institute (SANBI); Foundational
   Biodiversity Information Programme (FBIP); South African National
   Research Foundation (NRF)National Research Foundation - South Africa;
   Fynbos Trust; Nelson Mandela University; Fairfields; Detroit Zoological
   Society; Zoo Miami; Cincinnati Zoo Angel Fund; Seneca Park Zoo; Living
   Desert; Snapshot South Africa
FX We thank all the Zooniverse volunteers who contribute classifications to
   Snapshot Safari, and the moderators who donate their time and expertise
   to our projects. We also thank sponsors of our work, including the South
   African National Biodiversity Institute (SANBI), Foundational
   Biodiversity Information Programme (FBIP), South African National
   Research Foundation (NRF), Fynbos Trust, Nelson Mandela University,
   Fairfields, Detroit Zoological Society, Zoo Miami, Cincinnati Zoo Angel
   Fund, Seneca Park Zoo, and The Living Desert. We thank the Minnesota
   Supercomputing Institute for providing resources for data storage and
   processing, among others. Finally, we thank all the people and
   institutions participating in Snapshot South Africa, including the
   reserve managers and owners for providing access and supporting this
   programme, National Parks Institutions of every country, students and
   volunteer groups helping to maintain the grids, and the governmental
   institutions overseeing these parks and reserves.
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Anderson TM, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0314
   Barnard P, 2017, BIOL CONSERV, V208, P183, DOI 10.1016/j.biocon.2016.09.011
   Ceballos G, 2017, P NATL ACAD SCI USA, V114, pE6089, DOI 10.1073/pnas.1704949114
   Child MF., 2016, 2016 RED LIST MAMMAL
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Diaz S, 2019, SCIENCE, V366, P1327, DOI 10.1126/science.aax3100
   Dickman AJ, 2014, BIOL CONSERV, V178, P19, DOI 10.1016/j.biocon.2014.07.011
   Do Linh San E, 2006, S AFR J WILDL RES, V36, P201
   Donlan CJ, 2003, CONSERV BIOL, V17, P1850, DOI 10.1111/j.1523-1739.2003.00012.x
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hepler SA, 2018, ECOLOGY, V99, P2152, DOI 10.1002/ecy.2396
   Hofmeester TR, 2020, REMOTE SENS ECOL CON, V6, P129, DOI 10.1002/rse2.136
   Jensen D, 2019, SCI POLICY BUSINESS
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Palmer MS, 2017, ECOL LETT, V20, P1364, DOI 10.1111/ele.12832
   Palmer MS, 2018, AFR J ECOL, V56, P882, DOI 10.1111/aje.12505
   Spiers H, 2019, JCOM-J SCI COMMUN, V18, DOI 10.22323/2.18010204
   Swanson A, 2016, ECOL EVOL, V6, P8534, DOI 10.1002/ece3.2569
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tilman D, 2017, NATURE, V546, P73, DOI 10.1038/nature22900
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
NR 25
TC 3
Z9 3
U1 3
U2 6
PU ACAD SCIENCE SOUTH AFRICA A S S AF
PI LYNWOOD RIDGE
PA PO BOX 72135, LYNWOOD RIDGE 0040, SOUTH AFRICA
SN 0038-2353
EI 1996-7489
J9 S AFR J SCI
JI S. Afr. J. Sci.
PD JAN-FEB
PY 2021
VL 117
IS 1-2
AR 8134
DI 10.17159/sajs.2021/8134
PG 4
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA ST8AQ
UT WOS:000662660800004
OA gold
DA 2022-02-10
ER

PT J
AU Erdem, UM
   Sclaroff, S
AF Erdem, Ugur Murat
   Sclaroff, Stan
TI Automated camera layout to satisfy task-specific and floor plan-specific
   coverage requirements
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article; Proceedings Paper
CT Workshop on Omnidirectional Vision, Camera Networks and Non-Classical
   Cameras held in conjunction with the 8th European Conference on Computer
   Vision
CY MAY 16, 2004
CL Prague, CZECH REPUBLIC
DE camera placement; sensor networks; visibility; best view
AB In many multi-camera vision systems the effect of camera locations on the task-specific quality of service is ignored. Researchers in Computational Geometry have proposed elegant solutions for some sensor location problem classes. Unfortunately, these solutions use unrealistic assumptions about the cameras' capabilities that make these algorithms unsuitable for many real world computer vision applications. In this paper, the general camera placement problem is first defined with assumptions that are more consistent with the capabilities of real world cameras. The region to be observed by cameras may be volumetric, static or dynamic, and may include holes. A subclass of this general problem can be formulated in terms of planar regions that are typical of building floor plans. Given a floor plan to be observed, the problem is then to reliably compute a camera layout such that certain task-specific constraints are met. A solution to this problem is obtained via binary optimization over a discrete problem space. In experiments the performance of the resulting system is demonstrated with different real indoor and outdoor floor plans. (c) 2006 Elsevier Inc. All rights reserved.
C1 Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
RP Erdem, UM (corresponding author), Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
EM merdem@cs.bu.edu; sclaroff@cs.bu.edu
CR Abrams S, 1999, INT J ROBOT RES, V18, P267, DOI 10.1177/02783649922066204
   Arbel T, 2001, IMAGE VISION COMPUT, V19, P779, DOI 10.1016/S0262-8856(00)00103-7
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   BATISTA J, 1998, IEEE WORKSH VIS SURV
   Bose P, 1997, INT J COMPUT GEOM AP, V7, P153, DOI 10.1142/S0218195997000090
   Cai Q, 1999, IEEE T PATTERN ANAL, V21, P1241, DOI 10.1109/34.809119
   CARLSSON S, 1991, WORKSH ALG DAT STRUC, P367
   CHVATAL V, 1975, J COMB THEORY B, V18, P39, DOI 10.1016/0095-8956(75)90061-1
   Cortes J, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1327, DOI 10.1109/ROBOT.2002.1014727
   Cui YT, 1998, 1998 IEEE WORKSHOP ON VISUAL SURVEILLANCE, PROCEEDINGS, P2
   DOUBEK P, 2004, P OMN 5 WORKSH OMN V, P17
   Efrat A, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P927
   ESTIVILLCASTRO V, 1995, INFORM PROCESS LETT, V56, P9, DOI 10.1016/0020-0190(95)00129-Z
   Etemad K, 1997, J OPT SOC AM A, V14, P1724, DOI 10.1364/JOSAA.14.001724
   FISK S, 1978, J COMB THEORY B, V24, P374, DOI 10.1016/0095-8956(78)90059-X
   GINDY HE, 1981, J ALGORITHMS, V2, P186
   Guibas LJ, 1999, INT J COMPUT GEOM AP, V9, P471, DOI 10.1142/S0218195999000273
   HUMPHREYS A, STAR CONVEX MATHWORL
   MAVER J, 1993, IEEE T PATTERN ANAL, V15, P417, DOI 10.1109/34.211463
   Mikic I, 2000, WORKSHOP ON HUMAN MOTION, PROCEEDINGS, P107, DOI 10.1109/HUMO.2000.897379
   MITTAL A, 2004, EUR C COMP VIS ECCV
   O'Rourke J., 1987, ART GALLERY THEOREMS
   Pito R, 1999, IEEE T PATTERN ANAL, V21, P1016, DOI 10.1109/34.799908
   Rahimi A, 2004, PROC CVPR IEEE, P187
   RAUSS MP, 1996, 25 AIPR WORKSH EM AP, P253
   Schwarzkopf O. C., 2000, COMPUTATIONAL GEOMET
   Suzuki I, 2001, INT J COMPUT GEOM AP, V11, P529, DOI 10.1142/S0218195901000638
   TARABANIS PKA, 1995, IEEE T ROBOTIC AUTOM, P86
   Wolsey L. A., 1998, INTEGER PROGRAMMING
NR 29
TC 141
Z9 145
U1 0
U2 17
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD SEP
PY 2006
VL 103
IS 3
BP 156
EP 169
DI 10.1016/j.cviu.2006.06.005
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA 081DV
UT WOS:000240298300002
DA 2022-02-10
ER

PT J
AU Doherty, TS
   Hall, ML
   Parkhurst, B
   Westcott, V
AF Doherty, Tim S.
   Hall, Michelle L.
   Parkhurst, Ben
   Westcott, Vanessa
TI Experimentally testing the response of feral cats and their prey to
   poison baiting
SO WILDLIFE RESEARCH
LA English
DT Article; Early Access
DE cat baiting; dynamic occupancy model; impact evaluation; invasive
   predator; lethal control; pest control
ID NEW-SOUTH-WALES; FELIS-CATUS; HOME-RANGE; RED FOXES; R PACKAGE;
   AUSTRALIA; MOVEMENTS; IMPACTS; MAMMALS; ISLAND
AB Context. Feral cats, Felis catus, have caused the decline and extinction of many species worldwide, particularly on islands and in Australia where native species are generally naive to the threat of this introduced predator. Effectively reducing cat populations to protect wildlife is challenging because cats have a cryptic nature, high reproductive rate and strong reinvasion ability.
   Aims. We experimentally tested the response of feral cats and their native prey to an Eradicat (R) poison baiting program at a conservation reserve.
   Methods. Baits were distributed by hand along roads and tracks every 50 m (similar to 10 baits km(-2)). We used camera traps to monitor the response of cats to baiting using a repeated before-after, control-impact design over 6 years. We also measured introduced rabbit, Oryctolagus cuniculus, activity by using sand pads and small mammal and reptile captures by using pitfall trapping.
   Key results. Dynamic occupancy modelling showed only modest effects of baiting on cats in 2 of 6 years, with occupancy in the baited area decreasing from 54% to 19% in 2014 (-35%) and from 89% to 63% in 2017 (-26%). Baiting effectiveness was not related to antecedent rainfall or prey availability. Bait availability was reduced by non-target interference; 73% of 41 monitored baits were removed by non-target species. We found no evidence for persistent changes in small mammal or reptile capture rates in the baited area relative to the unbaited area over the life of the project.
   Conclusions. Relatively low baiting density and non-target interference with baits are likely to have reduced baiting efficacy. Further testing and refinement of ground baiting is needed, including trialling higher baiting densities and/or frequencies.
C1 [Doherty, Tim S.] Univ Sydney, Sch Life & Environm Sci, Camperdown, NSW 2006, Australia.
   [Doherty, Tim S.] Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, 221 Burwood Highway, Burwood, Vic 3125, Australia.
   [Hall, Michelle L.; Parkhurst, Ben; Westcott, Vanessa] Bush Heritage Australia, 1-395 Collins St, Melbourne, Vic 3000, Australia.
   [Hall, Michelle L.] Univ Western Australia, Sch Biol Sci, 35 Stirling Highway, Perth, WA 6009, Australia.
   [Hall, Michelle L.] Univ Melbourne, Sch BioSci, Melbourne, Vic 3010, Australia.
RP Doherty, TS (corresponding author), Univ Sydney, Sch Life & Environm Sci, Camperdown, NSW 2006, Australia.; Doherty, TS (corresponding author), Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, 221 Burwood Highway, Burwood, Vic 3125, Australia.
EM tim.doherty@sydney.edu.au
RI Doherty, Tim S./G-9354-2015; Hall, Michelle/A-1904-2010
OI Doherty, Tim S./0000-0001-7745-0251; Hall, Michelle/0000-0002-1263-8314
FU Earthwatch Institute Australia; Bush Heritage Australia; Edith Cowan
   University; Deakin University; Australian Research CouncilAustralian
   Research Council [DE200100157]
FX Financial support for this project was provided by Earthwatch Institute
   Australia, Bush Heritage Australia and Edith Cowan University. T. S.
   Doherty was supported by Edith Cowan University, Deakin University and
   the Australian Research Council (DE200100157) over the life of the
   project.
CR Algar D., 2007, Conservation Science Western Australia, V6, P109
   Algar Dave, 2011, Conservation Science Western Australia, V8, P367
   Allen BL, 2014, ENVIRON SCI POLLUT R, V21, P2178, DOI 10.1007/s11356-013-2118-7
   Allsop Sinead E., 2017, Pacific Conservation Biology, V23, P240, DOI 10.1071/PC17006
   Barton K, 2019, MUMIN MULTIMODEL INF
   Bell L., 2011, FIELD TRIAL COMP BAI
   Bengsen AJ, 2016, J ZOOL, V298, P112, DOI 10.1111/jzo.12290
   Bengsen AJ, 2012, WILDLIFE RES, V39, P258, DOI 10.1071/WR11097
   Berry O, 2013, WILDLIFE RES, V40, P615, DOI 10.1071/WR13073
   BLOOMER JP, 1992, BIOL CONSERV, V60, P211, DOI 10.1016/0006-3207(92)91253-O
   Bonnaud E, 2011, BIOL INVASIONS, V13, P581, DOI 10.1007/s10530-010-9851-3
   Bureau of Meteorology, 2020, CLIMATE DATA
   Burrows N., 2018, AERIAL GROUND BAITIN
   Burrows ND, 2003, J ARID ENVIRON, V55, P691, DOI 10.1016/S0140-1963(02)00317-8
   Christensen Per E. S., 2013, Ecological Management & Restoration, V14, P47, DOI 10.1111/emr.12025
   Christie AP, 2019, J APPL ECOL, V56, P2742, DOI 10.1111/1365-2664.13499
   Coates Terry D., 2008, Australian Mammalogy, V30, P51
   Comer S, 2020, WILDLIFE RES, V47, P762, DOI 10.1071/WR19217
   Comer S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23495-z
   Doherty T. S., 2015, THESIS E COWAN U PER
   Doherty TS, 2017, MAMMAL REV, V47, P83, DOI 10.1111/mam.12080
   Doherty TS, 2016, P NATL ACAD SCI USA, V113, P11261, DOI 10.1073/pnas.1602480113
   Doherty TS, 2015, AUST MAMMAL, V37, P219, DOI 10.1071/AM14038
   Doherty TS, 2015, ECOL MANAG RESTOR, V16, P124, DOI 10.1111/emr.12158
   Doherty TS, 2015, INT J WILDLAND FIRE, V24, P534, DOI 10.1071/WF14115
   Edwards GP, 2001, AUSTRAL ECOL, V26, P93, DOI 10.1111/j.1442-9993.2001.01091.pp.x
   Fancourt BA, 2021, J ENVIRON MANAGE, V280, DOI 10.1016/j.jenvman.2020.111691
   Fisher P, 2015, APPL ANIM BEHAV SCI, V173, P88, DOI 10.1016/j.applanim.2014.09.010
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Friend JA, 2020, WILDLIFE RES, V47, P747, DOI 10.1071/WR19087
   Geyle H. M., 2020, Ecological Solutions and Evidence, V1, DOI 10.1002/2688-8319.12018
   HARDEN RH, 1985, AUST WILDLIFE RES, V12, P25
   Hilmer S., 2010, THESIS GOETHE U FRAN
   Hohnen R, 2020, WILDLIFE RES, V47, P547, DOI 10.1071/WR19056
   Hone J, 2010, J APPL ECOL, V47, P507, DOI 10.1111/j.1365-2664.2010.01812.x
   Johnston M. J., 2014, FIELD EFFICACY CURIO
   JONES E, 1982, AUST WILDLIFE RES, V9, P409
   Lazenby BT, 2014, WILDLIFE RES, V41, P407, DOI 10.1071/WR14030
   Leahy L, 2015, WILDLIFE RES, V42, P705, DOI 10.1071/WR15011
   Legge S, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.52
   Legge S, 2018, WILDLIFE RES, V45, P627, DOI 10.1071/WR17172
   Leo BT, 2018, PAC SCI, V72, P57, DOI 10.2984/72.1.4
   Letnic M, 2010, BIOL REV, V85, P501, DOI 10.1111/j.1469-185X.2009.00113.x
   Little, 2013, FIELD ASSESSMENT CUR
   Lohr CA, 2020, SCI TOTAL ENVIRON, V720, DOI 10.1016/j.scitotenv.2020.137631
   Loss SR, 2017, FRONT ECOL ENVIRON, V15, P502, DOI 10.1002/fee.1633
   MacKenzie D. I., 2018, OCCUPANCY ESTIMATION
   McGregor H, 2020, BIOL INVASIONS, V22, P799, DOI 10.1007/s10530-019-02131-5
   Medina FM, 2011, GLOBAL CHANGE BIOL, V17, P3503, DOI 10.1111/j.1365-2486.2011.02464.x
   Molsher R, 2005, WILDLIFE RES, V32, P587, DOI 10.1071/WR04093
   Moseby KE, 2011, WILDLIFE RES, V38, P350, DOI 10.1071/WR10236
   Moseby KE, 2011, WILDLIFE RES, V38, P338, DOI 10.1071/WR10235
   Moseby KE, 2009, AUSTRAL ECOL, V34, P156, DOI 10.1111/j.1442-9993.2008.01916.x
   Newsome TM, 2013, ECOGRAPHY, V36, P914, DOI 10.1111/j.1600-0587.2013.00056.x
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Nogales M, 2013, BIOSCIENCE, V63, P804, DOI 10.1525/bio.2013.63.10.7
   Norbury GL, 2015, BIOL CONSERV, V191, P409, DOI 10.1016/j.biocon.2015.07.031
   Palmer R, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251304
   R Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
   Reddiex B, 2006, WILDLIFE RES, V33, P711, DOI 10.1071/WR05103
   Richards J., 2010, SUSTAINED INTRODUCED
   Robley Alan, 2010, Australian Mammalogy, V32, P23, DOI 10.1071/AM09030
   Roshier DA, 2020, WILDLIFE RES, V47, P570, DOI 10.1071/WR19153
   Ruscoe WA, 2011, ECOL LETT, V14, P1035, DOI 10.1111/j.1461-0248.2011.01673.x
   Salo P, 2007, P ROY SOC B-BIOL SCI, V274, P1237, DOI 10.1098/rspb.2006.0444
   Shionosaki K, 2015, WILDLIFE RES, V42, P343, DOI 10.1071/WR14161
   Short J, 1997, WILDLIFE RES, V24, P703, DOI 10.1071/WR96071
   Stewart A., 2019, CENTRAL ROCK RAT MON
   Stobo-Wilson AM, 2020, WILDLIFE RES, V47, P720, DOI 10.1071/WR19237
   Walsh JC, 2012, ANIM CONSERV, V15, P319, DOI 10.1111/j.1469-1795.2012.00537.x
   Weston MA, 2009, LANDSCAPE URBAN PLAN, V89, P98, DOI 10.1016/j.landurbplan.2008.10.009
   Woinarski J.C., 2019, CATS AUSTR COMPANION
   Woinarski JCZ, 2015, P NATL ACAD SCI USA, V112, P4531, DOI 10.1073/pnas.1417301112
   Wysong ML, 2020, WILDLIFE RES, V47, P557, DOI 10.1071/WR19175
   Wysong ML, 2020, MOV ECOL, V8, DOI 10.1186/s40462-020-00203-z
NR 75
TC 0
Z9 0
U1 4
U2 4
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
DI 10.1071/WR21008
EA AUG 2021
PG 10
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA UC5OS
UT WOS:000686575300001
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Elbroch, LM
   Wittmer, HU
AF Allen, Maximilian L.
   Elbroch, L. Mark
   Wittmer, Heiko U.
TI Can't bear the competition: Energetic losses from kleptoparasitism by a
   dominant scavenger may alter foraging behaviors of an apex predator
SO BASIC AND APPLIED ECOLOGY
LA English
DT Article
DE kleptoparasitism; Predation; Puma concolor; scavenging; Ursus americanus
AB The interspecific interactions of apex predators are integral to the function of ecological communities, but most studies have focused on understanding their top down effects. Kleptoparasitism (the stealing of procured food) by dominant scavengers can have negative effects on populations and behaviors of apex predators. We captured 7 pumas (Puma concolor) and fitted them with GPS collars to investigate potential kill sites (n = 352), some of which we monitored with camera traps (n = 58). We analyzed whether observed kleptoparasitism by American black bears (Ursus americanus) affected puma energetics and foraging behavior. We found that black bears were the most frequent scavenger of puma kills (72.4%), and we documented bears scavenging puma kills during every month. The top model for bear detection of puma kills included prey size, temperature, and canopy cover, with bears more likely to scavenge from adult black-tailed deer (Odocoileus hemionus columbianus) carcasses in warmer temperatures and under dense canopy cover. When black bear scavenging occurred, pumas spent 22% less time at their kill and incurred energetic losses. In response, pumas shortened their inter-kill intervals by 1.3 days thus increasing their kill rates. Our results demonstrate how a dominant scavenger directly mediates the foraging behavior of an apex predator. These results suggest that community interactions do not necessarily start at the top in top-down systems, and the effects of predators on prey populations can only be understood within their respective ecological communities. (C) 2021 Gesellschaft fur Okologie. Published by Elsevier GmbH. All rights reserved.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Elbroch, L. Mark] Panthera, 8 West 40th St,18th Floor, New York, NY 10018 USA.
   [Wittmer, Heiko U.] Victoria Univ Wellington, Sch Biol Sci, POB 600, Wellington 6140, New Zealand.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Wittmer, Heiko U/D-4172-2015
OI Wittmer, Heiko U/0000-0002-8861-188X
FU California Department of Fish and Wildlife; University of California at
   DavisUniversity of California System; California Deer Association;
   Victoria University of Wellington tuition scholarship
FX The California Department of Fish and Wildlife, the University of
   California at Davis, and the California Deer Association generously
   provided funding for the project. M. Allen was supported by a Victoria
   University of Wellington tuition scholarship. We thank B. Millsap, C.
   Wiley and D. Tichenor for their expertise and help in capturing pumas;
   and D. Casady, J. Golla, B. Evans, and many others for their help on the
   project. Constructive feedback from two anonymous reviewers improved
   previous drafts of our manuscript.
CR Ackerman B. B., 1982, THESIS
   Allen ML, 2015, CALIF FISH GAME, V101, P51
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Anderson K. A., 2002, MODEL SELECTION MULT
   Bacon MM, 2011, WILDLIFE SOC B, V35, P409, DOI 10.1002/wsb.85
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Carbone C, 1997, J ANIM ECOL, V66, P318, DOI 10.2307/5978
   Clark DA, 2014, J WILDLIFE MANAGE, V78, P1161, DOI 10.1002/jwmg.760
   Cristescu B., 2019, J ZOOL, V309, P259
   Cristescu B., 2020, BIIOGEOGRAPHICAL ECO, DOI [10.1101/2020.10.04.325779, DOI 10.1101/2020.10.04.325779]
   Cross PC, 2016, ECOLOGY, V97, P1938, DOI 10.1890/15-1346.1
   Danvir R.E., 1981, Encyclia, V58, P50
   De Roos AM, 2008, P NATL ACAD SCI USA, V105, P13930, DOI 10.1073/pnas.0803834105
   DeLong JP, 2015, AM NAT, V185, P354, DOI 10.1086/679735
   Elbroch LM, 2017, BIOL CONSERV, V215, P123, DOI 10.1016/j.biocon.2017.08.026
   Elbroch LM, 2015, BEHAV ECOL, V26, P247, DOI 10.1093/beheco/aru189
   Elbroch LM, 2014, ECOSPHERE, V5, DOI 10.1890/ES13-00373.1
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Forrester TD, 2019, WILDLIFE BIOL, DOI 10.2981/wlb.00510
   Hayward MW, 2006, J ZOOL, V270, P298, DOI 10.1111/j.1469-7998.2006.00139.x
   Heffelfinger J., 2010, ARIZONA GAME FISH DE
   Helldin JO, 2007, WILDLIFE BIOL, V13, P475, DOI 10.2981/0909-6396(2007)13[475:CIRFVV]2.0.CO;2
   Jameson E.W., 2004, MAMMALS CALIFORNIA
   Knopff KH, 2010, J WILDLIFE MANAGE, V74, P1435, DOI 10.2193/2009-314
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Krofel M, 2012, BEHAV ECOL SOCIOBIOL, V66, P1297, DOI 10.1007/s00265-012-1384-6
   Low W. A., 1963, Journal of Wildlife Management, V27, P466, DOI 10.2307/3798521
   Marescot L, 2015, POPUL ECOL, V57, P185, DOI 10.1007/s10144-014-0456-z
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   Metz MC, 2012, J ANIM ECOL, V81, P553, DOI 10.1111/j.1365-2656.2011.01945.x
   PARKER KL, 1993, CAN J ZOOL, V71, P1397, DOI 10.1139/z93-193
   Pinheiro J., 2013, NMLE LINEAR NONLINEA
   R Core Team, 2017, R LANG ENV STAT COMP
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Ripple WJ, 2001, BIOL CONSERV, V102, P227, DOI 10.1016/S0006-3207(01)00107-0
   Ruth TK, 2011, J WILDLIFE MANAGE, V75, P1381, DOI 10.1002/jwmg.190
   Ruth Toni K., 2010, P163
   Sibley D. A., 2005, SIBLEY FIELD GUIDE B
   Wilckens DT, 2016, J MAMMAL, V97, P373, DOI 10.1093/jmammal/gyv183
   Wilmers CC, 2003, J ANIM ECOL, V72, P909, DOI 10.1046/j.1365-2656.2003.00766.x
NR 40
TC 1
Z9 1
U1 4
U2 8
PU ELSEVIER GMBH
PI MUNICH
PA HACKERBRUCKE 6, 80335 MUNICH, GERMANY
SN 1439-1791
EI 1618-0089
J9 BASIC APPL ECOL
JI Basic Appl. Ecol.
PD MAR
PY 2021
VL 51
BP 1
EP 10
DI 10.1016/j.baae.2021.01.011
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QL3JK
UT WOS:000620976100001
DA 2022-02-10
ER

PT J
AU Francis, RJ
   Lyons, MB
   Kingsford, RT
   Brandis, KJ
AF Francis, Roxane J.
   Lyons, Mitchell B.
   Kingsford, Richard T.
   Brandis, Kate J.
TI Counting Mixed Breeding Aggregations of Animal Species Using Drones:
   Lessons from Waterbirds on Semi-Automation
SO REMOTE SENSING
LA English
DT Article
DE UAV; machine learning; colony; open source; GIS; avian; remote sensing;
   heronry
ID COLONIALLY-NESTING WATERBIRDS; MACQUARIE MARSHES; AERIAL SURVEYS; CAMERA
   TRAPS; MANAGEMENT; RIVER; POPULATIONS; ECOLOGY; FLORIDA; IMPACT
AB Using drones to count wildlife saves time and resources and allows access to difficult or dangerous areas. We collected drone imagery of breeding waterbirds at colonies in the Okavango Delta (Botswana) and Lowbidgee floodplain (Australia). We developed a semi-automated counting method, using machine learning, and compared effectiveness of freeware and payware in identifying and counting waterbird species (targets) in the Okavango Delta. We tested transferability to the Australian breeding colony. Our detection accuracy (targets), between the training and test data, was 91% for the Okavango Delta colony and 98% for the Lowbidgee floodplain colony. These estimates were within 1-5%, whether using freeware or payware for the different colonies. Our semi-automated method was 26% quicker, including development, and 500% quicker without development, than manual counting. Drone data of waterbird colonies can be collected quickly, allowing later counting with minimal disturbance. Our semi-automated methods efficiently provided accurate estimates of nesting species of waterbirds, even with complex backgrounds. This could be used to track breeding waterbird populations around the world, indicators of river and wetland health, with general applicability for monitoring other taxa.
C1 [Francis, Roxane J.; Lyons, Mitchell B.; Kingsford, Richard T.; Brandis, Kate J.] Univ New South Wales, Ctr Ecosyst Sci, Sydney, NSW 2052, Australia.
RP Francis, RJ (corresponding author), Univ New South Wales, Ctr Ecosyst Sci, Sydney, NSW 2052, Australia.
EM roxane.francis@unsw.edu.au; Mitchell.Lyons@unsw.edu.au;
   Richard.Kingsford@unsw.edu.au; Kate.Brandis@unsw.edu.au
OI Kingsford, Richard/0000-0001-6565-4134; Francis,
   Roxane/0000-0003-3172-5445; Brandis, Kate/0000-0001-6807-0142; Lyons,
   Mitchell/0000-0003-3960-3522
FU Elephants without Borders; Taronga Conservation Society; Australian
   Commonwealth Environmental Water Office; NSW Department of Primary
   Industries; University of New South Wales Sydney
FX This research received financial support from Elephants without Borders,
   Taronga Conservation Society, the Australian Commonwealth Environmental
   Water Office, the NSW Department of Primary Industries, and the
   University of New South Wales Sydney.
CR Arendt MD, 2012, MAR BIOL, V159, P101, DOI 10.1007/s00227-011-1793-5
   Arthur AD, 2012, WETLANDS, V32, P257, DOI 10.1007/s13157-011-0235-y
   Bennitt E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38610-x
   Bino G, 2014, ECOL APPL, V24, P142, DOI 10.1890/13-0202.1
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004
   Brandis KJ, 2011, ENVIRON MANAGE, V48, P489, DOI 10.1007/s00267-011-9705-5
   Brandis Kate J., 2014, Australian Field Ornithology, V31, P99
   Bregnballe T, 2014, ORNIS FENNICA, V91, P231
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brody S., UNMANNED INVESTIGATI
   Callaghan CT, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01825
   Carney KM, 1999, WATERBIRDS, V22, P68, DOI 10.2307/1521995
   Chabot D, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01205-130115
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Chambers LE, 2005, EMU, V105, P1, DOI 10.1071/MU04033
   Crutsinger GM, 2016, J UNMANNED VEH SYST, V4, P161, DOI 10.1139/juvs-2016-0008
   Descamps S, 2011, BIRD STUDY, V58, P302, DOI 10.1080/00063657.2011.588195
   Ezat MA, 2018, BIOL CONSERV, V223, P76, DOI 10.1016/j.biocon.2018.04.032
   Ferrari MA, 2013, MAR MAMMAL SCI, V29, P407, DOI 10.1111/j.1748-7692.2012.00585.x
   Frederick P, 2003, MONITORING ECOSYSTEMS, P321
   Green MC, 2008, J WILDLIFE MANAGE, V72, P697, DOI 10.2193/2006-391
   Groom G., P REM SENS PHOT SOC, DOI [10.1111/j.1477-9730.2007.00455.x, DOI 10.1111/J.1477-9730.2007.00455.X]
   Groom G, 2013, ECOL INFORM, V14, P2, DOI 10.1016/j.ecoinf.2012.12.001
   Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
   Hodgson JC, 2016, SCI REP-UK, V6, DOI 10.1038/srep22574
   Inman VL, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219652
   Kingsford RT, 2009, WILDLIFE RES, V36, P29, DOI 10.1071/WR08034
   Kingsford RT, 1999, FRESHWATER BIOL, V41, P425, DOI 10.1046/j.1365-2427.1999.00440.x
   Kingsford RT, 2005, RIVER RES APPL, V21, P187, DOI 10.1002/rra.840
   Kingsford RT, 1998, COLON WATERBIRD, V21, P159, DOI 10.2307/1521903
   Knight J, 2011, HUM ANIM STUD, V10, P1, DOI 10.1163/ej.9789004187931.i-629
   Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
   Leslie DJ, 2001, REGUL RIVER, V17, P21, DOI 10.1002/1099-1646(200101/02)17:1&lt;21::AID-RRR589&gt;3.0.CO;2-V
   Liu CC, 2015, ECOL INFORM, V30, P170, DOI 10.1016/j.ecoinf.2015.10.008
   Loots S., EVALUATION RADAR CAM
   Lopez R., 2006, EC DEV ENV SUSTAINAB
   Lowndes JSS, 2017, NAT ECOL EVOL, V1, DOI 10.1038/s41559-017-0160
   Lyons Mitchell, 2018, Australian Field Ornithology, V35, P51
   Lyons MB, 2019, METHODS ECOL EVOL, V10, P1024, DOI 10.1111/2041-210X.13194
   McEvoy JF, 2016, PEERJ, V4, DOI 10.7717/peerj.1831
   McNeill S, 2011, INT GEOSCI REMOTE SE, P4312, DOI 10.1109/IGARSS.2011.6050185
   Menkhorst P., AUSTR BIRD GUIDE CSI
   Mooii Tech, 2019, PHOT X
   Narayanan S. Prasanth, 2007, Podoces, V2, P22
   Niemczynwicz A, 2015, WATERBIRDS, V38, P282, DOI 10.1675/063.038.0308
   Ogden JC, 2014, ECOL INDIC, V44, P148, DOI 10.1016/j.ecolind.2014.03.007
   Pirotta V, 2017, FRONT MAR SCI, V4, DOI 10.3389/fmars.2017.00425
   Pix4d SA, 2019, PIX4DCAPTURE
   Pomeroy PP, 2000, J ZOOL, V250, P1
   R Core Team, R LANG ENV STAT COMP
   Rees AF, 2018, ENDANGER SPECIES RES, V35, P81, DOI 10.3354/esr00877
   Rodgers JA, 2005, WATERBIRDS, V28, P230, DOI 10.1675/1524-4695(2005)028[0230:AOASOW]2.0.CO;2
   Schofield G, 2017, FUNCT ECOL, V31, P2310, DOI 10.1111/1365-2435.12930
   Sen Gupta A, 2017, J GEOPHYS RES-SPACE, V122, P12353, DOI 10.1002/2017JA023949
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Teucher A., USING PRINCIPLES OPE
   Wakefield ED, 2017, ECOL APPL, V27, P2074, DOI 10.1002/eap.1591
   Wandler A, 2016, J CUTAN PATHOL, V43, P956, DOI 10.1111/cup.12778
   Wright MN, 2017, J STAT SOFTW, V77, P1, DOI 10.18637/jss.v077.i01
   Znidersic E, 2017, WATERBIRDS, V40, P417, DOI 10.1675/063.040.0414
NR 60
TC 13
Z9 13
U1 4
U2 6
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD APR
PY 2020
VL 12
IS 7
AR 1185
DI 10.3390/rs12071185
PG 17
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging
   Science & Photographic Technology
GA LU4EF
UT WOS:000537709600135
OA gold
DA 2022-02-10
ER

PT J
AU Phosri, K
   Tantipisanuh, N
   Chutipong, W
   Gore, ML
   Giordano, AJ
   Ngoprasert, D
AF Phosri, Kitipat
   Tantipisanuh, Naruemon
   Chutipong, Wanlop
   Gore, Meredith L.
   Giordano, Anthony J.
   Ngoprasert, Dusit
TI Fishing cats in an anthropogenic landscape: A multi-method assessment of
   local population status and threats
SO GLOBAL ECOLOGY AND CONSERVATION
LA English
DT Article
DE Camera-trapping; Human dimensions; Interviews; Prionailurus viverrinus;
   Spatially explicit capture-recapture; Thailand
ID PRIONAILURUS-VIVERRINUS BENNETT; 1833 CARNIVORA FELIDAE; CONSERVATION;
   WILDLIFE; BEAR; ECOLOGY
AB Fishing cat populations appear to have declined significantly in recent years due to the loss and fragmentation of inland and coastal wetland habitats. Moreover, there are still large gaps in data on population and density estimates, and threat evaluation, which are vital for conservation assessments. This research aimed to help fill these critical knowledge gaps. Our study is the first density estimate for fishing cat from mainland Southeast Asia. We conducted a camera-trap survey and used a spatially-explicit capture-recapture analytical framework to estimate the abundance of fishing cats in and around Khao Sam Roi Yot area (KSRY), which hosts an isolated, threatened population of the felid in Thailand. We also conducted interviews among adjacent communities to better understand local perspectives toward fishing cats, conflict with local people, and as a consequence of both, anthropogenic threats to the population. Over 6966 trap-days, we identified at least 33 individual adult cats and based on our top model (g0 similar to bk, sigma similar to h2), we estimated fishing cat density to be 18 +/- SE 6 individuals/100 km(2) (95% CI 10 - 33). Among 80 interviewees, we recorded 25 incidents of conflict, most relating to raids on poultry (n = 18) and damage to fishing gear in pursuit of fish (n = 5). Land use type, land use change, and human activity, did not significantly affect fishing cat density and movements. Our findings further suggest that a proposed tax policy governing land use may force landowners to convert suitable fishing cat habitat to unsuitable areas, resulting in the loss of up to 30% of existing suitable habitat from our study area. We also found that local communities would support either an exemption for landowners not wishing to develop suitable fishing cat habitat, and/or an additional policies that incentivize the maintenance and/or preservation of areas suitable for fishing cats. Finally, we conclude that the official presence of park officers in communities beyond the protected area would be beneficial, as would the implementation of public outreach programming to mitigate negative attitudes toward fishing cats, and provide recommendations on strategies for coexisting with them. (C) 2021 The Author(s). Published by Elsevier B.V. CC_BY_NC_ND_4.0
C1 [Phosri, Kitipat; Ngoprasert, Dusit] King Mongkuts Univ Technol Thonburi, Sch Bioresources & Technol, Conservat Ecol Program, Bangkok 10150, Thailand.
   [Tantipisanuh, Naruemon; Chutipong, Wanlop; Ngoprasert, Dusit] King Mongkuts Univ Technol Thonburi, Pilot Plant Dev & Training Inst, Conservat Ecol Program, Bangkok 10150, Thailand.
   [Gore, Meredith L.] Univ Maryland, Dept Geog Sci, College Pk, MD 20742 USA.
   [Phosri, Kitipat; Giordano, Anthony J.; Ngoprasert, Dusit] SPECIES Soc Preservat Endangered Carnivores & Int, POB 7403, Ventura, CA 93006 USA.
RP Ngoprasert, D (corresponding author), King Mongkuts Univ Technol Thonburi, Pilot Plant Dev & Training Inst, Conservat Ecol Program, Bangkok 10150, Thailand.
EM ndusit@gmail.com
FU King Mongkut's University of Technology Thonburi; ASAHI Glass
   Foundation, Japan; Society for the Preservation of Endangered Carnivores
   and their International Ecological Study
FX We appreciate financial supports from the King Mongkut's University of
   Technology Thonburi (KM2017) , the ASAHI Glass Foundation, Japan (2018)
   , and Society for the Preservation of Endangered Carnivores and their
   International Ecological Study (S.P.E.C.I.E.S.) . We would like to thank
   the Thai Department of National Parks, Wildlife and Plant Conservation,
   KSRY director R. Asawakultharin and all park staff for supporting our
   work. We thank M. Grainger for the useful comments and M. Namkhan for
   helping on GIS analysis. We especially thank our field assistants (J.
   Tananantayot, R. Angkaew, S. Khamngam, N. Sangpan and E. Sidum) for
   field work, and appreciate local villagers who participated in
   questionnaire survey.
CR Akaike H., 1998, P 2 INT S INF THEOR, P199, DOI [10.1007/978-1-4612-1694-0_15, 10.1007/978-1-4612-1694-0]
   Ali A, 2018, ETHOL ECOL EVOL, V30, P399, DOI 10.1080/03949370.2017.1423113
   Anderson D.R, 2002, TECHNOMETRICS
   [Anonymous], ROYAL THAI GOVT GAZE, V136, P104
   [Anonymous], ROYAL THAI GOVT GAZE, V136, P21
   Appel, 2016, P 1 INT FISH CAT CON, P48
   Augustine B, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2627
   Barbier EB, 2004, LAND ECON, V80, P389, DOI 10.2307/3654728
   Barrientos R., 2017, RAILWAY ECOLOGY, P43
   Barrientos R, 2019, EUR J WILDLIFE RES, V65, DOI 10.1007/s10344-018-1248-0
   Bauer H, 2017, ORYX, V51, P106, DOI 10.1017/S003060531500068X
   Braczkowski Alexander, 2013, Cat News, V58, P13
   Braczkowski AR, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151033
   Brodie JF, 2013, BIOL CONSERV, V163, P58, DOI 10.1016/j.biocon.2013.01.003
   Chapron G, 2014, SCIENCE, V346, P1517, DOI 10.1126/science.1257553
   Cherdymova EI, 2018, EKOLOJI, V27, P541
   Chowdhury Sayam U., 2015, Cat News, V62, P4
   Chutipong Wanlop, 2019, Journal of Threatened Taxa, V11, P13459, DOI 10.11609/jott.4557.11.4.13459-13469
   Cutter P, 2015, THESIS U MINNESOTA
   DAS KS, 2017, CAT, V66, P25
   Daszak P, 2000, SCIENCE, V287, P443, DOI 10.1126/science.287.5452.443
   Davis EO, 2019, BIOL CONSERV, V235, P119, DOI 10.1016/j.biocon.2019.04.003
   Department of National Parks Wildlife and Plant Conservation, 2015, KHAO SAM ROI YOT NAT
   Dickman AJ, 2011, P NATL ACAD SCI USA, V108, P13937, DOI 10.1073/pnas.1012972108
   Duckworth J.W, 2016, P 1 INT FISH CAT CON
   Duckworth J.W., 2016, P 1 INT FISH CAT CON, P25
   Efford M, 2019, SECRDESIGN SAMPLING
   Efford M.G, SECR SPATIALLY EXPLI
   Espinosa S, 2012, J ENVIRON EDUC, V43, P55, DOI 10.1080/00958964.2011.579642
   Fukuda S, 2017, WORLD C TRANSP RES W
   Gore ML, 2013, CONSERV LETT, V6, P430, DOI 10.1111/conl.12032
   Greenspan E, 2020, HUM DIMENS WILDL, V25, P301, DOI 10.1080/10871209.2020.1728789
   Havmoller RW, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0209541
   Huitric M, 2002, ECOL ECON, V40, P441, DOI 10.1016/S0921-8009(02)00011-3
   Jakes AF, 2018, BIOL CONSERV, V227, P310, DOI 10.1016/j.biocon.2018.09.026
   Jones S, 2019, CONSERV BIOL, V33, P895, DOI 10.1111/cobi.13275
   Kahler JS, 2013, CONSERV BIOL, V27, P177, DOI 10.1111/j.1523-1739.2012.01960.x
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   LeClerq AT, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.131
   Lin Naing, 2019, Journal of Threatened Taxa, V11, P13910, DOI 10.11609/jott.4795.11.7.13910-13914
   Lucas P.S., 2017, RAILW ECOL, P81, DOI [DOI 10.1007/978-3-319-57496-7_6, 10.1007/978-3-319-57496-7_6]
   Machado Renata F, 2017, Ecol. austral, V27, P232, DOI 10.25260/EA.17.27.2.0.416
   Mishra C, 2003, CONSERV BIOL, V17, P1512, DOI 10.1111/j.1523-1739.2003.00092.x
   Mohamed A, 2021, ORYX, V55, P56, DOI 10.1017/S0030605318001503
   Mukherjee S, 2010, IUCN RED LIST THREAT
   Mukherjee S, 2016, IUCN RED LIST THREAT
   Nair S, 2012, HABITAT USE ABUNDANC
   Nuno A, 2015, BIOL CONSERV, V189, P5, DOI 10.1016/j.biocon.2014.09.047
   Official Statistics Registration Systems, 2010, PRACH KHIR POP
   OTP, 2015, STRAT IMPR TRANSP IN
   Pathumratanathan S, 2015, THESIS GRADUATE SCH
   POLLOCK KH, 1990, WILDLIFE MONOGR, P1
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Ramsar, 2015, THAIL KHAO SAM ROI Y
   Schlexer Fredrick V., 2008, P263
   Setianto A., 2013, J APPL GEOL, V5, P21, DOI [10.22146/jag.7204, DOI 10.22146/JAG.7204]
   Shirley EA, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0207973
   Sidorovich A.A., 2020, ZOODIVERSITY, V54, P211
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Thai Meteorological Department, 2013, MONTHL ANN RAINF SEL
   Thaung R, 2018, ORYX, V52, P636, DOI 10.1017/S0030605317001491
   Ullah Z, 2020, GLOB ECOL CONSERV, V24, DOI 10.1016/j.gecco.2020.e01351
   Young JK, 2018, ANIM CONSERV, V21, P285, DOI 10.1111/acv.12438
   Young JC, 2018, METHODS ECOL EVOL, V9, P10, DOI 10.1111/2041-210X.12828
NR 64
TC 0
Z9 0
U1 4
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
EI 2351-9894
J9 GLOB ECOL CONSERV
JI Glob. Ecol. Conserv.
PD JUN
PY 2021
VL 27
AR e01615
DI 10.1016/j.gecco.2021.e01615
PG 16
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA SU9NB
UT WOS:000663455600004
OA gold
DA 2022-02-10
ER

PT C
AU Wahyudi, D
   Kusneti, M
   Suimah
AF Wahyudi, Deni
   Kusneti, Monica
   Suimah
BE Nugroho, RA
   Natalisanto, AI
   Tarigan, D
   Dharma, B
   Astuti, IF
   Wahyuningsih, S
TI Biodiversity Inventory and Conservation Opportunity of Suwi Wetlands,
   Muara Ancalong, East Kalimantan, Indonesia
SO 1ST INTERNATIONAL CONFERENCE ON MATHEMATICS, SCIENCE AND COMPUTER
   SCIENCE (ICMSC) 2016: SUSTAINABILITY AND ECO GREEN INNOVATION IN
   TROPICAL STUDIES FOR GLOBAL FUTURE
SE AIP Conference Proceedings
LA English
DT Proceedings Paper
CT 1st International Conference on Mathematics, Science and Computer
   Science (ICMSC)
CY OCT 19-21, 2016
CL Balikpapan, INDONESIA
AB Suwi wetlands lays in location permit of palm oil plantation, which has been cleared partially, but then abandoned because is not suitable for palm oil. Considering the biological richness and the usage, the wetlands is important to be conserved, the most possible is managed as an Essential Ecosystem. The main objective of this study was to conduct an inventory of species diversity of Suwi wetlands. Habitat condition and utilization was recorded as important supporting information. The fieldworks have been done from 2013 to 2016. Camera traps and mistnetts were used and randomly done several times in a place where animal were suspected presence. Direct observations were done in the morning and afternoon especially for bird and mammal inventory while dark night observations were done for the presence of crocodile. The result of fieldworks found 12 species of mammals, 63 species of birds, 9 species of reptiles and 38 species of fish, which 30 of the total 122 species are protected, based on Indonesian law as well as international rule. Proboscis monkey (Nasalis larvatus) is an endemic and one of conservation priority species of Indonesia. Meanwhile, Siamese crocodile (Crocodylus siamensis) is one of the most world's endangered crocodilians.
C1 [Wahyudi, Deni; Kusneti, Monica; Suimah] Yayasan Konservasi Khatulistiwa Indonesia, Jl Cendana,Gang Jamrud 678 2, Samarinda, Indonesia.
RP Wahyudi, D (corresponding author), Yayasan Konservasi Khatulistiwa Indonesia, Jl Cendana,Gang Jamrud 678 2, Samarinda, Indonesia.
EM denidailymail@gmail.com
FU ZGAP; IUCN-EAZA Southeast Asia Campaign; Dortmund Zoo, Germany
FX We would like to thank to Ingan, Kahang, Daud dan Nur Linda for
   assisting to collect the Data. We are gratefull to Iwan, Mohamad, Hai,
   Nanda and Ing for the assistantships. Financial support has been
   provided by ZGAP (JensOve Keckel); IUCN-EAZA Southeast Asia Campaign
   (Mikro Marseille) and Dortmund Zoo, Germany (Frank Brandstaetter).
CR COX JH, 1993, COPEIA, P564
   Erlandson JM, 2001, AM ANTIQUITY, V66, P413, DOI 10.2307/2694242
   Giesen W., 2012, MODELS SUSTAINABLE P
   Kurniati H., 2008, FAUNA INDONESIA, V8, P25
   Nugroho Rudy A., 2016, AACL Bioflux, V9, P345
   Simpson BK, 2010, CROCODILES STATUS SU, P120
   Wahyudi D, 2013, J INDONES NAT HIST, V1, P37
   Wowor D., 2009, AQUATIC FAUNA BASELI
NR 8
TC 0
Z9 0
U1 0
U2 14
PU AMER INST PHYSICS
PI MELVILLE
PA 2 HUNTINGTON QUADRANGLE, STE 1NO1, MELVILLE, NY 11747-4501 USA
SN 0094-243X
BN 978-0-7354-1481-5
J9 AIP CONF PROC
PY 2017
VL 1813
AR 020013
DI 10.1063/1.4975951
PG 7
WC Mathematics, Applied; Physics, Applied
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Mathematics; Physics
GA BH9PN
UT WOS:000404228900013
DA 2022-02-10
ER

PT J
AU Gelin, ML
   Branch, LC
   Thornton, DH
   Novaro, AJ
   Gould, MJ
   Caragiulo, A
AF Gelin, Maria L.
   Branch, Lyn C.
   Thornton, Daniel H.
   Novaro, Andres J.
   Gould, Matthew J.
   Caragiulo, Anthony
TI Response of pumas (Puma concolor) to migration of their primary prey in
   Patagonia
SO PLOS ONE
LA English
DT Article
ID LAMA-GUANICOE; MOUNTAIN LIONS; CAMERA TRAPS; SOCIAL-ORGANIZATION;
   DENSITY-ESTIMATION; CAPTURE-RECAPTURE; ACTIVITY PATTERNS; FEEDING
   ECOLOGY; HABITAT USE; POPULATION
AB Large-scale ungulate migrations result in changes in prey availability for top predators and, as a consequence, can alter predator behavior. Migration may include entire populations of prey species, but often prey populations exhibit partial migration with some individuals remaining resident and others migrating. Interactions of migratory prey and predators have been documented in North America and some other parts of the world, but are poorly studied in South America. We examined the response of pumas (Puma concolor) to seasonal migration of guanacos (Lama guanicoe) in La Payunia Reserve in northern Patagonia Argentina, which is the site of the longest known ungulate migration in South America. More than 15,000 guanacos migrate seasonally in this landscape, and some guanacos also are resident year-round. We hypothesized that pumas would respond to the guanaco migration by consuming more alternative prey rather than migrating with guanacos because of the territoriality of pumas and availability of alternative prey throughout the year at this site. To determine whether pumas moved seasonally with the guanacos, we conducted camera trapping in the summer and winter range of guanacos across both seasons and estimated density of pumas with spatial mark-resight (SMR) models. Also, we analyzed puma scats to assess changes in prey consumption in response to guanaco migration. Density estimates of pumas did not change significantly in the winter and summer range of guanacos when guanacos migrated to and from these areas, indicating that pumas do not follow the migration of guanacos. Pumas also did not consume more alternative native prey or livestock when guanaco availability was lower, but rather fed primarily on guanacos and some alternative prey during all seasons. Alternative prey were most common in the diet during summer when guanacos also were abundant on the summer range. The response of pumas to the migration of guanacos differs from sites in the western North America where entire prey populations migrate and pumas migrate with their prey or switch to more abundant prey when their primary prey migrates.
C1 [Gelin, Maria L.; Branch, Lyn C.] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.
   [Gelin, Maria L.; Branch, Lyn C.] Univ Florida, Sch Nat Resources & Environm, Gainesville, FL 32611 USA.
   [Thornton, Daniel H.] Washington State Univ, Sch Environm, Pullman, WA 99164 USA.
   [Novaro, Andres J.] INIBIOMA Univ Nacl Comahue CONICET, Programa Estepa Patagon & Andina, Wildlife Conservat Soc, Junin De Los Andes, Neuquen, Argentina.
   [Gould, Matthew J.] New Mexico State Univ, Dept Biol, Las Cruces, NM 88003 USA.
   [Caragiulo, Anthony] Amer Museum Nat Hist, Sackler Inst Comparat Genom, New York, NY 10024 USA.
RP Branch, LC (corresponding author), Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.; Branch, LC (corresponding author), Univ Florida, Sch Nat Resources & Environm, Gainesville, FL 32611 USA.
EM branchl@ufl.edu
OI Gould, Matthew/0000-0002-9703-4690
FU Conservation Food and Health Foundation; Pittsburgh Zoo & PPG Aquariums
   PPG Conservation and Sustainability Fund; Rufford Small Grants for
   Nature Conservation; IdeaWild; Tropical Conservation and Development
   Program, University of Florida; BEC.AR Argentina; School of Natural
   Resources and the Environment, University of FloridaUniversity of
   Florida
FX The authors gratefully acknowledge financial support and equipment from
   the Conservation Food and Health Foundation, Pittsburgh Zoo & PPG
   Aquariums PPG Conservation and Sustainability Fund, Rufford Small Grants
   for Nature Conservation, IdeaWild, and the Tropical Conservation and
   Development Program, University of Florida. During this work, M. L.
   Gelin was supported by scholarships from BEC.AR Argentina and the School
   of Natural Resources and the Environment, University of Florida. The
   funders had no role in study design, data collection and analysis,
   decision to publish, or preparation of the manuscript.; We thank the
   Argentinian program of the Wildlife Conservation Society and Direccion
   de Recursos Naturales Renovables de Mendoza for logistical support. We
   are grateful to Mariel Ruiz Blanco and Maria Jose Bolgeri for sharing
   their knowledge of La Payunia and for assistance throughout the
   fieldwork. We also thank all the volunteers on the project, and
   especially the following people for the many days they spent in the
   field: Uriel Menalled, Natanael Griffa, Bianca Bonaparte and park
   rangers Lucas Aros, Andres Castro, Anibal Soto, Edgardo Soto, Francisco
   Estive, Jairo Espinosa. Also, we are grateful to the park rangers of
   Malargue for providing lodging. We gratefully acknowledge financial
   support and equipment from the Conservation Food and Health Foundation,
   Pittsburgh Zoo & PPG Aquarium's PPG Conservation and Sustainability
   Fund, Rufford Small Grants for Nature Conservation, IdeaWild, and the
   Tropical Conservation and Development Program, University of Florida.
   During this work, M. L. Gelin was supported by scholarships from BEC.AR
   Argentina and the School of Natural Resources and the Environment,
   University of Florida. We thank John Goodrich and two anonymous
   reviewers for helpful comments on the manuscript.
CR Altrichter M, 2006, BIODIVERS CONSERV, V15, P2719, DOI 10.1007/s10531-005-0307-5
   Arroyo-Arce S, 2014, REV BIOL TROP, V62, P1449, DOI 10.15517/rbt.v62i4.13314
   Ball JP, 2001, WILDLIFE BIOL, V7, P39, DOI 10.2981/wlb.2001.007
   Ballard Warren B., 1997, Wildlife Monographs, V135, P1
   Bolgeri María J, 2015, Mastozool. neotrop., V22, P255
   Bolgeri MJ, 2015, THESIS
   Branch LC, 1996, J MAMMAL, V77, P453
   Candia R., 1993, MULTEQUINA, V2, P5
   Caragiulo A, 2014, MITOCHONDR DNA, V25, P304, DOI 10.3109/19401736.2013.800486
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chehebar C., 1989, Donana Acta Vertebrata, V16, P247
   Donadio E, 2010, J ZOOL, V280, P33, DOI 10.1111/j.1469-7998.2009.00638.x
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford M.G., 2016, SECR 2 10 SPATIALLY
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Elbroch LM, 2017, J ZOOL, V302, P164, DOI 10.1111/jzo.12442
   Elbroch LM, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083375
   Elbroch LM, 2013, J MAMMAL, V94, P259, DOI 10.1644/12-MAMM-A-041.1
   Elbroch LM, 2012, MAMM BIOL, V77, P377, DOI 10.1016/j.mambio.2012.02.010
   EMMONS LH, 1987, BEHAV ECOL SOCIOBIOL, V20, P271, DOI 10.1007/BF00292180
   Fernández Cynthia S, 2014, Mastozool. neotrop., V21, P331
   Frame PF, 2008, ARCTIC, V61, P134
   Franklin WL, 1999, BIOL CONSERV, V90, P33, DOI 10.1016/S0006-3207(99)00008-7
   FRYXELL JM, 1988, TRENDS ECOL EVOL, V3, P237, DOI 10.1016/0169-5347(88)90166-8
   Gannon WL, 2007, J MAMMAL, V88, P809, DOI 10.1644/06-MAMM-F-185R1.1
   Giroux MA, 2012, J ANIM ECOL, V81, P533, DOI 10.1111/j.1365-2656.2011.01944.x
   Guarda N, 2016, ORYX, V51, P1
   HOFER H, 1993, ANIM BEHAV, V46, P559, DOI 10.1006/anbe.1993.1223
   IRIARTE JA, 1990, OECOLOGIA, V85, P185, DOI 10.1007/BF00319400
   IRIARTE JA, 1991, REV CHIL HIST NAT, V64, P145
   Keehner JR, 2015, BIOL CONSERV, V192, P101, DOI 10.1016/j.biocon.2015.09.006
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Klare U, 2011, MAMMAL REV, V41, P294, DOI 10.1111/j.1365-2907.2011.00183.x
   Knopff KH, 2010, J WILDLIFE MANAGE, V74, P1435, DOI 10.2193/2009-314
   Laundre JW, 2010, WHAT WE KNOW PUMAS L, P76
   LINDZEY FG, 1994, J WILDLIFE MANAGE, V58, P619, DOI 10.2307/3809674
   Llambias EJ., 2008, DISTRITO VOLCANICO P, P263
   LOFT ER, 1984, J WILDLIFE MANAGE, V48, P1317, DOI 10.2307/3801792
   Logan K. A., 2001, DESERT PUMA EVOLUTIO
   LOGAN KA, 1986, J WILDLIFE MANAGE, V50, P648, DOI 10.2307/3800975
   Lyamuya RD, 2014, ORYX, V48, P378, DOI 10.1017/S0030605312001706
   Marker Laurie, 2003, Cat News, V38, P24
   Monteith KL, 2011, ECOSPHERE, V2, DOI 10.1890/ES10-00096.1
   Moraga CA, 2015, ORYX, V49, P30, DOI 10.1017/S0030605312001238
   Mueller T, 2011, GLOBAL ECOL BIOGEOGR, V20, P683, DOI 10.1111/j.1466-8238.2010.00638.x
   Mysterud A, 2013, ECOLOGY, V94, P1257, DOI 10.1890/12-0505.1
   Mysterud A, 2011, OIKOS, V120, P1817, DOI 10.1111/j.1600-0706.2010.19439.x
   Negroes N, 2010, J WILDLIFE MANAGE, V74, P1195, DOI 10.2193/2009-256
   Nelson AA, 2012, ECOL APPL, V22, P2293, DOI 10.1890/11-1829.1
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   Novaro A.J., 2005, LARGE CARNIVORES CON, P267
   Novaro AJ, 2000, BIOL CONSERV, V92, P25, DOI 10.1016/S0006-3207(99)00065-8
   Novaro AJ, 2006, TERCER INFORM AVANCE
   Nunez-Regueiro MM, 2015, BIOL CONSERV, V187, P19, DOI 10.1016/j.biocon.2015.04.001
   Ortega IM, 1995, REV CHIL HIST NAT, V68, P489
   Packer C, 1995, SERENGETI, P299
   Paviolo A, 2009, J MAMMAL, V90, P926, DOI 10.1644/08-MAMM-A-128.1
   Pierce BM, 1999, J MAMMAL, V80, P986, DOI 10.2307/1383269
   Pierce BM, 2000, ECOLOGY, V81, P1533
   Puig S, 1996, J ARID ENVIRON, V34, P215, DOI 10.1006/jare.1996.0103
   Puig Silvia, 2003, Multequina, V12, P37
   Quiroga VA, 2016, J NAT CONSERV, V31, P9, DOI 10.1016/j.jnc.2016.02.004
   Rich LN, 2014, J MAMMAL, V95, P382, DOI 10.1644/13-MAMM-A-126
   Royle JA, 2009, J APPL ECOL, V46, P118, DOI 10.1111/j.1365-2664.2008.01578.x
   Schroeder NM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0085960
   Seidensticker J.C. IV., 1973, Wildlife Monogr, VNo. 35, P1
   Silver S., 2004, ASSESSING JAGUAR ABU, P1
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Sollmann R, 2013, J APPL ECOL, V50, P961, DOI 10.1111/1365-2664.12098
   Soria-Diaz L, 2010, ANIM BIOL, V60, P361, DOI 10.1163/157075610X523251
   Stoner DC, 2006, J WILDLIFE MANAGE, V70, P1588, DOI 10.2193/0022-541X(2006)70[1588:CELIUI]2.0.CO;2
   Sunquist M., 2002, WILD CATS WORLD, DOI 10.1644/1545-1542(2004)0852.0.co;2
   Thornton DH, 2015, WILDLIFE RES, V42, P394, DOI 10.1071/WR15092
   Valeix M, 2012, J APPL ECOL, V49, P73, DOI 10.1111/j.1365-2664.2011.02099.x
   Villepique JT, 2011, SOUTHWEST NAT, V56, P187, DOI 10.1894/F07-TAL.1
   Walton Z, 2017, OIKOS, V126, P642, DOI 10.1111/oik.03374
   WEINGART E L, 1973, American Midland Naturalist, V90, P508, DOI 10.2307/2424481
   White PJ, 2007, BIOL CONSERV, V135, P502, DOI 10.1016/j.biocon.2006.10.041
   Wilmers CC, 2003, J ANIM ECOL, V72, P909, DOI 10.1046/j.1365-2656.2003.00766.x
   Martinez JIZ, 2012, ORYX, V46, P106, DOI 10.1017/S0030605310001821
   Zanon-Martinez JI, 2016, WILDLIFE RES, V43, P449, DOI 10.1071/WR16056
   Zar, 1999, BIOSTATISTICAL ANAL
NR 83
TC 12
Z9 14
U1 2
U2 23
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD DEC 6
PY 2017
VL 12
IS 12
AR e0188877
DI 10.1371/journal.pone.0188877
PG 16
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA FO9MS
UT WOS:000417212200050
PM 29211753
OA Green Published, Green Submitted, gold
DA 2022-02-10
ER

PT J
AU Caruso, N
   Lucherini, M
   Fortin, D
   Casanave, EB
AF Caruso, Nicolas
   Lucherini, Mauro
   Fortin, Daniel
   Casanave, Emma B.
TI Species-Specific Responses of Carnivores to Human-Induced Landscape
   Changes in Central Argentina
SO PLOS ONE
LA English
DT Article
ID CAT ONCIFELIS-GEOFFROYI; HABITAT SELECTION; CONEPATUS-CHINGA;
   HOME-RANGE; SPATIAL-ORGANIZATION; GRASSLAND RELICT; PAMPAS GRASSLAND;
   PUMA-CONCOLOR; FRAGMENTATION; FOREST
AB The role that mammalian carnivores play in ecosystems can be deeply altered by human-driven habitat disturbance. While most carnivore species are negatively affected, the impact of habitat changes is expected to depend on their ecological flexibility. We aimed to identify key factors affecting the habitat use by four sympatric carnivore species in landscapes of central Argentina. Camera trapping surveys were carried out at 49 sites from 2011 to 2013. Each site was characterized by 12 habitat attributes, including human disturbance and fragmentation. Four landscape gradients were created from Principal Component Analysis and their influence on species-specific habitat use was studied using Generalized Linear Models. We recorded 74 events of Conepatus chinga, 546 of Pseudalopex gymnocercus, 193 of Leopardus geoffroyi and 45 of Puma concolor. We found that the gradient describing sites away from urban settlements and with low levels of disturbance had the strongest influence. L. geoffroyi was the only species responding significantly to the four gradients and showing a positive response to modified habitats, which could be favored by the low level of persecution by humans. P. concolor made stronger use of most preserved sites with low proportion of cropland, even though the species also used sites with an intermediate level of fragmentation. A more flexible use of space was found for C. chinga and P. gymnocercus. Our results demonstrate that the impact of human activities spans across this guild of carnivores and that species-specific responses appear to be mediated by ecological and behavioral attributes.
C1 [Caruso, Nicolas; Lucherini, Mauro; Casanave, Emma B.] Univ Nacl Sur, Dept Biol Bioquim & Farm, Catedra Fisiol Anim, Grp Ecol Comportamental Mamiferos, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   [Caruso, Nicolas; Lucherini, Mauro] Consejo Nacl Invest Cient & Tecn, RA-1033 Buenos Aires, DF, Argentina.
   [Fortin, Daniel] Univ Laval, Dept Biol, Quebec City, PQ G1K 7P4, Canada.
   [Fortin, Daniel] Univ Laval, Dept Biol, Ctr Etud Foret, Quebec City, PQ G1K 7P4, Canada.
   [Casanave, Emma B.] Univ Nacl Sur, Inst Ciencias Biol & Biomed Sur INBIOSUR, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   [Casanave, Emma B.] Consejo Nacl Invest Cient & Tecn, Bahia Blanca, Buenos Aires, Argentina.
RP Caruso, N (corresponding author), Univ Nacl Sur, Dept Biol Bioquim & Farm, Catedra Fisiol Anim, Grp Ecol Comportamental Mamiferos, RA-8000 Bahia Blanca, Buenos Aires, Argentina.; Caruso, N (corresponding author), Consejo Nacl Invest Cient & Tecn, RA-1033 Buenos Aires, DF, Argentina.
EM nccaruso@gmail.com
RI Lucherini, Mauro/D-9668-2016; Fortin, Daniel/ABG-3554-2020
OI Casanave, Emma Beatriz/0000-0003-3482-8175; Lucherini,
   Mauro/0000-0002-9695-3943
FU Consejo Nacional de Investigaciones Cientificas y TecnicasConsejo
   Nacional de Investigaciones Cientificas y Tecnicas (CONICET);
   Universidad Nacional del Sur
FX This work was funded by Consejo Nacional de Investigaciones Cientificas
   y Tecnicas (http://www.conicet.gov.ar/) (recipient: ML), and Universidad
   Nacional del Sur (https://www.uns.edu.ar/) (recipient: EC). The funders
   had no role in study design, data collection and analysis, decision to
   publish, or preparation of the manuscript.
CR Andera M, 2009, LARGE MAMMALS CZECH
   Anderson K. A., 2002, MODEL SELECTION MULT
   Austen MJW, 2001, CONDOR, V103, P701, DOI 10.1650/0010-5422(2001)103[0701:LCAFEO]2.0.CO;2
   Borcard D, 2011, USE R, P1, DOI 10.1007/978-1-4419-7976-6
   Busso CA, 1997, J ARID ENVIRON, V36, P197, DOI 10.1006/jare.1996.0205
   Caruso N, 2015, ECOL MODEL, V297, P11, DOI 10.1016/j.ecolmodel.2014.11.004
   Caruso N, ORYX IN PRESS
   Castillo D, SMALL CARNI IN PRESS
   Castillo DF, 2011, CAN J ZOOL, V89, P229, DOI 10.1139/Z10-110
   Castillo DF, 2014, MAMMALIA, V78, P473, DOI 10.1515/mammalia-2013-0020
   Castillo Diego F, 2013, Mastozool. neotrop., V20, P373
   Castillo Diego F., 2008, Cat News, V49, P27
   Crooks KR, 2011, PHILOS T R SOC B, V366, P2642, DOI 10.1098/rstb.2011.0120
   Crooks KR, 1999, NATURE, V400, P563, DOI 10.1038/23028
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   De Angelo C, 2011, DIVERS DISTRIB, V17, P422, DOI 10.1111/j.1472-4642.2011.00746.x
   de Oliveira TG, 2014, J MAMM EVOL, V21, P427, DOI 10.1007/s10914-013-9251-4
   de Oliveira TG, 2005, GUIA CAMPO FELINOS
   Di Bitetti MS, 2006, J ZOOL, V270, P153, DOI 10.1111/j.1469-7998.2006.00102.x
   Di Bitetti MS, 2009, J MAMMAL, V90, P479, DOI 10.1644/08-MAMM-A-113.1
   Dobrovolski R, 2013, BIOL CONSERV, V165, P162, DOI 10.1016/j.biocon.2013.06.004
   Einseberg, 1992, MAMMALS NEOTROPICS S
   Emmons L, 2015, CONEPATUS CHINGA
   Castillo DF, 2012, J MAMMAL, V93, P716, DOI 10.1644/11-MAMM-A-300.2
   Fahrig L, 2003, ANNU REV ECOL EVOL S, V34, P487, DOI 10.1146/annurev.ecolsys.34.011802.132419
   FAHRIG L, 1995, BIOL CONSERV, V73, P177, DOI 10.1016/0006-3207(94)00102-V
   Fazey I, 2005, BIOL CONSERV, V124, P63, DOI 10.1016/j.biocon.2005.01.013
   Fernandez O., 1999, ARID SEMIARID RANGEL, V200, P41
   Fernandez OA, 2009, LAND DEGRAD DEV, V20, P431, DOI 10.1002/ldr.851
   Garcia VB, 2005, MAMM BIOL, V70, P218, DOI 10.1016/j.mambio.2004.11.019
   Gehring TM, 2003, BIOL CONSERV, V109, P283, DOI 10.1016/S0006-3207(02)00156-8
   Haila Y, 2002, ECOL APPL, V12, P321, DOI 10.1890/1051-0761(2002)012[0321:ACGOFR]2.0.CO;2
   Hanski I, 1998, NATURE, V396, P41, DOI 10.1038/23876
   Jimenez JE, 2015, IUCN RED LIST THREAT
   Kanagaraj R, 2011, ECOGRAPHY, V34, P970, DOI 10.1111/j.1600-0587.2010.06482.x
   Kasper CB, 2012, MAMM BIOL, V77, P358, DOI 10.1016/j.mambio.2012.03.006
   Knopff AA, 2014, BIOL CONSERV, V178, P136, DOI 10.1016/j.biocon.2014.07.017
   Lindenmayer D. B., 2006, HABITAT FRAGMENTATIO
   Linnell John D. C., 2000, Diversity and Distributions, V6, P169, DOI 10.1046/j.1472-4642.2000.00069.x
   Lucherini Mauro, 2008, Mammalian Species, P1, DOI 10.1644/820.1
   Vidal EML, 2012, J ZOOL, V287, P133, DOI 10.1111/j.1469-7998.2011.00896.x
   Lyra-Jorge MC, 2008, BIODIVERS CONSERV, V17, P1573, DOI 10.1007/s10531-008-9366-8
   Lyra-Jorge MC, 2010, EUR J WILDLIFE RES, V56, P359, DOI 10.1007/s10344-009-0324-x
   Manfredi C, 2006, J ZOOL, V268, P381, DOI 10.1111/j.1469-7998.2005.00033.x
   Manfredi C, 2006, NICHO TROFICO ESPACI
   Manfredi C, 2012, MAMMALIA, V76, P105, DOI 10.1515/mammalia-2011-0039
   May R, 2008, J APPL ECOL, V45, P1382, DOI 10.1111/j.1365-2664.2008.01527.x
   Mazzolli M, 2010, ENVIRON MANAGE, V46, P237, DOI 10.1007/s00267-010-9528-9
   Naves J, 2003, CONSERV BIOL, V17, P1276, DOI 10.1046/j.1523-1739.2003.02144.x
   Nielsen CK, 2015, IUCN RED LIST THREAT
   OJEDA R V, 2012, LIBRO ROJO MAMIFEROS
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Pereira JA, 2012, J ARID ENVIRON, V76, P36, DOI 10.1016/j.jaridenv.2011.08.006
   Pereira JA, 2015, IUCN RED LIST THREAT
   Pereira JA, 2006, J MAMMAL, V87, P1132, DOI 10.1644/05-MAMM-A-333R2.1
   Pezzola A., 2004, BOL TEC, V12, P1
   Porini G, 2007, ESTADO CONOCIMIENTO
   R Development Core Team, 2013, R LANG ENV STAT COMP
   Rondinini C, 2011, PHILOS T R SOC B, V366, P2633, DOI 10.1098/rstb.2011.0113
   Salek M, 2010, LANDSCAPE URBAN PLAN, V98, P86, DOI 10.1016/j.landurbplan.2010.07.013
   Schadt S, 2002, J APPL ECOL, V39, P189, DOI 10.1046/j.1365-2664.2002.00700.x
   Smith JA, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2014.2711
   TELLERIA JL, 1991, J MAMMAL, V72, P183, DOI 10.2307/1381994
   Vanak AT, 2013, ECOLOGY, V94, P2619, DOI 10.1890/13-0217.1
   Lantschner MV, 2011, MAMMALIA, V75, P249, DOI 10.1515/MAMM.2011.031
   Virgos E, 2002, BIODIVERS CONSERV, V11, P1063, DOI 10.1023/A:1015856703786
   Woodroffe R, 1998, SCIENCE, V280, P2126, DOI 10.1126/science.280.5372.2126
   Zar J.H., 2010, BIOSTATISTICAL ANAL, Vfifth
   Zuur Alain F., 2009, P1
NR 69
TC 18
Z9 21
U1 1
U2 38
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD MAR 7
PY 2016
VL 11
IS 3
AR e0150488
DI 10.1371/journal.pone.0150488
PG 16
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA DG3SL
UT WOS:000371990100036
PM 26950300
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Wang, YW
   Allen, ML
   Wilmers, CC
AF Wang, Yiwei
   Allen, Maximilian L.
   Wilmers, Christopher C.
TI Mesopredator spatial and temporal responses to large predators and human
   development in the Santa Cruz Mountains of California
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Mesopredator; Puma concolor; Occupancy; Habitat fragmentation;
   Mesopredator release; Trophic cascade; Canis latrans; Recreation;
   Competition
ID INTRAGUILD PREDATION; EXURBAN DEVELOPMENT; MODEL SELECTION; COYOTES;
   HABITAT; CARNIVORES; PATTERNS; RELEASE; URBANIZATION; COMPETITION
AB Human-driven declines of apex predators can trigger widespread impacts throughout ecological communities. Reduced apex predator occupancy or activity can release mesopredators from intraguild competition, with unknown repercussions on the ecological community. As exurban development continues to expand worldwide, it is important to document how mesopredators are impacted by the combined influences of apex predators and humans. We used motion-detecting camera traps to examine spatial and temporal patterns of meso- and apex predator occupancy and activity in a fragmented landscape in California. We hypothesized that both spatial and temporal partitioning among the carnivore guild would be affected by varied levels of human influence. We found that higher residential development reduced puma occupancy but was not related to the occupancy of mesopredators. Bobcats, grey foxes, and Virginia opossums were detected more often at sites occupied by pumas, whereas coyotes and raccoons were detected less often. The detection probabilities of smaller mesopredators were related to coyotes, a dominant mesopredator, but the magnitude and direction of these correlations differed depending upon puma occupancy. We also found that species altered their activities temporally in locations with higher human use, with pumas, bobcats and coyotes reducing diurnal activities and increasing nocturnal ones. These activity shifts were reflected in reduced temporal partitioning between intraguild competitors, with unknown effects on species interactions and repercussions to the prey community. Our results suggest that human development and activity alters predator community structure through both direct and indirect pathways. Therefore effective carnivore conservation requires an understanding of how mesopredators respond to varying levels of apex predator and anthropogenic influences. (C) 2015 The Authors. Published by Elsevier Ltd.
C1 [Wang, Yiwei; Allen, Maximilian L.; Wilmers, Christopher C.] Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Dept Environm Studies, Santa Cruz, CA 95064 USA.
RP Wang, YW (corresponding author), Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Dept Environm Studies, 1156 High St, Santa Cruz, CA 95064 USA.
EM yiweiwang@dataone.unm.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU American Mammalogy Association; Sigma Xi [G2009101017]; NSFNational
   Science Foundation (NSF) [0963022]; Gordon and Betty Moore
   FoundationGordon and Betty Moore Foundation; Department of Environmental
   Studies at University of California Santa Cruz
FX We thank the numerous public and private landowners for giving us access
   to their land and for their willingness to support science and
   conservation. Many undergraduate interns contributed to this project by
   maintaining cameras in the field and sorting camera photographs. In
   particular, we thank Lee Hibbeler, who helped coordinate many of these
   activities. We thank Barry Nickel and Aaron Cole for help obtaining GIS
   data, Yasaman Shakeri and Paul Houghtaling for providing field
   assistance, and Jim Estes for insightful comments on the manuscript.
   Funding was provided by the American Mammalogy Association, Sigma Xi
   Grant G2009101017, NSF Grant #0963022, the Gordon and Betty Moore
   Foundation, and the Department of Environmental Studies at University of
   California Santa Cruz.
CR Allen ML, 2015, CALIF FISH GAME, V101, P51
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155
   Arnold TW, 2010, J WILDLIFE MANAGE, V74, P1175, DOI 10.2193/2009-367
   Bateman PW, 2012, J ZOOL, V287, P1, DOI 10.1111/j.1469-7998.2011.00887.x
   Berger KM, 2008, ECOL APPL, V18, P599, DOI 10.1890/07-0308.1
   Bidlack A.L., 2007, ENV SCI POLICY MANAG, P201
   Brook LA, 2012, J APPL ECOL, V49, P1278, DOI 10.1111/j.1365-2664.2012.02207.x
   Carter NH, 2012, P NATL ACAD SCI USA, V109, P15360, DOI 10.1073/pnas.1210490109
   Crooks KR, 1999, NATURE, V400, P563, DOI 10.1038/23028
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   Cypher B.L, 2010, URBAN CARNIVORES ECO
   Elmhagen B, 2010, J ANIM ECOL, V79, P785, DOI 10.1111/j.1365-2656.2010.01678.x
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Fedriani JM, 2000, OECOLOGIA, V125, P258, DOI 10.1007/s004420000448
   Gehrt SD, 2003, WILDLIFE SOC B, V31, P836
   Gehrt SD, 2007, BEHAV ECOL, V18, P204, DOI 10.1093/beheco/arl075
   Gehrt SD, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075718
   George SL, 2006, BIOL CONSERV, V133, P107, DOI 10.1016/j.biocon.2006.05.024
   Ginger SM, 2003, J MAMMAL, V84, P1279, DOI 10.1644/103
   Goad EH, 2014, BIOL CONSERV, V176, P172, DOI 10.1016/j.biocon.2014.05.016
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Gosselink TE, 2003, J WILDLIFE MANAGE, V67, P90, DOI 10.2307/3803065
   Hansen AJ, 2005, ECOL APPL, V15, P1893, DOI 10.1890/05-5221
   Hass CC, 2009, J ZOOL, V278, P174, DOI 10.1111/j.1469-7998.2009.00565.x
   Hayward MW, 2009, S AFR J WILDL RES, V39, P109, DOI 10.3957/056.039.0207
   Johnson CN, 2007, P R SOC B, V274, P341, DOI 10.1098/rspb.2006.3711
   Johnson JB, 2004, TRENDS ECOL EVOL, V19, P101, DOI 10.1016/j.tree.2003.10.013
   Lennartz S., 2008, FINAL REPORT LAND CO
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Logan K. A., 2001, DESERT PUMA EVOLUTIO
   Lucherini M, 2009, J MAMMAL, V90, P1404, DOI 10.1644/09-MAMM-A-002R.1
   MacKenzie DI, 2004, J ANIM ECOL, V73, P546, DOI 10.1111/j.0021-8790.2004.00828.x
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   McKinney ML, 2006, BIOL CONSERV, V127, P247, DOI 10.1016/j.biocon.2005.09.005
   Meredith M., 2014, OVERLAP ESTIMATES CO
   Monterroso P, 2013, ETHOLOGY, V119, P1044, DOI 10.1111/eth.12156
   Noss RF, 1996, CONSERV BIOL, V10, P949, DOI 10.1046/j.1523-1739.1996.10040949.x
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Pace ML, 1999, TRENDS ECOL EVOL, V14, P483, DOI 10.1016/S0169-5347(99)01723-1
   Palomares F, 1999, AM NAT, V153, P492, DOI 10.1086/303189
   POLIS GA, 1992, TRENDS ECOL EVOL, V7, P151, DOI 10.1016/0169-5347(92)90208-S
   Prange S, 2004, CAN J ZOOL, V82, P1804, DOI 10.1139/Z04-179
   R Development Core Team, 2013, R LANG ENV STAT COMP
   Recio M.R., CURRENT ZOO IN PRESS
   Reed SE, 2008, CONSERV LETT, V1, P146, DOI 10.1111/j.1755-263X.2008.00019.x
   Richmond OMW, 2010, ECOL APPL, V20, P2036, DOI 10.1890/09-0470.1
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Riley SPD, 2006, J WILDLIFE MANAGE, V70, P1425, DOI 10.2193/0022-541X(2006)70[1425:SEOBAG]2.0.CO;2
   Riley SPD, 2003, CONSERV BIOL, V17, P566, DOI 10.1046/j.1523-1739.2003.01458.x
   Rissman AR, 2008, ECOL SOC, V13
   Ritchie EG, 2009, ECOL LETT, V12, P982, DOI 10.1111/j.1461-0248.2009.01347.x
   Ruth T.K., 2009, COUGARS ECOLOGY CONS, P163
   Salek M., 2014, MAMMAL REV
   Schuette P, 2013, BIOL CONSERV, V158, P301, DOI 10.1016/j.biocon.2012.08.008
   Selva N, 2005, CAN J ZOOL, V83, P1590, DOI 10.1139/Z05-158
   Smith JA, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2014.2711
   Wang YW, 2012, WILDLIFE RES, V39, P611, DOI 10.1071/WR11210
   Wilmers CC, 2005, PLOS BIOL, V3, P571, DOI 10.1371/journal.pbio.0030092
   Wilmers CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060590
   Wilson RR, 2010, OECOLOGIA, V164, P921, DOI 10.1007/s00442-010-1797-8
   Zaradic PA, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007367
NR 63
TC 117
Z9 122
U1 5
U2 235
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD OCT
PY 2015
VL 190
BP 23
EP 33
DI 10.1016/j.biocon.2015.05.007
PG 11
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA CO2FS
UT WOS:000358972200004
OA hybrid
DA 2022-02-10
ER

PT J
AU Robinson, NJ
   Johnsen, S
   Brooks, A
   Frey, L
   Judkins, H
   Vecchione, M
   Widder, E
AF Robinson, Nathan J.
   Johnsen, Sonke
   Brooks, Annabelle
   Frey, Lee
   Judkins, Heather
   Vecchione, Michael
   Widder, Edith
TI Studying the swift, smart, and shy: Unobtrusive camera-platforms for
   observing large deep-sea squid
SO DEEP-SEA RESEARCH PART I-OCEANOGRAPHIC RESEARCH PAPERS
LA English
DT Article
DE Architeuthis; Promachoteuthis; Pholidoteuthis; Cephalopod; Gulf of
   Mexico; Pelagic; Twilight zone; Medusa; Eye-in-the-sea
ID LIVE GIANT-SQUID; ARCHITEUTHIS-DUX; CEPHALOPODS; ATLANTIC; OEGOPSIDA;
   KNOWLEDGE; BEHAVIOR; RECORDS; OCEAN; RED
AB The legend of the "kraken" has captivated humans for millennia, yet our knowledge of the large deep-sea cephalopods that inspired this myth remains limited. Conventional methods for exploring the deep sea, including the use of nets, manned submersibles, and remotely operated vehicles (ROVs), are primarily suited for studying slow-moving or sessile organisms, and baited camera-traps tend to attract scavengers rather than predators. To address these issues, unobtrusive deep-sea camera platforms were developed that used low-light cameras, red illuminators, and bioluminescence-mimicking lures. Here, we report on several opportunistic deployments of these devices in the Wider Caribbean Region where we recorded several encounters with large deep-sea squids, including the giant squid Architeuthis dux Steenstrup 1857, Pholidoteuthis adami Voss 1956, and two large squid that may be Promachoteuthis sp. (possibly P. sloani Young et al. 2006). These species were recorded between depths of 557 and 950 m. We estimate the Mantle Lengths (ML) of Promachoteuthis were -1.0 m, the ML of the Pholidoteuthis was -0.5 m, and the ML of the Archtiteuthis was -1.7 m. These encounters suggest that unobtrusive camera platforms with luminescent lures are effective tools for attracting and studying large deep-sea squids.
C1 [Robinson, Nathan J.; Brooks, Annabelle] Cape Eleuthera Inst, POB EL-26029, Rock Sound, Eleuthera, Bahamas.
   [Johnsen, Sonke] Duke Univ, Dept Biol, Durham, NC 27708 USA.
   [Frey, Lee] Arctic Rays, Groton, MA 01450 USA.
   [Judkins, Heather] Univ South Florida St Petersburg, Dept Integrat Biol, St Petersburg, FL 33701 USA.
   [Vecchione, Michael] NOAA, NMFS Natl Systemat Lab, Natl Museum Nat Hist, Washington, DC 20013 USA.
   [Widder, Edith] Ocean Res & Conservat Assoc, Ft Pierce, FL 34949 USA.
   [Robinson, Nathan J.] Fdn Oceanog Ciudad Artes & Ciencias, Valencia, Spain.
RP Robinson, NJ (corresponding author), Cape Eleuthera Inst, POB EL-26029, Rock Sound, Eleuthera, Bahamas.; Robinson, NJ (corresponding author), Fdn Oceanog Ciudad Artes & Ciencias, Valencia, Spain.
EM nathanjackrobinson@gmail.com
RI Robinson, Nathan/AAE-4717-2019
OI Robinson, Nathan/0000-0001-7384-3576
FU National Science FoundationNational Science Foundation (NSF)
   [OCE-1008145]; Monterey Bay Aquarium Research Institute; NOAA's Office
   of Ocean ExplorationNational Oceanic Atmospheric Admin (NOAA) - USA;
   Office of Naval ResearchOffice of Naval Research; National Oceanographic
   and Atmospheric Administration's Office of Exploration and Research
   [NA04OAR4600057, NA05OAR4601059, NA17OAR0110208]; Japan Broadcasting
   Corp. (NHK); Harbor Branch Oceanographic Institution; Discovery Channel;
   Cape Eleuthera Institute
FX Funding for Medusa was provided by National Science Foundation (Grant
   #OCE-1008145). Development of Eye-In-The-Sea was funded in part by the
   Monterey Bay Aquarium Research Institute and Harbor Branch Oceanographic
   Institution and grants from NOAA's Office of Ocean Exploration and the
   Office of Naval Research. The National Oceanographic and Atmospheric
   Administration's Office of Exploration and Research funded the 2004,
   2005, and 2019 expeditions in the Gulf of Mexico (Grants #`s
   NA04OAR4600057, NA05OAR4601059, and NA17OAR0110208 respectively). The
   Japan Broadcasting Corp. (NHK) and the Discovery Channel funded the 2012
   expedition off Japan. The Cape Eleuthera Institute funded the
   deployments of the Medusa in 2013 in the Exuma Sound. We thank the
   captain and crew of the R/V Point Sur as well as Alexander Davis, Dante
   Fenolio, Megan McCall, Heather Bracken-Grissom, Lorian Schweikert,
   Ruchao Qian, Tamara Frank, Tracey Sutton for assistance in the field
   during the 2019 expedition in the Gulf of Mexico. Additional field
   assistance was also provided by the interns and students at the Cape
   Eleuthera Island School for the Medusa deployments in the Exuma Sound.
   Edward Brooks provided essential support and served as academic advisor
   for the deep-sea research program at the Cape Eleuthera Institute.
CR Amante C., 2009, ETOPO1 1 ARC MINUTE, DOI 10.7289/V5C8276M
   Andre M, 2011, FRONT ECOL ENVIRON, V9, P489, DOI 10.1890/100124
   Bolstad KS, 2004, NEW ZEAL J ZOOL, V31, P15, DOI 10.1080/03014223.2004.9518354
   Bustamante P, 2008, MAR ENVIRON RES, V66, P278, DOI 10.1016/j.marenvres.2008.04.003
   Cherel Y, 2009, BIOL LETTERS, V5, P364, DOI 10.1098/rsbl.2009.0024
   Coro G, 2015, ECOL MODEL, V305, P29, DOI 10.1016/j.ecolmodel.2015.03.011
   Diete RL, 2015, AUST J ZOOL, V63, P376, DOI 10.1071/ZO15050
   Ellis Richard., 1998, SEARCH GIANT SQUID
   FRANK TM, 1988, BIOL BULL, V175, P261, DOI 10.2307/1541567
   Guerra A, 2004, J MAR BIOL ASSOC UK, V84, P427, DOI 10.1017/S0025315404009397h
   GUERRA A, 2009, CALAMAR GIGANTE
   Guerra A, 2018, ECOLOGY, V99, P755, DOI 10.1002/ecy.2073
   Guerra A, 2011, BIOL CONSERV, V144, P1989, DOI 10.1016/j.biocon.2011.04.021
   Hanlon R., 2018, OCTOPUS SQUID CUTTLE
   Herring P., 2002, BIOL DEEP OCEAN
   Hoving HJT, 2012, BIOL BULL-US, V223, P263, DOI 10.1086/BBLv223n3p263
   Hoving HJT, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.1463
   Hoving HJT, 2014, ADV MAR BIOL, V67, P235, DOI 10.1016/B978-0-12-800287-2.00003-2
   Hoving HJT, 2004, J ZOOL, V264, P153, DOI 10.1017/S0952836904005710
   Jamieson AJ, 2020, MAR BIOL, V167, DOI 10.1007/s00227-020-03701-1
   Judkins H, 2017, MAR BIODIVERS, V47, P647, DOI 10.1007/s12526-016-0597-8
   Judkins H, 2015, J NAT HIST, V49, P1267, DOI 10.1080/00222933.2013.802045
   Judkins H, 2009, P BIOL SOC WASH, V122, P162, DOI 10.2988/08-30.1
   Kaartvedt S, 2012, MAR ECOL PROG SER, V456, P1, DOI 10.3354/meps09785
   Kubodera T, 2005, P ROY SOC B-BIOL SCI, V272, P2583, DOI 10.1098/rspb.2005.3158
   Kubodera T, 2007, P R SOC B, V274, P1029, DOI 10.1098/rspb.2006.0236
   Kubodera T, 2018, MAR BIODIVERS, V48, P1391, DOI 10.1007/s12526-016-0618-7
   Leite Luciana, 2016, Marine Biodiversity Records, V9, P26, DOI 10.1186/s41200-016-0028-3
   Levin LA, 2015, SCIENCE, V350, P766, DOI 10.1126/science.aad0126
   Lindsay DJ, 2020, DIVERSITY-BASEL, V12, DOI 10.3390/d12120449
   Mooney TA, 2010, J EXP BIOL, V213, P3748, DOI 10.1242/jeb.048348
   Nixon M., 2003, BRAINS LIVES CEPHALO
   NORMAN M, 2000, CEPHALOPODS WORLD GU
   Osterhage D, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0241066
   Paxton CGM, 2016, J ZOOL, V300, P82, DOI 10.1111/jzo.12347
   Raymond EH, 2007, MAR ECOL PROG SER, V350, P291, DOI 10.3354/meps07196
   Remeslo A, 2019, DEEP-SEA RES PT I, V147, P121, DOI 10.1016/j.dsr.2019.04.008
   Roper C.F.E., 2010, FAO SPECIES CATALOGU, V2, P370
   ROPER CFE, 1982, SCI AM, V246, P96, DOI 10.1038/scientificamerican0482-96
   Roper CFE, 2015, AM MALACOL BULL, V33, P78, DOI 10.4003/006.033.0116
   Roper CFE, 2013, AM MALACOL BULL, V31, P109, DOI 10.4003/006.031.0104
   Salgado TG, 2003, NEXUS NETW J, V5, P22, DOI DOI 10.1007/S00004-002-0003-7
   SEIDOU M, 1990, J COMP PHYSIOL A, V166, P769
   Vecchione M, 2001, AM FISH S S, V25, P153
   Vecchione M, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00403
   Vecchione M, 2010, MAR BIOL RES, V6, P25, DOI 10.1080/17451000902810751
   Webb TJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010223
   Widder EA, 2005, DEEP-SEA RES PT I, V52, P2077, DOI 10.1016/j.dsr.2005.06.007
   Widder E, 2013, SEA TECHNOL, V54, P49
   Widder EA, 2007, OCEANOGRAPHY, V20, P46, DOI 10.5670/oceanog.2007.04
   Young RE, 2006, P BIOL SOC WASH, V119, P287, DOI 10.2988/0006-324X(2006)119[287:PSANSO]2.0.CO;2
NR 51
TC 0
Z9 0
U1 1
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0967-0637
EI 1879-0119
J9 DEEP-SEA RES PT I
JI Deep-Sea Res. Part I-Oceanogr. Res. Pap.
PD JUN
PY 2021
VL 172
AR 103538
DI 10.1016/j.dsr.2021.103538
PG 8
WC Oceanography
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Oceanography
GA SM4IX
UT WOS:000657572400001
OA hybrid
DA 2022-02-10
ER

PT J
AU Tan, L
   Wang, YN
   Yu, HS
   Zhu, J
AF Tan, Lei
   Wang, Yaonan
   Yu, Hongshan
   Zhu, Jiang
TI Automatic Camera Calibration Using Active Displays of a Virtual Pattern
SO SENSORS
LA English
DT Article
DE camera calibration; 2D pattern; active display; lens distortion;
   closed-form solution; maximum likelihood estimation
AB Camera calibration plays a critical role in 3D computer vision tasks. The most commonly used calibration method utilizes a planar checkerboard and can be done nearly fully automatically. However, it requires the user to move either the camera or the checkerboard during the capture step. This manual operation is time consuming and makes the calibration results unstable. In order to solve the above problems caused by manual operation, this paper presents a full-automatic camera calibration method using a virtual pattern instead of a physical one. The virtual pattern is actively transformed and displayed on a screen so that the control points of the pattern can be uniformly observed in the camera view. The proposed method estimates the camera parameters from point correspondences between 2D image points and the virtual pattern. The camera and the screen are fixed during the whole process; therefore, the proposed method does not require any manual operations. Performance of the proposed method is evaluated through experiments on both synthetic and real data. Experimental results show that the proposed method can achieve stable results and its accuracy is comparable to the standard method by Zhang.
C1 [Tan, Lei; Wang, Yaonan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
   [Wang, Yaonan; Yu, Hongshan] Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
   [Zhu, Jiang] Xiangtan Univ, Coll Informat Engn, Xiangtan 411105, Peoples R China.
RP Tan, L (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.; Yu, HS (corresponding author), Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
EM leit@hnu.edu.cn; wangyaonan@hnu.edu.cn; yuhongshan@hnu.edu.cn;
   jiang126@126.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61573134, 61573135]; National Key Technology
   Support ProgramNational Key Technology R&D Program [2015BAF11B01];
   National Key Scientific Instrument and Equipment Development Project of
   China [2013YQ140517]; Key Research and Development Project of Science
   and Technology Plan of Hunan Province [2015GK3008]; Key Project of
   Science and Technology Plan of Guangdong Province [2013B011301014]
FX This work has been supported by National Natural Science Foundation of
   China (Grant No. 61573134, 61573135), National Key Technology Support
   Program (Grant No. 2015BAF11B01), National Key Scientific Instrument and
   Equipment Development Project of China (Grant No. 2013YQ140517), Key
   Research and Development Project of Science and Technology Plan of Hunan
   Province(Grant No. 2015GK3008), Key Project of Science and Technology
   Plan of Guangdong Province(Grant No. 2013B011301014).
CR Agrawal M., 2003, P 2003 9 IEEE INT C, P1
   Atcheson B., 2010, P VIS MOD VIS WORKSH, V10, P41
   Bergamasco F, 2014, INT C PATT RECOG, P2137, DOI 10.1109/ICPR.2014.372
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chen Q, 2004, LECT NOTES COMPUT SC, V3023, P521
   Donne S, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111858
   Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164
   Li B, 2013, IEEE INT C INT ROBOT, P1301, DOI 10.1109/IROS.2013.6696517
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Moreno D, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P464, DOI 10.1109/3DIMPVT.2012.77
   Orghidan R, 2012, FED CONF COMPUT SCI, P123
   Oyamada Y., 2012, P IEEE ISMAR 2012 WO
   Pilet J., 2006, P 5 IEEE ACM INT S M, P69, DOI 10.1109/ISMAR.2006.297796
   Raposo C, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P342, DOI 10.1109/3DV.2013.52
   Rufli M, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3121, DOI 10.1109/IROS.2008.4650703
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Vezhnevets V., OPENCV CALIBRATION O
   Wong KYK, 2011, IEEE T IMAGE PROCESS, V20, P305, DOI 10.1109/TIP.2010.2063035
   ZHANG Z, 1998, MSRTR9871
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 21
TC 17
Z9 17
U1 1
U2 12
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD APR
PY 2017
VL 17
IS 4
AR 685
DI 10.3390/s17040685
PG 13
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA EU1YU
UT WOS:000400822900026
PM 28346366
OA gold, Green Published, Green Submitted
DA 2022-02-10
ER

PT C
AU Wadenback, M
   Karlsson, M
   Heyden, A
   Robertsson, A
   Johansson, R
AF Wadenback, Marten
   Karlsson, Martin
   Heyden, Anders
   Robertsson, Anders
   Johansson, Rolf
BE Imai, F
   Tremeau, A
   Braz, J
TI Visual Odometry from Two Point Correspondences and Initial Automatic
   Camera Tilt Calibration
SO PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER
   VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP
   2017), VOL 6
LA English
DT Proceedings Paper
CT 12th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP)
CY FEB 27-MAR 01, 2017
CL Porto, PORTUGAL
SP Inst Syst & Technologies Informat, Control & Commun, ACM SIGGRAPH, AFIG, Eurographics
DE Visual Odometry; Tilted Camera; Trajectory Recovery
ID INTEGRATION; MOTION
AB Ego-motion estimation is an important step towards fully autonomous mobile robots. In this paper we propose the use of an initial but automatic camera tilt calibration, which transforms the subsequent motion estimation to a 2D rigid body motion problem. This transformed problem is solved '2-optimally using RANSAC and a two-point method for rigid body motion. The method is experimentally evaluated using a camera mounted onto a mobile platform. The results are compared to measurements from a highly accurate external camera positioning system which are used as gold standard. The experiments show promising results on real data.
C1 [Wadenback, Marten; Heyden, Anders] Lund Univ, Ctr Math Sci, Lund, Sweden.
   [Karlsson, Martin; Robertsson, Anders; Johansson, Rolf] Lund Univ, Dept Automat Control, Lund, Sweden.
RP Wadenback, M (corresponding author), Lund Univ, Ctr Math Sci, Lund, Sweden.
RI Johansson, Rolf/C-1845-2013
OI Johansson, Rolf/0000-0002-0786-8561; Wadenback,
   Marten/0000-0002-0675-2794
FU Swedish Foundation for Strategic Research through the SSF project
   ENGROSS
FX This work has been partly funded by the Swedish Foundation for Strategic
   Research through the SSF project ENGROSS (www.engross.lth.se). Among the
   authors are members of the LCCC Linnaeus Center and the ELLIIT
   Excellence Center at Lund University.
CR [Anonymous], 2005, P IEEE INT C ROB AUT
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Axis Communications AB, 2012, AXIS P3364 VE NETW C
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   DURRANTWHYTE HF, 1987, INT J ROBOT RES, V6, P3, DOI 10.1177/027836498700600301
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fraunhofer IPA, 2012, COMPACT DRIVE MODULE
   Gustafsson F., 2012, STAT SENSOR FUSION
   Hajjdiab H, 2004, 1ST CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P155, DOI 10.1109/CCCRV.2004.1301439
   HARRIS CG, 1988, IMAGE VISION COMPUT, V6, P87, DOI 10.1016/0262-8856(88)90003-0
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Jones ES, 2011, INT J ROBOT RES, V30, P407, DOI 10.1177/0278364910388963
   Karlsson N, 2005, IEEE INT CONF ROBOT, P24
   Liang BJ, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P205, DOI 10.1109/ROBOT.2002.1013362
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Newman P, 2005, IEEE INT CONF ROBOT, P635
   Nikon Corporation, 2011, K SER OPT CMM SOL SU
   Ortin D, 2001, ROBOTICA, V19, P331, DOI 10.1017/S0263574700003143
   Scaramuzza D, 2011, INT J COMPUT VISION, V95, P74, DOI 10.1007/s11263-011-0441-3
   Scaramuzza D, 2011, J FIELD ROBOT, V28, P792, DOI 10.1002/rob.20411
   Wadenback M, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P635
   Zienkiewicz J, 2015, J FIELD ROBOT, V32, P803, DOI 10.1002/rob.21547
NR 24
TC 4
Z9 4
U1 1
U2 1
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-227-1
PY 2017
BP 340
EP 346
DI 10.5220/0006079903400346
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BK9RU
UT WOS:000444903900035
OA hybrid, Green Submitted
DA 2022-02-10
ER

PT J
AU Stommel, C
   Hofer, H
   East, ML
AF Stommel, Claudia
   Hofer, Heribert
   East, Marion L.
TI The Effect of Reduced Water Availability in the Great Ruaha River on the
   Vulnerable Common Hippopotamus in the Ruaha National Park, Tanzania
SO PLOS ONE
LA English
DT Article
ID MAXIMUM-LIKELIHOOD ANALYSIS; GENERALIZED LINEAR-MODELS; RAINFALL;
   ECOLOGY; AFRICA
AB In semi-arid environments, 'permanent' rivers are essential sources of surface water for wildlife during 'dry' seasons when rainfall is limited or absent, particularly for species whose resilience to water scarcity is low. The hippopotamus (Hippopotamus amphibius) requires submersion in water to aid thermoregulation and prevent skin damage by solar radiation; the largest threat to its viability are human alterations of aquatic habitats. In the Ruaha National Park (NP), Tanzania, the Great Ruaha River (GRR) is the main source of surface water for wildlife during the dry season. Recent, large-scale water extraction from the GRR by people upstream of Ruaha NP is thought to be responsible for a profound decrease in dry season water-flow and the absence of surface water along large sections of the river inside the NP. We investigated the impact of decreased water flow on daytime hippo distribution using regular censuses at monitoring locations, transects and camera trap records along a 104km section of the GRR within the Ruaha NP during two dry seasons. The minimum number of hippos per monitoring location increased with the expanse of surface water as the dry seasons progressed, and was not affected by water quality. Hippo distribution significantly changed throughout the dry season, leading to the accumulation of large numbers in very few locations. If surface water loss from the GRR continues to increase in future years, this will have serious implications for the hippo population and other water dependent species in Ruaha NP.
C1 [Stommel, Claudia; Hofer, Heribert; East, Marion L.] Leibniz Inst Zoo & Wildlife Res, Alfred Kowalke Str 17, D-10315 Berlin, Germany.
RP Stommel, C (corresponding author), Leibniz Inst Zoo & Wildlife Res, Alfred Kowalke Str 17, D-10315 Berlin, Germany.
EM stommel@izw-berlin.de
RI Hofer, Heribert/AAF-7854-2021
OI Hofer, Heribert/0000-0002-2813-7442
FU Leibniz Institute for Zoo and Wildlife Research IZW institutional
   budget; German Academic Exchange Service (DAAD)Deutscher Akademischer
   Austausch Dienst (DAAD) [D/11/44168]; Leibniz Association
FX Funding for this study came from the Leibniz Institute for Zoo and
   Wildlife Research IZW institutional budget and the German Academic
   Exchange Service (DAAD), https://www.daad.de, grant number: D/11/44168,
   to CS. The publication of this article was funded by the Open Access
   Fund of the Leibniz Association. The funders had no role in study
   design, data collection and analysis, decision to publish, or
   preparation of the manuscript.
CR Agresti Alan., 2013, CATEGORICAL DATA ANA, V3rd ed
   Aitkin M, 1999, BIOMETRICS, V55, P117, DOI 10.1111/j.0006-341X.1999.00117.x
   Aitkin M, 1996, STAT COMPUT, V6, P251, DOI 10.1007/BF00140869
   Ardia DR, 2011, FUNCT ECOL, V25, P61, DOI 10.1111/j.1365-2435.2010.01759.x
   BARNES RFW, 1983, AFR J ECOL, V21, P185, DOI 10.1111/j.1365-2028.1983.tb01180.x
   Bates D., PACKAGE LME4
   Bobbink R, 2006, ECOL STUD-ANAL SYNTH, V191, P1
   Cerioli A, 1997, BIOMETRICS, V53, P619, DOI 10.2307/2533962
   Cerling TE, 2008, J ZOOL, V276, P204, DOI 10.1111/j.1469-7998.2008.00450.x
   Chen CJ, 2015, INT J CLIMATOL, V35, P2698, DOI 10.1002/joc.4165
   Dudley JP, 2016, MAMMAL REV, V46, P191, DOI 10.1111/mam.12056
   East ML, 2015, BEHAV ECOL SOCIOBIOL, V69, P805, DOI 10.1007/s00265-015-1897-x
   Eltringham, 1999, HIPPOS NATURAL HIST
   Eltringham SK, 1993, PIGS PECCARIES HIPPO, P161
   FESTABIANCHET M, 1989, J ANIM ECOL, V58, P785, DOI 10.2307/5124
   HANNAN EJ, 1979, J ROY STAT SOC B MET, V41, P190
   Harrison ME, 2008, AFR J ECOL, V46, P507, DOI 10.1111/j.1365-2028.2007.00887.x
   Hijmans R. J., DIVA GIS VERSION 7 5
   Hilbe JM, 2011, NEGATIVE BINOMIAL RE
   Hilbe JM, 2009, CH CRC TEXT STAT SCI, P1
   Hofer H., 2012, NEW DIRECTIONS CONSE, P82
   JACOBSEN NHG, 1993, WATER SA, V19, P301
   KARSTAD EL, 1986, MAMMALIA, V50, P153, DOI 10.1515/mamm.1986.50.2.153
   Klingel H., 2013, MAMMALS AFRICA HIPP, P68
   KLINGEL H, 1991, AFRICAN WILDLIFE RES, P73
   LEIVESTAD H, 1973, RESP PHYSIOL, V19, P19, DOI 10.1016/0034-5687(73)90086-8
   Lewison R, 1998, ETHOL ECOL EVOL, V10, P277, DOI 10.1080/08927014.1998.9522857
   Lewison R, 2015, IUCN RED LIST THREAT
   Lewison R, 2007, AFR J ECOL, V45, P407, DOI 10.1111/j.1365-2028.2006.00747.x
   LOCK JM, 1972, J ECOL, V60, P445, DOI 10.2307/2258356
   Lyon B, 2012, GEOPHYS RES LETT, V39, DOI 10.1029/2011GL050337
   Mtahiko M. G. G., 2006, Wetlands Ecology and Management, V14, P489, DOI 10.1007/s11273-006-9002-x
   OLIVIER R C D, 1974, East African Wildlife Journal, V12, P249
   Rigby RA, 2005, J R STAT SOC C-APPL, V54, P507, DOI 10.1111/j.1467-9876.2005.00510.x
   Ripley, 2002, MODERN APPL STAT S, DOI DOI 10.1007/978-0-387-21706-2
   Roberts L, 2011, SCIENCE, V333, P540, DOI 10.1126/science.333.6042.540
   Rowell DP, 2015, J CLIMATE, V28, P9768, DOI 10.1175/JCLI-D-15-0140.1
   Saikawa Y, 2004, NATURE, V429, P363, DOI 10.1038/429363a
   SMUTS G.L., 1981, KOEDOE, V24, P169, DOI DOI 10.4102/koedoe.v24i1.626
   Stommel C, 2016, MAMM BIOL, V81, P21, DOI 10.1016/j.mambio.2015.08.005
   Sturdivant RodneyX., 2013, APPL LOGISTIC REGRES
   Tanzania Wildlife Research Institute, 2006, TECHNICAL REPORT
   Tanzania Wildlife Research Institute, 2009, TECHNICAL REPORT
   Team R core, 2015, R LANG ENV STAT COMP
   Timbuka CD, 2012, THESIS U E ANGLIA
   Viljoen P. C., 1995, Koedoe, V38, P115
   Vorosmarty CJ, 2010, NATURE, V467, P555, DOI 10.1038/nature09440
   Wafula MM, 2008, AFR J ECOL, V46, P24, DOI 10.1111/j.1365-2028.2007.00796.x
NR 48
TC 15
Z9 15
U1 3
U2 47
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD JUN 8
PY 2016
VL 11
IS 6
AR e0157145
DI 10.1371/journal.pone.0157145
PG 18
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA DO1TJ
UT WOS:000377561700074
PM 27276362
OA Green Published, gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Mikhelson, IV
   Lee, PG
   Sahakian, AV
   Wu, Y
   Katsaggelos, AK
AF Mikhelson, Ilya V.
   Lee, Philip G.
   Sahakian, Alan V.
   Wu, Ying
   Katsaggelos, Aggelos K.
TI Automatic, fast, online calibration between depth and color cameras
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Calibration; Depth cameras; Color cameras; Automatic; Online; Fast;
   Point cloud; Point correspondence
AB Automatic camera calibration has remained a hard topic in computer vision since its inception due to its reliance on the image correspondence problem. This problem becomes even more pronounced when calibrating a depth image with a color image due to a lack of simple correspondences between the two modalities. In this work, we develop a completely automatic, very fast, online algorithm that demonstrates how a consumer-grade depth camera can be calibrated with a color camera with minimal user interaction. (C) 2013 Elsevier Inc. All rights reserved.
C1 [Mikhelson, Ilya V.; Lee, Philip G.; Sahakian, Alan V.; Wu, Ying; Katsaggelos, Aggelos K.] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
   [Sahakian, Alan V.] Northwestern Univ, Dept Biomed Engn, Evanston, IL 60208 USA.
RP Mikhelson, IV (corresponding author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
EM i-mikhelson@u.northwestern.edu
RI Katsaggelos, Aggelos K/B-7233-2009; Wu, Ying/B-7283-2009; Sahakian, Alan
   V/B-7268-2009
OI Mikhelson, Ilya/0000-0001-5111-791X; Sahakian, Alan/0000-0003-3090-0328
CR [Anonymous], 2012, MATLAB VERS 8 0 0 R2
   Bouguet J. Y., 2004, CAMERA CALIBRATION T
   Bradski G, 2000, OPENCV LIB
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fuchs S., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587828
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125
   KASSIR A., 2010, AUSTR C ROB AUT
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   Mikhelson I., 2013, CALIBRT TOOLBOX MATL
   Mikhelson IV, 2012, IEEE T INF TECHNOL B, V16, P927, DOI 10.1109/TITB.2012.2204760
   Mikhelson IV, 2011, IEEE T BIO-MED ENG, V58, P1671, DOI 10.1109/TBME.2011.2111371
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Shim H., 2011, VISUAL COMPUT, P1
   Silva M., 2012, WORKSH COL DEPTH CAM
   Smisek J., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1154, DOI 10.1109/ICCVW.2011.6130380
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stephens M, 1988, COMBINED CORNER EDGE, V15, P10, DOI DOI 10.5244/C.2.23
   Wang ZS, 2007, APPL MATH COMPUT, V185, P894, DOI 10.1016/j.amc.2006.05.210
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Zhang XG, 2011, PROCEEDINGS OF THE 2011 INTERNATIONAL CONFERENCE ON ENGINEERING AND RISK MANAGEMENT, P1
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 23
TC 14
Z9 15
U1 0
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JAN
PY 2014
VL 25
IS 1
SI SI
BP 218
EP 226
DI 10.1016/j.jvcir.2013.03.010
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 297MP
UT WOS:000330259900019
DA 2022-02-10
ER

PT C
AU Tsoy, T
   Safin, R
   Magid, E
   Saha, SK
AF Tsoy, Tatyana
   Safin, Ramil
   Magid, Evgeni
   Saha, Subir Kumar
GP IEEE
TI Estimation of 4-DoF manipulator optimal configuration for autonomous
   camera calibration of a mobile robot using on-board templates
SO INTERNATIONAL SIBERIAN CONFERENCE ON CONTROL AND COMMUNICATIONS (SIBCON
   2021 )
SE IEEE International Siberian Conference on Control and Communications
LA English
DT Proceedings Paper
CT International Siberian Conference on Control and Communications (SIBCON)
CY MAY 13-15, 2021
CL Kazan Fed Univ, Kazan, RUSSIA
SP Tomsk IEEE Chapter & Student Branch, Inst Elect & Elect Engineers, EEE Electron Devices Soc, eSystems Eng Soc, Russian Branch
HO Kazan Fed Univ
DE camera calibration; manipulator; kinematics; mobile robot
AB Camera calibration is one of the important tasks in the field of robotics and computer vision. It enables to increase the accuracy of metric measurements in photogrammetry applications and provides higher performance in computer vision algorithms such as stereo matching and motion estimation. It is known that regardless of the calibration method used variation of given estimated camera parameters their is inevitable due to multiple reasons: varying weather conditions, temperature fluctuations or severe operation mode of a robotic system. In order to alleviate the aforementioned issues recalibration is required. In this work we employed 4-DoF manipulator Servosila Engineer mobile robotic system equipped with an on-board camera. In order to automate the process of calibration an algorithm is developed which enables to estimate optimal manipulator joint configurations. Forward kinematics is solved on a discretized set of joint angles, subsequently estimating possible camera positions for its further calibration. Experiments on real robot demonstrate the possibility of usage of the developed algorithm to find optimal joint configurations for automatic camera calibration.
C1 [Tsoy, Tatyana; Safin, Ramil; Magid, Evgeni] Kazan Fed Univ, Inst Informat Technol & Intelligent Syst, Intelligent Robot Dept, Lab Intelligent Robot Syst LIRS, Kazan, Russia.
   [Saha, Subir Kumar] Indian Inst Technol Delhi, Mech Engn Dept, New Delhi, India.
RP Tsoy, T (corresponding author), Kazan Fed Univ, Inst Informat Technol & Intelligent Syst, Intelligent Robot Dept, Lab Intelligent Robot Syst LIRS, Kazan, Russia.
RI Magid, Evgeni/B-9697-2014
OI Magid, Evgeni/0000-0001-7316-5664
FU Russian Foundation for Basic Research (RFBR)Russian Foundation for Basic
   Research (RFBR) [20-38-90257]
FX The reported study was funded by the Russian Foundation for Basic
   Research (RFBR), project number 20-38-90257.
CR Atcheson B., 2010, P VIS MOD VIS WORKSH, V10, P41
   Brand C, 2014, IEEE INT C INT ROBOT, P1846, DOI 10.1109/IROS.2014.6942805
   Brandt, 2007, WILEY ENCY COMPUTER, P1
   Brandt S. S., 2008, WILEY ENCY COMPUTER, V13, P1, DOI DOI 10.1002/9780470050118.ECSE589
   Carrera G, 2011, IEEE INT CONF ROBOT, P2652, DOI 10.1109/ICRA.2011.5980294
   Cmarada M, 2012, ANN P DAAAM INT 2012
   Corke P., 2017, ROBOTICS VISION CONT, V118
   Fang T, 2018, J ELECTRON IMAGING, V27
   FAUGERAS OD, 1992, LECT NOTES COMPUT SC, V588, P321
   Fraser, 2006, INT ARCH PHOTOGRAMM, V36, P266, DOI DOI 10.3929/ETHZ-B-000158067
   Magid E, 2010, LECT NOTES ARTIF INT, V6472, P423, DOI 10.1007/978-3-642-17319-6_39
   Martinez-Garcia EA, 2019, I C DEV ESYST ENG, P64, DOI 10.1109/DeSE.2019.00022
   Mingachev E, 2020, LECT NOTES ARTIF INT, V12336, P222, DOI 10.1007/978-3-030-60337-3_22
   Moskvin Ilya, 2020, Proceedings of 14th International Conference on Electromechanics and Robotics ldquoZavalishin's Readingsrdquo. ER(ZR) 2019. Smart Innovation, Systems and Technologies (SIST 154), P411, DOI 10.1007/978-981-13-9267-2_33
   Sablatnig R., 2004, COMP METHODS GEOMETR
   Sagitov A, 2017, ICINCO: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS - VOL 2, P182, DOI 10.5220/0006478901820191
   Salvi J, 2002, PATTERN RECOGN, V35, P1617, DOI 10.1016/S0031-3203(01)00126-1
   Shu C., 2005, 48306ERB1130 NRC CAN, V48306, P26
   Sturm P.F., 1999, P 1999 IEEE COMP SOC, V1, P432, DOI DOI 10.1109/CVPR.1999.786974
   Tsoy T, 2019, I C DEV ESYST ENG, P495, DOI 10.1109/DeSE.2019.00096
   Zhang H, 2007, IEEE T PATTERN ANAL, V29, P499, DOI 10.1109/TPAMI.2007.45
   Zhang Z., 2014, COMPUTER VISION, P76
NR 22
TC 0
Z9 0
U1 3
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2380-6508
BN 978-1-7281-8504-0
J9 IEEE INT SIBER CONF
PY 2021
DI 10.1109/SIBCON50419.2021.9438925
PG 6
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BS0BS
UT WOS:000680842100073
DA 2022-02-10
ER

PT J
AU Shirazi, MS
   Morris, BT
AF Shirazi, Mohammad Shokrolah
   Morris, Brendan Tran
TI Investigation of safety analysis methods using computer vision
   techniques
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE safety analysis methods; vision-based tracking system; conflict events;
   real time
ID PEDESTRIAN SAFETY; INTERSECTIONS; BEHAVIOR; FRAMEWORK; TRACKING
AB This work investigates safety analysis methods using computer vision techniques. The vision-based tracking system is developed to provide the trajectory of road users including vehicles and pedestrians. Safety analysis methods are developed to estimate time to collision (TTC) and postencroachment time (PET) that are two important safety measurements. Corresponding algorithms are presented and their advantages and drawbacks are shown through their success in capturing the conflict events in real time. The performance of the tracking system is evaluated first, and probability density estimation of TTC and PET are shown for 1-h monitoring of a Las Vegas intersection. Finally, an idea of an intersection safety map is introduced, and TTC values of two different intersections are estimated for 1 day from 8:00 a.m. to 6:00 p.m. (C) 2017 SPIE and IS&T
C1 [Shirazi, Mohammad Shokrolah] Cleveland State Univ, Elect Engn & Comp Sci Dept, Cleveland, OH 44115 USA.
   [Morris, Brendan Tran] Univ Nevada, Dept Elect & Comp Engn, Las Vegas, NV 89154 USA.
RP Shirazi, MS (corresponding author), Cleveland State Univ, Elect Engn & Comp Sci Dept, Cleveland, OH 44115 USA.
EM m.shokrolahshirazi@csuohio.edu
OI Morris, Brendan/0000-0002-8592-8806
FU Nevada Department of Transportation
FX The authors thank the Nevada Department of Transportation for supporting
   the research project: "An Automated Camera-Based Pedestrian-Vehicle
   Conflict Evaluation System."
CR Alhajyaseen WKM, 2012, IATSS RES, V36, P66, DOI 10.1016/j.iatssr.2012.03.002
   Amundsen F, 1977, P 1 WORKSH TRAFF CON
   Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Chin HC, 1997, SAFETY SCI, V26, P169, DOI 10.1016/S0925-7535(97)00041-6
   de Leur P, 2003, CAN J CIVIL ENG, V30, P711, DOI 10.1139/L03-034
   Hayward J.C., 1972, NEAR MISS DETERMINAT, P24
   Hu WM, 2004, IEEE T VEH TECHNOL, V53, P677, DOI 10.1109/TVT.2004.825772
   Hussein M, 2015, TRANSPORT RES REC, P17, DOI 10.3141/2519-03
   Ismail K, 2010, TRANSPORT RES REC, P52, DOI 10.3141/2198-07
   Ismail K, 2009, TRANSPORT RES REC, P44, DOI 10.3141/2140-05
   Li S, 2012, TRANSPORT RES REC, P121, DOI 10.3141/2299-13
   Morris B, 2009, PROC CVPR IEEE, P312, DOI 10.1109/CVPRW.2009.5206559
   Perkins S., 1966, CRITERIA TRAFFIC CON
   Pin C, 2015, TRANSPORT RES REC, P58, DOI 10.3141/2514-07
   Sacchi E, 2016, J TRANSP SAF SECUR, V8, P266, DOI 10.1080/19439962.2015.1030807
   Sayed T., 2013, TRANSPORTATION RES B
   Sayed T, 2012, TRANSPORT RES REC, P18, DOI 10.3141/2280-03
   Shen XH, 2013, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2013.444
   Shirazi MS, 2017, IEEE T INTELL TRANSP, V18, P4, DOI 10.1109/TITS.2016.2568920
   Shirazi MS, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.5.051203
   Shirazi MS, 2016, INT J ARTIF INTELL T, V25, DOI 10.1142/S0218213016400042
   Shirazi MS, 2015, LECT NOTES COMPUT SC, V9474, P752, DOI 10.1007/978-3-319-27857-5_67
   Shirazi MS, 2016, IEEE INTEL TRANSP SY, V8, P23, DOI 10.1109/MITS.2015.2477474
   Shirazi MS, 2015, IEEE INT VEH SYM, P1264, DOI 10.1109/IVS.2015.7225856
   Shirazi MS, 2015, IEEE INT VEH SYM, P1258, DOI 10.1109/IVS.2015.7225855
   Shirazi MS, 2014, LECT NOTES COMPUT SC, V8887, P708, DOI 10.1007/978-3-319-14249-4_68
   Shirazi MS, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P3100, DOI 10.1109/ITSC.2014.6958188
   Svensson A, 2006, ACCIDENT ANAL PREV, V38, P379, DOI 10.1016/j.aap.2005.10.009
   Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7
   Zaki MH, 2013, TRANSPORT RES REC, P75, DOI 10.3141/2393-09
   Zangenehpour S, 2016, ACCIDENT ANAL PREV, V86, P161, DOI 10.1016/j.aap.2015.10.025
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 32
TC 2
Z9 2
U1 0
U2 3
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD SEP
PY 2017
VL 26
IS 5
AR 051404
DI 10.1117/1.JEI.26.5.051404
PG 11
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Optics; Imaging Science & Photographic Technology
GA FL5DJ
UT WOS:000414251400005
DA 2022-02-10
ER

PT J
AU Andersen, ML
   Bennett, DE
   Holbrook, JD
AF Andersen, Megan L.
   Bennett, Drew E.
   Holbrook, Joseph D.
TI Burrow webs: Clawing the surface of interactions with burrows excavated
   by American badgers
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE American badgers; burrow web; ecological network; ecosystem engineers;
   species interactions; subterranean habitat; Taxidea taxus
ID CAVITY-NESTING COMMUNITIES; ECOLOGICAL ROLES; TAXIDEA-TAXUS;
   CONSERVATION; ORGANISMS; LONGEVITY; DENSITIES; NETWORKS; RESOURCE;
   SUCCESS
AB Ecosystem engineers are organisms that influence their environment, which includes alterations leading to habitat provisioning for other species. Perhaps the most well-examined guild of species provisioning habitat for other species is tree cavity excavators or woodpeckers (Picidae). Many studies have examined the suite of secondary cavity users that rely on woodpeckers, and how the ecological network of secondary users, collectively referred to as the nest web, changes across communities. Despite similar habitat provisioning processes, fewer studies have assessed the suite of species associated with burrowers providing access to subterranean habitat. Here, we begin to characterize the burrow web provisioned by American badgers (Taxidea taxus) and evaluate the diversity and frequency of species interactions we detected at abandoned badger burrows in Wyoming, USA. We deployed camera traps at 23 badger burrows and identified interactions with the burrow by birds, mammals, and reptiles. Overall, we discovered 31 other species utilizing badger burrows, consisting of 12 mammals, 18 birds, and 1 reptile. Mammals, other than American badgers themselves and other fossorial species such as ground squirrels (Urocitellus sp.), frequently using burrows included mice (Peromyscus sp.), long-tailed weasel (Mustela frenata), pygmy rabbit (Brachylagus idahoensis), and desert cottontail (Sylvilagus audubonii). Of the 18 bird species detected, most accounted for <5% of overall detections, besides chipping sparrows (Spizella passerina) at 7.2%-11.5% of detections. The most common category of detection by bird species was foraging, contrary to mammals, which used the burrow frequently and were commonly observed entering and exiting the burrow. This work provides additional context on the ecological role of American badgers within their environment. More broadly, this work scratches the surface of many remaining questions to explore with the aim of advancing our understandings about burrow webs across the diversity of burrowing species and the communities in which they occur.
C1 [Andersen, Megan L.; Bennett, Drew E.; Holbrook, Joseph D.] Univ Wyoming, Haub Sch Environm & Nat Resources, Laramie, WY 82071 USA.
   [Holbrook, Joseph D.] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
RP Holbrook, JD (corresponding author), Univ Wyoming, Haub Sch Environm & Nat Resources, Laramie, WY 82071 USA.
EM Joe.Holbrook@uwyo.edu
FU Whitney MacMillan program; Haub School of Environment and Natural
   Resources; University of Wyoming
FX We sincerely thank the landowners and ranch managers for allowing access
   and aiding our research efforts. Support for this work was provided by
   the Whitney MacMillan program for Private Lands Stewardship, the Haub
   School of Environment and Natural Resources, and the University of
   Wyoming.
CR Bylo LN, 2014, RANGELAND ECOL MANAG, V67, P247, DOI 10.2111/REM-D-13-00152.1
   Cockle KL, 2019, BIODIVERS CONSERV, V28, P3371, DOI 10.1007/s10531-019-01826-4
   Cockle KL, 2019, ECOL APPL, V29, DOI 10.1002/eap.1916
   Cockle KL, 2011, FRONT ECOL ENVIRON, V9, P377, DOI 10.1890/110013
   Davidson AD, 2008, J ARID ENVIRON, V72, P2142, DOI 10.1016/j.jaridenv.2008.07.006
   Davidson AD, 2012, FRONT ECOL ENVIRON, V10, P477, DOI 10.1890/110054
   Dawson SJ, 2019, J ZOOL, V308, P149, DOI 10.1111/jzo.12663
   Desmond MJ, 1996, AM MIDL NAT, V136, P143, DOI 10.2307/2426639
   Di Blanco YE, 2020, J ZOOL, V311, P227, DOI 10.1111/jzo.12782
   Edworthy AB, 2012, ECOL APPL, V22, P1733, DOI 10.1890/11-1594.1
   Eldridge DJ, 2009, J ARID ENVIRON, V73, P66, DOI 10.1016/j.jaridenv.2008.09.004
   Eldridge DJ, 2004, J MAMMAL, V85, P1060, DOI 10.1644/BEH-105.1
   GLEASON RS, 1985, GREAT BASIN NAT, V45, P81
   Goodman SJ, 2018, SOUTHEAST NAT, V17, P531, DOI 10.1656/058.017.0310
   Grassel SM, 2015, ECOL EVOL, V5, P2762, DOI 10.1002/ece3.1561
   Haynes G, 2012, GEOMORPHOLOGY, V157, P99, DOI 10.1016/j.geomorph.2011.04.045
   Holbrook JD, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1307
   Holmes AL, 2003, WEST N AM NATURALIST, V63, P244
   Ings TC, 2009, J ANIM ECOL, V78, P253, DOI 10.1111/j.1365-2656.2008.01460.x
   Desbiez ALJ, 2013, BIOTROPICA, V45, P537, DOI 10.1111/btp.12052
   Jones CG, 1997, ECOLOGY, V78, P1946, DOI 10.1890/0012-9658(1997)078[1946:PANEOO]2.0.CO;2
   JONES CG, 1994, OIKOS, V69, P373, DOI 10.2307/3545850
   Kefi S, 2012, ECOL LETT, V15, P291, DOI 10.1111/j.1461-0248.2011.01732.x
   Kinlaw A, 2012, GEOMORPHOLOGY, V157, P108, DOI 10.1016/j.geomorph.2011.06.030
   Kucheravy CE, 2021, BASIC APPL ECOL, V51, P11, DOI 10.1016/j.baae.2021.01.012
   Kurek P, 2014, ECOL RES, V29, P1, DOI 10.1007/s11284-013-1094-1
   Lohr K, 2013, J WILDLIFE MANAGE, V77, P983, DOI 10.1002/jwmg.541
   Lorenz TJ, 2015, ECOL APPL, V25, P1016, DOI 10.1890/14-1042.1
   Martin K, 2004, CONDOR, V106, P5, DOI 10.1650/7482
   Martin K, 1999, FOREST ECOL MANAG, V115, P243, DOI 10.1016/S0378-1127(98)00403-4
   MESSICK JP, 1981, WILDLIFE MONOGR, P1
   Milling CR, 2018, PEERJ, V6, DOI 10.7717/peerj.4511
   Murphy CM, 2021, FOREST ECOL MANAG, V482, DOI 10.1016/j.foreco.2020.118809
   Natural Resource Conservation Service, 2019, ECOLOGICAL SITE R034, P30
   Natural Resource Conservation Service, 2020, EC SIT RO32XY162WY S, P5
   Olsson IAS, 2005, APPL ANIM BEHAV SCI, V93, P259, DOI 10.1016/j.applanim.2004.11.018
   Pagliai M., 2002, ADV GEOECOLOGY, V35, P69
   R Core Team, 2020, R FDN STAT COMP
   Read JL, 2008, J ARID ENVIRON, V72, P2124, DOI 10.1016/j.jaridenv.2008.06.018
   Symes SA, 2019, WILDLIFE BIOL, DOI 10.2981/wlb.00528
   United States Fish and Wildlife Service, 2008, FED REGISTER, V73, P1312
   Vierling KT, 2018, INT J BIOMETEOROL, V62, P553, DOI 10.1007/s00484-017-1464-4
   Whittington-Jones GM, 2011, AFR ZOOL, V46, P362, DOI 10.3377/004.046.0215
NR 43
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD SEP
PY 2021
VL 11
IS 17
BP 11559
EP 11568
DI 10.1002/ece3.7962
EA JUL 2021
PG 10
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA UN4RM
UT WOS:000679457800001
PM 34522324
OA Green Published, gold
DA 2022-02-10
ER

PT C
AU Teutsch, C
   Berndt, D
   Trostmann, E
   Weber, M
AF Teutsch, C
   Berndt, D
   Trostmann, E
   Weber, M
BE Meriaudeau, F
   Niel, KS
TI Real-time detection of elliptic shapes for automated object recognition
   and object tracking
SO MACHINE VISION APPLICATIONS IN INDUSTRIAL INSPECTION XIV
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Machine Vision Applications in Industrial Inspection XIV
CY JAN 16-17, 2006
CL San Jose, CA
SP Soc Imaging Sci & Technol, SPIE
DE real-time ellipse detection; shape recognition; shape classification
ID ROBUST
AB The detection of varying 2D shapes is a recurrent task for Computer Vision applications, and camera based object recognition has become a standard procedure. Due to the discrete nature of digital images and aliasing effects, shape recognition can be complicated. There are many existing algorithms that discuss the identification of circles and ellipses, but they are very often limited in flexibility or speed or require high quality input data. Our work considers the application of shape recognition for processes in industrial environments and, especially the automatization requires reliable and fast algorithms at the same time. We take a very practical look at the automated shape recognition for common industrial tasks and present a very fast novel approach for the detection of deformed shapes which are in the broadest sense elliptic. Furthermore, we consider the automated recognition of bacteria colonies and coded markers for both 3D object tracking and an automated camera calibration procedure.
C1 Fraunhofer Inst Factory Operat & Automat, Sandtorstr 22, D-39106 Magdeburg, Germany.
RP Teutsch, C (corresponding author), Fraunhofer Inst Factory Operat & Automat, Sandtorstr 22, D-39106 Magdeburg, Germany.
EM christian.teutsch@iff.fraunhofer.de
CR AHN SJ, 1997, P 4 ABW WORKSH OPT 3
   AHN SJ, 1999, P 6 ABW WORKSH OPT 3
   Bachelder I. A., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P798, DOI 10.1109/CVPR.1992.223169
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   BERNDT D, 2005, P OPT 3 D MEAS TEC 7, V1, P317
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   COHEN I, 1992, LECT NOTES COMPUT SC, V588, P458
   DAVIS ER, 1997, MACHINE VISION THEOR, P245
   JACOBS CE, 1995, P ACM SIGGRAPH AUG, P00277
   Lei YW, 1999, PATTERN RECOGN LETT, V20, P41, DOI 10.1016/S0167-8655(98)00127-5
   LEUNG T, 1998, LECT NOTES COMPUTER, V1406, P544
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   SCLAROFF S, 1995, IEEE T PATTERN ANAL, V17, P545, DOI 10.1109/34.387502
   SERRA B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P402, DOI 10.1109/ICCV.1995.466911
   SERRA B, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P202, DOI 10.1109/CVPR.1994.323830
   SHEN J, 1992, CVGIP-GRAPH MODEL IM, V54, P112, DOI 10.1016/1049-9652(92)90060-B
   Xie YH, 2002, INT C PATT RECOG, P957, DOI 10.1109/ICPR.2002.1048464
   XU J, 2003, P INT C IM PROC ICIP, V1, P973
   Yao J, 2004, INT C PATT RECOG, P859, DOI 10.1109/ICPR.2004.1334394
   Zhang SC, 2005, PATTERN RECOGN, V38, P273, DOI 10.1016/j.patcog.2004.03.014
   Zhu CR, 2004, PATTERN RECOGN LETT, V25, P1471, DOI 10.1016/j.patrec.2004.05.023
NR 22
TC 5
Z9 5
U1 0
U2 2
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 0-8194-6110-5
J9 PROC SPIE
PY 2006
VL 6070
AR 60700J
DI 10.1117/12.642167
PG 9
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BEE27
UT WOS:000236912300019
DA 2022-02-10
ER

PT J
AU Stommel, C
   Hofer, H
   Grobbel, M
   East, ML
AF Stommel, Claudia
   Hofer, Heribert
   Grobbel, Mirjam
   East, Marion L.
TI Large mammals in Ruaha National Park, Tanzania, dig for water when water
   stops flowing and water bacterial load increases
SO MAMMALIAN BIOLOGY
LA English
DT Article
DE Great Ruaha River; Wildlife; Water holes; Salinity; Bacterial
   contamination
ID NAMIBIA; ELEPHANTS
AB As water is essential for life, animals have adaptations that increase their ability to survive during periods of water shortage. Accessing water by digging is one behavioural adaptation to water shortage used by some African mammals. Digging might also provide access to higher quality water below ground when surface water quality is poor. We investigated the digging of waterholes by wildlife in the Ruaha National Park (NP), in central Tanzania, during three dry seasons (June to November from 2011 to 2013). We monitored surface water availability and water quality at 10 sites along the Great Ruaha River (GRR) and eight non-GRR sites. We used camera-traps and direct observations to determine when and where digging to access water occurred. Elephant (Loxodonta africana), plains zebra (Equus quagga), warthog (Phacochoerus africanus) and yellow baboon (Papio cynocephalus) dug waterholes and a further four species drunk from these holes. Waterholes were dug later in the dry season along the GRR (October) than at other sites (July). The likelihood of digging and drinking from waterholes was lower along the GRR than at non-GRR sites and did not depend on the absence of surface water but increased when surface water stopped flowing. Digging of waterholes was also significantly more likely when the bacterial load in available surface water increased but was independent of salinity levels. Escherichia coli load, indicative of faecal contamination, significantly increased with total aerobic bacterial load. Our results suggest that digging is an adaptation to avoid the ingestion of poor quality surface water highly contaminated with faeces, and thereby possibly also potentially pathogenic microbes, in addition to providing access to water when surface water is absent. Our findings also highlight (1) the essential role of the GRR as the key water source for wildlife in the Ruaha NP during the dry season, and (2) that maintenance of water flow throughout the dry season is essential to prevent deterioration of water quality in the GRR. (C) 2015 Deutsche Gesellschaft fur Saugetierkunde. Published by Elsevier GmbH. All rights reserved.
C1 [Stommel, Claudia; Hofer, Heribert; Grobbel, Mirjam; East, Marion L.] Leibniz Inst Zoo & Wildlife Res, Alfred Kowalke Str 17, D-10315 Berlin, Germany.
   [Grobbel, Mirjam] Bundesinst Risikobewertung, Abt Biol Sicherheit, Diedersdorfer Weg 1, D-12277 Berlin, Germany.
RP Stommel, C (corresponding author), Leibniz Inst Zoo & Wildlife Res, Alfred Kowalke Str 17, D-10315 Berlin, Germany.
EM stommel@izw-berlin.de
RI Grobbel, Mirjam/AAP-2541-2021; Hofer, Heribert/AAF-7854-2021
OI Grobbel, Mirjam/0000-0002-8619-1498; Hofer, Heribert/0000-0002-2813-7442
FU Leibniz Institute for Zoo and Wildlife Research; German Academic
   Exchange Service (DAAD)Deutscher Akademischer Austausch Dienst (DAAD)
   [D/11/44168]
FX We are grateful to the Tanzanian Commission of Science and Technology,
   the Tanzania Wildlife Research Institute and Tanzania National Parks for
   permission to conduct this study and the Ruaha National Park ecologists
   Paul Banga and Godwell Elias Ole Meing'ataki for their support. We thank
   Luisa Ilse, Dagmar Thierer and Kerstin Wilhelm for their assistance.
   This work was financed by the Leibniz Institute for Zoo and Wildlife
   Research and a grant from the German Academic Exchange Service (DAAD)
   D/11/44168. Additionally we thank two anonymous reviewers for their
   useful comments.
CR BARNES RFW, 1983, AFR J ECOL, V21, P185, DOI 10.1111/j.1365-2028.1983.tb01180.x
   Bengis RG, 2002, REV SCI TECH OIE, V21, P53
   Bjornstad A., 1976, VEGETATION RUAHA NAT
   Dudley JP, 2001, AFR J ECOL, V39, P187, DOI 10.1046/j.0141-6707.2000.00297.x
   East M. L., 2010, Ungulate management in Europe: problems and practices, P319
   Epaphras A. M., 2008, Wetlands Ecology and Management, V16, P183, DOI 10.1007/s11273-007-9065-3
   Galat-Luong A, 2009, GEOGR TECH, V4, P199
   Gersberg RM, 2006, APPL ENVIRON MICROB, V72, P7438, DOI 10.1128/AEM.01024-06
   HAMILTON WJ, 1985, INT J PRIMATOL, V6, P451, DOI 10.1007/BF02735570
   Hellberg RS, 2015, CRIT REV MICROBIOL, P1
   Hilbe JM, 2011, NEGATIVE BINOMIAL RE
   Hilbe JM, 2009, CH CRC TEXT STAT SCI, P1
   Johnson LK, 2004, APPL ENVIRON MICROB, V70, P4478, DOI 10.1128/AEM.70.8.4478-4485.2004
   Keet DF, 1996, ONDERSTEPOORT J VET, V63, P239
   LINDEQUE PM, 1994, ONDERSTEPOORT J VET, V61, P71
   McGrew WC, 2007, FOLIA PRIMATOL, V78, P240, DOI 10.1159/000102319
   Mtahiko M. G. G., 2006, Wetlands Ecology and Management, V14, P489, DOI 10.1007/s11273-006-9002-x
   Poche R.M., 1974, Mammalia, V38, P567, DOI 10.1515/mamm.1974.38.4.567
   Ramey EM, 2013, PACHYDERM, P66
   Redfern JV, 2003, ECOLOGY, V84, P2092, DOI 10.1890/01-0625
   SAFFERMAN ROBERT S., 1967, ENVIRON SCI TECHNOL, V1, P429, DOI 10.1021/es60005a009
   Tieleman BI, 2003, P ROY SOC B-BIOL SCI, V270, P207, DOI 10.1098/rspb.2002.2205
   Wanke H, 2007, J ARID ENVIRON, V70, P553, DOI 10.1016/j.jaridenv.2007.01.011
   Western D, 1975, AFR J ECOL, V13, P265, DOI DOI 10.1111/J.1365-2028.1975.TB00139.X
   Willmer P, 2005, ENV PHYSL ANIMALS, V2nd
   Withers PC, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2014.0149
   Wolanski E, 1999, AFR J ECOL, V37, P419, DOI 10.1046/j.1365-2028.1999.00198.x
   Young WF, 1996, WATER RES, V30, P331, DOI 10.1016/0043-1354(95)00173-5
NR 28
TC 9
Z9 10
U1 1
U2 43
PU ELSEVIER GMBH, URBAN & FISCHER VERLAG
PI JENA
PA OFFICE JENA, P O BOX 100537, 07705 JENA, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
PD JAN
PY 2016
VL 81
IS 1
BP 21
EP 30
DI 10.1016/j.mambio.2015.08.005
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA DC8IH
UT WOS:000369462600003
DA 2022-02-10
ER

PT C
AU Chittaro, L
   Ieronutti, L
   Ranon, R
AF Chittaro, Luca
   Ieronutti, Lucio
   Ranon, Roberto
BE Taylor, R
   Boulanger, P
   Kruger, A
   Olivier, P
TI VEX-CMS: A Tool to Design Virtual Exhibitions and Walkthroughs That
   Integrates Automatic Camera Control Capabilities
SO SMART GRAPHICS, PROCEEDINGS
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 10th International Symposium on Smart Graphics
CY JUN 24-26, 2010
CL Banff, CANADA
SP Univ Alberta, Adv Man Machine Interface Lab, Banff New Media Inst, Eurograph Assoc, Amer Assoc Artificial Intelligence, ACM SIGGRAPH, ACM SIGCHI, ACM SIGART
DE 3D virtual museums; automatic camera control; 3D authoring tools
AB This paper presents VEX-CMS, a tool to build 3D virtual museums and exhibitions, architectural walkthroughs and, more generally, applications where 3D and 2D content is presented to the user inside a 3D Virtual Environment. In this paper, we concentrate on the task of designing tours of the virtual environment to be followed by visitors, either through manual or automatic navigation, and show how a recent automatic camera control approach [3], coupled with path planning, can be exploited to accomplish the task with minimal knowledge and manual effort on the curator's side.
C1 [Chittaro, Luca; Ieronutti, Lucio; Ranon, Roberto] Univ Udine, HCI Lab, I-33100 Udine, Italy.
RP Chittaro, L (corresponding author), Univ Udine, HCI Lab, Via Sci 206, I-33100 Udine, Italy.
RI Ranon, Roberto/D-6145-2018
OI Ranon, Roberto/0000-0002-9744-0197; CHITTARO, Luca/0000-0001-5975-4294
CR Arnold D., 2009, P EVA 2009 FLOR, V21, P94
   Bares W., 2000, Proceedings ACM Multimedia 2000, P177, DOI 10.1145/354384.354463
   Burelli P, 2008, LECT NOTES COMPUT SC, V5166, P130, DOI 10.1007/978-3-540-85412-8_12
   Chittaro L., 2009, P 16 ACM S VIRT REAL, P171
   CONWAY M, 2000, P SIGCHI C HUM FACT, P486
   DRUCKER SM, 1994, GRAPH INTER, P190
   Elmqvist N, 2007, VRST 2007: ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY, PROCEEDINGS, P207
   IERONUTTI L, 2004, P 9 INT C 3D WEB TEC, P61
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   KUFFNER JJ, 1999, THESIS STANFORD U
   Lepouras George, 2004, VIRTUAL REALITY, V8, P96, DOI DOI 10.1007/S10055-004-0141-1
   MOURKOUSSIS N, 2003, P 2003 INT C DUBL CO, P1
   Patel M, 2003, P INT C VIS VID GRAP, P189
   Veron Eliseo., 1983, ETHNOGRAPHIE EXPOSIT
   Wang XM, 2007, STRUCT INFRASTRUCT E, V3, P3, DOI 10.1080/15732470500103682
NR 15
TC 2
Z9 2
U1 0
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-642-13543-9
J9 LECT NOTES COMPUT SC
PY 2010
VL 6133
BP 103
EP 114
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BPP93
UT WOS:000279615200010
DA 2022-02-10
ER

PT C
AU Nakayama, Y
   Inoue, Y
   Katsurai, M
AF Nakayama, Yu
   Inoue, Yoshiaki
   Katsurai, Marie
GP IEEE
TI RAMNe: Realtime Animal Monitoring over Network with Age of Information
SO 2020 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS WORKSHOPS (ICC
   WORKSHOPS)
SE IEEE International Conference on Communications Workshops
LA English
DT Proceedings Paper
CT IEEE International Conference on Communications (IEEE ICC) / Workshop on
   NOMA for 5G and Beyond
CY JUN 07-11, 2020
CL ELECTR NETWORK
SP IEEE, Huawei, ZTE, Qualcomm
DE IoT; Age of Information; scheduling; computer vision; realtime system
ID CAMERA-TRAPS
AB The ever-improving Internet of things (IoT) and computer vision technologies have enabled automated monitoring of animals, which is essential for understanding animal behavior and conservation of ecosystem. The tradeoff between survey cost and sampling variability is a significant issue in designing a camera survey considering the risk of losing informative images; the monitoring accuracy tends to decrease in accordance with data reduction. However, there has been no designing method for time-lapse realtime monitoring over networks to guarantee monitoring accuracy. To address this problem, this paper proposes a Realtime Animal Monitoring over Network (RAMNe). The goal of RAMNe is to efficiently detect target animals in realtime using network cameras. We propose a determination method for the monitoring interval to guarantee the target value of monitoring accuracy based on a formal theoretical analysis using the Age of Information (AoI). The proposed scheme can minimize the amount of transferred data to enable efficient and stable monitoring even in resource-limited environments. The performance of RAMNe was evaluated with ns-3 simulations to confirm the relationship between monitoring accuracy and interval.
C1 [Nakayama, Yu] Tokyo Univ Agr & Technol, Inst Engn, Tokyo, Japan.
   [Inoue, Yoshiaki] Osaka Univ, Dept Informat & Commun Technol, Osaka, Japan.
   [Katsurai, Marie] Doshisha Univ, Dept Intelligent Informat Engn & Sci, Kyoto, Japan.
RP Nakayama, Y (corresponding author), Tokyo Univ Agr & Technol, Inst Engn, Tokyo, Japan.
RI Nakayama, Yu/J-4484-2019; Katsurai, Marie/ABF-1378-2021
OI Nakayama, Yu/0000-0002-6945-7055; 
CR Bengsen A, 2011, WILDLIFE RES, V38, P732, DOI 10.1071/WR11134
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Costa M, 2016, IEEE T INFORM THEORY, V62, P1897, DOI 10.1109/TIT.2016.2533395
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hamel S, 2013, METHODS ECOL EVOL, V4, P105, DOI 10.1111/j.2041-210x.2012.00262.x
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Inoue Y, 2019, IEEE T INFORM THEORY, V65, P8305, DOI 10.1109/TIT.2019.2938171
   Inoue Y, 2019, IEEE CONF COMPUT, P183, DOI 10.1109/INFCOMW.2019.8845262
   Kaul S, 2012, IEEE INFOCOM SER, P2731, DOI 10.1109/INFCOM.2012.6195689
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   McIntosh DMD, 2019, ROUT RES COMMUN STUD, P1
   Nakayama Y, 2018, IEEE ACCESS, V6, P78829, DOI 10.1109/ACCESS.2018.2885346
   Nakayama Y, 2017, IEEE GLOB COMM CONF
   Nakayama Y, 2018, J OPT COMMUN NETW, V10, P14, DOI 10.1364/JOCN.10.000014
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Sun Y, 2017, IEEE T INFORM THEORY, V63, P7492, DOI 10.1109/TIT.2017.2735804
   Swann DE, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P27, DOI 10.1007/978-4-431-99495-4_3
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Yokoi S, 2012, HYDROL PROCESS, V26, P834, DOI 10.1002/hyp.8297
NR 21
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2164-7038
BN 978-1-7281-7440-2
J9 IEEE INT CONF COMM
PY 2020
PG 6
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BQ5SJ
UT WOS:000607199300139
DA 2022-02-10
ER

PT J
AU Clare, JDJ
   Townsend, PA
   Anhalt-Depies, C
   Locke, C
   Stenglein, JL
   Frett, S
   Martin, KJ
   Singh, A
   Van Deelen, TR
   Zuckerberg, B
AF Clare, John D. J.
   Townsend, Philip A.
   Anhalt-Depies, Christine
   Locke, Christina
   Stenglein, Jennifer L.
   Frett, Susan
   Martin, Karl J.
   Singh, Aditya
   Van Deelen, Timothy R.
   Zuckerberg, Benjamin
TI Making inference with messy (citizen science) data: when are data
   accurate enough and how can they be improved?
SO ECOLOGICAL APPLICATIONS
LA English
DT Article
DE automated classification; citizen science; crowdsourcing; false-positive
   error; misclassification; remote camera; species distribution model
ID DATA QUALITY; CAMERA TRAPS; ERROR; BIODIVERSITY; CHALLENGES; VOLUNTEER;
   MODELS; STATE; TOOL
AB Measurement or observation error is common in ecological data: as citizen scientists and automated algorithms play larger roles processing growing volumes of data to address problems at large scales, concerns about data quality and strategies for improving it have received greater focus. However, practical guidance pertaining to fundamental data quality questions for data users or managers-how accurate do data need to be and what is the best or most efficient way to improve it?-remains limited. We present a generalizable framework for evaluating data quality and identifying remediation practices, and demonstrate the framework using trail camera images classified using crowdsourcing to determine acceptable rates of misclassification and identify optimal remediation strategies for analysis using occupancy models. We used expert validation to estimate baseline classification accuracy and simulation to determine the sensitivity of two occupancy estimators (standard and false-positive extensions) to different empirical misclassification rates. We used regression techniques to identify important predictors of misclassification and prioritize remediation strategies. More than 93% of images were accurately classified, but simulation results suggested that most species were not identified accurately enough to permit distribution estimation at our predefined threshold for accuracy (<5% absolute bias). A model developed to screen incorrect classifications predicted misclassified images with >97% accuracy: enough to meet our accuracy threshold. Occupancy models that accounted for false-positive error provided even more accurate inference even at high rates of misclassification (30%). As simulation suggested occupancy models were less sensitive to additional false-negative error, screening models or fitting occupancy models accounting for false-positive error emerged as efficient data remediation solutions. Combining simulation-based sensitivity analysis with empirical estimation of baseline error and its variability allows users and managers of potentially error-prone data to identify and fix problematic data more efficiently. It may be particularly helpful for "big data" efforts dependent upon citizen scientists or automated classification algorithms with many downstream users, but given the ubiquity of observation or measurement error, even conventional studies may benefit from focusing more attention upon data quality.
C1 [Clare, John D. J.; Townsend, Philip A.; Anhalt-Depies, Christine; Singh, Aditya; Van Deelen, Timothy R.; Zuckerberg, Benjamin] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Locke, Christina; Stenglein, Jennifer L.; Frett, Susan] Wisconsin Dept Nat Resources, Off Appl Sci, Madison, WI 53716 USA.
   [Martin, Karl J.] Univ Wisconsin, Div Cooperat Extens, Madison, WI 53706 USA.
   [Singh, Aditya] Univ Florida, Dept Agr & Biol Engn, Gainesville, FL 32611 USA.
RP Clare, JDJ (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM jclare2@wisc.edu
RI Singh, Aditya/H-5325-2019; Zuckerberg, Benjamin/AAL-9623-2021
OI Singh, Aditya/0000-0001-5559-9151; 
FU NASA Ecological Forecasting [NNX14AC36G]; NESSF [NNX16A061H]; University
   of Wisconsin Cooperative Extension; Federal Aid in Wildlife Restoration
   act; Global Impact Award from GoogleGoogle Incorporated; Department of
   Forest and Wildlife Ecology; Alfred P. Sloan FoundationAlfred P. Sloan
   Foundation; Wisconsin Citizen-based Monitoring Network Partnership
   Program
FX We acknowledge funding and other support from the Wisconsin
   Citizen-based Monitoring Network Partnership Program, NASA Ecological
   Forecasting #NNX14AC36G, and NESSF #NNX16A061H, the University of
   Wisconsin Cooperative Extension, and a grant from the Federal Aid in
   Wildlife Restoration act awarded to WDNR. This publication uses data
   generated via the Zooniverse. org platform, funded by in part by a grant
   from the Alfred P. Sloan Foundation and a Global Impact Award from
   Google. We thank the Department of Forest and Wildlife Ecology for their
   support. We thank A. Johnston, A. Wiggins, and V. Radeloff for comments
   that greatly improved the manuscript.
CR Abra FD, 2018, BIOL CONSERV, V225, P42, DOI 10.1016/j.biocon.2018.06.019
   Alldredge MW, 2007, J WILDLIFE MANAGE, V71, P2759, DOI 10.2193/2006-161
   Anderson K. A., 2002, MODEL SELECTION MULT
   Bird TJ, 2014, BIOL CONSERV, V173, P144, DOI 10.1016/j.biocon.2013.07.037
   Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
   Bonter DN, 2012, FRONT ECOL ENVIRON, V10, P305, DOI 10.1890/110273
   Butt N, 2013, ECOL APPL, V23, P936, DOI 10.1890/11-2059.1
   Chambert T, 2015, ECOLOGY, V96, P332, DOI 10.1890/14-1507.1
   Chandler M, 2017, BIOL CONSERV, V213, P280, DOI 10.1016/j.biocon.2016.09.004
   Clare J., 2018, BIORXIV, DOI [10. 1101/422527, DOI 10.1101/422527]
   Clare J, 2017, ECOL APPL, V27, P2031, DOI 10.1002/eap.1587
   Clare JDJ, 2016, ECOL EVOL, V6, P3884, DOI 10.1002/ece3.2170
   Clare JDJ, 2015, J WILDLIFE MANAGE, V79, P469, DOI 10.1002/jwmg.844
   Crall AW, 2011, CONSERV LETT, V4, P433, DOI 10.1111/j.1755-263X.2011.00196.x
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Ellis MM, 2014, CONSERV BIOL, V28, P52, DOI 10.1111/cobi.12139
   Farmer RG, 2012, AUK, V129, P76, DOI 10.1525/auk.2012.11129
   Gabry J, 2016, RSTANARM BAYESIAN AP
   Gardiner MM, 2012, FRONT ECOL ENVIRON, V10, P471, DOI 10.1890/110185
   Gelman A., 1992, STAT SCI, V7, P457, DOI [10.1214/ss/1177011136., 10.1214/ss/1177011136, DOI 10.1214/SS/1177011136]
   Guillera-Arroita G, 2015, GLOBAL ECOL BIOGEOGR, V24, P276, DOI 10.1111/geb.12268
   Isaac NJB, 2014, METHODS ECOL EVOL, V5, P1052, DOI 10.1111/2041-210X.12254
   Johnston A, 2018, METHODS ECOL EVOL, V9, P88, DOI 10.1111/2041-210X.12838
   Kellner K., 2015, JAGSUI WRAPPER RJAGS
   Kissling WD, 2018, BIOL REV, V93, P600, DOI 10.1111/brv.12359
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   La Sorte FA, 2018, CONDOR, V120, P414, DOI 10.1650/CONDOR-17-206.1
   Lewandowski E, 2015, CONSERV BIOL, V29, P713, DOI 10.1111/cobi.12481
   Linden DW, 2017, J APPL ECOL, V54, P2043, DOI 10.1111/1365-2664.12883
   Mac Aodha O, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005995
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Mcclintock BT, 2010, J WILDLIFE MANAGE, V74, P1882, DOI 10.2193/2009-321
   MCCLISH DK, 1989, MED DECIS MAKING, V9, P190, DOI 10.1177/0272989X8900900307
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Miller DA, 2011, ECOLOGY, V92, P1422, DOI 10.1890/10-1396.1
   Miller DAW, 2015, METHODS ECOL EVOL, V6, P557, DOI 10.1111/2041-210X.12342
   Miller DAW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0065808
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Plummer M., 2003, P 3 INT WORKSH DISTR, V3, P20
   Prince K, 2015, GLOBAL CHANGE BIOL, V21, P572, DOI 10.1111/gcb.12740
   Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
   R CoreTeam, 2015, R LANG ENV STAT COMP
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Royle JA, 2007, ECOLOGY, V88, P1813, DOI 10.1890/06-0669.1
   Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
   Ruiz-Gutierrez V, 2016, METHODS ECOL EVOL, V7, P900, DOI 10.1111/2041-210X.12542
   Simons TR, 2007, AUK, V124, P986, DOI 10.1642/0004-8038(2007)124[986:EAOTAD]2.0.CO;2
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Tabak M. A., 2018, BIORXIV, DOI 10. 1101/346809
   Wiggins A., 2014, 1 MONDAY, V20, DOI [10.5210/fm.v20i1.5520., DOI 10.5210/FM.V20I1.5520, 10.5210/fm.v20i1.5520]
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
NR 54
TC 14
Z9 14
U1 1
U2 37
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1051-0761
EI 1939-5582
J9 ECOL APPL
JI Ecol. Appl.
PD MAR
PY 2019
VL 29
IS 2
AR e01849
DI 10.1002/eap.1849
PG 15
WC Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HN4RC
UT WOS:000460170200001
PM 30656779
DA 2022-02-10
ER

PT J
AU Bi, QL
   Liu, ZJ
   Wang, MH
   Lai, ML
   Xiao, LM
   Yan, YP
   Liu, XG
AF Bi, Qilin
   Liu, Zhijun
   Wang, Miaohui
   Lai, Minling
   Xiao, Leming
   Yan, Yipu
   Liu, Xiaoguang
TI An automatic camera calibration method based on checkerboard
SO TRAITEMENT DU SIGNAL
LA English
DT Article
DE computer vision; camera calibration; checkerboard; corner recognition;
   corner matching
AB The traditional camera calibration methods faces many problems, such as the need for manual operation and high-quality images as well as the heavy time consumption. To solve these problems, this paper puts forward an adaptive extraction and matching algorithm for checkerboard inner-corners for camera calibration. Firstly, the coordinates of all corner points of the checkerboard were derived by the Harris algorithm. Then, the four vertices of the checkerboard were acquired in the image coordinate system based on polygonal convexity. After that, the coordinates of the inner-corner points of the checkerboard image were obtained against the judgement rules that distinguish inner-corner points from other points on that image. On this basis, the matching relationship was established between the inner-corner points of the checkerboard image in the image coordinate system and those in the checkerboard coordinate system. Finally, the theoretical modelling, judgement rules and a mature camera calibration model were integrated for automatic camera calibration experiments. The results show that the automatic camera calibration method based on the proposed algorithm consumed 75% less time than the Matlab toolbox and controlled the error within 0.3 pixels. This research provides a real-time, robust and accurate automatic camera calibration method for engineering applications.
C1 [Bi, Qilin; Liu, Zhijun; Lai, Minling; Xiao, Leming; Yan, Yipu] Guangzhou Maritime Univ, Guangzhou 510725, Guangdong, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Natl Engn Lab Big Data Syst Comp Technol, Shenzhen 518060, Peoples R China.
   [Liu, Xiaoguang] Guangdong Inst Intelligent Mfg, Guangzhou 510070, Guangdong, Peoples R China.
RP Liu, ZJ (corresponding author), Guangzhou Maritime Univ, Guangzhou 510725, Guangdong, Peoples R China.
EM hbbql@163.com
FU National Natural Science Foundation of Guangdong ProvinceNational
   Natural Science Foundation of Guangdong Province [2016A030310309];
   Guangdong Province Science and Technology Project [2017A010102009,
   20178010118004]; Guangzhou City Science and Technology Project
   [201804010354, 201707010187]; Guangdong Applied Science and Technology
   Research and Development Special Fund Project [20168020243012];
   Innovation and Entrepreneurship Education Project in Colleges and
   Universities in Guangzhou [201709P09]; Guangdong Provincial Department
   of Transportation Science and Technology Project [2017-02-025]
FX Fund project: National Natural Science Foundation of Guangdong Province
   (2016A030310309), Guangdong Province Science and Technology Project
   (2017A010102009, 20178010118004), Guangzhou City Science and Technology
   Project (201804010354, 201707010187), Guangdong Applied Science and
   Technology Research and Development Special Fund Project
   (20168020243012), Innovation and Entrepreneurship Education Project in
   Colleges and Universities in Guangzhou (201709P09).Guangdong Provincial
   Department of Transportation Science and Technology
   Project(Technology-2017-02-025).
CR Baataoui A, 2016, ROBUST METHOD CAMERA
   Bushnevskiy A, 2016, IEEE IMAGE PROC, P1165, DOI 10.1109/ICIP.2016.7532541
   ENGELENLEE J, 2017, IEEE T PATTERN ANAL, V18, P210, DOI DOI 10.1080/21678421.2016.1245757
   Geiger A, 2012, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2012.6224570
   Hou ZJ, 2012, INFORMATION-TOKYO, V15, P4393
   [黄风山 HUANG Fengshan], 2006, [光电子·激光, Journal of Optoelectronics·Laser], V17, P705
   Jin J, 2013, J OPT SOC AM A, V30, P287, DOI 10.1364/JOSAA.30.000287
   Kruger L, 2011, PATTERN RECOGN LETT, V32, P1428, DOI 10.1016/j.patrec.2011.04.002
   Leal-Taixe L, 2012, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2012.6247901
   Li Hai, 2015, Optics and Precision Engineering, V23, P3480, DOI 10.3788/OPE.20152312.3480
   Liu Z, 2013, ACTA OPT SINICA, V33
   Shan B H, 2016, ACTA OPTICA SINICA, V36
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Stephens M, 1988, COMBINED CORNER EDGE, V15, P10, DOI DOI 10.5244/C.2.23
   Wei G.-Q., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P133, DOI 10.1109/CVPR.1991.139675
   Yang Xingfang, 2010, China Mechanical Engineering, V21, P2541
   Zhang YJ, 2014, OPT ENG, V53, DOI 10.1117/1.OE.53.11.112203
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 18
TC 1
Z9 1
U1 0
U2 7
PU PRESSES UNIV GRENOBLE
PI GRENOBLE
PA 1041 RUE DES RESIDENCES, GRENOBLE, 38040, FRANCE
SN 0765-0019
J9 TRAIT SIGNAL
JI Trait. Signal
PY 2017
VL 34
IS 3-4
BP 209
EP 226
DI 10.3166/TS.34.209-226
PG 18
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA VI3DA
UT WOS:000469311500007
DA 2022-02-10
ER

PT J
AU Campos, CM
   Moreno, MC
   Cappa, FM
   Ontiveros, Y
   Cona, MI
   Torres, ML
AF Campos, Claudia M.
   Carolina Moreno, M.
   Cappa, Flavio M.
   Ontiveros, Yamila
   Cona, Monica, I
   Laura Torres, M.
TI "Weaving" Different Knowledge Systems through Studying Salience of Wild
   Animals in a Dryland Area of Argentina
SO JOURNAL OF ETHNOBIOLOGY
LA English
DT Article
DE cognitive salience index; cultural salience; ecological salience;
   occupancy; protected areas
ID NATURES CONTRIBUTIONS; COMMUNITIES
AB The current biodiversity conservation framework explores "nature-people" relationships, recognizing culture's central role. This study aimed to combine local knowledge with scientific ecological data to better understand the relationships between wild animals and local people. We worked in a village (Los Baldecitos) located in the area of influence of Ischigualasto Provincial Park (San Juan, Argentina). We conducted 20 free listing interviews and 12 semi-structured and open ones. We analyzed how the overall salience of different species (established through free listing and cognitive salience index) can be explained by ecological (measured through species occupancy models) and cultural (expressed in interviews) aspects of salience. The cognitive salience index and estimated animal occupancy showed a positive correlation, although it was not statistically significant (Spearman's Rho = 0.48, P = 0.095, N = 17). This could mean that cultural aspects (faunal uses, perception related to attitudes and to nature conservation) were relevant in explaining overall salience. Ten species had the highest and most statistically significant salience and were recorded by camera traps. Some of them share spaces with people (village, water points, corrals, and domestic animal areas), and others were less likely to share habitats where people are present. Wild species have cultural value related to uses and acceptance due to material (tangible benefits, ecological functions) and non-material (affectionate, emotional, aesthetic, presence in oral expression) values. Two carnivores elicited negative reactions because of their predatory damage to domestic animals. This study demonstrates methods to interweave local and scientific knowledge to understand peoplenature relationships in context.
C1 [Campos, Claudia M.; Carolina Moreno, M.; Cona, Monica, I; Laura Torres, M.] Univ Nacl Cuyo, CONICET, Gobierno Mendoza, IADIZA,Inst Argentino Invest Zonas Aridas, RA-5500 Mendoza, Argentina.
   [Cappa, Flavio M.; Ontiveros, Yamila] Univ Nacl San Juan, CONICET, Ctr Invest Geosfera & Biosfera, CIGEOBIO, San Juan, Argentina.
RP Campos, CM (corresponding author), Univ Nacl Cuyo, CONICET, Gobierno Mendoza, IADIZA,Inst Argentino Invest Zonas Aridas, RA-5500 Mendoza, Argentina.
EM ccampos@mendoza-conicet.gob.ar
FU Gobierno de San Juan; CONICETConsejo Nacional de Investigaciones
   Cientificas y Tecnicas (CONICET); Presidencia de la Nacion Argentina
FX S We thank the financial support of Presidencia de la Nacion Argentina
   and Gobierno de San Juan (Project Native Forest 2012-2022) and the
   doctoral fellowship from CONICET to MCM. We thank the administration and
   all the staff of Ischigualasto Provincial Park and the local people of
   Los Baldecitos for their cooperation and help. Yamila Andrada helped us
   during the interviews. Juan Jose Ciarlante assisted us in computational
   programming. Nelida Horak assisted us in drafting the English version.
CR Acebes P, 2010, REV CHIL HIST NAT, V83, P395, DOI 10.4067/S0716-078X2010000300007
   Albuquerque U.P, 2014, METHODS TECHNIQUES E, DOI DOI 10.1007/978-1-4614-8636-7_2
   Anderson, 2010, MODEL SELECTION MULT
   Brewer DD, 2002, FIELD METHOD, V14, P108, DOI DOI 10.1177/1525822X02014001007
   Campos CM, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162551
   Chaves LD, 2019, ACTA BOT BRAS, V33, P360, DOI 10.1590/0102-33062018abb0330
   Neto BCD, 2017, ENVIRON DEV SUSTAIN, V19, P1795, DOI 10.1007/s10668-016-9827-2
   del Solar RG, 1997, MAMMALIA, V61, P617
   Diaz S, 2018, SCIENCE, V359, P270, DOI 10.1126/science.aap8826
   Diaz S, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002040
   Giaccardi M., 2015, PLAN MANEJO PARQUE N
   Gosler AG, 2017, J ETHNOBIOL, V37, P637, DOI 10.2993/0278-0771-37.4.637
   Hernandez J, 2015, J ETHNOBIOL ETHNOMED, V11, DOI 10.1186/1746-4269-11-15
   Hunn E, 1999, FOLKBIOLOGY, P47
   Jofre R.C., 2008, REALIDAD TENDENCIAS, P63
   Liamputtong P, 2017, HDB RES METHODS HLTH, P1, DOI DOI 10.1007/978-981-10-2779-6_12-1
   Lucherini Mauro, 2008, Cat News, V49, P29
   Mace GM, 2014, SCIENCE, V345, P1558, DOI 10.1126/science.1254704
   MacKenzie D. I., 2018, OCCUPANCY ESTIMATION
   Mastrangelo ME, 2019, NAT SUSTAIN, V2, P1115, DOI 10.1038/s41893-019-0412-1
   Medinaceli A, 2018, ETHNOBIOL LETT, V9, P86, DOI 10.14237/ebl.9.1.2018.1121
   Ministerio de Ambiente y Desarrollo Sustentable de la Republica Argentina and Aves Argentinas, 2017, CATEGORIZACION AVES
   Morello J, 2012, ECORREGIONES COMPLEJ
   Nagy-Reis M. B., 2017, PLOS ONE, V12, DOI [10.1371/journal.pone.0168441, DOI 10.1371/journal.pone.0168441]
   Narosky T, 2010, AVES ARGENTINA URUGU
   Zambrana NYP, 2018, NAT PLANTS, V4, P201, DOI 10.1038/s41477-018-0128-7
   Pascual U, 2017, CURR OPIN ENV SUST, V26-27, P7, DOI 10.1016/j.cosust.2016.12.006
   Quiroga VA, 2016, J NAT CONSERV, V31, P9, DOI 10.1016/j.jnc.2016.02.004
   R Core Team, 2019, R LANGUAGE ENV STAT
   Rodrigues TF, 2020, BIOL REV, V95, P1, DOI 10.1111/brv.12551
   Roubik, 2017, POT POLLEN STINGLESS, P283, DOI 10.1007/978-3-319-61839
   Rovero F., 2016, CAMERA TRAPPING WILD
   Sacristan I., 2018, Journal of Threatened Taxa, V10, P11566, DOI 10.11609/jott.4030.10.5.11566-11573
   Sampieri R., 2010, METODOLOGIA INVESTIG, V5a
   Scolaro A., 2006, REPTILES PATAGONICOS
   Secretaria de Ambiente y Desarrollo Sustentable de la Republica Argentina and Sociedad Argentina para el Estudio de los Mamiferos, 2019, CAT 2019 MAM ARG SEG
   Smith J.J., 1997, J LINGUIST ANTHROPOL, V7, P208, DOI DOI 10.1525/JLIN.1997.7.2.208
   Sutrop Urmas, 2001, FIELD METHOD, V13, P263, DOI [10.1177/1525822X0101300303, DOI 10.1177/1525822X0101300303]
   Trillo Cecilia, 2016, Ecol. austral, V26, P7
   Vaccaro, 2007, GUIA MAMIFEROS AM
   Vilela A, 2009, J ARID ENVIRON, V73, P238, DOI 10.1016/j.jaridenv.2007.10.013
   Wajner M, 2019, ETHNOBIOL CONSERV, V8, DOI 10.15451/ec2019-07-8.09-1-23
NR 42
TC 0
Z9 0
U1 1
U2 1
PU SOC ETHNOBIOLOGY
PI DENTON
PA UNIV NORTH TEXAS, DEPT GEOGRAPHY, 1155 UNION CIRCLE 305279, DENTON, TX
   76203-5017 USA
SN 0278-0771
EI 2162-4496
J9 J ETHNOBIOL
JI J. Ethnobiol.
PD JUL
PY 2021
VL 41
IS 2
BP 292
EP 306
PG 15
WC Anthropology; Biology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Anthropology; Life Sciences & Biomedicine - Other Topics
GA TF6HH
UT WOS:000670819600010
DA 2022-02-10
ER

PT C
AU Xu, RYD
   Jin, JS
AF Xu, Richard Y. D.
   Jin, Jesse S.
GP IEEE
TI Individual object interaction for camera control and multimedia
   synchronization
SO 2006 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-13
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
LA English
DT Proceedings Paper
CT 31st IEEE International Conference on Acoustics, Speech and Signal
   Processing
CY MAY 14-19, 2006
CL Toulouse, FRANCE
SP IEEE Signal Proc Soc
AB In recent times, most of the computer-vision assisted automatic camera control policies are based on human events, such as speaker position changes. In addition to these events, in this paper, we introduce a set of natural camera control and multimedia synchronization schemes based on individual object interaction. We present our methods in detail, including head-pose calculation and laser pointer guidance, which are used to estimate the region of interest (ROI) for both hand-held and object-at-distance. We explain, from our results, of how these set of approaches have achieved robustness, efficiency and unambiguous object interaction during real-time video shooting.
C1 Univ Technol Sydney, Fac Infomat Technol, Sydney, NSW, Australia.
RP Xu, RYD (corresponding author), Univ Technol Sydney, Fac Infomat Technol, Sydney, NSW, Australia.
EM richardx@it.uts.edu.au; jesse.jin@newcastle.edu.au
OI Xu, Richard Yi Da/0000-0003-2080-4762
CR Comaniciu D., 2003, IEEE T PATTERN ANAL, V25
   Gemmell J., 2000, IEEE Multimedia, V7, P26, DOI 10.1109/93.895152
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Onishi M, 2004, INT C PATT RECOG, P781, DOI 10.1109/ICPR.2004.1334333
   OZEKI M, 2004, 1 EUR C VIS MED PROD, P211
   SHIMADA A, 2004, 7 IASTED INT C COMP, P106
   XU RYD, 2005, IN PRESS WORLD C E L
   XU RYD, 2005, IN PRESS INT J DISTA
   Yip B, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P130, DOI 10.1109/MULMM.2004.1264977
   Yuanchun Shi, 2002, Advances in Web-Based Learning. First International Conference, ICWL 2002. Proceedings (Lecture Notes in Computer Science Vol.2436), P130
NR 11
TC 0
Z9 0
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1520-6149
BN 978-1-4244-0468-1
J9 INT CONF ACOUST SPEE
PY 2006
BP 5339
EP 5342
PG 4
WC Acoustics; Computer Science, Artificial Intelligence; Engineering,
   Electrical & Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Acoustics; Computer Science; Engineering; Imaging Science & Photographic
   Technology
GA BFZ22
UT WOS:000245559906038
DA 2022-02-10
ER

PT C
AU Cai, Q
   Aggarwal, JK
AF Cai, Q
   Aggarwal, JK
GP IEEE
TI Automatic tracking of human motion in indoor scenes across multiple
   synchronized video streams
SO SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION
LA English
DT Proceedings Paper
CT 6th International Conference on Computer Vision
CY JAN 04-07, 1998
CL BOMBAY, INDIA
SP IEEE Comp Soc
AB This paper presents a comprehensive framework for tracking moving humans in an indoor environment from sequences of synchronized monocular grayscale images captured from multiple fixed cameras. The proposed framework consists of three main modules: Single View Tracking (SVT), Multiple View Transition Tracking (MVTT), and Automatic Camera Switching (ACS). Bayesian classification schemes based on motion analysis of human features are used to track (spatially and temporally) a subject image of interest between consecutive frames. The automatic camera switching module predicts the position of the subject along a spatial-temporal domain, and then selects the camera which provides the best view and requires the least switching to continue tracking. Limited degrees of occlusion are tolerated within the system. Tracking is based upon the images of upper human bodies captured from various viewing angles, and non-human moving objects are excluded using Principal Component Analysis (PGA). experimental results are presented to evaluate the performance of the tracking system.
C1 Univ Texas, Dept Elect & Comp Engn, Comp & Vis Res Ctr, Austin, TX 78712 USA.
RP Cai, Q (corresponding author), Univ Texas, Dept Elect & Comp Engn, Comp & Vis Res Ctr, Austin, TX 78712 USA.
NR 0
TC 45
Z9 45
U1 0
U2 0
PU NAROSA PUBLISHING HOUSE
PI NEW DELHI
PA 22 DARYAGANJ, DELHI MEDICAL ASSOCIATION RD, NEW DELHI 110 002, INDIA
BN 81-7319-221-9
PY 1998
BP 356
EP 362
DI 10.1109/ICCV.1998.710743
PG 7
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Interdisciplinary Applications; Computer Science,
   Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science
GA BL47P
UT WOS:000075656000050
DA 2022-02-10
ER

PT J
AU Shao, S
   Zhou, ZX
   Deng, GJ
   Du, P
   Jian, CY
   Yu, ZR
AF Shao, Shuai
   Zhou, Zhixiang
   Deng, Guojun
   Du, Peng
   Jian, Chuanyi
   Yu, Zhongru
TI Experiment of Structural Geometric Morphology Monitoring for Bridges
   Using Holographic Visual Sensor
SO SENSORS
LA English
DT Article
DE structural geometry monitoring; computer-vision-based measurement
   technology; bridge safety; holographic visual sensor; dense full-field
   measurement; digital twins
ID SUBSTRUCTURAL DAMAGE DETECTION; DISPLACEMENT MEASUREMENT; DYNAMIC
   DISPLACEMENT; COMPUTER VISION; OPTICAL-FLOW; ARMAX MODEL; IDENTIFICATION
AB To further improve the precision and efficiency of structural health monitoring technology and the theory of large-scale structures, full-field non-contact structural geometry morphology monitoring is expected to be a breakthrough technology in structural safety state monitoring and digital twins, owing to its economic, credible, high frequency, and holographic advantages. This study validates a proposed holographic visual sensor and algorithms in a computer-vision-based full-field non-contact displacement and vibration measurement. Using an automatic camera patrol experimental device, original segmental dynamic and static video monitoring data of a model bridge under various damage/activities were collected. According to the temporal and spatial characteristics of the series data, the holographic geometric morphology tracking algorithm was introduced. Additionally, the feature points set of the structural holography geometry and the holography feature contours were established. Experimental results show that the holographic visual sensor and the proposed algorithms can extract an accurate holographic full-field displacement signal, and factually and sensitively accomplish vibration measurement, while accurately reflecting the real change in structural properties under various damage/action conditions. The proposed method can serve as a foundation for further research on digital twins for large-scale structures, structural condition assessment, and intelligent damage identification.
C1 [Shao, Shuai; Deng, Guojun; Du, Peng; Jian, Chuanyi; Yu, Zhongru] Chongqing Jiaotong Univ, Sch Civil Engn, Chongqing 400074, Peoples R China.
   [Shao, Shuai; Zhou, Zhixiang] Shenzhen Univ, Coll Civil & Transportat Engn, Shenzhen 518061, Peoples R China.
RP Zhou, ZX (corresponding author), Shenzhen Univ, Coll Civil & Transportat Engn, Shenzhen 518061, Peoples R China.
EM 622150086086@mails.cqjtu.edu.cn; zhixiangzhou@szu.edu.cn;
   guojunforsea@gmail.com; dupeng_cgjtu@163.com;
   chuanyi_jian_cqjtu@163.com; zhongru_yu_cqjtu@163.com
RI Yu, Zhongru/ABE-1572-2021
OI Yu, Zhongru/0000-0002-4104-1365; Shao, Shuai/0000-0002-9243-6666; ,
   Guojun/0000-0001-7883-2585
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [51778094]; National Science Foundation for
   Distinguished Young Scholars of ChinaNational Natural Science Foundation
   of China (NSFC)National Science Fund for Distinguished Young Scholars
   [51708068]; Science and Technology Innovation Project of Chongqing
   Jiaotong University [2019S0141]
FX This research was funded by the National Natural Science Foundation of
   China (Grant No. 51778094), the National Science Foundation for
   Distinguished Young Scholars of China (Grant No. 51608080), and the
   National Science Foundation for Distinguished Young Scholars of China
   (Grant No. 51708068), and the Science and Technology Innovation Project
   of Chongqing Jiaotong University (Grant No. 2019S0141).
CR Abe D., 2015, ACM T GRAPHIC, V34, P1
   Artese S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020338
   Bao YQ, 2019, ENGINEERING-PRC, V5, P234, DOI 10.1016/j.eng.2018.11.027
   Bao YQ, 2014, J CIV STRUCT HEALTH, V4, P77, DOI 10.1007/s13349-013-0064-1
   Bao YQ, 2015, STRUCT CONTROL HLTH, V22, P433, DOI 10.1002/stc.1681
   Cha YJ, 2017, ENG STRUCT, V132, P300, DOI 10.1016/j.engstruct.2016.11.038
   Cha YJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16071016
   Chang XL, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051360
   Chen JG, 2015, J SOUND VIB, V345, P58, DOI 10.1016/j.jsv.2015.01.024
   Chu X, 2019, SAINS MALAYS, V48, P2777, DOI 10.17576/jsm-2019-4812-19
   Chu X, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9214532
   Clough R., 2003, DYNAMICS STRUCTURES
   Dai KS, 2017, WIND ENERGY, V20, P1687, DOI 10.1002/we.2117
   Deng G.J., ADV CIV ENG
   Editorial Department of China Journal of Highway and Transport China's 2014, 2014, CHINA J HIGHWAY TRAN, V27, P1
   Feng DM, 2018, ENG STRUCT, V156, P105, DOI 10.1016/j.engstruct.2017.11.018
   Feng DM, 2017, MECH SYST SIGNAL PR, V88, P199, DOI 10.1016/j.ymssp.2016.11.021
   Feng DM, 2015, J BRIDGE ENG, V20, DOI 10.1061/(ASCE)BE.1943-5592.0000765
   Feng DM, 2015, SENSORS-BASEL, V15, P16557, DOI 10.3390/s150716557
   Ghorbani R, 2015, EXP MECH, V55, P227, DOI 10.1007/s11340-014-9906-y
   Guo J, 2016, MECH SYST SIGNAL PR, V66-67, P425, DOI 10.1016/j.ymssp.2015.06.004
   Hartley R., 2008, MULTIPLE VIEW GEOMET
   Javh J, 2017, MECH SYST SIGNAL PR, V88, P89, DOI 10.1016/j.ymssp.2016.11.009
   Jiang Teng-Jiao, 2016, Research and Exploration in Laboratory, V35, P26
   Kromanis R, 2019, INVENTIONS-BASEL, V4, DOI 10.3390/inventions4030047
   [李惠 Li Hui], 2015, [工程力学, Engineering Mechanics], V32, P1
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   [刘智 Liu Zhi], 2015, [公路交通科技, Journal of Highway and Transportation Research and Development], V32, P88
   LUHMANN T, 2015, PHOTOGRAMM ENG REMOT, V0081, P00273
   Mei L, 2019, ENG STRUCT, V191, P625, DOI 10.1016/j.engstruct.2019.04.084
   Mei L, 2016, STRUCT CONTROL HLTH, V23, P218, DOI 10.1002/stc.1766
   Mei QP, 2019, MECH SYST SIGNAL PR, V119, P523, DOI 10.1016/j.ymssp.2018.10.006
   Park JW, 2010, NDT&E INT, V43, P642, DOI 10.1016/j.ndteint.2010.06.009
   Ribeiro D, 2014, ENG STRUCT, V75, P164, DOI 10.1016/j.engstruct.2014.04.051
   Ronda JI, 2019, J MATH IMAGING VIS, V61, P252, DOI 10.1007/s10851-018-0833-x
   [邵帅 Shao Shuai], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P91
   SMITH J, 2008, US PHARM S       MAR, P4
   [孙利民 Sun Limin], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P1
   [孙倩 Sun Qian], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P83
   Wadhwa N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461966
   Wang S.R., 2013, IABSE S REP, V101, P1, DOI [10.2749/222137813808627857, DOI 10.2749/222137813808627857]
   Wang SR, 2016, J BRIDGE ENG, V21, DOI 10.1061/(ASCE)BE.1943-5592.0000956
   [王邵锐 Wang Shaorui], 2015, [计算力学学报, Chinese Journal of Computational Mechanics], V32, P627
   [王邵锐 Wang Shaorui], 2015, [土木工程学报, China Civil Engineering Journal], V48, P70
   [王邵锐 Wang Shaorui], 2014, [土木工程学报, China Civil Engineering Journal], V47, P70
   Wang Shuai, 2016, Horticultural Plant Journal, V2, P82, DOI 10.1016/j.hpj.2016.03.001
   Wang Y, 2020, J PERFORM CONSTR FAC, V34, DOI 10.1061/(ASCE)CF.1943-5509.0001366
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xu Y, 2018, STRUCT CONTROL HLTH, V25, DOI 10.1002/stc.2155
   Yang YC, 2017, MECH SYST SIGNAL PR, V85, P567, DOI 10.1016/j.ymssp.2016.08.041
   Ye X.W., 2019, CHINA J HIGHWAY TRAN, V32, P20
   YIBIN HE, 2018, INT J MOD PHYS C, V29, P169
   Zhang S.B., 2017, CHINA J HIGHW TRANSP, V30, P251
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   [周志祥 Zhou Zhixiang], 2018, [应用基础与工程科学学报, Journal of Basic Science and Engineering], V26, P1078
   [宗周红 Zong Zhouhong], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P40
NR 56
TC 6
Z9 6
U1 26
U2 38
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD FEB
PY 2020
VL 20
IS 4
AR 1187
DI 10.3390/s20041187
PG 24
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA KY3CM
UT WOS:000522448600238
PM 32098079
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Sun, YX
   Liu, M
   Meng, MQH
AF Sun, Yuxiang
   Liu, Ming
   Meng, Max Q. -H.
TI Active Perception for Foreground Segmentation: An RGB-D Data-Based
   Background Modeling Method
SO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
LA English
DT Article
DE Active perception; Image color analysis; Sensors; Computer vision; Image
   segmentation; Active perception; background modeling; foreground
   segmentation; RGB-D camera
ID MOTION REMOVAL; D SLAM; KINECT
AB Foreground moving object segmentation is a fundamental problem in many computer vision applications. As a solution for foreground segmentation, background modeling has been intensively studied over past years and many effective algorithms have been developed. However, accurate foreground segmentation is still a difficult problem. Currently, most of the algorithms work solely within the color space, in which the segmentation performance is prone to be degraded by a multitude of challenges, such as illumination changes, shadows, automatic camera adjustments, and color camouflage. RGB-D cameras are active visual sensors that provide depth measurements along with color images. We present in this paper an innovative background modeling method by using both the color and depth information from an RGB-D camera. The proposed method is evaluated using a public RGB-D data set. Various experiments confirm that our method is able to achieve superior performance compared with existing well-known methods. Note to Practitioners-This paper investigates background modeling for foreground segmentation with active perception. Recent RGB-D cameras that leverage the active perception technology have advanced many computer vision algorithms. In this paper, we develop a background modeling method to achieve superior performance by using an RGB-D camera instead of a color camera. Due to the use of the active sensing technology, the proposed method is characterized by its robustness to common challenges. Our method could be used for improving existing infrastructures, such as visual surveillance systems for parking spaces. Moreover, the simple design of our method allows it to be easily deployed on various computing platforms, which facilitates many practical applications that usually require embedded computing devices. However, our method cannot run real timely at the current status. We believe that it can be further improved using parallel programming techniques to meet the real-time requirement.
C1 [Sun, Yuxiang] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Inst Robot, Hong Kong, Peoples R China.
   [Liu, Ming] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
   [Meng, Max Q. -H.] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
RP Liu, M (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.; Meng, MQH (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM eeyxsun@ust.hk; eelium@ust.hk; max.meng@cuhk.edu.hk
RI Liu, Ming/AAC-9891-2020
OI Liu, Ming/0000-0002-4500-238X; Sun, Yuxiang/0000-0002-7704-0559; Meng,
   Max Q.-H./0000-0002-5255-5898
FU Shenzhen Science and Technology Innovation Project
   [JCYJ20160428154842603, JCYJ20170413161616163]; Hong Kong Research Grant
   Council (RGC)Hong Kong Research Grants Council [11210017, 16212815,
   21202816, 14205914, 14200618]; ITC ITF Project [ITS/236/15]; National
   Natural Science Foundation of ChinaNational Natural Science Foundation
   of China (NSFC) [U1713211]
FX This work was supported by the Shenzhen Science and Technology
   Innovation Project JCYJ20160428154842603, JCYJ20170413161616163, the
   Hong Kong Research Grant Council (RGC) Project 11210017, 16212815,
   21202816, 14205914, 14200618, the ITC ITF Project ITS/236/15, the
   National Natural Science Foundation of China Project U1713211.
CR Amamra A., 2014, P INT S ELM ZAD CROA, P1, DOI DOI 10.1109/ELMAR.2014.6923325
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   BAJCSY R, 1992, CVGIP-IMAG UNDERSTAN, V56, P31, DOI 10.1016/1049-9660(92)90083-F
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bouwmans T., 2014, BACKGROUND MODELING
   Cai ZY, 2017, MULTIMED TOOLS APPL, V76, P4313, DOI 10.1007/s11042-016-3374-6
   Camplani M, 2014, J VIS COMMUN IMAGE R, V25, P122, DOI 10.1016/j.jvcir.2013.03.009
   Cheng JY, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P589, DOI 10.1109/ICAR.2017.8023671
   Davis L., 2000, COMPUTER VISION ECCV, P751, DOI DOI 10.1007/3-540-45053-X_48
   Eveland C, 1998, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.1998.698619
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fernandez-Sanchez EJ, 2013, SENSORS-BASEL, V13, P8895, DOI 10.3390/s130708895
   Godbehere AB, 2012, P AMER CONTR CONF, P4305
   Gordon G., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P459, DOI 10.1109/CVPR.1999.784721
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941
   Hofmann M., 2012, P IEEE C COMP VIS PA, P38, DOI DOI 10.1109/CVPRW.2012.6238925
   Horaud R, 2016, MACH VISION APPL, V27, P1005, DOI 10.1007/s00138-016-0784-4
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Keselman L, 2017, IEEE COMPUT SOC CONF, P1267, DOI 10.1109/CVPRW.2017.167
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Liu H., IEEE T IND ELECT
   Murgia J, 2014, LECT NOTES ARTIF INT, V8856, P380, DOI 10.1007/978-3-319-13647-9_35
   Nageli T, 2017, IEEE ROBOT AUTOM LET, V2, P1696, DOI 10.1109/LRA.2017.2665693
   Ren XF, 2013, IEEE ROBOT AUTOM MAG, V20, P49, DOI 10.1109/MRA.2013.2253409
   Roushdy M., 2006, GVIP J, V6, P17
   Shah M, 2014, MACH VISION APPL, V25, P1105, DOI 10.1007/s00138-013-0552-7
   Shao L, 2013, IEEE T CYBERNETICS, V43, P1314, DOI 10.1109/TCYB.2013.2276144
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Sun YX, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1377, DOI 10.1109/ROBIO.2015.7418963
   Sun YX, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P1617, DOI 10.1109/ICInfA.2015.7279544
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Yan TF, 2018, IEEE INT CONF ROBOT, P6766
   Zahzah, 2015, ROBUST LOW RANK SPAR
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 42
TC 18
Z9 18
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-5955
EI 1558-3783
J9 IEEE T AUTOM SCI ENG
JI IEEE Trans. Autom. Sci. Eng.
PD OCT
PY 2019
VL 16
IS 4
BP 1596
EP 1609
DI 10.1109/TASE.2019.2893414
PG 14
WC Automation & Control Systems
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems
GA JG9XE
UT WOS:000492428500011
DA 2022-02-10
ER

PT C
AU Khosravi, H
   Fazl-Ersi, E
AF Khosravi, Hooman
   Fazl-Ersi, Ehsan
GP IEEE
TI Trackerbot: A Robotic Surveillance System based on Stereo-Vision and
   Artificial Neural Networks
SO 2016 4TH RSI INTERNATIONAL CONFERENCE ON ROBOTICS AND MECHATRONICS
   (ICROM)
SE RSI International Conference on Robotics and Mechatronics ICRoM
LA English
DT Proceedings Paper
CT 4th RSI International Conference on Robotics and Mechatronics (ICROM)
CY OCT 26-28, 2016
CL Tehran, IRAN
SP Robot Soc Iran, Univ Tehran
DE Inteligent surveillance; Master-slave camera system; camera control; PTZ
   parameters
AB Master-slave camera systems are ideal for detailed surveillance of desired targets in wide scenes. In this paper, we propose a fully automatic camera system that is structurally unconstrained and independent from both intrinsic and extrinsic camera parameters for detecting activities in indoor or outdoor environments. Unlike traditional models in our system, two wide cameras are used to reach the minimum error in estimating the pan, tilt and zoom parameters (PTZ) which then would be only affected by the resolution of the wide cameras or the PTZ motor system. After an initial automatic calibration, a feed-forward artificial neural network (ANN) takes charge of controlling the PTZ unit according to the information extracted from the frames of the wide cameras.
C1 [Khosravi, Hooman; Fazl-Ersi, Ehsan] Ferdowsi Univ Mashhad, Dept Comp Engn, Mashhad, Iran.
RP Khosravi, H (corresponding author), Ferdowsi Univ Mashhad, Dept Comp Engn, Mashhad, Iran.
EM hooman.khosravi@stu-mail.um.ac.ir; fazlersi@ferdowsi.um.ac.ir
CR Bastanlar Y, 2016, PATTERN RECOGN LETT, V71, P1, DOI 10.1016/j.patrec.2015.11.013
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bodor R., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P643
   Chen P., 2016, P 2015 CHIN INT SYST
   Choi H.C., 2011, IEEE TCSVT, V8, P1665
   Choi H.-C., 2010, BIOM THEOR APPL SYST
   Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Hu J., 2013, CONTR DEC C CCDC 201
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Lain B., 2011, HUM SYST INT HSI 201
   Li XZ, 2014, COMM COM INF SC, V483, P293
   Liao HC, 2010, INF TECHNOL CONTROL, V39, P227
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meyer M., 1998, SEC TECHN 1998 P 32
   Neves JC, 2015, LECT NOTES COMPUT SC, V9117, P552, DOI 10.1007/978-3-319-19390-8_62
   Sato K., 1994, CAD BAS VIS WORKSH 1
   Senior A. W., 2005, APPL COMP VIS 2005 W, V1
   Shakeri M., 2015, FIELD SERVICE ROBOTI
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
NR 21
TC 2
Z9 2
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2377-679X
EI 2572-6889
BN 978-1-5090-3222-8
J9 RSI INT CONF ROBOT M
PY 2016
BP 449
EP 454
PG 6
WC Engineering, Electrical & Electronic; Engineering, Mechanical; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Robotics
GA BH8YZ
UT WOS:000403766200075
DA 2022-02-10
ER

PT J
AU Sarkar, D
   Chapman, CA
AF Sarkar, Dipto
   Chapman, Colin A.
TI The Smart Forest Conundrum: Contextualizing Pitfalls of Sensors and AI
   in Conservation Science for Tropical Forests
SO TROPICAL CONSERVATION SCIENCE
LA English
DT Article
DE sensors; monitoring technologies; poaching; protected area; Kibale
   National Park; drones; camera-traps; remote sensing; AI
ID CAMERA TRAPS; WAR; BIODIVERSITY; BIG; SURVEILLANCE; DRONES; DONT
AB The term 'smart forest' is not yet common, but the proliferation of sensors, algorithms, and technocentric thinking in conservation, as in most other aspects of our lives, suggests we are at the brink of this evolution. While there has been some critical discussion about the value of using smart technology in conservation, a holistic discussion about the broader technological, social, and economic interactions involved with using big data, sensors, artificial intelligence, and global corporations is largely missing. Here, we explore the pitfalls that are useful to consider as forests are gradually converted to technological sites of data production for optimized biodiversity conservation and are consequently incorporated in the digital economy. We consider who are the enablers of the technologically enhanced forests and how the gradual operationalization of smart forests will impact the traditional stakeholders of conservation. We also look at the implications of carpeting forests with sensors and the type of questions that will be encouraged. To contextualize our arguments, we provide examples from our work in Kibale National Park, Uganda which hosts the one of the longest continuously running research field station in Africa.
C1 [Sarkar, Dipto] Carleton Univ, Dept Geog & Environm Studies, Ottawa, ON K1S 5B6, Canada.
   [Chapman, Colin A.] George Washington Univ, Dept Anthropol, Ctr Adv Study Human Paleobiol, Washington, DC 20052 USA.
   [Chapman, Colin A.] Northwest Univ, Shaanxi Key Lab Anim Conservat, Xian, Peoples R China.
   [Chapman, Colin A.] Univ KwaZulu Natal, Sch Life Sci, Pietermaritzburg, South Africa.
RP Sarkar, D (corresponding author), Carleton Univ, Dept Geog & Environm Studies, Ottawa, ON K1S 5B6, Canada.
EM dipto.sarkar@carleton.ca
CR Abdelnour S., 2015, SUSTAINABLE ACCESS E, P205
   Adams WM, 2019, PROG HUM GEOG, V43, P337, DOI 10.1177/0309132517740220
   Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Arvidsson A, 2016, THEOR CULT SOC, V33, P3, DOI 10.1177/0263276416658104
   Bakker K, 2018, GLOBAL ENVIRON CHANG, V52, P201, DOI 10.1016/j.gloenvcha.2018.07.011
   Barns S, 2017, URBAN POLICY RES, V35, P20, DOI 10.1080/08111146.2016.1235032
   Begg CM, 2005, J ZOOL, V265, P23, DOI 10.1017/S0952836904005989
   Benkler Y, 2019, NATURE, V569, P161, DOI 10.1038/d41586-019-01413-1
   Bennett EL, 2015, CONSERV BIOL, V29, P54, DOI 10.1111/cobi.12377
   Birhane A, 2020, SCRIPT ED, V17, P389
   Bonnin N, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2020017
   Bortolamiol S., CONSERV SCI PRACT
   Bradshaw CJA, 2009, FRONT ECOL ENVIRON, V7, P79, DOI 10.1890/070193
   Buscher B, 2016, NEW MEDIA SOC, V18, P726, DOI 10.1177/1461444814545841
   Buscher B, 2016, AFR AFFAIRS, V115, P1, DOI 10.1093/afraf/adv058
   Calzada I, 2020, SMART CITIES-BASEL, V3, P1145, DOI 10.3390/smartcities3040057
   Dechmann DKN, 2011, J EXP BIOL, V214, P3605, DOI 10.1242/jeb.056010
   Ditmer MA, 2015, CURR BIOL, V25, P2278, DOI 10.1016/j.cub.2015.07.024
   Dourish P, 2018, BIG DATA SOC, V5, DOI 10.1177/2053951718784083
   Duffy R, 2016, GEOFORUM, V69, P238, DOI 10.1016/j.geoforum.2015.09.014
   Duffy R, 2014, INT AFF, V90, P819, DOI 10.1111/1468-2346.12142
   Faraway JJ, 2018, STAT PROBABIL LETT, V136, P142, DOI 10.1016/j.spl.2018.02.031
   Fletcher R., 2014, NATURE INC ENV CONSE
   Fourcade M, 2017, SOCIO-ECON REV, V15, P9, DOI 10.1093/ser/mww033
   Gabrys J., 2016, PROGRAM EARTH ENV SE
   Gabrys J, 2020, BIG DATA SOC, V7, DOI 10.1177/2053951720904871
   Galaz V, 2017, TRENDS ECOL EVOL, V32, P628, DOI 10.1016/j.tree.2017.06.013
   Garstang M, 2004, J COMP PHYSIOL A, V190, P791, DOI 10.1007/s00359-004-0553-0
   Gautam H, 2019, BIOTROPICA, V51, P443, DOI 10.1111/btp.12651
   Getzin S, 2012, METHODS ECOL EVOL, V3, P397, DOI 10.1111/j.2041-210X.2011.00158.x
   Goetz Scott J, 2009, Carbon Balance Manag, V4, P2, DOI 10.1186/1750-0680-4-2
   Guo ST, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101412
   Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1215904, 10.1126/science.1244693]
   Harvey D., 2014, 17 CONTRADICTIONS EN
   Howard A, 2018, SCI ENG ETHICS, V24, P1521, DOI 10.1007/s11948-017-9975-2
   Huesemann M., 2011, TECHNO FIX WHY TECHN
   Humle T, 2014, SCIENCE, V344, P1351, DOI 10.1126/science.344.6190.1351-a
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7_24
   Joppa LN, 2015, AMBIO, V44, pS522, DOI 10.1007/s13280-015-0702-4
   Kang C., 2020, NEW YORK TIMES
   Karagounis B., 2020, INTRO MICROSOFT AZUR
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Keeping D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096598
   Kirumira D, 2019, CONSERV SOC, V17, P51, DOI 10.4103/cs.cs_17_72
   Kitchin R., 2014, DATA REVOLUTION BIG
   Kolowski JM, 2021, ECOSPHERE, V12, DOI 10.1002/ecs2.3350
   Kull CA, 2007, SOC NATUR RESOUR, V20, P723, DOI 10.1080/08941920701329702
   Leyland C., 2020, THE GUARDIAN
   Linkie M, 2013, BIOL CONSERV, V162, P107, DOI 10.1016/j.biocon.2013.03.028
   Lyon D, 2014, BIG DATA SOC, V1, DOI 10.1177/2053951714541861
   Lyubenova M., 2015, Journal of Balkan Ecology, V18, P363
   Maisels F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059469
   Marvin DC, 2016, GLOB ECOL CONSERV, V7, P262, DOI 10.1016/j.gecco.2016.07.002
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Meek PD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110832
   Merritt SM, 2013, HUM FACTORS, V55, P520, DOI 10.1177/0018720812465081
   Morozov E., 2013, SAVE EVERYTHING CLIC
   Morrissey L.F., 2012, DEVELOPMENT, V55, P13
   Naughton-Treves L, 2011, P NATL ACAD SCI USA, V108, P13919, DOI 10.1073/pnas.1013332108
   Neumann RP, 2004, POLIT GEOGR, V23, P813, DOI 10.1016/j.polgeo.2004.05.011
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell F., 2011, CAMERA TRAPS ANIMAL
   O'Neil Cathy., 2016, WEAPONS MATH DESTRUC
   Park JY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131534
   Pasquale F., 2015, BLACK BOX SOC
   Pimm SL, 2014, SCIENCE, V344, P987, DOI 10.1126/science.1246752
   Rebolo-Ifran N, 2019, ENVIRON CONSERV, V46, P205, DOI 10.1017/S0376892919000080
   Redford KH, 2013, CONSERV BIOL, V27, P437, DOI 10.1111/cobi.12071
   Rivas-Romero JA, 2015, SOUTHWEST NAT, V60, P366, DOI 10.1894/0038-4909-60.4.366
   Roderick L, 2014, CRIT SOCIOL, V40, P729, DOI 10.1177/0896920513501350
   Sadowski J, 2019, BIG DATA SOC, V6, DOI 10.1177/2053951718820549
   Sandbrook C, 2018, CONSERV SOC, V16, P493, DOI 10.4103/cs.cs_17_165
   Sandbrook C, 2015, AMBIO, V44, pS636, DOI 10.1007/s13280-015-0714-0
   Sarkar D, 2019, PROF GEOGR, V71, P422, DOI 10.1080/00330124.2018.1547976
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   Shelton T, 2019, CITY, V23, P35
   Simonson WD, 2014, METHODS ECOL EVOL, V5, P719, DOI 10.1111/2041-210X.12219
   Sosnowski MC, 2019, BIOL CONSERV, V237, P392, DOI 10.1016/j.biocon.2019.07.020
   Srnicek N., 2017, PLATFORM CAPITALISM
   Succi S, 2019, PHILOS T R SOC A, V377, DOI 10.1098/rsta.2018.0145
   Tembon M., 2019, INVESTING BHUTANS FO
   Vanolo A, 2014, URBAN STUD, V51, P883, DOI 10.1177/0042098013494427
   Vas E, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2014.0754
   Wasser SK, 2015, SCIENCE, V349, P84, DOI 10.1126/science.aaa2457
   Wasser SK, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat0625
   Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
   Wiig A, 2018, ENVIRON PLAN C-POLIT, V36, P403, DOI 10.1177/2399654417743767
   Wrege PH, 2017, METHODS ECOL EVOL, V8, P1292, DOI 10.1111/2041-210X.12730
   Xiao Y., 2016, TRAFFIC BULL, V17
   Xu FF, 2020, J SUSTAIN TOUR, V28, P144, DOI 10.1080/09669582.2019.1631318
   Yoccoz NG, 2001, TRENDS ECOL EVOL, V16, P446, DOI 10.1016/S0169-5347(01)02205-4
   ZIEGLER TE, 1995, HORM BEHAV, V29, P407, DOI 10.1006/hbeh.1995.1028
   Zuboff S, 2015, J INF TECHNOL-UK, V30, P75, DOI 10.1057/jit.2015.5
NR 94
TC 3
Z9 3
U1 5
U2 6
PU SAGE PUBLICATIONS INC
PI THOUSAND OAKS
PA 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN 1940-0829
J9 TROP CONSERV SCI
JI Trop. Conserv. Sci.
PD APR
PY 2021
VL 14
AR 19400829211014740
DI 10.1177/19400829211014740
PG 11
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA SJ2QF
UT WOS:000655374400001
OA gold
DA 2022-02-10
ER

PT C
AU Radig, B
   Follmann, P
AF Radig, Bernd
   Follmann, Patrick
GP CENPARMI
TI Training a classifier for automatic flash detection in million images
   from camera-traps
SO PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND
   ARTIFICIAL INTELLIGENCE (ICPRAI 2018)
LA English
DT Proceedings Paper
CT International Conference on Pattern Recognition and Artificial
   Intelligence (ICPRAI)
CY MAY 13-17, 2018
CL Montreal, CANADA
SP Concordia Univ, Ctr Pattern Recognit & Machine Intelligence
C1 [Radig, Bernd; Follmann, Patrick] Tech Univ Munich, Informat, Munich, Germany.
   [Follmann, Patrick] Tech Univ Munich, MVTec Software GmbH, Munich, Germany.
RP Radig, B (corresponding author), Tech Univ Munich, Informat, Munich, Germany.
EM radig@in.tum.de; follmann@mvtec.com
CR Barz B., 2018, IMTA WORKSH UNPUB
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Krizhevsky A., 2012, IMAGENET CLASSIFICAT, V25
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
NR 4
TC 0
Z9 0
U1 0
U2 0
PU CENPARMI
PI Montreal, QC
PA 1455 Maisonneuve Blvd, West EV 003 403, Montreal, QC, CANADA
BN 978-1-895193-04-6
PY 2018
BP 589
EP 591
PG 3
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS3YW
UT WOS:000716970300102
DA 2022-02-10
ER

PT C
AU Dingli, A
   Abela, C
AF Dingli, Alexiei
   Abela, Charlie
BE Ghallab, M
   Spyropoulos, CD
   Fakotakis, N
   Avouris, N
TI A pervasive assistant for nursing and doctoral staff
SO ECAI 2008, PROCEEDINGS
SE Frontiers in Artificial Intelligence and Applications
LA English
DT Proceedings Paper
CT 18th European Conference on Artificial Intelligence
CY JUL 21-25, 2008
CL Univ Patras, Patras, GREECE
SP European Comm Artificial Intelligence, Hellen Artificial Intelligence Soc
HO Univ Patras
AB The goal of health-care institutions is to provide patient-centric health care services. Unfortunately, this goal is frequently undermined due to human-related aspects. The Pervaslve Nursing And docToral Assistant (PINATA) provides a patient-centric system powered with Ambience Intelligence techniques and Semantic Web technologies. Through PINATA, the movement of patients and medical staff is tracked via RFID sensors while an automated camera system monitors the interaction of people within their environment. The system reacts to particular situations autonomously by directing medical staff towards emergencies in a timely manner and providing them with just the information they require on their handheld devices. This ensures that patients are given the best care possible on a 24/7 basis especially when the medical staff is not around.
C1 [Dingli, Alexiei; Abela, Charlie] Univ Malta, Fac ICT, Dept Artificial Intelligence, Msida, Malta.
RP Dingli, A (corresponding author), Univ Malta, Fac ICT, Dept Artificial Intelligence, Msida, Malta.
EM alexiei.dingli@um.edu.mt; charlie.abela@um.edu.mt
OI Dingli, Alexiei/0000-0002-5951-8299; Abela, Charlie/0000-0002-7574-4296
FU Malta Council for Science and Technology; Ministry of Technology
FX This work was carried out within the PINATA project, funded by the Malta
   Council for Science and Technology (http://www.mcst.org.mt) and done in
   collaboration with St.James Hospital Malta (http://stjameshospital.com).
   The project was also supported by the Ministry of Technology
   (http://www.miti.gov.mt).
CR AHOLA J, 2002, EDBT 02
   BRAVO J, 2006, PTA2006 WORKSH
   Capgemini, 2007, GAIN SOL RES RFID HE
   CHOWDHURY B, 2007, ACIS ICIS, P363
   FIELD MJ, 1990, CLIN PRACTICE HOSPIT
   HUSSAIN S, 2007, ONTOLOGY DRIVEN CPG
   ISSARNY V, 2005, DEV AMBIENT INTELLIG
   JERVIS C, 2005, TAG TEAM CARE RFID C
   Reiner J., 2005, RFID HEALTHCARE PANA
   SNIDARO L, 2005, SYSTEMS MAN CYBERN A
NR 10
TC 1
Z9 1
U1 0
U2 2
PU IOS PRESS
PI AMSTERDAM
PA NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
SN 0922-6389
BN 978-1-58603-891-5
J9 FRONT ARTIF INTEL AP
PY 2008
VL 178
BP 829
EP +
DI 10.3233/978-1-58603-891-5-829
PG 2
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BMY54
UT WOS:000273903100188
DA 2022-02-10
ER

PT C
AU Brouwers, GMYE
   Zwemer, MH
   Wijnhoven, RGJ
   de With, PHN
AF Brouwers, Guido M. Y. E.
   Zwemer, Matthijs H.
   Wijnhoven, Rob G. J.
   de With, Peter H. N.
BE Hua, G
   Jegou, H
TI Automatic Calibration of Stationary Surveillance Cameras in the Wild
SO COMPUTER VISION - ECCV 2016 WORKSHOPS, PT II
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 14th European Conference on Computer Vision (ECCV)
CY OCT 08-16, 2016
CL Amsterdam, NETHERLANDS
DE Automatic camera calibration; Vanishing points
ID SELF-CALIBRATION; AUTOCALIBRATION; VIDEO
AB We present a fully automatic camera calibration algorithm for monocular stationary surveillance cameras. We exploit only information from pedestrians tracks and generate a full camera calibration matrix based on vanishing-point geometry. This paper presents the first combination of several existing components of calibration systems from literature. The algorithm introduces novel pre- and post-processing stages that improve estimation of the horizon line and the vertical vanishing point. The scale factor is determined using an average body height, enabling extraction of metric information without manual measurement in the scene. Instead of evaluating performance on a limited number of camera configurations (video seq.) as in literature, we have performed extensive simulations of the calibration algorithm for a large range of camera configurations. Simulations reveal that metric information can be extracted with an average error of 1.95% and the derived focal length is more accurate than the reported systems in literature. Calibration experiments with real-world surveillance datasets in which no restrictions are made on pedestrian movement and position, show that the performance is comparable (max. error 3.7 %) to the simulations, thereby confirming feasibility of the system.
C1 [Brouwers, Guido M. Y. E.; Zwemer, Matthijs H.; Wijnhoven, Rob G. J.] ViNot BV, Eindhoven, Netherlands.
   [Zwemer, Matthijs H.; de With, Peter H. N.] Eindhoven Univ Technol, Eindhoven, Netherlands.
RP Brouwers, GMYE (corresponding author), ViNot BV, Eindhoven, Netherlands.
EM guido.brouwers@vinotion.nl; m.zwemer@tue.nl; rob.wijnhoven@vinotion.nl;
   p.h.n.de.With@tue.nl
OI Zwemer, Matthijs/0000-0003-0835-202X
CR Berclaz J., 2011, IEEE T PATTERN ANAL
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dubska M., 2014, P BRIT MACH VIS C
   Faugeras O. D., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P15
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Hartley R. I., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P471
   Huang S, 2016, IEEE IPCCC
   Krahnstoever N, 2005, IEEE I CONF COMP VIS, P1858
   Kusakunniran W, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P250, DOI 10.1109/DICTA.2009.49
   Liu J., 2011, BRIT MACH VIS C BMVC
   Liu JC, 2013, IEEE WORK APP COMP, P433, DOI 10.1109/WACV.2013.6475051
   Lv FJ, 2006, IEEE T PATTERN ANAL, V28, P1513, DOI 10.1109/TPAMI.2006.178
   Lv FJ, 2002, INT C PATT RECOG, P562, DOI 10.1109/ICPR.2002.1044793
   MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171
   Micusik B, 2010, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2010.5539786
   MILLAR WJ, 1986, J EPIDEMIOL COMMUN H, V40, P319, DOI 10.1136/jech.40.4.319
   Orghidan R, 2012, FED CONF COMPUT SCI, P123
   Possegger Horst, 2012, P COMP VIS WINT WORK
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 19
TC 6
Z9 6
U1 0
U2 3
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-48881-3; 978-3-319-48880-6
J9 LECT NOTES COMPUT SC
PY 2016
VL 9914
BP 743
EP 759
DI 10.1007/978-3-319-48881-3_52
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BG5HT
UT WOS:000389501700052
DA 2022-02-10
ER

PT J
AU Nagy-Reis, M
   Oshima, JED
   Kanda, CZ
   Palmeira, FBL
   de Melo, FR
   Morato, RG
   Bonjorne, L
   Magioli, M
   Leuchtenberger, C
   Rohe, F
   Lemos, FG
   Martello, F
   Alves-Eigenheer, M
   da Silva, RA
   dos Santos, JS
   Priante, CF
   Bernardo, R
   Rogeri, P
   Assis, JC
   Gaspar, LP
   Tonetti, VR
   Trinca, CT
   Ribeiro, AD
   Bocchiglieri, A
   Hass, A
   Canteri, A
   Chiarello, AG
   Paglia, AP
   Pereira, AA
   de Souza, AC
   Gatica, A
   Medeiro, AZ
   Eriksson, A
   Costa, AN
   Gonzalez-Gallina, A
   Yanosky, AA
   de la Cruz, AJ
   Bertassoni, A
   Bager, A
   Bovo, AAA
   Mol, AC
   Bezerra, AMR
   Percequillo, A
   Vogliotti, A
   Lopes, AMC
   Keuroghlian, A
   Hartley, ACZ
   Devlin, AL
   de Paula, A
   Garcia-Olaechea, A
   Sanchez, A
   Aquino, ACMM
   Srbek-Araujo, AC
   Ochoa, AC
   Tomazzoni, AC
   Lacerda, ACR
   Bacellar, AED
   Campelo, AKN
   Victoria, AMH
   Paschoal, AMD
   Potrich, AP
   Gomes, APN
   Olimpio, APM
   Costa, ARC
   Jacomo, ATD
   Calaca, AM
   Jesus, AS
   Barban, AD
   Feijo, A
   Pagoto, A
   Rolim, AC
   Hermann, AP
   Souza, ASMDE
   Alonso, AC
   Monteiro, A
   Mendonca, AF
   Luza, AL
   Moura, ALB
   da Silva, ALF
   Lanna, AM
   Antunes, AP
   Nunes, AV
   Dechner, A
   Carvalho, AS
   Novaro, AJ
   Scabin, AB
   Gatti, A
   Nobre, AB
   Montanarin, A
   Deffaci, AC
   de Albuquerque, ACF
   Mangione, AM
   Pinto, AMS
   Pontes, ARM
   Bertoldi, AT
   Calouro, AM
   Fernandes, A
   Ferreira, AN
   Ferreguetti, AC
   Rosa, ALM
   Banhos, A
   Francisco, BDD
   Cezila, BA
   Beisiegel, BD
   de Thoisy, B
   Ingberman, B
   Neves, BD
   Pereira-Silva, B
   de Camargo, BB
   Andrade, BD
   Santos, BS
   Leles, B
   Campos, BATP
   Kubiak, BB
   Franca, BRD
   Saranholi, BH
   Mendes, CP
   Devids, CC
   Pianca, C
   Rodrigues, C
   Islas, CA
   de Lima, CA
   de Lima, CR
   Gestich, CC
   Tedesco, CD
   De Angelo, C
   Fonseca, C
   Hass, C
   Peres, CA
   Kasper, CB
   Durigan, CC
   Fragoso, CE
   Verona, CE
   Rocha, CFD
   Salvador, CH
   Vieira, CL
   Ruiz, CEB
   Cheida, CC
   Sartor, CC
   Espinosa, CD
   Fieker, CZ
   Braga, C
   Sanchez-Lalinde, C
   Machado, CIC
   Cronemberger, C
   Luna, CL
   Del Vechio, C
   Bernardo, CSS
   Hurtado, CM
   Lopes, CM
   da Rosa, CA
   Cinta, CC
   Costa, CG
   Zarate-Castaneda, CP
   Novaes, CL
   Jenkins, CN
   Seixas, CS
   Martin, C
   Zaniratto, CP
   Lopez-Fuerte, CF
   da Cunha, CJ
   De-Carvalho, CB
   Chavez, C
   Santos, CC
   Polli, DJ
   Buscariol, D
   Carreira, DC
   Galiano, D
   Thornton, D
   Ferraz, DD
   Lamattina, D
   Moreno, DJ
   Moreira, DO
   Farias, DA
   Barros-Battesti, DM
   Tavares, DC
   Braga, DC
   Gaspar, DA
   Friedeberg, D
   Astua, D
   Silva, DA
   Viana, DC
   Lizcano, DJ
   Varela, DM
   Loretto, D
   Grabin, DM
   Eaton, DP
   da Silva, DM
   Dias, DD
   Camara, EMVC
   Barbier, E
   Chavez-Gonzalez, E
   Rocha, EC
   Lima, ED
   Carrano, E
   Eizirik, E
   Nakano-Oliveira, E
   Rigacci, ED
   Santos, EM
   Venticinque, EM
   Alexandrino, ER
   Ribeiro, EA
   Setz, E
   Rocha, ECLD
   Carvalho, EAR
   Rechenberg, E
   Fraga, ED
   Mendonca, EN
   D'Bastiani, E
   Isasi-Catala, E
   Guijosa-Guadarrama, E
   Ramalho, EE
   Gonzalez, E
   Hasui, E
   Saito, EN
   Fischer, E
   Aguiar, EF
   Rocha, ES
   Nambo, EDM
   de la Pena-Cuellar, E
   Castro, EP
   de Freitas, EB
   Pedo, E
   Rocha, FL
   Girardi, F
   Pereira, FD
   Soares, FAM
   Roque, FD
   Diaz-Santos, FG
   Patiu, FM
   do Nascimento, FO
   Ferreira, FK
   Diaz-Santos, F
   Fantacini, FM
   Pedrosa, F
   da Silva, FP
   Velez-Garcia, F
   Gomes, FBR
   da Silva, FG
   Michalski, F
   de Azevedo, FC
   de Barros, FC
   Santos, FD
   Abra, FD
   Ramalho, FD
   Hatano, FM
   Anaguano-Yancha, F
   Goncalves, F
   Pedroni, F
   Passos, FC
   Jacinavicius, FD
   Bonfim, FCG
   Puertas, FH
   Contreras-Moreno, FM
   Tortato, FR
   Santos, FM
   Chaves, FG
   Tirelli, FP
   Boas, FEV
   Rodrigues, FHG
   Ubaid, FK
   Grotta-Neto, F
   Palomares, F
   Souza, FL
   Costa, FE
   Franca, FGR
   Pinto, FR
   Aguiar, GL
   Hofmann, GS
   Heliodoro, G
   Duarte, GT
   de Andrade, GR
   Beca, G
   Zapata-Rios, G
   Gine, GAF
   Powell, GVN
   Fernandes, GW
   Forero-Medina, G
   Melo, GL
   Santana, GG
   Ciocheti, G
   Alves, GB
   Souto, GHBD
   Villarroel, GJ
   Porfirio, GED
   Batista, GO
   Behling, GM
   Crespo, GMA
   Mourao, GD
   Rezende, GZ
   Toledo, GAD
   Herrera, HM
   Prado, HA
   Bergallo, HD
   Secco, H
   Rajao, H
   Roig, HL
   Concone, HVB
   Duarte, H
   Ermenegildo, H
   Neto, HFP
   Quigley, H
   Lemos, HM
   Cabral, H
   Fernandes-Ferreira, H
   del Castillo, HF
   Ribeiro, IK
   Coelho, IP
   Franceschi, IC
   Melo, I
   Oliveira-Bevan, I
   Mourthe, I
   Bernardi, I
   de la Torre, JA
   Marinho-FIlho, J
   Martinez, J
   Perez, JXP
   Perez-Torres, J
   Bubadue, J
   Silveira, JR
   Seibert, JB
   Oliveira, JF
   Assis, JR
   De la Maza, J
   Hinojosa, J
   Metzger, JP
   Thompson, JJ
   Svenning, JC
   Gouvea, JA
   Souza, JRD
   Pincheira-Ulbrich, J
   Nodari, JZ
   Miranda, J
   Gebin, JCZ
   Giovanelli, JGR
   Rossi, JL
   Favoretti, JPP
   Villani, JP
   Just, JPG
   Souza-Alves, JP
   Costa, JF
   Rocha, J
   Polisar, J
   Sponchiado, J
   Cherem, JJ
   Marinho, JR
   Ziegler, J
   Cordeiro, J
   Silva, JDE
   Rodriguez-Pulido, JA
   dos Santos, JCC
   dos Reis, JC
   Mantovani, JE
   Ramirez, JFM
   Sarasola, JH
   Cartes, JL
   Duarte, JMB
   Longo, JM
   Dantas, JO
   Venancio, JO
   de Matos, JR
   Pires, JSR
   Hawes, JE
   Santos, JG
   Ruiz-Esparza, J
   Lanfranco, JAM
   Rudolf, JC
   Charre-Medellin, JF
   Zanon-Martinez, JI
   Pena-Mondragon, JL
   Krauer, JMC
   Arrabal, JP
   Beduschi, J
   Ilha, J
   Mata, JC
   Bonanomi, J
   Jordao, J
   de Almeida-Rocha, JM
   Pereira-Ribeiro, J
   Zanoni, JB
   Bogoni, JA
   Pacheco, JJC
   Palma, KMC
   Strier, KB
   Castro, KGR
   Didier, K
   Schuchmann, KL
   Chavez-Congrains, K
   Burs, K
   Ferraz, KMPMB
   Juarez, KM
   Flesher, K
   Morais, KDR
   Lautenschlager, L
   Grossel, LA
   Dahmer, LC
   de Almeida, LR
   Fornitano, L
   Barbosa, LDB
   Bailey, LL
   Barreto, LN
   Villalba, LM
   Magalhaes, LM
   Cullen, L
   Marques, L
   Costa, LM
   Silveira, L
   Moreira, LS
   Sartorello, L
   Oliveira, LD
   Gomes, LD
   Aguiar, LD
   da Silva, LH
   Mendonca, LS
   Valenzuela, LA
   Benavalli, L
   Dias, LCS
   Munhoes, LP
   Catenacci, L
   Rampim, LE
   de Paula, LM
   Nascimento, LA
   da Silva, LG
   Quintilham, L
   Segura, LR
   Perillo, LN
   Rezende, LR
   Retta, LM
   Rojas, LNS
   Guimaraes, LN
   Araujo, L
   da Silva, LZ
   Querido, LCD
   Verdade, LM
   Perera-Romero, LE
   Carvalho-Leite, LJ
   Hufnagel, L
   Bernardo, LRR
   Oliveira, LF
   Santos, LGRO
   Lyra, LH
   Borges, LHM
   Severo, MM
   Benchimol, M
   Quatrocchi, MG
   Martins, MZA
   Rodrigues, M
   Penteado, MJF
   Moraes, MFD
   Oliveira, MA
   Lima, MGM
   Ponzio, MD
   Cervini, M
   da Silva, M
   Passamani, M
   Villegas, MA
   dos Santos, MA
   Yamane, MH
   Jardim, MMD
   de Oliveira, ML
   Silveira, M
   Tortato, MA
   Figueiredo, MDL
   Vieira, MV
   Sekiama, ML
   da Silva, MAA
   Nunez, MB
   Siviero, MB
   Carrizo, MC
   Barros, MC
   Barros, MAS
   do Rosario, MCF
   Mora, MCP
   Jover, MDF
   Morandi, MED
   Huerta, ME
   Fernandes, MEA
   Sinani, MEV
   Iezzi, ME
   Pereira, MJR
   Vinassa, MLG
   Lorini, ML
   Jorge, MLSP
   Morini, MS
   Guenther, M
   Landis, MB
   Vale, MM
   Xavier, MS
   Tavares, MS
   Kaizer, M
   Velilla, M
   Bergel, MM
   Hartmann, MT
   da Silva, ML
   Rivero, M
   Munerato, MS
   da Silva, MX
   Zanin, M
   Marques, MI
   Haberfeld, M
   Di Bitetti, MS
   Bowler, M
   Galliez, M
   Ortiz-Moreno, ML
   Buschiazzo, M
   Montes, MA
   Alvarez, MR
   Melo-Dias, M
   Reis, MG
   Correa, MRJ
   Tobler, MW
   Gompper, ME
   Nunez-Regueiro, M
   Vecchi, MB
   Graipel, ME
   Godoi, MN
   Moura, MO
   Konzen, MQ
   Pardo, MV
   Beltrao, MG
   Mongelli, M
   Almeida, MO
   Gilmore, MP
   Schutte, M
   Faria, MB
   Luiz, MR
   de Paula, M
   Hidalgo-Mihart, MG
   Perilli, MLL
   Freitas-Junior, MC
   da Silva, MP
   Denkiewicz, NM
   Torres, NM
   Olifiers, N
   De Lima, ND
   de Albuquerque, NM
   Canassa, NF
   Curi, NHD
   Prestes, NP
   Falconi, N
   Gurgel-Filho, NM
   Pasqualotto, N
   Caceres, NC
   Peroni, N
   de la Sancha, NU
   Zanella, N
   Monroy-Vilchis, O
   Pays, O
   Arimoro, OA
   Ribeiro, OS
   Villalva, P
   Goncalves, PR
   Santos, PM
   Brennand, P
   Rocha, P
   Akkawi, P
   Cruz, P
   Ferreira, PM
   Prist, PR
   Martin, PS
   Arroyo-Gerala, P
   Auricchio, P
   Hartmann, PA
   Antas, PDZ
   Camargo, PHSA
   Marinho, PH
   Ruffino, PHP
   Prado, PI
   Martins, PW
   Cordeiro-Estrela, P
   Luna, P
   Sarmento, P
   Peres, PHF
   Galetti, PM
   de Castilho, PV
   Renaud, PC
   Scarascia, PO
   Cobra, PDA
   Lombardi, PM
   Bessa, R
   Reyna-Hurtado, R
   de Souza, RCC
   Hoogesteijn, RJ
   Alves, RSC
   Romagna, RS
   Silva, RL
   de Oliveira, R
   Beltrao-Mendes, R
   Alencar, RD
   Coutinho, R
   da Silva, RC
   Grando, RLSCC
   Matos, RG
   Araujo, RD
   Pedroso, RF
   Duraes, RMN
   Ribeiro, RLA
   Chagas, R
   Miotto, R
   Bonikowski, RTR
   Muylaert, RL
   Pagotto, RV
   Hilario, RR
   Faria, RT
   Bassini-Silva, R
   Sampaio, R
   Sartorello, R
   Pires, RA
   Hatakeyama, R
   Bianchi, RD
   Buitenwerf, R
   Wallace, R
   Paolino, RM
   Fusco-Costa, R
   Trovati, RG
   Tomasi, RJ
   Hack, ROE
   Magalhaes, RA
   Nobrega, RAD
   Nobre, RD
   Massara, RL
   Froes, RM
   Araujo, RPD
   Perez, RRL
   Jorge, RSP
   de Paula, RC
   Martins, R
   da Cunha, RGT
   Costa, RM
   Alves, RRN
   Garcia-Anleu, R
   Almeida, RPS
   Loachamin, RDC
   Andrade, RS
   Juarez, R
   Bordallo, SUA
   Guaragni, SA
   Carrillo-Percastegui, SE
   Seber, S
   Astete, S
   Hartz, SM
   Espinosa, S
   Solas, SA
   Lima, SR
   Silvestre, SM
   Machado, SAD
   Keuroghlian-Eaton, S
   Albanesi, S
   Costa, SA
   Bazilio, S
   Mendes, SL
   Althoff, SL
   Pinheiro, SD
   Napiwoski, SJ
   Ramirez, SF
   Talamoni, SA
   Age, SG
   Pereira, TC
   Moreira, TC
   Trigo, TC
   Gondim, TMD
   Karlovic, TC
   Cavalcante, T
   Maccarini, T
   Rodrigues, TF
   Timo, TPDE
   Monterrubio, TC
   Piovezan, U
   Cavarzere, V
   Towns, V
   Onofrio, VC
   Oliveira, VB
   Araujo, VC
   Melo, VL
   Kanaan, VT
   Iwakami, V
   Vale, V
   Picinatto, V
   Alberici, V
   Bastazini, VAG
   Orsini, VS
   Braz, VD
   Bonzi, VBR
   Layme, VMG
   Gaboardi, VTR
   Rocha, VJ
   Martins, WP
   Tomas, WM
   Hannibal, W
   Dattilo, W
   Silva, WR
   Endo, W
   Berce, W
   de la Cruz, YB
   Ribeiro, YGG
   Galetti, M
   Ribeiro, MC
AF Nagy-Reis, Mariana
   Oshima, Julia Emi de Faria
   Kanda, Claudia Zukeran
   Palmeira, Francesca Belem Lopes
   de Melo, Fabiano Rodrigues
   Morato, Ronaldo Goncalves
   Bonjorne, Lilian
   Magioli, Marcelo
   Leuchtenberger, Caroline
   Rohe, Fabio
   Lemos, Frederico Gemesio
   Martello, Felipe
   Alves-Eigenheer, Milene
   da Silva, Rafaela Aparecida
   Silveira dos Santos, Juliana
   Priante, Camila Fatima
   Bernardo, Rodrigo
   Rogeri, Patricia
   Assis, Julia Camara
   Gaspar, Lucas Pacciullio
   Tonetti, Vinicius Rodrigues
   Trinca, Cristiano Trape
   Ribeiro, Adauto de Souza
   Bocchiglieri, Adriana
   Hass, Adriani
   Canteri, Adriano
   Chiarello, Adriano Garcia
   Paglia, Adriano Pereira
   Pereira, Adriele Aparecida
   de Souza, Agnis Cristiane
   Gatica, Ailin
   Medeiro, Akyllam Zoppi
   Eriksson, Alan
   Costa, Alan Nilo
   Gonzalez-Gallina, Alberto
   Yanosky, Alberto A.
   Jesus de la Cruz, Alejandro
   Bertassoni, Alessandra
   Bager, Alex
   Bovo, Alex Augusto Abreu
   Cravino Mol, Alexandra
   Bezerra, Alexandra Maria Ramos
   Percequillo, Alexandre
   Vogliotti, Alexandre
   Costa Lopes, Alexandre Martins
   Keuroghlian, Alexine
   Zuniga Hartley, Alfonso Christopher
   Devlin, Allison L.
   de Paula, Almir
   Garcia-Olaechea, Alvaro
   Sanchez, Amadeo
   Aquino, Ana Carla Medeiros Morato
   Srbek-Araujo, Ana Carolina
   Ochoa, Ana Cecilia
   Tomazzoni, Ana Cristina
   Lacerda, Ana Cristyna Reis
   Bacellar, Ana Elisa de Faria
   Campelo, Ana Kellen Nogueira
   Herrera Victoria, Ana Maria
   Paschoal, Ana Maria de Oliveira
   Potrich, Ana Paula
   Gomes, Ana Paula Nascimento
   Olimpio, Ana Priscila Medeiros
   Cunha Costa, Ana Raissa
   Jacomo, Anah Tereza de Almeida
   Calaca, Analice Maria
   Jesus, Anamelia Souza
   de Barros Barban, Ananda
   Feijo, Anderson
   Pagoto, Anderson
   Rolim, Anderson Claudino
   Hermann, Andiara Paula
   Souza, Andiara Silos Moraes de Castro e
   Chein Alonso, Andre
   Monteiro, Andre
   Mendonca, Andre Faria
   Luza, Andre Luis
   Moura, Andre Luis Botelho
   da Silva, Andre Luiz Ferreira
   Lanna, Andre Monnerat
   Antunes, Andre Pinassi
   Nunes, Andre Valle
   Dechner, Andrea
   Carvalho, Andrea Siqueira
   Novaro, Andres Jose
   Scabin, Andressa Barbara
   Gatti, Andressa
   Nobre, Andrezza Bellotto
   Montanarin, Anelise
   Deffaci, Angela Camila
   de Albuquerque, Anna Carolina Figueiredo
   Mangione, Antonio Marcelo
   Pinto, Antonio Millas Silva
   Mendes Pontes, Antonio Rossano
   Bertoldi, Ariane Teixeira
   Calouro, Armando Muniz
   Fernandes, Arthur
   Ferreira, Arystene Nicodemo
   Ferreguetti, Atilla Colombo
   Rosa, Augusto Lisboa Martins
   Banhos, Aureo
   Francisco, Beatriz da Silva de Souza
   Cezila, Beatriz Azevedo
   Beisiegel, Beatriz de Mello
   de Thoisy, Benoit
   Ingberman, Bianca
   Neves, Bianca dos Santos
   Pereira-Silva, Brenda
   Bertagni de Camargo, Bruna
   Andrade, Bruna da Silva
   Santos, Bruna Silva
   Leles, Bruno
   Torres Parahyba Campos, Bruno Augusto
   Kubiak, Bruno Busnello
   Franca, Bruno Rodrigo de Albuquerque
   Saranholi, Bruno Henrique
   Pereira Mendes, Calebe
   Cantagallo Devids, Camila
   Pianca, Camila
   Rodrigues, Camila
   Islas, Camila Alvez
   de Lima, Camilla Angelica
   de Lima, Camilo Ribeiro
   Gestich, Carla Cristina
   Tedesco, Carla Denise
   De Angelo, Carlos
   Fonseca, Carlos
   Hass, Carlos
   Peres, Carlos A.
   Kasper, Carlos Benhur
   Durigan, Carlos Cesar
   Fragoso, Carlos Eduardo
   Verona, Carlos Eduardo
   Rocha, Carlos Frederico Duarte
   Salvador, Carlos Henrique
   Vieira, Carlos Leonardo
   Ruiz, Carmen Elena Barragan
   Cheida, Carolina Carvalho
   Sartor, Caroline Charao
   Espinosa, Caroline da Costa
   Fieker, Carolline Zatta
   Braga, Caryne
   Sanchez-Lalinde, Catalina
   Machado, Cauanne Iglesias Campos
   Cronemberger, Cecilia
   Luna, Cecilia Licariao
   Del Vechio, Christine
   Bernardo, Christine Steiner S.
   Hurtado, Cindy Meliza
   Lopes, Cintia M.
   da Rosa, Clarissa Alves
   Cinta, Claudia Cristina
   Costa, Claudia Guimaraes
   Zarate-Castaneda, Claudia Paola
   Novaes, Claudio Leite
   Jenkins, Clinton N.
   Seixas, Cristiana Simao
   Martin, Cristiane
   Zaniratto, Cristiane Patricia
   Lopez-Fuerte, Cristina Fabiola
   da Cunha, Cristina Jaques
   De-Carvalho, Crizanto Brito
   Chavez, Cuauhtemoc
   Santos, Cyntia Cavalcante
   Polli, Daiana Jeronimo
   Buscariol, Daiane
   Carreira, Daiane Cristina
   Galiano, Daniel
   Thornton, Daniel
   Ferraz, Daniel da Silva
   Lamattina, Daniela
   Moreno, Daniele Janina
   Moreira, Danielle Oliveira
   Farias, Danilo Augusto
   Barros-Battesti, Darci Moraes
   Tavares, Davi Castro
   Costa Braga, David
   Gaspar, Denise Alemar
   Friedeberg, Diana
   Astua, Diego
   Silva, Diego Afonso
   Viana, Diego Carvalho
   Lizcano, Diego J.
   Varela, Diego M.
   Loretto, Diogo
   Grabin, Diogo Maia
   Eaton, Donald P.
   Machado da Silva, Douglas
   Dias, Douglas de Matos
   Camara, Edeltrudes Maria Valadares Calaca
   Barbier, Eder
   Chavez-Gonzalez, Edgar
   Rocha, Ednaldo Candido
   Lima, Edson de Souza
   Carrano, Eduardo
   Eizirik, Eduardo
   Nakano-Oliveira, Eduardo
   Rigacci, Eduardo Delgado
   Santos, Eduardo Marques
   Venticinque, Eduardo Martins
   Alexandrino, Eduardo Roberto
   Abreu Ribeiro, Edvandro
   Setz, Eleonore
   Rocha, Eliana Cesar Laranjeira Duarte
   Carvalho, Elildo Alves Ribeiro, Jr.
   Rechenberg, Elisabete
   Fraga, Elmary da Costa
   Mendonca, Eloisa Neves
   D'Bastiani, Elvira
   Isasi-Catala, Emiliana
   Guijosa-Guadarrama, Emiliano
   Ramalho, Emiliano Esterci
   Gonzalez, Enrique
   Hasui, Erica
   Saito, Erica Naomi
   Fischer, Erich
   Aguiar, Erick Francisco
   Rocha, Erick Sekiama
   Martinez Nambo, Erik Daniel
   de la Pena-Cuellar, Erika
   Castro, Erika Paula
   de Freitas, Evellyn Borges
   Pedo, Ezequiel
   Rocha, Fabiana Lopes
   Girardi, Fabiane
   Pereira, Fabiane de Aguiar
   Soares, Fabio Angelo Melo
   Roque, Fabio de Oliveira
   Diaz-Santos, Fabio Gabriel
   Patiu, Fabio Mello
   do Nascimento, Fabio Oliveira
   Keesen Ferreira, Fabiola
   Diaz-Santos, Fabricio
   Moreli Fantacini, Felipe
   Pedrosa, Felipe
   Pessoa da Silva, Felipe
   Velez-Garcia, Felipe
   Gomes, Felipe Bittioli R.
   Guedes da Silva, Fernanda
   Michalski, Fernanda
   de Azevedo, Fernanda Cavalcanti
   de Barros, Fernanda Cristina
   Santos, Fernanda da Silva
   Abra, Fernanda Delborgo
   Ramalho, Fernanda do Passo
   Hatano, Fernanda Martins
   Anaguano-Yancha, Fernando
   Goncalves, Fernando
   Pedroni, Fernando
   Passos, Fernando C.
   Jacinavicius, Fernando de Castro
   Bonfim, Fernando Cesar Goncalves
   Puertas, Fernando Henrique
   Contreras-Moreno, Fernando M.
   Tortato, Fernando Rodrigo
   Santos, Filipe Martins
   Chaves, Flavia Guimaraes
   Tirelli, Flavia Pereira
   Vilas Boas, Flavio Eduardo
   Rodrigues, Flavio Henrique Guimaraes
   Ubaid, Flavio Kulaif
   Grotta-Neto, Francisco
   Palomares, Francisco
   Souza, Franco Leandro
   Costa, Francys Emanuelle
   Franca, Frederico G. R.
   Ramirez Pinto, Fredy
   Aguiar, Gabriel Lima
   Hofmann, Gabriel Selbach
   Heliodoro, Gabriela
   Duarte, Gabriela Teixeira
   Ribeiro de Andrade, Gabrielle
   Beca, Gabrielle
   Zapata-Rios, Galo
   Gine, Gaston Andres Fernandez
   Powell, George V. N.
   Wilson Fernandes, Geraldo
   Forero-Medina, German
   Melo, Geruza L.
   Santana, Gindomar Gomes
   Ciocheti, Giordano
   Alves, Giselle Bastos
   Souto, Glauber Henrique Borges de Oliveira
   Villarroel, Glenda Jessica
   Porfirio, Grasiela Edith de Oliveira
   Batista, Graziele Oliveira
   Behling, Greici Maia
   Ayala Crespo, Guido Marcos
   Mourao, Guilherme de Miranda
   Rezende, Guilherme Zamarian
   Toledo, Gustavo Alves da Costa
   Herrera, Heitor Miraglia
   Alves Prado, Helena
   Bergallo, Helena de Godoy
   Secco, Helio
   Rajao, Henrique
   Roig, Henrique Llacer
   Concone, Henrique Villas Boas
   Duarte, Herbert
   Ermenegildo, Hiago
   Ferreira Paulino Neto, Hipolito
   Quigley, Howard
   Lemos, Hudson Macedo
   Cabral, Hugo
   Fernandes-Ferreira, Hugo
   del Castillo, Hugo Fernando
   Ribeiro, Igor Kintopp
   Coelho, Igor Pfeifer
   Franceschi, Ingridi Camboim
   Melo, Isabel
   Oliveira-Bevan, Isabella
   Mourthe, Italo
   Bernardi, Itibere
   de la Torre, J. Antonio
   Marinho-Filho, Jader
   Martinez, Jaime
   Palacios Perez, Jaime Xavier
   Perez-Torres, Jairo
   Bubadue, Jamile
   Silveira, Jana Rangel
   Seibert, Jardel Brandao
   Oliveira, Jasmim Felipe
   Assis, Jasmine Resende
   De la Maza, Javier
   Hinojosa, Javier
   Metzger, Jean Paul
   Thompson, Jeffrey James
   Svenning, Jens-Christian
   Gouvea, Jessica Abonizio
   Souza, Jesus Rodrigues Domingos
   Pincheira-Ulbrich, Jimmy
   Nodari, Joana Zorzal
   Miranda, Joao
   Zecchini Gebin, Joao Carlos
   Giovanelli, Joao Gabriel Ribeiro
   Rossi Junior, Joao Luiz
   Pandini Favoretti, Joao Paulo
   Villani, Joao Paulo
   Just, Joao Paulo Gava
   Souza-Alves, Joao Pedro
   Costa, Jociel Ferreira
   Rocha, Joedison
   Polisar, John
   Sponchiado, Jonas
   Cherem, Jorge Jose
   Marinho, Jorge Reppold
   Ziegler, Jorn
   Cordeiro, Jose
   Silva Junior, Jose DeSousa e
   Rodriguez-Pulido, Jose Ariel
   Chaves dos Santos, Jose Carlos
   dos Reis Junior, Jose Clemensou
   Mantovani, Jose Eduardo
   Moreira Ramirez, Jose Fernando
   Sarasola, Jose Hernan
   Cartes, Jose Luis
   Duarte, Jose Mauricio Barbanti
   Longo, Jose Milton
   Dantas, Jose Oliveira
   Venancio, Jose Otavio
   de Matos, Jose Roberto
   Pires, Jose Salatiel Rodrigues
   Hawes, Joseph E.
   Santos, Joyce Goncalves
   Ruiz-Esparza, Juan
   Martinez Lanfranco, Juan Andres
   Rudolf, Juan Carlos
   Charre-Medellin, Juan Felipe
   Zanon-Martinez, Juan Ignacio
   Pena-Mondragon, Juan L.
   Campos Krauer, Juan Manuel
   Arrabal, Juan Pablo
   Beduschi, Julia
   Ilha, Julia
   Mata, Julia Carolina
   Bonanomi, Juliana
   Jordao, Juliana
   de Almeida-Rocha, Juliana Monteiro
   Pereira-Ribeiro, Juliane
   Zanoni, Juliani Bruna
   Bogoni, Juliano Andre
   Chacon Pacheco, Julio Javier
   Contreras Palma, Kamila Marianne
   Strier, Karen B.
   Rodriguez Castro, Karen Giselle
   Didier, Karl
   Schuchmann, Karl L.
   Chavez-Congrains, Karla
   Burs, Kathrin
   Ferraz, Katia M. P. M. B.
   Juarez, Keila Macfadem
   Flesher, Kevin
   Morais, Kimberly Danielle Rodrigues
   Lautenschlager, Lais
   Grossel, Lais Aline
   Dahmer, Lais Camila
   de Almeida, Lana Resende
   Fornitano, Larissa
   Barbosa, Larissa de Nazare Barros
   Bailey, Larissa L.
   Barreto, Larissa Nascimento
   Villalba, Laura Magnolia
   Magalhaes, Laura Martins
   Cullen, Laury, Jr.
   Marques, Leandro
   Marques Costa, Leonardo
   Silveira, Leandro
   Moreira, Leandro Santana
   Sartorello, Leonardo
   Oliveira, Leonardo de Carvalho
   Gomes, Leonardo de Paula
   Aguiar, Leonardo dos Santos
   da Silva, Leonardo Henrique
   Mendonca, Leonardo Siqueira
   Valenzuela, Leonor Adriana
   Benavalli, Leticia
   Dias, Leticia Coutinho Sangy
   Munhoes, Leticia Prado
   Catenacci, Lilian
   Rampim, Lilian Elaine
   de Paula, Livia Maria
   Nascimento, Lorena Anne
   Goncalves da Silva, Lucas
   Quintilham, Lucas
   Ramis Segura, Lucas
   Perillo, Lucas Neves
   Rezende, Lucas Rodrigo
   Martinez Retta, Lucia
   Rojas, Lucia Nathaly Stefany
   Guimaraes, Luiza Neves
   Araujo, Luciana
   Zago da Silva, Luciana
   Querido, Luciano Carramaschi de Alagao
   Verdade, Luciano Martins
   Perera-Romero, Lucy E.
   Carvalho-Leite, Ludimila Juliele
   Hufnagel, Ludmila
   Rezende Bernardo, Luis Renato
   Oliveira, Luiz Flamarion
   Oliveira Santos, Luiz Gustavo Rodrigues
   Lyra, Luiz Henrique
   Borges, Luiz Henrique Medeiros
   Severo, Magnus Machado
   Benchimol, Maira
   Quatrocchi, Maira Giuliana
   Martins, Maisa Ziviani Alves
   Rodrigues, Manoel
   Penteado, Marcel Jose Franco
   Figueredo Duarte Moraes, Marcela
   Oliveira, Marcela Alvares
   Lima, Marcela Guimaraes Moreira
   Ponzio, Marcella do Carmo
   Cervini, Marcelo
   da Silva, Marcelo
   Passamani, Marcelo
   Villegas, Marcelo Alejandro
   dos Santos Junior, Marcelo Augusto
   Yamane, Marcelo Hideki
   Jardim, Marcia Maria de Assis
   Leite de Oliveira, Marcio
   Silveira, Marcos
   Tortato, Marcos Adriano
   Figueiredo, Marcos de Souza Lima
   Vieira, Marcus Vinicius
   Sekiama, Margareth L.
   Andrade da Silva, Maria Augusta
   Nunez, Maria Beatriz
   Siviero, Maria Brunini
   Carrizo, Maria Celina
   Barros, Maria Claudene
   Barros, Marilia A. S.
   do Rosario, Maria Cristina Ferreira
   Penuela Mora, Maria Cristina
   Fleytas Jover, Maria del Carmen
   Morandi, Maria Elisa de Freitas
   Huerta, Maria Emilia
   Fernandes, Maria Emilia Avelar
   Viscarra Sinani, Maria Estela
   Iezzi, Maria Eugenia
   Ramos Pereira, Maria Joao
   Gomez Vinassa, Maria Laura
   Lorini, Maria Lucia
   Jorge, Maria Luisa S. P.
   Morini, Maria Santina
   Guenther, Mariana
   Landis, Mariana Bueno
   Vale, Mariana M.
   Xavier, Mariana Sampaio
   Tavares, Mariana Silva
   Kaizer, Mariane
   Velilla, Marianela
   Bergel, Mariano Maudet
   Hartmann, Marilia Teresinha
   Lima da Silva, Marina
   Rivero, Marina
   Salles Munerato, Marina
   Xavier da Silva, Marina
   Zanin, Marina
   Marques, Marinez Isaac
   Haberfeld, Mario
   Di Bitetti, Mario S.
   Bowler, Mark
   Galliez, Maron
   Ortiz-Moreno, Martha Lucia
   Buschiazzo, Martin
   Montes, Martin Alejandro
   Alvarez, Martin R.
   Melo-Dias, Mateus
   Reis, Matheus Goncalves
   Correa, Matheus Rocha Jorge
   Tobler, Mathias W.
   Gompper, Matthew E.
   Nunez-Regueiro, Mauricio
   Brandao Vecchi, Mauricio
   Graipel, Mauricio Eduardo
   Godoi, Mauricio Neves
   Moura, Mauricio O.
   Konzen, Mauricio Quoos
   Pardo, Maximiliano Victor
   Beltrao, Mayara Guimaraes
   Mongelli, Melissa
   Almeida, Meyline Oliveira
   Gilmore, Michael P.
   Schutte, Michel
   Faria, Michel Barros
   Luiz, Micheli Ribeiro
   de Paula, Milton
   Hidalgo-Mihart, Mircea G.
   Perilli, Miriam Lucia Lages
   Freitas-Junior, Mozart Caetano
   da Silva, Murillo Prado
   Denkiewicz, Natalia Mariana
   Torres, Natalia Mundim
   Olifiers, Natalie
   De Lima, Natani Da Silva
   de Albuquerque, Natasha Moraes
   Canassa, Nathalia Fernandes
   de Almeida Curi, Nelson Henrique
   Prestes, Nemora Pauletti
   Falconi, Nereyda
   Gurgel-Filho, Newton Mota
   Pasqualotto, Nielson
   Caceres, Nilton C.
   Peroni, Nivaldo
   de la Sancha, Noe U.
   Zanella, Noeli
   Monroy-Vilchis, Octavio
   Pays, Olivier
   Arimoro, Omolabake Alhambra
   Ribeiro, Otavio Santi
   Villalva, Pablo
   Goncalves, Pablo Rodrigues
   Santos, Paloma Marques
   Brennand, Pamella
   Rocha, Patricio
   Akkawi, Paula
   Cruz, Paula
   Ferreira, Paula Modenesi
   Prist, Paula Ribeiro
   Martin, Paula Sanches
   Arroyo-Gerala, Paulina
   Auricchio, Paulo
   Hartmann, Paulo Afonso
   Antas, Paulo de Tarso Zuquim
   Camargo, Paulo H. S. A.
   Marinho, Paulo Henrique
   Ruffino, Paulo Henrique Peira
   Prado, Paulo Inacio
   Martins, Paulo Wesley
   Cordeiro-Estrela, Pedro
   Luna, Pedro
   Sarmento, Pedro
   Faria Peres, Pedro Henrique
   Galetti, Pedro Manoel, Jr.
   de Castilho, Pedro Volkmer
   Renaud, Pierre-Cyril
   Scarascia, Pietro Oliveira
   Cobra, Priscilla De Paula Andrade
   Lombardi, Pryscilla Moura
   Bessa, Rafael
   Reyna-Hurtado, Rafael
   de Souza, Rafael Cerqueira Castro
   Hoogesteijn, Rafael Jan
   Alves, Rafael Souza Cruz
   Romagna, Rafael Spilere
   Silva, Ramon Lima
   de Oliveira, Ramonna
   Beltrao-Mendes, Raone
   Alencar, Raony de Macedo
   Coutinho, Raphaella
   da Silva, Raquel Costa
   Caribe Grando, Raquel L. S. C.
   Matos, Rayanne Gama
   Araujo, Raylenne da Silva
   Pedroso, Rayssa Faria
   Duraes, Rayssa Mainette Nantes
   Ribeiro, Renan Lieto Alves
   Chagas, Renata
   Miotto, Renata
   Twardowsky Ramalho Bonikowski, Renata
   Muylaert, Renata Lara
   Pagotto, Renata Valls
   Hilario, Renato Richard
   Faria, Rhayssa Terra
   Bassini-Silva, Ricardo
   Sampaio, Ricardo
   Sartorello, Ricardo
   Pires, Ricardo Araujo
   Hatakeyama, Richard
   Bianchi, Rita de Cassia
   Buitenwerf, Robert
   Wallace, Robert
   Paolino, Roberta Montanheiro
   Fusco-Costa, Roberto
   Trovati, Roberto Guilherme
   Tomasi, Roberto Junior
   Espindola Hack, Robson Odeli
   Magalhaes, Rodolfo Assis
   Nobrega, Rodrigo Affonso de Albuquerque
   Nobre, Rodrigo de Almeida
   Massara, Rodrigo Lima
   Froes, Rodrigo Medina
   Araujo, Rodrigo Paulo da Cunha
   Leon Perez, Rodrigo Raul
   Jorge, Rodrigo Silva Pinto
   de Paula, Rogerio Cunha
   Martins, Rogerio
   da Cunha, Rogerio Grassetto Teixeira
   Costa, Romulo
   Alves, Romulo Romeu Nobrega
   Garcia-Anleu, Rony
   Santos Almeida, Rony Peterson
   Cueva Loachamin, Ruben Dario
   Andrade, Rubia Santana
   Juarez, Rugieri
   Bordallo, Samanta Uchoa
   Guaragni, Samara Arsego
   Carrillo-Percastegui, Samia E.
   Seber, Samile
   Astete, Samuel
   Hartz, Sandra Maria
   Espinosa, Santiago
   Alvarez Solas, Sara
   Ramos Lima, Saulo
   Silvestre, Saulo Meneses
   Machado, Savio Augusto de Souza
   Keuroghlian-Eaton, Sean
   Albanesi, Sebastian
   Costa, Sebastian Andres
   Bazilio, Sergio
   Mendes, Sergio Lucena
   Althoff, Sergio Luiz
   Pinheiro, Shery Duque
   Napiwoski, Silvio Junior
   Fernandez Ramirez, Sixto
   Talamoni, Sonia Aparecida
   Age, Stefani Gabrieli
   Pereira, Taigua Correa
   Moreira, Tainah Cruz
   Trigo, Tatiane Campos
   Gondim, Tayana Mendonca da Silva
   Karlovic, Thamiris Christina
   Cavalcante, Thiago
   Maccarini, Thiago
   Rodrigues, Thiago Ferreira
   Timo, Thiago Philipe
   Monterrubio, Tiberio Cesar
   Piovezan, Ubiratan
   Cavarzere, Vagner
   Towns, Valeria
   Onofrio, Valeria Castilho
   Oliveira, Valeska Buchemi
   Araujo, Valquiria Cabral
   Melo, Vanessa Lazaro
   Kanaan, Vanessa Tavares
   Iwakami, Victor
   Vale, Victor
   Picinatto Filho, Vilmar
   Alberici, Vinicius
   Bastazini, Vinicius A. G.
   Orsini, Vinicius Santana
   Braz, Vivian da Silva
   Rojas Bonzi, Viviana B.
   Guedes Layme, Viviane Maria
   Gaboardi, Viviane Telles Rodrigues
   Rocha, Vlamir Jose
   Martins, Waldney Pereira
   Tomas, Walfrido Moraes
   Hannibal, Wellington
   Dattilo, Wesley
   Silva, Wesley R.
   Endo, Whaldener
   Berce, William
   Bravata de la Cruz, Yaribeth
   Ribeiro, Yuri Geraldo Gomes
   Galetti, Mauro
   Ribeiro, Milton C.
TI NEOTROPICAL CARNIVORES: a data set on carnivore distribution in the
   Neotropics
SO ECOLOGY
LA English
DT Article; Data Paper
DE canidae; carnivores; conservation; data paper; felidae; mammal;
   neotropical region; occurrence; predator; species distribution
AB Mammalian carnivores are considered a key group in maintaining ecological health and can indicate potential ecological integrity in landscapes where they occur. Carnivores also hold high conservation value and their habitat requirements can guide management and conservation plans. The order Carnivora has 84 species from 8 families in the Neotropical region: Canidae; Felidae; Mephitidae; Mustelidae; Otariidae; Phocidae; Procyonidae; and Ursidae. Herein, we include published and unpublished data on native terrestrial Neotropical carnivores (Canidae; Felidae; Mephitidae; Mustelidae; Procyonidae; and Ursidae). NEOTROPICAL CARNIVORES is a publicly available data set that includes 99,605 data entries from 35,511 unique georeferenced coordinates. Detection/non-detection and quantitative data were obtained from 1818 to 2018 by researchers, governmental agencies, non-governmental organizations, and private consultants. Data were collected using several methods including camera trapping, museum collections, roadkill, line transect, and opportunistic records. Literature (peer-reviewed and grey literature) from Portuguese, Spanish and English were incorporated in this compilation. Most of the data set consists of detection data entries (n = 79,343; 79.7%) but also includes non-detection data (n = 20,262; 20.3%). Of those, 43.3% also include count data (n = 43,151). The information available in NEOTROPICAL CARNIVORES will contribute to macroecological, ecological, and conservation questions in multiple spatio-temporal perspectives. As carnivores play key roles in trophic interactions, a better understanding of their distribution and habitat requirements are essential to establish conservation management plans and safeguard the future ecological health of Neotropical ecosystems. Our data paper, combined with other large-scale data sets, has great potential to clarify species distribution and related ecological processes within the Neotropics. There are no copyright restrictions and no restriction for using data from this data paper, as long as the data paper is cited as the source of the information used. We also request that users inform us of how they intend to use the data.
EM mariana.nbreis@gmail.com
RI Prado, Paulo I/G-3353-2012; Magioli, Marcelo/L-7809-2014; Cáceres,
   Nilton C./H-6899-2012; Passamani, Marcelo/ABG-8886-2020; Monroy-Vilchis,
   Octavio/H-8942-2019; Hartz, Sandra Maria/A-8052-2012; Benchimol, Maíra
   MB/I-7664-2014; Lizcano, Diego J./E-6893-2011; Tavares, Davi
   Castro/H-7960-2015; Massara, Rodrigo Lima/Q-2223-2015; Grossel,
   Laís/AAR-5708-2021; SARANHOLI, BRUNO HENRIQUE/AAK-2395-2020; de
   Oliveira, Márcio L/F-3792-2012; Sponchiado, Jonas/E-4501-2013;
   Figueiredo, Marcos S. L./A-9720-2014; Setz, Eleonore Z F/C-1050-2012;
   Nobrega, Rodrigo Affonso Albuquerque/M-9668-2013; Viana,
   Diego/AAG-6445-2021; Oliveira, Marcela Alvares/X-7458-2019; Peroni,
   Nivaldo/AAU-4701-2020; Cronemberger, Cecilia/O-5663-2016; Chiarello,
   Adriano Garcia/G-2510-2012; Gestich, Carla Cristina/Q-2833-2017; Braga,
   Caryne/G-2626-2014; Junior, Vagner A Cavarzere/S-3435-2016; Sartor,
   Caroline Charão/D-7509-2018; PACHECO, JULIO JAVIER CHACÓN/L-2550-2013;
   Layme, Viviane/AAR-1800-2021; Peres, Carlos Augusto/N-8275-2019;
   Rigacci, Eduardo/AAC-1793-2021; CARRANO, EDUARDO/ABA-5037-2021; Alves,
   Rômulo Romeu Nóbrega/A-7026-2009; Battesti, Darci Moraes
   Barros/A-6804-2018; Tonetti, Vinicius Rodrigues/I-3018-2015; Azevedo,
   Fernanda Cavalcanti/J-1734-2014; Nunes, André/AAF-5792-2021; da Cunha,
   Rogério Grassetto Teixeira/J-1738-2012; Pasqualotto,
   Nielson/AAF-7015-2021; Percequillo, Alexandre R/C-3067-2012; Passos,
   Fernando C/H-1073-2012; DE TARSO ZUQUIM ANTAS, PAULO/AAK-9499-2021;
   CHAVEZ, CUAUHTÉMOC/C-5754-2019; Novaes, Claudio/AAH-2137-2021;
   /AAT-4881-2021; Ferreguetti, Atilla/D-1163-2019; da Silva Ferraz,
   Daniel/F-3669-2016; Silva, Rafaela/D-8593-2019; Bassini-Silva,
   Ricardo/X-3935-2019; Perillo, Lucas/ABG-5303-2020; Jenkins,
   Clinton/D-6134-2011; Paglia, Adriano P/A-7965-2012; Azevedo,
   Fernanda/AAB-4277-2019; Vale, Mariana M./I-9408-2012; Bonikowski, Renata
   Twardowsky Ramalho/AAO-8303-2021; Kaizer, Mariane C/P-2116-2017; Santos,
   Filipe Martins/M-1061-2019; da Silva, Felipe Pessoa/AAV-6536-2020;
   Fonseca, Carlos MMS/D-9744-2011; Oliveira-Santos, Luiz Gustavo
   Rodrigues/F-9562-2012; Bezerra, Alexandra Maria Ramos/H-2180-2012;
   Souza-Alves, João Pedro/Q-9316-2019; Fernandes, Geraldo/AAN-5602-2021;
   Metzger, Jean Paul/C-2514-2012; Bergallo, Helena Godoy/F-9257-2011;
   Bailey, Larissa/ABH-3513-2020; Talamoni, Sônia Aparecida/B-9422-2013;
   Sponchiado, Jonas/AAT-3039-2021; Paschoal, Ana Maria O/N-6631-2016;
   Zanin, Marina/AAN-6327-2020; Svenning, Jens-Christian/C-8977-2012; Melo,
   Geruza Leal/P-7449-2016; da Silva, Lucas Gonçalves/B-8702-2014; Soares,
   Fábio Angelo/ABD-6311-2020; Beltrão, Mayara Guimarães/K-1432-2014;
   Ortiz-Moreno, Martha Lucia/K-1481-2017; Hilário, Renato/A-7158-2013;
   Luza, André Luís/O-1134-2017; Guenther, Mariana/A-5695-2013; de C
   Jacinavicius, Fernando/F-2856-2015; Garcia-Anleu, Rony/AAA-6992-2022;
   Cobra, Priscilla/AAC-2486-2022; Rocha, Ednaldo Cândido/K-3031-2015;
   Rodrigues, Thiago F/L-4273-2018; Mendonça, André F./D-5997-2013; Rocha,
   Patrício/B-3875-2013; Cavarzere, Vagner/I-2971-2015; Camargo, Paulo H.
   S. A./C-3271-2018; Barbier, Eder/C-3515-2012; Pônzio,
   Marcella/AAW-1617-2020; Lemos, Frederico Gemesio/G-1991-2014; Peroni,
   Nivaldo/ABA-2663-2021; Costa, Alan Nilo/F-2836-2014; Ubaid,
   Flávio/ABA-8231-2021; Michalski, Fernanda/J-4691-2012; de Moura Bubadué,
   Jamile/G-5378-2015; Prado, Paulo Inácio/AAN-6512-2021; Moura, Mauricio
   O/B-1063-2013; Eizirik, Eduardo/K-8034-2012; Pereira-Ribeiro,
   Juliane/D-4371-2017; Karlovic, Thamíris C./M-5088-2017; Astúa,
   Diego/A-3583-2010; Cordeiro, Jose/D-1943-2016; Paulo da Cunha Araujo,
   Rodrigo/U-9219-2018; Zuniga Hartley, Alfonso Christopher/O-4283-2016;
   Goncalves Bonfim, Fernando Cesar/D-3356-2019; Perez-Torres,
   Jairo/F-1395-2010; Alexandrino, Eduardo Roberto/F-8261-2013; Galiano,
   Daniel/O-1671-2013; Astete, Samuel/M-8585-2018; Assis, Julia
   Camara/P-4264-2018; Fragoso, Carlos Eduardo/U-6488-2017; Palmeira,
   Francesca B. L./B-3646-2015; Bonjorne de Almeida, Lilian/P-3746-2018;
   Venticinque, Eduardo/G-8961-2015
OI Magioli, Marcelo/0000-0003-0865-102X; Cáceres, Nilton
   C./0000-0003-4904-0604; Passamani, Marcelo/0000-0002-0940-4074;
   Monroy-Vilchis, Octavio/0000-0003-3159-6014; Hartz, Sandra
   Maria/0000-0002-6536-1072; Benchimol, Maíra MB/0000-0002-1238-1619;
   Lizcano, Diego J./0000-0002-9648-0576; Tavares, Davi
   Castro/0000-0002-6811-9572; Massara, Rodrigo Lima/0000-0003-1221-2185;
   SARANHOLI, BRUNO HENRIQUE/0000-0002-8221-3557; de Oliveira, Márcio
   L/0000-0002-7705-0626; Sponchiado, Jonas/0000-0002-1267-1763;
   Figueiredo, Marcos S. L./0000-0003-0558-911X; Setz, Eleonore Z
   F/0000-0001-7638-7086; Viana, Diego/0000-0002-3302-9892; Oliveira,
   Marcela Alvares/0000-0002-4129-993X; Peroni,
   Nivaldo/0000-0002-6770-5377; Cronemberger, Cecilia/0000-0002-0704-0262;
   Chiarello, Adriano Garcia/0000-0003-1914-5480; Gestich, Carla
   Cristina/0000-0002-3906-025X; Braga, Caryne/0000-0003-2671-0206;
   PACHECO, JULIO JAVIER CHACÓN/0000-0002-7770-3615; Layme,
   Viviane/0000-0002-2490-777X; Peres, Carlos Augusto/0000-0002-1588-8765;
   Alves, Rômulo Romeu Nóbrega/0000-0001-6824-0797; Battesti, Darci Moraes
   Barros/0000-0002-8541-2252; Tonetti, Vinicius
   Rodrigues/0000-0003-2263-5608; Azevedo, Fernanda
   Cavalcanti/0000-0002-2424-6860; da Cunha, Rogério Grassetto
   Teixeira/0000-0002-2368-3540; Pasqualotto, Nielson/0000-0002-8283-4759;
   Percequillo, Alexandre R/0000-0002-7892-8912; DE TARSO ZUQUIM ANTAS,
   PAULO/0000-0002-8295-6297; CHAVEZ, CUAUHTÉMOC/0000-0003-2201-4748;
   Novaes, Claudio/0000-0002-1692-369X; Ferreguetti,
   Atilla/0000-0002-5139-8835; da Silva Ferraz, Daniel/0000-0001-7919-1433;
   Silva, Rafaela/0000-0002-0132-0124; Bassini-Silva,
   Ricardo/0000-0002-9568-4120; Perillo, Lucas/0000-0003-4291-4452;
   Jenkins, Clinton/0000-0003-2198-0637; Paglia, Adriano
   P/0000-0001-9957-5506; Azevedo, Fernanda/0000-0002-2424-6860; Vale,
   Mariana M./0000-0003-0734-4925; Kaizer, Mariane C/0000-0001-9105-9478;
   Santos, Filipe Martins/0000-0003-2032-8129; da Silva, Felipe
   Pessoa/0000-0003-1411-1249; Fonseca, Carlos MMS/0000-0001-6559-7133;
   Oliveira-Santos, Luiz Gustavo Rodrigues/0000-0001-9632-5173; Bezerra,
   Alexandra Maria Ramos/0000-0002-7972-5535; Souza-Alves, João
   Pedro/0000-0002-8517-1276; Fernandes, Geraldo/0000-0003-1559-6049;
   Metzger, Jean Paul/0000-0002-0087-5240; Bergallo, Helena
   Godoy/0000-0001-9771-965X; Talamoni, Sônia
   Aparecida/0000-0002-6411-1805; Paschoal, Ana Maria
   O/0000-0001-8560-1056; Zanin, Marina/0000-0001-7112-7381; Svenning,
   Jens-Christian/0000-0002-3415-0862; Melo, Geruza
   Leal/0000-0002-2384-3786; da Silva, Lucas Gonçalves/0000-0002-7993-9015;
   Ortiz-Moreno, Martha Lucia/0000-0003-0172-9111; Hilário,
   Renato/0000-0002-0346-0921; Luza, André Luís/0000-0003-0302-529X;
   Guenther, Mariana/0000-0002-3104-3105; de C Jacinavicius,
   Fernando/0000-0002-5503-3120; Cobra, Priscilla/0000-0002-7553-0212;
   Rocha, Ednaldo Cândido/0000-0002-2554-777X; Rodrigues, Thiago
   F/0000-0003-1972-0043; Mendonça, André F./0000-0002-8248-0639; Rocha,
   Patrício/0000-0003-1661-3779; Cavarzere, Vagner/0000-0003-0510-4557;
   Camargo, Paulo H. S. A./0000-0001-9081-4558; Barbier,
   Eder/0000-0001-5068-7048; Pônzio, Marcella/0000-0003-2901-5794; Lemos,
   Frederico Gemesio/0000-0002-3027-5713; Peroni,
   Nivaldo/0000-0002-6770-5377; Costa, Alan Nilo/0000-0002-7396-6370;
   Ubaid, Flávio/0000-0001-8604-1206; Michalski,
   Fernanda/0000-0002-8074-9964; de Moura Bubadué,
   Jamile/0000-0001-7069-996X; Prado, Paulo Inácio/0000-0002-7174-5005;
   Moura, Mauricio O/0000-0001-7948-2986; Eizirik,
   Eduardo/0000-0002-9658-0999; Pereira-Ribeiro,
   Juliane/0000-0002-0762-337X; Karlovic, Thamíris C./0000-0001-8409-2958;
   Astúa, Diego/0000-0002-9573-6437; CONTRERAS MORENO, FERNANDO
   MARCOS/0000-0002-5927-4925; Peres, Pedro Henrique/0000-0002-3158-0963;
   Barros, Marilia/0000-0002-3828-6433; Souza, Jesus/0000-0002-8384-3294;
   Cordeiro, Jose/0000-0001-5821-8764; Bowler, Mark/0000-0001-5236-3477;
   Garcia-Olaechea, Alvaro/0000-0002-2288-8923; Paulo da Cunha Araujo,
   Rodrigo/0000-0002-9250-6767; Santos, Paloma Marques/0000-0002-6932-1406;
   Alvarez Solas, Sara/0000-0002-8267-9816; Nobrega,
   Rodrigo/0000-0001-7058-5903; Zuniga Hartley, Alfonso
   Christopher/0000-0002-3302-1268; Trigo, Tatiane/0000-0003-3694-6802;
   Hurtado, Cindy/0000-0002-7958-236X; Oshima, Julia Emi de
   Faria/0000-0003-1545-768X; Galetti, Mauro/0000-0002-8187-8696; Tobler,
   Mathias/0000-0002-8587-0560; Sartor, Caroline/0000-0002-3552-0140;
   Morato, Ronaldo/0000-0002-8304-9779; Lamattina,
   Daniela/0000-0002-5926-8234; Goncalves Bonfim, Fernando
   Cesar/0000-0002-9924-830X; Perez-Torres, Jairo/0000-0001-7121-6210;
   Carramaschi de Alagao Querido, Luciano/0000-0002-7687-8413; Alexandrino,
   Eduardo Roberto/0000-0003-3088-4524; Grotta Neto,
   Francisco/0000-0002-2390-936X; Galiano, Daniel/0000-0003-1428-8634;
   Rodriguez Castro, Karen Giselle/0000-0002-0333-6402; Astete,
   Samuel/0000-0003-0774-9068; Espinosa, Santiago/0000-0002-7416-7167;
   Penuela Mora, Maria Cristina/0000-0002-9611-1359; Assis, Julia
   Camara/0000-0003-1104-7851; Fragoso, Carlos Eduardo/0000-0001-8971-2896;
   Palmeira, Francesca B. L./0000-0002-7597-1157; Reis,
   AlessanRSS/0000-0001-8486-7469; Hawes, Joseph/0000-0003-0053-2018;
   Kasper, Carlos Benhur/0000-0002-7089-6119; Campelo,
   Ana/0000-0002-4493-8434; Bonjorne de Almeida,
   Lilian/0000-0003-2485-5399; Schuchmann, Karl-L/0000-0002-3233-8917;
   Venticinque, Eduardo/0000-0002-3455-9107; Galvao Bastazini, Vinicius
   Augusto/0000-0001-5270-0621; Melo, Fabiano Rodrigues
   de/0000-0001-9958-2036; Endo, Whaldener/0000-0002-7305-4398;
   Guijosa-Guadarrama, Emiliano/0000-0003-2833-4646
FU Agencia Nacional de Promoción Científica y Tecnológica, Argentina
   (ANPCyT)ANPCyT [1908, PICT 2013 #1904, 1905, 1906, 1907] Funding Source:
   Medline; Brazilian National Council for Scientific and Technological
   Development (CNPq)Conselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPQ) [Edital 06/2008 CNPQ Jovens Pesquisadores,
   CNPq/PIBIC, CNPq/PELD/ILTER Site 5, CNPq/PCI #300670/2019-2, CNPq/DCR
   #300461/2016-0, CNPq/Casadinho/PROCAD Project UESC-UFRJ #552198/20,
   CNPq/CAPES/FAPs/PELD #88887.140649/2017-00, CNPq-PROBIO #680037/02-0,
   CNPq for N.C.C. (Research Fellow in Ecology), Brazilian Program for
   Biodiversity Research (PPBio, Bolsa produtividade CNPq P2
   #308503/2014-7, Bolsa produtividade CNPq P2 #308040/2017-1,
   503372/2014-5, 457451/2012-9, 441435/2017-3, 312292/2016-3,
   312045/2013-1, 308385/2014-4, 306700/2015-8, 306695/2015-4,
   303006/2014-5, 216938/2014-7, 168234/2014-9, 141667/2016-8,
   140689/2013-3, 140040/2016-1, 300917/2019-8] Funding Source: Medline;
   Brazilian Program for Biodiversity Research (PPBio) Atlantic Forest
   Network (CNPq)Conselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPQ)Fundacao de Apoio a Pesquisa do Distrito Federal
   (FAPDF) [#457451/2012-9] Funding Source: Medline; Coordenação de
   Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES)Coordenacao
   de Aperfeicoamento de Pessoal de Nivel Superior (CAPES) [Sv875/2017,
   #872, #88881.162169/2017-0, 88882.180491/2018-01, #88881.162169/2017-0,
   #88881.068425/2014-01, BEX 1298/15-1] Funding Source: Medline; Darwin
   Initiative (DEFRA, UK)Department for Environment, Food & Rural Affairs
   (DEFRA) Funding Source: Medline; Deutscher Akademischer Austauschdienst
   (DAAD)Deutscher Akademischer Austausch Dienst (DAAD) Funding Source:
   Medline; EMBRAPAEmpresa Brasileira de Pesquisa Agropecuaria (EMBRAPA)
   Funding Source: Medline; EMBRAPA PantanalEmpresa Brasileira de Pesquisa
   Agropecuaria (EMBRAPA) [#03.09.00.077.00.00] Funding Source: Medline;
   FAPDFFundacao de Apoio a Pesquisa do Distrito Federal (FAPDF) Funding
   Source: Medline; FAPEMAFundacao de Amparo a Pesquisa e Desenvolvimento
   Cientifico do Maranhao (FAPEMA) Funding Source: Medline; FAPEMATFundacao
   de Amparo a Pesquisa do Estado de Mato Grosso (FAPEMAT) Funding Source:
   Medline; FAPEMIG/CNPqConselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPQ)Fundacao de Amparo a Pesquisa do Estado de Minas
   Gerais (FAPEMIG) Funding Source: Medline; FAPERGSFundacao de Amparo a
   Ciencia e Tecnologia do Estado do Rio Grande do Sul (FAPERGS) Funding
   Source: Medline; FAPERJFundacao Carlos Chagas Filho de Amparo a Pesquisa
   do Estado do Rio De Janeiro (FAPERJ) Funding Source: Medline;
   FAPESBFundacao de Amparo a Pesquisa do Estado da Bahia (FAPESB)
   [#2366/2012, 1760/2013] Funding Source: Medline; Foundation for Research
   of the State of Minas Gerais (FAPEMIG)Fundacao de Amparo a Pesquisa do
   Estado de Minas Gerais (FAPEMIG) [CRA APQ 00604-17, APQ-00839-15,
   PPM-00139-14, CRA- RDP-00104-10] Funding Source: Medline;
   FundectFundacao de Apoio ao Desenvolvimento do Ensino Ciencia e
   Tecnologia do Estado de Mato Grosso do Sul (FUNDECT MS) [06/2016]
   Funding Source: Medline; NERC (Natural Environment Research Council,
   UK)UK Research & Innovation (UKRI)Natural Environment Research Council
   (NERC) Funding Source: Medline; NSFNational Science Foundation (NSF)
   [#BCS-0921013] Funding Source: Medline; São Paulo Research Foundation
   (FAPESP) #05/60016-1Fundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP) [2014/23095-0, 2019/04851-1, 2017/21816-0, 2016/11595-3,
   2015/22844-1, 2015/19439-8, 2008/03500-6, 2010/05343-5, 2006/04878-7,
   2005/00405-4, 05/60016-1, 2012/00534-2, 2011/22449-4, 2011/06782-5,
   2013/07162-6, 2015/18381-6, 2014/09300-0, 2013/24453-4, 2015/17739-4,
   2014/23132-2, 2012/14245-2, 2013/04957-8, 2013/50421-2, 2014/01986-0,
   2014/10192-7, 2014/14925-9] Funding Source: Medline; The Gordon and
   Betty Moore FoundationGordon and Betty Moore Foundation Funding Source:
   Medline; USAIDUnited States Agency for International Development (USAID)
   Funding Source: Medline; USFWSUS Fish & Wildlife Service Funding Source:
   Medline; Planta Funding Source: Medline; Votorantim Funding Source:
   Medline; Mohamed bin Zayed Species Conservation Fund [162512917,
   12055114] Funding Source: Medline; Brehm Funds for International Bird
   Conservation (Germany) Funding Source: Medline; Woodland Park Zoo
   Funding Source: Medline; Cambuhy Agrícola Ltda. Funding Source: Medline;
   Conservation Program of Species at Risk (PROCER-CONANP)
   [PROCER/CCER/RFSIPS/04/2016, PROCER/RFSIPS/10/2015,
   PROCER/RFSIPS/04/2015, PROCER/CCER/RFSIPS/14/2016] Funding Source:
   Medline; People&apos;s Trust for Endangered Species (PTES) Funding
   Source: Medline; Zoological Society of San Diego Funding Source:
   Medline; Overbrook Funding Source: Medline; Lion Tamarin of Brazil Fund
   (LTBF) Funding Source: Medline; Reserva Ecológica Michelin - Bahia -
   Brazil Funding Source: Medline; Consejo Nacional de Investigaciones
   Científicas y Técnicas, Argentina (CONICET) [Project UE IBS
   #22920160100130CO, PIP 2012-2014 #112-201101-00616] Funding Source:
   Medline; Global Ecotours &amp; Expeditions volunteers Funding Source:
   Medline; UFG Funding Source: Medline; Sidney José Damiani (Área
   Particular de Preservação Ambiental São Francisco) Funding Source:
   Medline; Fundação Estadual do Meio Ambiente e Recursos Hídricos de
   Roraima Funding Source: Medline; CHTP Funding Source: Medline; VILLUM
   Investigator project &quot;Biodiversity Dynamics in a Changing
   World&quot; funded by VILLUM FONDEN Funding Source: Medline; Panthera
   Foundation Funding Source: Medline; PAP-UDESC/FAPESC [#2017TR744]
   Funding Source: Medline; CASEST Funding Source: Medline; FCT/MEC CESAM
   [(UID/AMB/50017)] Funding Source: Medline; Funape Funding Source:
   Medline; University of Aveiro - Portugal Funding Source: Medline;
   Primate Conservation Inc. [(Project 1158)] Funding Source: Medline;
   Ministerio de Agroindustria Funding Source: Medline; Fundação para o
   Desenvolvimento Sustentável da Terra Potiguar-FUNDEP Funding Source:
   Medline; World Wildlife Fund (WWF) Funding Source: Medline; Departamento
   de Recursos Naturales - Universidad Nacional de La Pampa [PI #R018]
   Funding Source: Medline; Norte Energia Funding Source: Medline; Programa
   Áreas Protegidas da Amazônia (ARPA) Funding Source: Medline; Wildlife
   Conservation Society (WCS) Funding Source: Medline; TFCA/Funbio Funding
   Source: Medline; University of Wisconsin-Madison Funding Source:
   Medline; IFRJ Funding Source: Medline; Earthwatch Institute Funding
   Source: Medline; Iunes Habib Funding Source: Medline; Resource Award
   Funding Source: Medline; IBAMA Funding Source: Medline; The Rufford
   Foundation [11495-1, 20950-1] Funding Source: Medline; Comisión Nacional
   de Áreas Naturales Protegidas Área de Protección de Flora y Fauna
   Funding Source: Medline; Ministry of Culture and Science of North
   Rhine-Westphalia Funding Source: Medline; Fundação Grupo Boticário de
   Proteção à Natureza [0522-2012, 1037-20151, 0939-20121] Funding Source:
   Medline; CI-Brazil Funding Source: Medline; Heinrich Hertz-Foundation
   Funding Source: Medline; Consórcio Capim Branco Energia (CCBE) Funding
   Source: Medline; Wild Felid Legacy Scholarship Funding Source: Medline;
   Alexander Koenig Society (Bonn, Germany) Funding Source: Medline;
   CONACYT through the Programa PROCIENCIA with resources from the Fondo
   para la Excelencia de la Educacion e Investigacion (FEEI) Funding
   Source: Medline; Fundação de Amparo à Pesquisa do Estado do Pará
   (FAPESPA) [ICAAF #018/2016] Funding Source: Medline; Programa de
   Pesquisa em Biodiversidade-PPBio Rede BioM.A. Funding Source: Medline;
   Secretaría de Ciencia y Técnica-Facultad de Química Bioquímica y
   Farmacia-Universidad Nacional de San Luis Funding Source: Medline; Rede
   Sisbiota-ComCerrado [#107-CAP-2011] Funding Source: Medline; Politrade
   Funding Source: Medline; Júnior Santos (Instituto Felinos do Aguaí)
   Funding Source: Medline; Belmond Hotel Funding Source: Medline; Wildlife
   Research Funding Source: Medline; Région Pays de la Loire Funding
   Source: Medline; Laguna de Términos Funding Source: Medline; Zoological
   Society of London Funding Source: Medline; Carlsberg Foundation Semper
   Ardens project MegaPast2Future [(CF16-0005)] Funding Source: Medline;
   VRAC/PUC-Rio Funding Source: Medline; Pró-Reitorias de Pesquisa e
   Extensão da UEMG by the scholarship PIBIC and PAEx Funding Source:
   Medline; Agencia Nacional de Promoción Científica y Tecnológica de
   Argentina [PICT-2010-1256] Funding Source: Medline; UCAR (Unidad para el
   Cambio Rural), Ministerio de Agroindustria, Argentina through PIA 2011
   [10106, 10103, 10102, 10104, 10105] Funding Source: Medline; FAEP
   Funding Source: Medline; International Paper Co. of Brazil Funding
   Source: Medline; Chinese Academy of Sciences President&apos;s
   International Fellowship Initiative [(Grant #2018PB0040)] Funding
   Source: Medline; Primate Action Fund [(Project 1001257)] Funding Source:
   Medline; Pró-Reitorias de Pesquisa e Extensão da UEMG and FAPEMIG by the
   scholarship PIBIC Funding Source: Medline; Disney World Conservation
   Fund Funding Source: Medline; Neotropical Grassland Conservancy (NGC)
   Funding Source: Medline; FAPES (BPIG/I Biologia da Conservação) Funding
   Source: Medline; Tropical Conservation and Development Program Funding
   Source: Medline; Oswaldo Cruz Foundation Funding Source: Medline;
   ZGAP/SPS/FbP Funding Source: Medline; PROPe UNESP Funding Source:
   Medline; Superintendência da Zona Franca de Manaus-Coordenação Regional
   de Rio Branco Funding Source: Medline; Conservation Leadership Programme
   [(Project #02224115)] Funding Source: Medline; Fundação de Amparo à
   Ciência e Tecnologia do Estado de Pernambuco (FACEPE)
   [#BCT-0025-2.05/17] Funding Source: Medline; Consejo Nacional de Ciencia
   y Tecnología de México Funding Source: Medline; DIBIO/ICMBio [(Projeto
   PEM 011.034)] Funding Source: Medline; von Humboldt Foundation/CAPES
   [#88881.162169/2017-01] Funding Source: Medline; Fundação Mary Brown
   Funding Source: Medline; Biofaces Funding Source: Medline; Instituto
   Arapyaú Funding Source: Medline; Comisión Nacional de Áreas Naturales
   Protegidas Reserva de la Biosfera Pantanos de Centla Funding Source:
   Medline; Universidad Autónoma del Estado de México Funding Source:
   Medline; Fundação Monsanto Funding Source: Medline; Liz Claiborne &amp;
   Art Ortenberg Foundation Funding Source: Medline; Reitoria USP Funding
   Source: Medline; Biota - Projetos e Consultoria Ambiental Funding
   Source: Medline; IEMA [#9003/2014] Funding Source: Medline; Elguero Farm
   Funding Source: Medline; Programa de las Naciones Unidas para el
   Desarrollo (PNUD)/Comisión Nacional Forestal (CONAFOR) Funding Source:
   Medline; Parrot Wildlife Foundation Funding Source: Medline; Legado das
   Águas Funding Source: Medline; International Association for Bear
   Research and Management - IBA Funding Source: Medline; Idea Wild Funding
   Source: Medline; PROBIO/MMA Funding Source: Medline; Fibria Celulose
   S.A. Funding Source: Medline; CEMIG Funding Source: Medline; San Diego
   Zoo Global Funding Source: Medline; The Social Sciences and Humanities
   Research Council (SSHRC, Canadá) Funding Source: Medline; EW volunteers
   Funding Source: Medline; MRN - Mineração Rio do Norte Funding Source:
   Medline; Landowners that have participated in the research Funding
   Source: Medline; Concessionária Auto Raposo Tavares (CART) Funding
   Source: Medline; Ecopetrol and Fundación Marío Santo Domingo Funding
   Source: Medline
NR 0
TC 7
Z9 7
U1 7
U2 18
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0012-9658
EI 1939-9170
J9 ECOLOGY
JI Ecology
PD NOV
PY 2020
VL 101
IS 11
AR e03128
DI 10.1002/ecy.3128
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA OP9CN
UT WOS:000588387700012
PM 32862433
OA Bronze, Green Published
DA 2022-02-10
ER

PT J
AU Singh, S
   Shekhar, C
   Vohra, A
AF Singh, Sanjay
   Shekhar, Chandra
   Vohra, Anil
TI Real-Time FPGA-Based Object Tracker with Automatic Pan-Tilt Features for
   Smart Video Surveillance Systems
SO JOURNAL OF IMAGING
LA English
DT Article
DE real-time object tracking; VLSI architecture; FPGA implementation; video
   surveillance system; smart camera system
ID PARTICLE FILTERS; SEGMENTATION; IMPLEMENTATION
AB The design of smart video surveillance systems is an active research field among the computer vision community because of their ability to perform automatic scene analysis by selecting and tracking the objects of interest. In this paper, we present the design and implementation of an FPGA-based standalone working prototype system for real-time tracking of an object of interest in live video streams for such systems. In addition to real-time tracking of the object of interest, the implemented system is also capable of providing purposive automatic camera movement (pan-tilt) in the direction determined by movement of the tracked object. The complete system, including camera interface, DDR2 external memory interface controller, designed object tracking VLSI architecture, camera movement controller and display interface, has been implemented on the Xilinx ML510 (Virtex-5 FX130T) FPGA Board. Our proposed, designed and implemented system robustly tracks the target object present in the scene in real time for standard PAL (720 x 576) resolution color video and automatically controls camera movement in the direction determined by the movement of the tracked object.
C1 [Singh, Sanjay; Shekhar, Chandra] CSIR, Cent Elect Engn Res Inst, Pilani 333031, Rajasthan, India.
   [Vohra, Anil] Kurukshetra Univ, Elect Sci Dept, Kurukshetra 136119, Haryana, India.
RP Singh, S (corresponding author), CSIR, Cent Elect Engn Res Inst, Pilani 333031, Rajasthan, India.
EM sanjay.csirceeri@gmail.com; chandra@ceeri.ernet.in; vohra64@gmail.com
RI ; Shekhar, Chandra/O-3381-2017
OI Singh, Sanjay/0000-0002-2249-799X; Shekhar, Chandra/0000-0002-2114-9096
FU Department of Electronics & Information Technology (DeitY)/Ministry of
   Communications and Information Technology (MCIT), the Government of
   India
FX The financial support of Department of Electronics & Information
   Technology (DeitY)/Ministry of Communications and Information Technology
   (MCIT), the Government of India, is gratefully acknowledged.
CR Abd El-Halym HA, 2010, IEEE IMAGE PROC, P4497, DOI 10.1109/ICIP.2010.5653817
   Adam A., 2006, P IEEE C COMP VIS PA, P798, DOI DOI 10.1109/CVPR.2006.256
   Agrawal S, 2012, 2012 INTERNATIONAL SYMPOSIUM ON ELECTRONIC SYSTEM DESIGN (ISED 2012), P82, DOI 10.1109/ISED.2012.41
   Ahmed J., 2007, P NAT C ART INT, P1077
   Ahmed J., 2005, P INT C MACH VIS PAT, P1
   Ahmed J., 2007, INT J COMPUT INF SYS, V1, P1825
   Ahmed J, 2016, J REAL-TIME IMAGE PR, V11, P315, DOI 10.1007/s11554-012-0251-z
   Al Haj Murad, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1690, DOI 10.1109/ICPR.2010.418
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436
   Chan SC, 2013, IEEE J EM SEL TOP C, V3, P248, DOI 10.1109/JETCAS.2013.2256822
   Chen XL, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P423, DOI 10.1109/ICMI.2002.1167032
   Cho J. U., 2007, P INT C CONTR AUT SY, P1163
   Cho JU, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-5, P172
   Coifman B, 1998, TRANSPORT RES C-EMER, V6, P271, DOI 10.1016/S0968-090X(98)00019-9
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dargazany A., 2010, ADV ARTIF INTELL, V2010
   Doulamis A, 2003, IEEE T NEURAL NETWOR, V14, P616, DOI 10.1109/TNN.2003.810605
   ELEFTHERIADIS A, 1995, SIGNAL PROCESS-IMAGE, V7, P231, DOI 10.1016/0923-5965(95)00028-U
   Elkhatib LN, 2012, 2012 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT AND ADVANCED SYSTEMS (ICIAS), VOLS 1-2, P745, DOI 10.1109/ICIAS.2012.6306112
   Erdem CE, 2007, SIGNAL PROCESS-IMAGE, V22, P891, DOI 10.1016/j.image.2007.09.001
   Gevers T, 2004, IEEE T CIRC SYST VID, V14, P776, DOI 10.1109/TCSVT.2004.828347
   Gupta N, 2004, IEEE IMAGE PROC, P1041
   Gustafsson F, 2002, IEEE T SIGNAL PROCES, V50, P425, DOI 10.1109/78.978396
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Ho J, 2004, PROC CVPR IEEE, P782
   Intille SS, 1997, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.1997.609402
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Johnston T., 2005, P INT C SENS TECHN I, P66
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kang S, 2003, PROC SPIE, V5132, P103, DOI 10.1117/12.514945
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P122, DOI 10.1109/76.988659
   Kristensen F, 2008, J SIGNAL PROCESS SYS, V52, P75, DOI 10.1007/s11265-007-0100-7
   Lee SG, 2011, COMM COM INF SC, V206, P121
   Li CM, 2005, Proceedings of 2005 International Conference on Machine Learning and Cybernetics, Vols 1-9, P4957
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   McErlean M, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1 AND 2, P242, DOI 10.1109/ISSPIT.2006.270805
   Namboodiri VP, 2006, LECT NOTES COMPUT SC, V4338, P504
   Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4
   Papoutsakis KE, 2010, LECT NOTES COMPUT SC, V6453, P405
   Paschalakis S, 2004, REAL-TIME IMAGING, V10, P81, DOI 10.1016/j.rti.2004.02.004
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Peddigari V, 2007, J REAL-TIME IMAGE PR, V2, P45, DOI 10.1007/s11554-007-0036-y
   Perez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Popescu D, 2010, UNIV POLIT BUCHAR S, V72, P121
   Porikli F, 2005, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2005.188
   Porikli F, 2006, P IEEE COMP SOC C CO, DOI [DOI 10.1109/CVPR.2006.94, 10.1109/CVPR.2006.94]
   Porikli F, 2006, J REAL-TIME IMAGE PR, V1, P33, DOI 10.1007/s11554-006-0011-z
   Raju K. S., 2012, INT J COMPUT SCI ISS, V9, P43
   Raju K. S., 2013, INT J COMPUT APPL, V69, P41
   Shahzad M, 2009, ICDIP 2009: INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING, PROCEEDINGS, P220, DOI 10.1109/ICDIP.2009.81
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Su Liu, 2011, Proceedings of the 2011 Symposium on Application Accelerators in High-Performance Computing (SAAHPC 2011), P1, DOI 10.1109/SAAHPC.2011.22
   Tai JC, 2004, IMAGE VISION COMPUT, V22, P485, DOI 10.1016/j.imavis.2003.12.001
   Dinh T, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3786, DOI 10.1109/IROS.2009.5353915
   Tripathi S, 2009, ICAPR 2009: SEVENTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, PROCEEDINGS, P278, DOI 10.1109/ICAPR.2009.39
   Varcheie PDZ, 2009, 2009 IEEE INTERNATIONAL WORKSHOP ON ROBOTIC AND SENSORS ENVIRONMENTS (ROSE 2009), P98, DOI 10.1109/ROSE.2009.5355997
   Wong S, 2012, I C MECH MACH VIS PR, P156
   Wong S., 2005, P SOC PHOTO-OPT INS, V5810, P1
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Xiaofeng Lu, 2010, 2010 International Conference on Audio, Language and Image Processing (ICALIP), P1657, DOI 10.1109/ICALIP.2010.5685091
   Xu JB, 2007, DSD 2007: 10TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN ARCHITECTURES, METHODS AND TOOLS, PROCEEDINGS, P432, DOI 10.1109/DSD.2007.4341504
   Yamaoka K, 2006, IEEE INT SYMP CIRC S, P5575
   Yilmaz A, 2004, IEEE T PATTERN ANAL, V26, P1531, DOI 10.1109/TPAMI.2004.96
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yuan-Pao Hsu, 2010, Proceedings of the SICE 2010 - 49th Annual Conference of the Society of Instrument and Control Engineers of Japan, P2878
NR 70
TC 9
Z9 9
U1 1
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2313-433X
J9 J IMAGING
JI J. Imaging
PD JUN
PY 2017
VL 3
IS 2
AR 18
DI 10.3390/jimaging3020018
PG 28
WC Imaging Science & Photographic Technology
WE Emerging Sources Citation Index (ESCI)
SC Imaging Science & Photographic Technology
GA FF4KN
UT WOS:000408913500005
OA gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Bartl, V
   Spanhel, J
   Dobes, P
   Juranek, R
   Herout, A
AF Bartl, Vojtech
   Spanhel, Jakub
   Dobes, Petr
   Juranek, Roman
   Herout, Adam
TI Automatic camera calibration by landmarks on rigid objects
SO MACHINE VISION AND APPLICATIONS
LA English
DT Article
DE Camera calibration; Optimization; Surveillance
ID POSE
AB This article presents a new method for automatic calibration of surveillance cameras. We are dealing with traffic surveillance, and therefore, the camera is calibrated by observing vehicles; however, other rigid objects can be used instead. The proposed method is usingkeypointsorlandmarksautomatically detected on the observed objects by a convolutional neural network. By using fine-grained recognition of the vehicles (calibration objects), and by knowing the 3D positions of the landmarks for the (very limited) set of known objects, the extracted keypoints are used for calibration of the camera, resulting in internal (focal length) and external (rotation, translation) parameters and scene scale of the surveillance camera. We collected a dataset in two parking lots and equipped it with a calibration ground truth by measuring multiple distances in the ground plane. This dataset seems to be more accurate than the existing comparable data (GT calibration error reduced from 4.62 % to 0.99 %). Also, the experiments show that our method overcomes the best existing alternative in terms of accuracy (error reduced from 6.56 % to 4.03%) and our solution is also more flexible in terms of viewpoint change and other.
C1 [Bartl, Vojtech; Spanhel, Jakub; Dobes, Petr; Juranek, Roman; Herout, Adam] Brno Univ Technol, Fac Informat Technol, Ctr Excellence IT4Innovat, Brno, Czech Republic.
RP Bartl, V (corresponding author), Brno Univ Technol, Fac Informat Technol, Ctr Excellence IT4Innovat, Brno, Czech Republic.
EM ibartl@fit.vutbr.cz
RI Herout, Adam/B-5651-2014
OI Bartl, Vojtech/0000-0003-1792-1028
FU Ministry of Education, Youth and Sports of the Czech Republic from the
   National Programme of Sustainability (NPU II); project IT4Innovations
   excellence in science [LQ1602]
FX This work was supported by The Ministry of Education, Youth and Sports
   of the Czech Republic from the National Programme of Sustainability (NPU
   II); project IT4Innovations excellence in science-LQ1602.
CR Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bhardwaj R., 2017, 4 ACM INT C SYST EN
   Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2
   Cathey FW, 2005, 2005 IEEE Intelligent Vehicles Symposium Proceedings, P777
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dailey D. J., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P98, DOI 10.1109/6979.880967
   Darrell, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   de Villiers Jason P, 2008, Proceedings of the SPIE - The International Society for Optical Engineering, V7266, DOI 10.1117/12.804771
   Dubska M., 2014, BMVC, DOI DOI 10.5244/C.28.42
   Dubska M, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.90
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Filipiak P, 2016, LECT NOTES COMPUT SC, V9597, P803, DOI 10.1007/978-3-319-31204-0_51
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Grammatikopoulos L., 2005, P INT S MOD TECHN ED, P332
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   He X. C., 2007, IEEE WORKSH APPL COM, DOI DOI 10.1109/WACV.2007.7
   Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266
   Howard A.G., 2017, MOBILENETS EFFICIENT
   Ioffe S, 2015, PR MACH LEARN RES, P448, DOI DOI 10.1109/CVPR.2016.90
   Juranek R, 2015, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2015.274
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Kneip L, 2014, LECT NOTES COMPUT SC, V8689, P127, DOI 10.1007/978-3-319-10590-1_9
   Komodakis, 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Lan JH, 2014, OPTIK, V125, P289, DOI 10.1016/j.ijleo.2013.06.036
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lin YL, 2014, LECT NOTES COMPUT SC, V8692, P466, DOI 10.1007/978-3-319-10593-2_31
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Llorca D.F., 2016, ITSC, DOI [10.1109/ITSC.2014.6958187, DOI 10.1109/ITSC.2014.6958187]
   Luvizon Diogo C., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6563, DOI 10.1109/ICASSP.2014.6854869
   Maduro C, 2008, IEEE IMAGE PROC, P777, DOI 10.1109/ICIP.2008.4711870
   Meng XQ, 2003, PATTERN RECOGN, V36, P1155, DOI 10.1016/S0031-3203(02)00225-X
   Moulon P, 2013, IEEE I CONF COMP VIS, P3248, DOI 10.1109/ICCV.2013.403
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nurhadiyatna A, 2013, INT C ADV COMP SCI I, P451, DOI 10.1109/ICACSIS.2013.6761617
   Pearce G., 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P373, DOI 10.1109/AVSS.2011.6027353
   Penate-Sanchez A, 2013, IEEE T PATTERN ANAL, V35, P2387, DOI 10.1109/TPAMI.2013.36
   Pirsiavash H., 2009, ADV NEURAL INF PROCE, P1482
   REDMON J, 2016, PROC CVPR IEEE, P779, DOI DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schoepflin TN, 2003, IEEE T INTELL TRANSP, V4, P90, DOI 10.1109/TITS.2003.821213
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Sochor J., 2017, ARXIV170300686
   Sochor J, 2019, IEEE T INTELL TRANSP, V20, P1633, DOI 10.1109/TITS.2018.2825609
   Sochor J, 2017, COMPUT VIS IMAGE UND, V161, P87, DOI 10.1016/j.cviu.2017.05.015
   Sochor J, 2016, PROC CVPR IEEE, P3006, DOI 10.1109/CVPR.2016.328
   Song KT, 2006, IEEE T SYST MAN CY B, V36, P1091, DOI 10.1109/TSMCB.2006.872271
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   SZEGEDY C, 2016, PROC CVPR IEEE, P2818, DOI DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Do VH, 2015, 2015 12TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING/ELECTRONICS, COMPUTER, TELECOMMUNICATIONS AND INFORMATION TECHNOLOGY (ECTI-CON)
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   You XH, 2016, NEUROCOMPUTING, V204, P222, DOI 10.1016/j.neucom.2015.09.132
   Zhang B., 2014, INT J COMPUT VIS ROB, V4, P195
   Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   ZHENG Y, 2016, CVPR
   Zheng YQ, 2014, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2014.62
   Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291
NR 69
TC 1
Z9 1
U1 4
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0932-8092
EI 1432-1769
J9 MACH VISION APPL
JI Mach. Vis. Appl.
PD OCT 6
PY 2020
VL 32
IS 1
AR 2
DI 10.1007/s00138-020-01125-x
PG 13
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA NX0RA
UT WOS:000575425400001
DA 2022-02-10
ER

PT J
AU Garau, N
   De Natale, FGB
   Conci, N
AF Garau, Nicola
   De Natale, Francesco G. B.
   Conci, Nicola
TI Fast automatic camera network calibration through human mesh recovery
SO JOURNAL OF REAL-TIME IMAGE PROCESSING
LA English
DT Article
DE Camera calibration; Pose estimation; Human mesh recovery; 3D matching
ID SELF-CALIBRATION
AB Camera calibration is a necessary preliminary step in computer vision for the estimation of the position of objects in the 3D world. Despite the intrinsic camera parameters can be easily computed offline, extrinsic parameters need to be computed each time a camera changes its position, thus not allowing for fast and dynamic network re-configuration. In this paper we present an unsupervised and automatic framework for the estimation of the extrinsic parameters of a camera network, which leverages on optimised 3D human mesh recovery from a single image, and which does not require the use of additional markers. We show how it is possible to retrieve the real-world position of the cameras in the network together with the floor plane, exploiting regular RGB images and with a weak prior knowledge of the internal parameters. Our framework can also work with a single camera and in real-time, allowing the user to add, re-position, or remove cameras from the network in a dynamic fashion.
C1 [Garau, Nicola; De Natale, Francesco G. B.; Conci, Nicola] Univ Trento, Via Sommar 9, I-38123 Trento, TN, Italy.
RP Garau, N (corresponding author), Univ Trento, Via Sommar 9, I-38123 Trento, TN, Italy.
EM nicola.garau@unitn.it; francesco.denatale@unitn.it;
   nicola.conci@unitn.it
FU UniversitA degli Studi di Trento within the CRUI-CARE Agreement
FX Open access funding provided by UniversitA degli Studi di Trento within
   the CRUI-CARE Agreement.
CR Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   Cao Z., 2018, ARXIV181208008
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Coughlan JM, 2001, ADV NEUR IN, V13, P845
   Desai K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P250, DOI 10.1145/3204949.3204969
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Garau N., 2019, P 13 INT C DISTR SMA
   Geiger A, 2012, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2012.6224570
   Hidalgo G., 2019, ARXIV190913423
   Hold-Geoffroy Y, 2018, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR.2018.00250
   Inomata Ryo, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P325
   Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381
   Kanazawa A, 2019, IEEE C COMP VIS PATT, P5614
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kim H, 2001, IEE P-VIS IMAGE SIGN, V148, P349, DOI 10.1049/ip-vis:20010574
   Kolotouros N., 2019, ARXIV190912828
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Lucas B. D., 1981, P INT JOINT C ART IN, V2, P674, DOI 10.5555/1623264.1623280
   Miyata S, 2018, IEEE T CIRC SYST VID, V28, P2210, DOI 10.1109/TCSVT.2017.2731792
   Nister D., 2004, COMP VIS PATT REC 20, V1, pI
   Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014
   Ramakrishna V, 2014, COMPUTER VISION ECCV, V33, P47
   Seo Y, 2001, IEE P-VIS IMAGE SIGN, V148, P166, DOI 10.1049/ip-vis:20010078
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Simek K, 2013, PINHOLE CAMERA DIAGR
   Tang Z, 2019, IEEE ACCESS, V7, P10754, DOI 10.1109/ACCESS.2019.2891224
   Tang Z, 2016, INT CONF ACOUST SPEE, P1115, DOI 10.1109/ICASSP.2016.7471849
   Tome D., 2017, P IEEE C COMP VIS PA, P2500
   Vasconcelos F, 2018, IEEE T PATTERN ANAL, V40, P791, DOI 10.1109/TPAMI.2017.2699648
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Zhang GP, 2018, MEAS SCI TECHNOL, V29, DOI 10.1088/1361-6501/aab4d6
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao FD, 2018, IMAGE VISION COMPUT, V70, P46, DOI 10.1016/j.imavis.2017.12.006
NR 36
TC 2
Z9 2
U1 4
U2 4
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1861-8200
EI 1861-8219
J9 J REAL-TIME IMAGE PR
JI J. Real-Time Image Process.
PD DEC
PY 2020
VL 17
IS 6
SI SI
BP 1757
EP 1768
DI 10.1007/s11554-020-01002-w
EA SEP 2020
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA OP5TA
UT WOS:000566041200001
OA hybrid
DA 2022-02-10
ER

PT C
AU Micusik, B
   Pajdla, T
AF Micusik, Branislav
   Pajdla, Tomas
GP IEEE
TI Simultaneous surveillance camera calibration and foot-head homology
   estimation from human detections
SO 2010 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
LA English
DT Proceedings Paper
CT 23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 13-18, 2010
CL San Francisco, CA
SP IEEE Comp Soc
AB We propose a novel method for automatic camera calibration and foot-head homology estimation by observing persons standing at several positions in the camera field of view. We demonstrate that human body can be considered as a calibration target thus avoiding special calibration objects or manually established fiducial points. First, by assuming roughly parallel human poses we derive a new constraint which allows to formulate the calibration of internal and external camera parameters as a Quadratic Eigenvalue Problem. Secondly, we couple the calibration with an imp roved effective integral contour based human detector and use 3D projected models to capture a large variety of person and camera mutual positions. The resulting camera autocalibration method is very robust and efficient, and thus well suited for surveillance applications where the camera calibration process cannot use special calibration targets and must be simple.
C1 [Micusik, Branislav] AIT Austrian Inst Technol, Safety & Secur Dept, Vienna, Austria.
   [Pajdla, Tomas] Czech Tech Univ, Prague, Czech Republic.
RP Micusik, B (corresponding author), AIT Austrian Inst Technol, Safety & Secur Dept, Vienna, Austria.
RI Pajdla, Tomas/K-7954-2013
OI Pajdla, Tomas/0000-0001-6325-0072
FU Wiener Wissenschafts-,Forschungs- und Technologiefonds - WWTF
   [ICT08-030];  [FP6-IST-027787 DIRAC];  [MSM6840770038]
FX This research received funding from Wiener Wissenschafts-,Forschungs-
   und Technologiefonds - WWTF, Project No ICT08-030, and from the projects
   FP6-IST-027787 DIRAC and MSM6840770038.
CR Bai Z., 2000, TEMPLATES SOLUTION A
   BELEZNAI C, 2009, CVPR
   Boutry G, 2005, SIAM J MATRIX ANAL A, V27, P582, DOI 10.1137/S0895479803428795
   BUJNAK M, 2009, ICCV
   CRIMINISI A, 2001, ACCURATE VISUAL METR
   Fitzgibbon AW, 2001, PROC CVPR IEEE, P125
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Junejo I. N., 2007, ICCV
   Junejo IN, 2008, IMAGE VISION COMPUT, V26, P512, DOI 10.1016/j.imavis.2007.07.006
   Kanatani K, 2005, IEICE T INF SYST, VE88D, P2260, DOI 10.1093/ietisy/e88-d.10.2260
   Krahnstoever N, 2005, IEEE I CONF COMP VIS, P1858
   LI L, 2008, CVPR
   LIEBELT J, 2008, CVPR
   Liebowitz D., 2001, THESIS U OXFORD
   LV F, 2006, PAMI, V28
   MICUSIK B, 2006, PAMI, V28
   Steele RM, 2006, LECT NOTES COMPUT SC, V3951, P253
   Toshev A., 2009, CVPR
NR 19
TC 16
Z9 17
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 1063-6919
BN 978-1-4244-6984-0
J9 PROC CVPR IEEE
PY 2010
BP 1562
EP 1569
DI 10.1109/CVPR.2010.5539786
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Mathematics, Applied; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematics; Imaging Science & Photographic Technology
GA BTN70
UT WOS:000287417501077
DA 2022-02-10
ER

PT J
AU Assadzadeh, A
   Arashpour, M
   Bab-Hadiashar, A
   Ngo, T
   Li, H
AF Assadzadeh, Amin
   Arashpour, Mehrdad
   Bab-Hadiashar, Alireza
   Ngo, Tuan
   Li, Heng
TI Automatic far-field camera calibration for construction scene analysis
SO COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING
LA English
DT Article
AB The use of cameras for safety monitoring, progress tracking, and site security has grown significantly on construction and civil infrastructure sites over the past decade. Localization of construction resources is a crucial prerequisite for many applications in automated construction management. However, most existing vision-based methods perform the analysis in the image plane, overlooking the effect of perspective and depth. The manual and labor-intensive process of traditional calibration techniques, as well as the busy and restrictive construction environment, makes this a challenging task. This study proposes a framework for automatic camera calibration with no manual intervention. The framework utilizes convolutional neural networks for geometrical scene analysis and object detection, which are used to estimate the location of horizon line, vertical vanishing point, as well as objects with known height distributions. This enables automatic estimation of camera parameters and retrieval of scale. The proposed framework is evaluated on images from two major construction projects in Melbourne, Australia. Results show that the proposed method achieves a minimum accuracy of 90% in estimating proximity of points on the ground and can facilitate further development of vision-based solutions for safety and productivity analysis.
C1 [Assadzadeh, Amin; Arashpour, Mehrdad] Monash Univ, Dept Civil Engn, Melbourne, Vic 3800, Australia.
   [Bab-Hadiashar, Alireza] RMIT Univ, Sch Engn, Melbourne, Vic, Australia.
   [Ngo, Tuan] Univ Melbourne, Dept Infrastruct Engn, Melbourne, Vic, Australia.
   [Li, Heng] Hong Kong Polytech Univ, Dept Bldg & Real Estate, Hong Kong, Peoples R China.
RP Arashpour, M (corresponding author), Monash Univ, Dept Civil Engn, Melbourne, Vic 3800, Australia.
EM mehrdad.arashpour@monash.edu
RI Ngo, Tuan D/P-8184-2014; Bab-Hadiashar, Alireza/A-9157-2010; Li,
   Heng/B-2821-2015
OI Ngo, Tuan D/0000-0002-9831-8580; Bab-Hadiashar,
   Alireza/0000-0002-6192-2303; Li, Heng/0000-0002-3187-9041
FU Monash Infrastructure (MI) grant
FX This work was supported by a Monash Infrastructure (MI) grant. The
   authors would also like to acknowledge the support of the industry
   partners of this research. Any opinions, findings, conclusions, and
   recommendations expressed in this paper are those of the authors and do
   not necessarily reflect the views of the industry partners or Monash
   Infrastructure (MI).
CR Abbas A, 2019, IEEE INT CONF COMP V, P4095, DOI 10.1109/ICCVW.2019.00504
   Acharya UR, 2018, COMPUT BIOL MED, V100, P270, DOI 10.1016/j.compbiomed.2017.09.017
   Ackermann H, 2007, P IAPR C MACH VIS AP, P178
   Arabi S, 2020, COMPUT-AIDED CIV INF, V35, P753, DOI 10.1111/mice.12530
   Arashpour M, 2021, J BUILD ENG, V33, DOI 10.1016/j.jobe.2020.101672
   Asadzadeh A, 2020, AUTOMAT CONSTR, V113, DOI 10.1016/j.autcon.2020.103128
   Assadzadeh A, 2021, COMPUT-AIDED CIV INF, V36, P1073, DOI 10.1111/mice.12660
   Australian Bureau of Statistics, 2012, AUSTR HLTH SURVEY 1
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chang H, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010063
   Chen HT, 2017, IEEE T CIRC SYST VID, V27, P2555, DOI 10.1109/TCSVT.2016.2595319
   Chuah WQ, 2019, IEEE INT C INTELL TR, P4255, DOI 10.1109/ITSC.2019.8917194
   Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15
   Edirisinghe R, 2019, ENG CONSTR ARCHIT MA, V26, P184, DOI 10.1108/ECAM-04-2017-0066
   Fang Q, 2018, AUTOMAT CONSTR, V85, P1, DOI 10.1016/j.autcon.2017.09.018
   Fang WL, 2020, ADV ENG INFORM, V43, DOI 10.1016/j.aei.2019.100980
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gomez-de-Gabriel JM, 2019, MEASUREMENT, V131, P329, DOI 10.1016/j.measurement.2018.07.093
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Heinrich SB, 2011, IMAGE VISION COMPUT, V29, P653, DOI 10.1016/j.imavis.2011.07.003
   Jacobs, 2016, 27 BRIT MACH VIS C B
   Kamoona AM, 2019, IEEE ACCESS, V7, P105710, DOI 10.1109/ACCESS.2019.2932137
   KANADE T, 1983, P IEEE, V71, P789, DOI 10.1109/PROC.1983.12679
   Kim D, 2019, AUTOMAT CONSTR, V99, P168, DOI 10.1016/j.autcon.2018.12.014
   Kim J, 2017, J COMPUT CIVIL ENG, V31, DOI 10.1061/(ASCE)CP.1943-5487.0000677
   Kluger F, 2017, LECT NOTES COMPUT SC, V10496, P17, DOI 10.1007/978-3-319-66709-6_2
   Lezama J, 2014, PROC CVPR IEEE, P509, DOI 10.1109/CVPR.2014.72
   Liebowitz D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P293, DOI 10.1109/ICCV.1999.791233
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo HB, 2020, ADV ENG INFORM, V45, DOI 10.1016/j.aei.2020.101100
   Luo XC, 2019, COMPUT-AIDED CIV INF, V34, P333, DOI 10.1111/mice.12419
   Lv FJ, 2006, IEEE T PATTERN ANAL, V28, P1513, DOI 10.1109/TPAMI.2006.178
   Manzanera OM, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500102
   Mckee M, 2020, NAT MED, V26, P640, DOI 10.1038/s41591-020-0863-y
   Nearmap, 2019, NEARMAP DOCUMENTATIO
   Pan X, 2020, COMPUT-AIDED CIV INF, V35, P495, DOI 10.1111/mice.12549
   Rafiei MH, 2018, ENG STRUCT, V156, P598, DOI 10.1016/j.engstruct.2017.10.070
   Rafiei MH, 2017, STRUCT DES TALL SPEC, V26, DOI 10.1002/tal.1400
   Redmon J., 2018, ARXIV PREPRINT ARXIV
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28
   Roberts D, 2019, AUTOMAT CONSTR, V105, DOI 10.1016/j.autcon.2019.04.006
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Son H, 2019, AUTOMAT CONSTR, V99, P27, DOI 10.1016/j.autcon.2018.11.033
   Statistics B.o.L., 2019, CENSUS FATAL OCCUPAT
   Steger C., 2018, MACHINE VISION ALGOR
   Tang Z, 2019, IEEE ACCESS, V7, P10754, DOI 10.1109/ACCESS.2019.2891224
   Tardif JP, 2009, IEEE I CONF COMP VIS, P1250, DOI 10.1109/ICCV.2009.5459328
   Ting DSW, 2020, NAT MED, V26, P459, DOI 10.1038/s41591-020-0824-5
   Tretyak E, 2012, INT J COMPUT VISION, V97, P305, DOI 10.1007/s11263-011-0488-1
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Valikhani A, 2021, COMPUT-AIDED CIV INF, V36, P213, DOI 10.1111/mice.12605
   Vera-Olmos FJ, 2019, INTEGR COMPUT-AID E, V26, P85, DOI 10.3233/ICA-180584
   Wildenauer H, 2012, PROC CVPR IEEE, P2831, DOI 10.1109/CVPR.2012.6248008
   World Health Organization, 2020, COR DIS COVID 19 SIT, P60
   Wu JX, 2019, AUTOMAT CONSTR, V106, DOI 10.1016/j.autcon.2019.102894
   Wu WW, 2013, AUTOMAT CONSTR, V34, P67, DOI 10.1016/j.autcon.2012.10.010
   Xu YL, 2013, PROC CVPR IEEE, P1376, DOI 10.1109/CVPR.2013.181
   Yan XZ, 2020, COMPUT-AIDED CIV INF, V35, P1023, DOI 10.1111/mice.12536
   Yang J, 2015, ADV ENG INFORM, V29, P211, DOI 10.1016/j.aei.2015.01.011
   Yang T, 2019, INTEGR COMPUT-AID E, V26, P273, DOI 10.3233/ICA-180596
   Yang XC, 2018, COMPUT-AIDED CIV INF, V33, P1110, DOI 10.1111/mice.12385
   Zhai MH, 2016, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2016.610
   Zhang B, 2019, COMPUT-AIDED CIV INF, V34, P471, DOI 10.1111/mice.12434
   Zhang MY, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17081841
   Zhang XD, 2018, NEUROCOMPUTING, V311, P260, DOI 10.1016/j.neucom.2018.05.071
   Zhang ZX, 2013, IEEE T CIRC SYST VID, V23, P518, DOI 10.1109/TCSVT.2012.2210670
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhu ZH, 2016, AUTOMAT CONSTR, V68, P95, DOI 10.1016/j.autcon.2016.04.009
NR 71
TC 3
Z9 3
U1 9
U2 19
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1093-9687
EI 1467-8667
J9 COMPUT-AIDED CIV INF
JI Comput.-Aided Civil Infrastruct. Eng.
PD AUG
PY 2021
VL 36
IS 8
BP 1073
EP 1090
DI 10.1111/mice.12660
EA FEB 2021
PG 18
WC Computer Science, Interdisciplinary Applications; Construction &
   Building Technology; Engineering, Civil; Transportation Science &
   Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Construction & Building Technology; Engineering;
   Transportation
GA TF3FD
UT WOS:000621891200001
DA 2022-02-10
ER

PT C
AU Nishio, T
   Inoue, Y
   Nakayama, Y
   Katsurai, M
AF Nishio, Takayuki
   Inoue, Yoshiaki
   Nakayama, Yu
   Katsurai, Marie
GP IEEE
TI Joint Computation Offloading and Sampling Interval Optimization for
   Accuracy-Guaranteed Surveillance
SO 2021 IEEE 18TH ANNUAL CONSUMER COMMUNICATIONS & NETWORKING CONFERENCE
   (CCNC)
SE IEEE Consumer Communications and Networking Conference
LA English
DT Proceedings Paper
CT IEEE 18th Annual Consumer Communications and Networking Conference
   (CCNC)
CY JAN 09-13, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Commun Soc
ID CAMERA-TRAPS
AB A key aspect to realize Internet of things applications such as industry automation and smart agriculture is to enable realtime and networked automatic monitoring via cloud computing and computer vision. However, to design a networked monitoring system, it is necessary to realize a balance between the monitoring accuracy and monitoring cost, for instance, between the network traffic to transmit images and the computation load. Although the monitoring cost can be decreased by increasing the sampling interval of cameras, it becomes more likely that informative images cannot be obtained; in other words, the monitoring accuracy decreases with a reduction in the amount of data. Moreover, although on-device image processing can decrease the network traffic, a large computation delay may be incurred, limiting the sampling rate of the monitoring system. The objective of this study was to examine the balance between the monitoring accuracy and cost and to develop a joint optimization technique for the sampling interval and computation offloading to minimize the monitoring cost in a networked monitoring system while ensuring a high monitoring accuracy. The main contributions of this paper are that we prove the joint optimization problem can be solved explicitly and to develop an algorithm to obtain the solution of the joint optimization problem. The simulation results demonstrated that the proposed algorithm can reduce the monitoring cost by 24-48% while maximizing the number of nodes ensured to achieve high monitoring accuracy.
C1 [Nishio, Takayuki] Kyoto Univ, Grad Sch Informat, Sakyo Ku, Kyoto 6068501, Japan.
   [Inoue, Yoshiaki] Osaka Univ, Dept Informat & Commun Technol, Osaka, Japan.
   [Nakayama, Yu] Tokyo Univ Agr & Technol, Inst Engn, Tokyo, Japan.
   [Katsurai, Marie] Doshisha Univ, Dept Intelligent Informat Engn & Sci, Kyoto, Japan.
RP Nishio, T (corresponding author), Kyoto Univ, Grad Sch Informat, Sakyo Ku, Kyoto 6068501, Japan.
EM nishio@i.kyoto-u.ac.jp
RI Katsurai, Marie/ABF-1378-2021
OI Nishio, Takayuki/0000-0003-1026-319X
FU JSPS KAKENHIMinistry of Education, Culture, Sports, Science and
   Technology, Japan (MEXT)Japan Society for the Promotion of
   ScienceGrants-in-Aid for Scientific Research (KAKENHI) [JP18K13757];
   KDDI FoundationKDDI Corporation
FX This work was supported in part by JSPS KAKENHI Grant Number JP18K13757
   and the KDDI Foundation.
CR [Anonymous], 2020, P 2020 IEEE 17 ANN C
   Das J, 2015, IEEE INT CON AUTO SC, P462, DOI 10.1109/CoASE.2015.7294123
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hamel S, 2013, METHODS ECOL EVOL, V4, P105, DOI 10.1111/j.2041-210x.2012.00262.x
   Javed O, 2002, LECT NOTES COMPUT SC, V2353, P343
   Kaul S, 2012, IEEE INFOCOM SER, P2731, DOI 10.1109/INFCOM.2012.6195689
   Morabito R, 2018, IEEE NETWORK, V32, P102, DOI 10.1109/MNET.2018.1700175
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Park M, 2015, IEEE COMMUN MAG, V53, P145, DOI 10.1109/MCOM.2015.7263359
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Samie F, 2016, 2016 IEEE 3RD WORLD FORUM ON INTERNET OF THINGS (WF-IOT), P7, DOI 10.1109/WF-IoT.2016.7845499
   Swann DE, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P27, DOI 10.1007/978-4-431-99495-4_3
   Zhang F, 2016, IEEE INTERNET THINGS, V3, P1355, DOI 10.1109/JIOT.2016.2600630
NR 13
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2331-9852
BN 978-1-7281-9794-4
J9 CONSUM COMM NETWORK
PY 2021
DI 10.1109/CCNC49032.2021.9369655
PG 6
WC Computer Science, Hardware & Architecture; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BR7NG
UT WOS:000668563500202
DA 2022-02-10
ER

PT J
AU Lee, SJ
   Lee, JW
   Lee, W
   Jang, C
AF Lee, Sang Jun
   Lee, Jae-Woo
   Lee, Wonju
   Jang, Cheolhun
TI Constrained Multiple Planar Reconstruction for Automatic Camera
   Calibration of Intelligent Vehicles
SO SENSORS
LA English
DT Article
DE computer vision; intelligent vehicles; extrinsic camera calibration;
   structure from motion; convex optimization
ID EGO-MOTION
AB In intelligent vehicles, extrinsic camera calibration is preferable to be conducted on a regular basis to deal with unpredictable mechanical changes or variations on weight load distribution. Specifically, high-precision extrinsic parameters between the camera coordinate and the world coordinate are essential to implement high-level functions in intelligent vehicles such as distance estimation and lane departure warning. However, conventional calibration methods, which solve a Perspective-n-Point problem, require laborious work to measure the positions of 3D points in the world coordinate. To reduce this inconvenience, this paper proposes an automatic camera calibration method based on 3D reconstruction. The main contribution of this paper is a novel reconstruction method to recover 3D points on planes perpendicular to the ground. The proposed method jointly optimizes reprojection errors of image features projected from multiple planar surfaces, and finally, it significantly reduces errors in camera extrinsic parameters. Experiments were conducted in synthetic simulation and real calibration environments to demonstrate the effectiveness of the proposed method.
C1 [Lee, Sang Jun] Jeonbuk Natl Univ, Div Elect Engn, 567 Baekje Daero, Jeonju Si, Jeollabuk Do, South Korea.
   [Lee, Jae-Woo; Lee, Wonju; Jang, Cheolhun] Samsung Adv Inst Technol SAIT, 130 Samsung Ro, Suwon 16678, Gyeonggi Do, South Korea.
RP Lee, JW (corresponding author), Samsung Adv Inst Technol SAIT, 130 Samsung Ro, Suwon 16678, Gyeonggi Do, South Korea.
EM sj.lee@jbnu.ac.kr; magic0ad@gmail.com; wonjulee@kaist.ac.kr;
   c_h.jang@samsung.com
OI Lee, Sang Jun/0000-0002-9312-6299
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2021R1G1A1009792]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIT) (No.
   2021R1G1A1009792). This paper was supported by research funds for newly
   appointed professors of Jeonbuk National University in 2020.
CR Antunes M, 2017, PROC CVPR IEEE, P6691, DOI 10.1109/CVPR.2017.708
   Bazargani H, 2015, IEEE INSTRU MEAS MAG, V18, P20, DOI 10.1109/MIM.2015.7335834
   Bustos AP, 2019, IEEE INT CONF ROBOT, P2385, DOI 10.1109/ICRA.2019.8793749
   Chum O, 2005, COMPUT VIS IMAGE UND, V97, P86, DOI 10.1016/j.cviu.2004.03.004
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599
   Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   HORAUD R, 1989, COMPUT VISION GRAPH, V47, P33, DOI 10.1016/0734-189X(89)90052-2
   Itu R, 2017, INT C INTELL COMP CO, P273, DOI 10.1109/ICCP.2017.8117016
   Jinwoo Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P541, DOI 10.1007/978-3-030-58610-2_32
   Kanatani K, 2016, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-48493-8
   Kanatani Kenichi, 2011, IPSJ Transactions on Computer Vision and Applications, V3, P67, DOI 10.2197/ipsjtcva.3.67
   Kanatani K., 2008, 19 BRIT MACH VIS C B, P173, DOI DOI 10.5244/C.22.18
   Kolupaev A.V., 2018, P 2018 IEEE E W DES, P1, DOI [10.1109/EWDTS.2018.8524815, DOI 10.1109/EWDTS.2018.8524815]
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li Y, 2019, IEEE INT CONF ROBOT, P5439, DOI 10.1109/ICRA.2019.8793706
   Lindstrom P, 2010, PROC CVPR IEEE, P1554, DOI 10.1109/CVPR.2010.5539785
   Lucas B.D., 1981, P IJCAI, P121
   Miksch M, 2010, IEEE INT VEH SYM, P832, DOI 10.1109/IVS.2010.5548048
   Mouragnon E., 2006, P IEEE COMP SOC C CO, V1, P363
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Song SY, 2014, PROC CVPR IEEE, P1566, DOI 10.1109/CVPR.2014.203
   Wang XB, 2009, IEEE SYS MAN CYBERN, P1770, DOI 10.1109/ICSMC.2009.5346611
   Yamaguchi K, 2006, INT C PATT RECOG, P610
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao Ji, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3030161
   Zhou ZH, 2012, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2012.6247837
NR 33
TC 0
Z9 0
U1 1
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JUL
PY 2021
VL 21
IS 14
AR 4643
DI 10.3390/s21144643
PG 13
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA TO6AX
UT WOS:000676993000001
PM 34300383
OA Green Published, gold
DA 2022-02-10
ER

PT C
AU Tingdahl, D
   Van Gool, L
AF Tingdahl, David
   Van Gool, Luc
BE Gagalowicz, A
   Philips, W
TI A Public System for Image Based 3D Model Generation
SO COMPUTER VISION/COMPUTER GRAPHICS COLLABORATION TECHNIQUES, MIRAGE 2011
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 5th International Conference on Computer Vision/Computer Graphics
   Collaboration Techniques (MIRAGE)
CY OCT 10-11, 2011
CL INRIA, Rocquencourt, FRANCE
SP Ghent Univ
HO INRIA
AB This paper presents a service that creates complete and realistic 3D models out of a set of photographs taken with a consumer camera. In contrast to other systems which produce sparse point clouds or individual depth maps, our system automatically generates textured and dense models that require little or no post-processing. Our reconstruction pipeline features automatic camera parameter retrieval from the web and intelligent view selection. This ARC3D system is available as a public, free-to-use web service (http://www.arc3d.be). Results are made available both as a full-resolution model and as a low-resolution for web browser viewing using WebGL.
C1 [Tingdahl, David; Van Gool, Luc] Katholieke Univ Leuven, ESAT PSI, Louvain, Belgium.
RP Tingdahl, D (corresponding author), Katholieke Univ Leuven, ESAT PSI, Louvain, Belgium.
CR Agarwal S., 2009, ICCV
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   Furukawa Y., 2010, CVPR
   Furukawa Y., 2007, CVPR
   Goesele M., 2007, ICCV
   HARALICK RM, 1994, INT J COMPUT VISION, V13, P331
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Hoppe H., 1996, SIGGRAPH
   Kazhdan M., 2006, SGP
   Moons T, 2008, FOUND TRENDS COMPUT, V4, P287, DOI 10.1561/0600000007
   Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Pollefeys M, 2002, LECT NOTES COMPUT SC, V2351, P837
   Snavely N., 2006, SIGGRAPH
   Sons K., 2010, WEB3D
   Strecha C., 2008, CVPR
   Vergauwen M, 2006, MACH VISION APPL, V17, P411, DOI 10.1007/s00138-006-0027-1
   [No title captured]
NR 19
TC 18
Z9 18
U1 0
U2 6
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-642-24135-2; 978-3-642-24136-9
J9 LECT NOTES COMPUT SC
PY 2011
VL 6930
BP 262
EP 273
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BDJ61
UT WOS:000313551100023
DA 2022-02-10
ER

PT C
AU Gnatyuk, V
   Zavalishin, S
   Petrova, X
   Odinokikh, G
   Fartukov, A
   Danilevich, A
   Eremeev, V
   Yoo, J
   Lee, K
   Lee, H
   Shin, D
   Solomatin, I
AF Gnatyuk, Vitaly
   Zavalishin, Sergey
   Petrova, Xenya
   Odinokikh, Gleb
   Fartukov, Alexey
   Danilevich, Alexey
   Eremeev, Vladimir
   Yoo, Juwoan
   Lee, Kwanghyun
   Lee, Heejun
   Shin, Daekyu
   Solomatin, Ivan
GP IEEE
TI Fast Automatic Exposure Adjustment Method for Iris Recognition System
SO PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON ELECTRONICS,
   COMPUTERS AND ARTIFICIAL INTELLIGENCE (ECAI-2019)
SE International Conference on Electronics Computers and Artificial
   Intelligence
LA English
DT Proceedings Paper
CT 11th International Conference on Electronics, Computers and Artificial
   Intelligence (ECAI)
CY JUN 27-29, 2019
CL Pitesti, ROMANIA
SP IEEE, IEEE Romania Sect, IEEE Ind Applicat Soc, Univ Pitesti, Fac Elect, Commun & Comp, Guvernul Romaniei, Ministerul Educatiei Cercetarii Stiintifice
DE iris recognition; auto exposure; computer vision
AB In this paper, we propose a novel algorithm for automatic camera parameter adjustment, which is exploited for getting the correct image exposure required for iris recognition. We use two-step processing, where the first step adjusts the camera parameters on the basis of a single shot, and the second step applies precise iterative adjustment. In order to get the correct iris exposure, we use a weighted mask, which is constructed offline using a set of face images. In contrast to the existing algorithms, our method does not need to be calibrated for a particular camera sensor. We show that the proposed method significantly decreases false rejection rate caused by incorrect image exposure and reduces recognition time.
C1 [Gnatyuk, Vitaly; Zavalishin, Sergey; Petrova, Xenya; Odinokikh, Gleb; Fartukov, Alexey; Danilevich, Alexey; Eremeev, Vladimir; Solomatin, Ivan] Samsung R&D Inst Russia, Bio Recognit Lab, Moscow, Russia.
   [Yoo, Juwoan; Lee, Kwanghyun; Lee, Heejun] Samsung Elect, Multimedia R&D Grp, Suwon, South Korea.
   [Shin, Daekyu] Samsung Elect, Visual SW R&D Grp, Suwon, South Korea.
RP Gnatyuk, V (corresponding author), Samsung R&D Inst Russia, Bio Recognit Lab, Moscow, Russia.
EM v.gnatyuk@samsung.com; s.zavalishin@samsung.com;
   xenya.petrova@samsung.com; g.odinokikh@samsung.com;
   a.fartukov@samsung.com; a.danilevich@samsung.com; v.eremeev@samsung.com;
   juwoan.yoo@samsung.com; kwangh86.lee@samsung.com;
   heejun_lee@samsung.com; daekyu.shin@samsung.com; i.solomatin@samsung.com
CR Battiato S, 2009, IMAGE PROCESS SER, P323
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Dunstone T., 2009, BIOMETRICS SYSTEM DA, DOI [10.1007/978-0-387-77627-9, DOI 10.1007/978-0-387-77627-9]
   Ilstrup D, 2010, LECT NOTES COMPUT SC, V6311, P200, DOI 10.1007/978-3-642-15549-9_15
   Liang JY, 2007, ASICON 2007: 2007 7TH INTERNATIONAL CONFERENCE ON ASIC, VOLS 1 AND 2, PROCEEDINGS, P725, DOI 10.1109/ICASIC.2007.4415733
   Messina G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P549
   Nourani-Vatani N., 2007, AUSTR C ROB AUT 2007
   Odinokikh G. A., 2018, Pattern Recognition and Image Analysis, V28, P516, DOI 10.1134/S105466181803015X
   Shim I, 2019, IEEE T CIRC SYST VID, V29, P1569, DOI 10.1109/TCSVT.2018.2846292
   Su YH, 2015, I SYMP CONSUM ELECTR, P13, DOI 10.1109/ICCE.2015.7066300
   Yang H, 2019, IEEE T VIS COMPUT GR, V25, P2953, DOI 10.1109/TVCG.2018.2865555
NR 11
TC 0
Z9 0
U1 1
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2378-7147
BN 978-1-7281-1624-2
J9 INT C ELECT COMPUT
PY 2019
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BP9NX
UT WOS:000569985400106
DA 2022-02-10
ER

PT C
AU Rother, D
   Patwardhan, KA
   Sapiro, G
AF Rother, Diego
   Patwardhan, Kedar A.
   Sapiro, Guillermo
GP IEEE
TI What can casual walkers tell us about a 3D scene?
SO 2007 IEEE 11TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS 1-6
SE IEEE International Conference on Computer Vision
LA English
DT Proceedings Paper
CT 11th IEEE International Conference on Computer Vision
CY OCT 14-21, 2007
CL Rio de Janeiro, BRAZIL
SP IEEE
AB An approach for incremental learning of a 3D scene from a single static video camera is presented in this paper In particular, we exploit the presence of casual people walking in the scene to infer relative depth, learn shadows, and segment the critical ground structure. Considering that this type of video data is so ubiquitous, this work provides an important step towards 3D scene analysis from single cameras in readily available ordinary videos and movies. On-line 3D scene learning, as presented here, is very important for applications such as scene analysis, foreground refinement, tracking, biometrics, automated camera collaboration, activity analysis, identification, and real-time computer-graphics applications. The main contributions of this work are then two-fold. First, we use the people in the scene to continuously learn and update the 3D scene parameters using an incremental robust (L-1) error minimization. Secondly, models of shadows in the scene are learned using a statistical framework. A symbiotic relationship between the shadow model and the estimated scene geometry is exploited towards incremental mutual improvement. We illustrate the effectiveness of the proposed framework with applications in foreground refinement, automatic segmentation as well as relative depth mapping of the floor/ground, and estimation of 3D trajectories of people in the scene.
C1 [Rother, Diego; Patwardhan, Kedar A.; Sapiro, Guillermo] Univ Minnesota, Minneapolis, MN 55455 USA.
RP Rother, D (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.
EM diroth@umn.edu; kedar@umn.edu; guille@umn.edu
CR CAO X, 2005, P IEEE CVPR, V2, P918
   CRIMINISI A, 2001, ACCURATE VISUAL METR
   Hartley R., 2000, MULTIPLE VIEW GEOMET
   Hoiem D., 2006, CVPR, DOI DOI 10.1109/CVPR.2006.232
   HORPRASERT T, 1999, IEEE ICCV FRAM RAT W
   JUNEJO I, 2006, P IEEE ICVSS
   Krahnstoever N, 2005, IEEE I CONF COMP VIS, P1858
   Mittal A, 2004, PROC CVPR IEEE, P302
   Porikli F, 2005, IEEE I CONF COMP VIS, P891
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   Salvador E, 2001, INT CONF ACOUST SPEE, P1545, DOI 10.1109/ICASSP.2001.941227
   Yatziv L, 2006, J COMPUT PHYS, V212, P393, DOI 10.1016/j.jcp.2005.08.005
NR 12
TC 3
Z9 3
U1 1
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1550-5499
BN 978-1-4244-1630-1
J9 IEEE I CONF COMP VIS
PY 2007
BP 1987
EP 1994
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA BHP11
UT WOS:000255099301128
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Singh, M
   Parameswaran, V
   Ramesh, V
AF Singh, Maneesh
   Parameswaran, Vasu
   Ramesh, Visvanathan
GP IEEE
TI Order consistent change detection via fast statistical significance
   testing
SO 2008 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOLS
   1-12
SE IEEE Conference on Computer Vision and Pattern Recognition
LA English
DT Proceedings Paper
CT IEEE Conference on Computer Vision and Pattern Recognition
CY JUN 23-28, 2008
CL Anchorage, AK
SP IEEE Comp Soc
AB Robustness to illumination variations is a key requirement for the problem of change detection which in turn is a fundamental building block for many visual surveillance applications. The use of ordinal measures is a powerful way of filtering out illumination dependency in representing appearance, and several such measures have been proposed in the past for change detection. By design, these measures are invariant to unknown monotonic transformations that may be caused due to global illumination changes or automatic camera gain. However, previous work has left theoretical and practical gaps that limit their full potential from being realized. For instance, random noise has not been given a principled treatment. In this paper, we formulate the change detection problem in terms of order consistency and show that in the presence of noise with known statistical properties, significance tests for order consistency yield much better results than the state of the art. Since ordinal measures require a reordering of patches, they are usually expensive in practice (O(n*log n) at best). We improve upon this by connecting the problem to monotonic regression, and applying a fast algorithm from the corresponding literature. We also show that good trade offs between speed and accuracy can be made by quantization to achieve accurate and very fast matching algorithms in practice. We demonstrate superior performance on statistical simulations as well as real image sequences.
C1 [Singh, Maneesh; Parameswaran, Vasu; Ramesh, Visvanathan] Siemens Corp Res, Princeton, NJ 08540 USA.
RP Singh, M (corresponding author), Siemens Corp Res, 755 Coll Rd E, Princeton, NJ 08540 USA.
EM maneesh.singh@siemens.com; vasu.parameswaran@siemens.com;
   visvanathan.ramesh@siemens.com
CR ALEFS B, 2006, P IEEE INT T SYST C
   Barlow RE., 1972, STAT INFERENCE ORDER
   BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873
   Bhat DN, 1998, IEEE T PATTERN ANAL, V20, P415, DOI 10.1109/34.677275
   BICEGO M, 2006, IEEE CVPR 06 WORKSH
   Hardle W., 1990, APPL NONPARAMETRIC R
   KE Y, 2004, P IEEE CVPR
   LOWE D, 2004, IJCV, V60
   Marden J. I., 1995, ANAL MODELING RANK D
   MIAN A, 2007, IEEE PAMI IN PRESS
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K., 2005, P ICCV
   MITTAL A, 2006, P IEEE CVPR
   SEZAN MI, 1982, IEEE T MED IMAGING, V1
   TANG F, 2005, IEEE INT WORKSH VS P
   Xie BL, 2004, IMAGE VISION COMPUT, V22, P117, DOI 10.1016/j.imavis.2003.07.003
   YANG RDC, 2005, ICCV
   ZABIH R, 1994, P EUR C COMP VIS, P82
NR 18
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1063-6919
BN 978-1-4244-2242-5
J9 PROC CVPR IEEE
PY 2008
BP 2538
EP 2545
PG 8
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BII46
UT WOS:000259736802033
DA 2022-02-10
ER

PT C
AU Giannakeris, P
   Kaltsa, V
   Avgerinakis, K
   Briassouli, A
   Vrochidis, S
   Kompatsiaris, L
AF Giannakeris, Panagiotis
   Kaltsa, Vagia
   Avgerinakis, Konstantinos
   Briassouli, Alexia
   Vrochidis, Stefanos
   Kompatsiaris, Loannis
GP IEEE
TI Speed Estimation and Abnormality Detection from Surveillance Cameras
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
SP IEEE Comp Soc
ID ANOMALY DETECTION; MODEL
AB Motivated by the increasing industry trends towards autonomous driving, vehicles, and transportation we focus on developing a traffic analysis framework for the automatic exploitation of a large pool of available data relative to traffic applications. We propose a cooperative detection and tracking algorithm for the retrieval of vehicle trajectories in video surveillance footage based on deep CNN features that is ultimately used for two separate traffic analysis modalities: (a) vehicle speed estimation based on a state of the art fully automatic camera calibration algorithm and (b) the detection of possibly abnormal events in the scene using robust optical flow descriptors of the detected vehicles and Fisher vector representations of spatiotemporal visual volumes. Finally we measure the performance of our proposed methods in the NVIDIA AI CITY challenge evaluation dataset.
C1 [Giannakeris, Panagiotis; Kaltsa, Vagia; Avgerinakis, Konstantinos; Vrochidis, Stefanos; Kompatsiaris, Loannis] CERTH ITI, Thessaloniki, Greece.
   [Briassouli, Alexia] Maastricht Univ, Dept Data Sci & Knowledge Engn, Maastricht, Netherlands.
RP Giannakeris, P (corresponding author), CERTH ITI, Thessaloniki, Greece.
EM giannakeris@iti.gr; vagiakal@iti.gr; koafgeri@iti.gr;
   alexia.briassouli@maastrichtuniversity.nl; stefanos@iti.gr; ikom@iti.gr
OI Briassouli, Alexia/0000-0002-0545-3215; Vrochidis,
   Stefanos/0000-0002-2505-9178
FU European CommissionEuropean CommissionEuropean Commission Joint Research
   Centre [700475, 740593]
FX This work was supported by beAWARE [1] and ROBORDER [3] projects,
   partially funded by the European Commission under grant agreement No
   700475 and No 740593.
CR Cheng KW, 2015, IEEE T IMAGE PROCESS, V24, P5288, DOI 10.1109/TIP.2015.2479561
   Dubska M., 2014, BMVC, DOI DOI 10.5244/C.28.42
   Dubska M, 2015, IEEE T INTELL TRANSP, V16, P1162, DOI 10.1109/TITS.2014.2352854
   Filipiak P, 2016, LECT NOTES COMPUT SC, V9597, P803, DOI 10.1007/978-3-319-31204-0_51
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He X. C., 2007, IEEE WORKSH APPL COM, DOI DOI 10.1109/WACV.2007.7
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hospedales T, 2012, INT J COMPUT VISION, V98, P303, DOI 10.1007/s11263-011-0510-7
   Huang JT, 2017, IEEE ICC
   Jeong H, 2014, MACH VISION APPL, V25, P1501, DOI 10.1007/s00138-014-0629-y
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869
   Lan JH, 2014, OPTIK, V125, P289, DOI 10.1016/j.ijleo.2013.06.036
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Nurhadiyatna A, 2013, INT C ADV COMP SCI I, P451, DOI 10.1109/ICACSIS.2013.6761617
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saleemi I, 2009, IEEE T PATTERN ANAL, V31, P1472, DOI 10.1109/TPAMI.2008.175
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Sochor J, 2017, COMPUT VIS IMAGE UND, V161, P87, DOI 10.1016/j.cviu.2017.05.015
   Varadarajan J, 2013, INT J COMPUT VISION, V103, P100, DOI 10.1007/s11263-012-0596-6
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Wen L., 2015, ARXIV151104136
   Yang WQ, 2013, COMPUT VIS IMAGE UND, V117, P1273, DOI 10.1016/j.cviu.2012.08.010
NR 24
TC 5
Z9 6
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 93
EP 99
DI 10.1109/CVPRW.2018.00020
PG 7
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800013
DA 2022-02-10
ER

PT J
AU Ghanem, B
   Cao, YH
   Wonka, P
AF Ghanem, Bernard
   Cao, Yuanhao
   Wonka, Peter
TI Designing Camera Networks by Convex Quadratic Programming
SO COMPUTER GRAPHICS FORUM
LA English
DT Article; Proceedings Paper
CT 36th Annual Conference of the European-Association-for-Computer-Graphics
CY MAY 04-08, 2015
CL Zurich, SWITZERLAND
SP European Assoc Comp Graph
ID PLACEMENT; ALGORITHM
AB In this paper, we study the problem of automatic camera placement for computer graphics and computer vision applications. We extend the problem formulations of previous work by proposing a novel way to incorporate visibility constraints and camera-to-camera relationships. For example, the placement solution can be encouraged to have cameras that image the same important locations from different viewing directions, which can enable reconstruction and surveillance tasks to perform better. We show that the general camera placement problem can be formulated mathematically as a convex binary quadratic program (BQP) under linear constraints. Moreover, we propose an optimization strategy with a favorable trade-off between speed and solution quality. Our solution is almost as fast as a greedy treatment of the problem, but the quality is significantly higher, so much so that it is comparable to exact solutions that take orders of magnitude more computation time. Because it is computationally attractive, our method also allows users to explore the space of solutions for variations in input parameters. To evaluate its effectiveness, we show a range of 3D results on real-world floorplans (garage, hotel, mall, and airport).
C1 [Ghanem, Bernard; Cao, Yuanhao; Wonka, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
RP Ghanem, B (corresponding author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
RI Ghanem, Bernard/J-7605-2017
OI Ghanem, Bernard/0000-0002-5534-587X
FU King Abdullah University of Science and Technology (KAUST)King Abdullah
   University of Science & Technology
FX We thank the anonymous reviewers for their valuable comments and
   suggestions. Special thanks goes to Yoshihiro Kobayashi and Christopher
   Grasso for generating the 3D renderings. Research reported in this
   publication was supported by competitive research funding from King
   Abdullah University of Science and Technology (KAUST).
CR Amriki K, 2011, IEEE INT CON MULTI
   Bodor R, 2007, J INTELL ROBOT SYST, V50, P257, DOI 10.1007/s10846-007-9164-7
   Buchheim C, 2010, LECT NOTES COMPUT SC, V6080, P285, DOI 10.1007/978-3-642-13036-6_22
   Delbos F, 2005, J CONVEX ANAL, V12, P45
   Dunn E, 2006, PATTERN RECOGN LETT, V27, P1209, DOI 10.1016/j.patrec.2005.07.019
   Ercan AO, 2006, LECT NOTES COMPUT SC, V4026, P389
   Erdem UM, 2006, COMPUT VIS IMAGE UND, V103, P156, DOI 10.1016/j.cviu.2006.06.005
   Gonzalez-Barbosa JJ, 2009, IEEE INT CONF ROBOT, P3672
   HORSTER E., 2006, INT WORKSH VID SURV
   Jian Zhao, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1705, DOI 10.1109/ICCVW.2011.6130455
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Mavrinac A, 2014, ACM T SENSOR NETWORK, V10, DOI 10.1145/2530373
   Mavrinac A, 2013, INT J COMPUT VISION, V101, P205, DOI 10.1007/s11263-012-0587-7
   MITTAL A., 2004, EUR C COMP VIS 2004
   O'Rourke J., 1987, ART GALLERY THEOREMS
   Olsson C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383202
   RAM S., 2006, INT WORKSH VID SURV
   Sivaram GSVS, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556140
   STEINITZ A., 2012, THESIS U CALIF
   TARABANIS P., 1995, IEEE T ROBOTICS AUTO
   van der Vlies AE, 2009, PSYCHOL MED, V39, P1907, DOI 10.1017/S0033291709005492
   Yabuta K, 2008, IEEE INT SYMP CIRC S, P2114
   YAO Y., 2008, P IEEE COMP SOC C CO, P1
NR 23
TC 6
Z9 6
U1 0
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0167-7055
EI 1467-8659
J9 COMPUT GRAPH FORUM
JI Comput. Graph. Forum
PD MAY
PY 2015
VL 34
IS 2
BP 69
EP 80
DI 10.1111/cgf.12542
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CN3LB
UT WOS:000358326600011
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Bartl, V
   Herout, A
AF Bartl, Vojtech
   Herout, Adam
GP IEEE
TI OptInOpt: Dual Optimization for Automatic Camera Calibration by
   Multi-Target Observations
SO 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL
   BASED SURVEILLANCE (AVSS)
LA English
DT Proceedings Paper
CT 16th IEEE International Conference on Advanced Video and Signal Based
   Surveillance (AVSS)
CY SEP 18-21, 2019
CL Taipei, TAIWAN
SP IEEE
AB In this paper, we propose a new approach to automatic calibration of surveillance cameras. The proposed method is based on observing rigid objects in the scene and automatically estimating landmarks on these objects. The proposed approach can use arbitrary rigid objects, as was verified by experiments with a synthetic dataset, but vehicles were used during our experiments with real-life data. Landmarks on objects automatically detected by a convolutional neural network together with corresponding 3D positions in the object coordinate system are exploited during the camera calibration process. To determine 3D positions of the landmarks, fine-grained classification of the detected vehicles in the image plane is necessary. The proposed calibration method consists of dual optimization - optimization of objects' positions in the world coordinate system and also optimization of the calibration parameters to minimize the re-projection error of the localized landmarks. The experiments show improvement in calibration accuracy over the existing method solving a similar problem furthermore with fewer restrictions on the input data. The calibration error on a real world dataset decreased from 6.88% to 2.85%.
C1 [Bartl, Vojtech; Herout, Adam] Brno Univ Technol, Graph FIT, Brno, Czech Republic.
RP Bartl, V (corresponding author), Brno Univ Technol, Graph FIT, Brno, Czech Republic.
EM ibartl@fit.vutbr.cz; herout@fit.vutbr.cz
RI Herout, Adam/B-5651-2014
OI Bartl, Vojtech/0000-0003-1792-1028
FU TACR project "SMARTCarPark" [TH03010529]; Ministry of Education, Youth
   and Sports of the Czech Republic from the National Programme of
   Sustainability (NPU II); project IT4Innovations excellence in science
   [LQ1602]
FX This work was supported by TACR project "SMARTCarPark", TH03010529.
   Also, this work was supported by The Ministry of Education, Youth and
   Sports of the Czech Republic from the National Programme of
   Sustainability (NPU II); project IT4Innovations excellence in science -
   LQ1602.
CR Bhardwaj R., 2017, 4 ACM INT C SYST EN
   Cathey FW, 2005, 2005 IEEE Intelligent Vehicles Symposium Proceedings, P777
   Dailey D. J., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P98, DOI 10.1109/6979.880967
   Do V.H., P 2015 12 INT C EL E, P1, DOI [10.1109/ECTICon.2015.7207027, DOI 10.1109/ECTICON.2015.7207027]
   Dubsk M., 2014, BRIT MACH VIS C BMVC
   Filipiak P., 2016, EVOAPPLICATIONS 2016
   Fischler M. A., 1981, RANDOM SAMPLE CONSEN
   Grammatikopoulos L., 2005, P INT S MOD TECHN ED, P332
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He X. C., 2007, IEEE WORKSH APPL COM, DOI DOI 10.1109/WACV.2007.7
   Hesch J. A., 2011, ICCV
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Juranek R, 2015, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2015.274
   Kneip L, 2014, LECT NOTES COMPUT SC, V8689, P127, DOI 10.1007/978-3-319-10590-1_9
   Lan J., 2014, OPTIK INT J LIGHT EL
   Lepetit V., 2008, IJCV
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Llorca D.F., 2016, ITSC, DOI [10.1109/ITSC.2014.6958187, DOI 10.1109/ITSC.2014.6958187]
   Luvizon D., 2000, ICASSP
   Maduro C., 2008, ICIP
   Meng X., 2003, NEW EASY CAMERA CALI
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nurhadiyatna A., 2013, ICACSIS
   Pearce G., 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P373, DOI 10.1109/AVSS.2011.6027353
   Penate-Sanchez A., 2013, T PAMI
   Ren SQ, 2015, ADV NEUR IN, V28
   Sattler T., 2014, ECCV
   Schoepflin T., 2003, T ITS
   Sochor J., 2017, CVIU
   Sochor J., 2016, IEEE C COMP VIS PATT
   Sochor J, 2019, IEEE T INTELL TRANSP, V20, P1633, DOI 10.1109/TITS.2018.2825609
   Song K.-T., 2006, IEEE T SYSTEMMAN B
   Stom R, 1997, J GLOBAL OPTIM, V11, P341, DOI DOI 10.1023/A:1008202821328
   Wang Z., 2017, IEEE INT C COMP VIS
   You XH, 2016, NEUROCOMPUTING, V204, P222, DOI 10.1016/j.neucom.2015.09.132
   Zhang B., 2014, INT J COMPUT VIS ROB, V4, P195
   Zhang Z., 2004, T PAMI
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zheng Y., 2014, CVPR
   ZHENG Y, 2016, CVPR
   Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291
NR 41
TC 0
Z9 0
U1 1
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-0990-9
PY 2019
PG 8
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BO7KL
UT WOS:000524684300085
DA 2022-02-10
ER

PT C
AU Pouget, A
   Ramesh, S
   Giang, M
   Chandrapalan, R
   Tanner, T
   Prussing, M
   Timofte, R
   Ignatov, A
AF Pouget, Angeline
   Ramesh, Sidharth
   Giang, Maximilian
   Chandrapalan, Ramithan
   Tanner, Toni
   Prussing, Moritz
   Timofte, Radu
   Ignatov, Andrey
GP IEEE Comp Soc
TI Fast and Accurate Camera Scene Detection on Smartphones
SO 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGITION
   WORKSHOPS (CVPRW 2021)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 19-25, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, CVF
AB AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone, though the problem of accurate scene prediction has not yet been addressed by the research community. This paper for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD) containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper are available on the project website.
C1 [Pouget, Angeline; Ramesh, Sidharth; Giang, Maximilian; Chandrapalan, Ramithan; Tanner, Toni; Prussing, Moritz; Timofte, Radu; Ignatov, Andrey] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Timofte, R (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM radu.timofte@vision.ee.ethz.ch; andrey@vision.ee.ethz.ch
CR Abadi Martin, 2016, arXiv
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Chiang C.-M., 2020, P IEEE CVF C COMP VI, P502
   Darlow L.N., 2018, ABS181003505 CORR
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dick, 2019, ARXIV190308066
   Garnett, 2018, ADV NEURAL INFORM PR, P1967
   He FJ, 2019, WOODHEAD PUBL FOOD S, P3, DOI 10.1016/B978-0-08-100890-4.00001-9
   Howard A. G, 2017, ARXIV PREPRINT ARXIV
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Ignatov A, 2018, CVPR WORKSH, P691
   Ignatov A, 2021, IEEE COMPUT SOC CONF, P2558, DOI 10.1109/CVPRW53098.2021.00289
   Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P288, DOI 10.1007/978-3-030-11021-5_19
   Ignatov A, 2019, IEEE INT CONF COMP V, P3617, DOI 10.1109/ICCVW.2019.00447
   Ignatov Andrey, 2020, P IEEE CVF C COMP VI
   Ignatov D, 2020, PATTERN RECOGN LETT, V138, P276, DOI 10.1016/j.patrec.2020.07.033
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Krizhevsky A, 2009, THESIS U TORONTO, DOI 10.1.1.222.9220
   Li YW, 2019, IEEE I CONF COMP VIS, P5622, DOI 10.1109/ICCV.2019.00572
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339D\, 10.1109/ICCV.2019.00339]
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293
   Van Gool L., 2020, INT C MACH LEARN, P7392
   Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie SN, 2019, IEEE I CONF COMP VIS, P1284, DOI 10.1109/ICCV.2019.00137
   Yang JW, 2019, PROC CVPR IEEE, P7300, DOI 10.1109/CVPR.2019.00748
   Yaohui Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13166, DOI 10.1109/CVPR42600.2020.01318
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197
   Yu F., 2015, CONSTRUCTION LARGE S
   Zhou B., 2014, ADV NEURAL INFORM PR, P487
   ZHOU GB, 2017, P IEEE C COMP VIS PA, P633
NR 37
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2160-7508
BN 978-1-6654-4899-4
J9 IEEE COMPUT SOC CONF
PY 2021
BP 2569
EP 2580
DI 10.1109/CVPRW53098.2021.00290
PG 12
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS2PO
UT WOS:000705890202075
OA Green Submitted
DA 2022-02-10
ER

PT C
AU John, V
   Englebienne, G
   Krose, B
AF John, Vijay
   Englebienne, Gwenn
   Krose, Ben
BE Fusiello, A
   Murino, V
   Cucchiara, R
TI Relative Camera Localisation in Non-overlapping Camera Networks Using
   Multiple Trajectories
SO COMPUTER VISION - ECCV 2012, PT III
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 12th European Conference on Computer Vision (ECCV)
CY OCT 07-13, 2012
CL Florence, ITALY
SP Google, Natl Robot Engn Ctr, Adobe, Microsoft Res, Mitsubishi Elect, Mobileye, Nvidia, Point Grey, Technicolor, Toshiba, Toyota, Datalogic, IBM Res, ST, Univ Studi Firenze, Univ Cambridge, Ente Cassa Risparmio Firenze
AB In this article we present an automatic camera calibration algorithm using multiple trajectories in a multiple camera network with non-overlapping field-of-views (FOV). Visible trajectories within a camera FOV are assumed to be measured with respect to the camera local co-ordinate system. Calibration is performed by aligning each camera local co-ordinate system with a pre-defined global co-ordinate system using three steps. Firstly, extrinsic pair-wise calibration parameters are estimated using particle swarm optimisation and Kalman filtering. The resulting pair-wise calibration estimates are used to generate an initial estimate of network calibration parameters, which are corrected to account for accumulation errors using particle swarm optimisation-based local search. Finally, a Bayesian framework with Metropolis algorithm is adopted and the posterior distribution over the network calibration parameters are estimated. We validate our algorithm using studio and synthetic datasets and compare our approach with existing state-of-the-art algorithms.
C1 [John, Vijay; Englebienne, Gwenn; Krose, Ben] Univ Amsterdam, Intelligent Autonomous Syst Grp, Amsterdam, Netherlands.
RP John, V (corresponding author), Univ Amsterdam, Intelligent Autonomous Syst Grp, Amsterdam, Netherlands.
EM v.c.k.john@uva.nl; englebienne@uva.nl; b.j.a.krose@uva.nl
OI John, Vijay/0000-0002-9553-0906
CR Anjum N, 2011, J ELECTR COMPUT ENG, V2011, DOI 10.1155/2011/604647
   BEICHL I, 2000, COMPUTING SCI ENG
   Chen G., 1990, IEEE T AEROSPACE ELE
   Ghahramani Z., 1996, CRGTR962 U TOR DEP C
   JAVED O, 2003, ICME
   John V, 2010, IMAGE VISION COMPUT, V28, P1530, DOI 10.1016/j.imavis.2010.03.008
   Kelley C., 2003, SOLVING NONLINEAR EQ
   KUMAR R, 2008, CVPR
   Micusik B., 2011, CVPR
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   RAHIMI A, 2004, CVPR
   Shi Y., 1998, IEEE INT C EV COMP P
NR 12
TC 3
Z9 3
U1 1
U2 3
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-642-33885-4; 978-3-642-33884-7
J9 LECT NOTES COMPUT SC
PY 2012
VL 7585
BP 141
EP 150
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods;
   Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BE3TK
UT WOS:000371261400015
OA Bronze
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Despres-Einspenner, ML
   Farine, DR
   Samuni, L
   Angedakin, S
   Arandjelovic, M
   Boesch, C
   Dieguez, P
   Havercamp, K
   Knight, A
   Langergraber, KE
   Kuhl, HS
AF McCarthy, Maureen S.
   Despres-Einspenner, Marie-Lyne
   Farine, Damien R.
   Samuni, Liran
   Angedakin, Samuel
   Arandjelovic, Mimi
   Boesch, Christophe
   Dieguez, Paula
   Havercamp, Kristin
   Knight, Alex
   Langergraber, Kevin E.
   Kuehl, Hjalmar S.
TI Camera Traps Offer a Robust Means for Social Network Analysis in Wild
   Chimpanzees
SO FOLIA PRIMATOLOGICA
LA English
DT Meeting Abstract
C1 [McCarthy, Maureen S.; Despres-Einspenner, Marie-Lyne; Samuni, Liran; Angedakin, Samuel; Arandjelovic, Mimi; Boesch, Christophe; Dieguez, Paula; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Farine, Damien R.] Max Planck Inst Ornithol, Dept Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Konstanz, Dept Biol, Chair Biodivers & Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Oxford, Dept Zool, Edward Grey Inst Field Ornithol, Oxford, England.
   [Samuni, Liran] CSRS, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Havercamp, Kristin] Kyoto Univ, Wildlife Res Ctr, Kyoto, Japan.
   [Knight, Alex] Univ Auckland, Sch Biol Sci, Auckland, New Zealand.
   [Langergraber, Kevin E.] Arizona State Univ, Sch Human Evolut & Social Change, Tempe, AZ USA.
   [Langergraber, Kevin E.] Arizona State Univ, Inst Human Origins, Tempe, AZ USA.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Halle, Germany.
EM maureen_mc@eva.mpg.de
RI Farine, Damien R./Y-2454-2019
OI Farine, Damien R./0000-0003-2208-7613
NR 0
TC 0
Z9 0
U1 0
U2 2
PU KARGER
PI BASEL
PA ALLSCHWILERSTRASSE 10, CH-4009 BASEL, SWITZERLAND
SN 0015-5713
EI 1421-9980
J9 FOLIA PRIMATOL
JI Folia Primatol.
PD MAY
PY 2020
VL 91
IS 3
BP 347
EP 347
PG 1
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA LU6HU
UT WOS:000537855200173
DA 2022-02-10
ER

PT J
AU Marcon, A
   Bongi, P
   Battocchio, D
   Apollonio, M
AF Marcon, Andrea
   Bongi, Paolo
   Battocchio, Daniele
   Apollonio, Marco
TI REM: performance on a high-density fallow deer (Dama dama) population
SO MAMMAL RESEARCH
LA English
DT Article
DE Field test; Fenced area; Total count; Known density; Camera trapping
ID ESTIMATING ANIMAL DENSITY; CAMERA TRAPS; ACTIVITY PATTERNS; DISTANCE;
   DEFENSE
AB We present an application of the random encounter model (REM) to estimate population density of a closed population of fallow deer (Dama dama). REM promises to be a powerful tool for providing density estimates for species which are not individually recognisable, but it still requires thorough testing to assess its limits and performance. In our study area, effective fencing prevents animals from migrating in or out of the area thus assuring a closed population; still the application of REM presented several challenges, including a very high density (more than 600 ind/km(2)) and a constant fission/fusion of groups of the study species. We applied both stratified and unstratified analysis approach, and we estimated density over a range of daily range values. The REM approach underestimates the true density value of about 31% and 28% the CV of the estimates was 0.39 and 0.52, for stratified and unstratified approach, respectively. Still it provides evidence in support of REM as a method for providing density estimates of free-ranging unmarked individuals with a fission-fusion social structure.
C1 [Marcon, Andrea; Battocchio, Daniele; Apollonio, Marco] Univ Sassari, Dept Vet Med, I-07100 Sassari, Italy.
   [Marcon, Andrea] Via Cavin Sala 39-A, I-30036 Santa Maria Di Sala, VE, Italy.
   [Bongi, Paolo] Pzza Don S Venturini 3 Olivola, I-54010 Aulla, Massa, Italy.
RP Marcon, A (corresponding author), Univ Sassari, Dept Vet Med, I-07100 Sassari, Italy.; Marcon, A (corresponding author), Via Cavin Sala 39-A, I-30036 Santa Maria Di Sala, VE, Italy.
EM amarcon.work@gmail.com
OI Marcon, Andrea/0000-0003-3434-1279
FU Italian Navy, La Spezia North Command
FX We are thankful to the Italian Navy, La Spezia North Command, and the
   personnel of the branch office Filatteria Communication Centre,
   Massa-Carrara, Italy, for letting us perform the field work inside the
   Navy Base. We would like to thank Milena Baruffetti and Martina Marin
   for their help with data collection.
CR ALVAREZ F, 1990, J MAMMAL, V71, P692, DOI 10.2307/1381810
   APOLLONIO M, 1989, BEHAV ECOL SOCIOBIOL, V25, P89, DOI 10.1007/BF00302925
   Berzi D., 2010, P INT C WOLV PEOPL T
   Campos-Candela A, 2018, J ANIM ECOL, V87, P825, DOI 10.1111/1365-2656.12787
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chauvenet ALM, 2017, ECOL MODEL, V350, P79, DOI 10.1016/j.ecolmodel.2017.02.007
   Ciuti S, 2008, BEHAV ECOL SOCIOBIOL, V62, P1747, DOI 10.1007/s00265-008-0603-7
   CLUTTONBROCK TH, 1988, BEHAV ECOL SOCIOBIOL, V23, P281, DOI 10.1007/BF00300575
   Core Team R., 2020, R LANG ENV STAT COMP
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Farina A, 1980, LUNIGIANA AMBIENTE S, P103
   Goswami VR, 2012, ANIM CONSERV, V15, P174, DOI 10.1111/j.1469-1795.2011.00501.x
   Hofmeester TR, 2017, REMOTE SENS ECOL CON, V3, P81, DOI 10.1002/rse2.25
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Karanth KU, 2004, ANIM CONSERV, V7, P285, DOI 10.1017/S1367943004001477
   Manzo E, 2012, ACTA THERIOL, V57, P165, DOI 10.1007/s13364-011-0055-8
   Marcon A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0222349
   Mayle B.A, 1999, MANY DEER FIELD GUID
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Meek PD, 2012, AUST MAMMAL, V34, P223, DOI 10.1071/AM11032
   Moeller AK, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2331
   Morellet N, 2007, J APPL ECOL, V44, P634, DOI 10.1111/j.1365-2664.2007.01307.x
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4_1
   Oliveira-Santos LGR, 2008, J TROP ECOL, V24, P563, DOI 10.1017/S0266467408005324
   Carbajal-Borges JP, 2014, TROP CONSERV SCI, V7, P100, DOI 10.1177/194008291400700102
   Rademaker M, 2017, J BIODIVERS ENDANGER, V5, P200, DOI [10.4172/2332-2543.1000200, DOI 10.4172/2332-2543.1000200]
   Ramsey David S.L., 2015, Journal of Wildlife Management, V79, P491
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Seber G. A., 1982, ESTIMATION ANIMAL AB
   Zero VH, 2013, ORYX, V47, P410, DOI 10.1017/S0030605312000324
NR 35
TC 1
Z9 1
U1 0
U2 12
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2199-2401
EI 2199-241X
J9 MAMMAL RES
JI Mammal Res.
PD OCT
PY 2020
VL 65
IS 4
BP 835
EP 841
DI 10.1007/s13364-020-00522-x
EA JUL 2020
PG 7
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA NO3BE
UT WOS:000552760000001
OA Bronze
DA 2022-02-10
ER

PT C
AU Vogels, T
   van Gastel, M
   Wang, WJ
   de Haan, G
AF Vogels, Tom
   van Gastel, Mark
   Wang, Wenjin
   de Haan, Gerard
GP IEEE
TI Fully-automatic camera-based pulse-oximetry during sleep
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
SP IEEE Comp Soc
ID REMOTE-PPG
AB Current routines for the monitoring of sleep require many sensors attached to the patient during a nocturnal observational study, limiting mobility and causing stress and discomfort. Cameras have shown promise in the remote monitoring of pulse rate, respiration and oxygen saturation, which potentially allows a reduction in the number of sensors. Applying these techniques in a sleep setting is challenging, as it is unknown upfront which portion of the skin will be visible, there is no unique skin-color outside the visible range, and the pulsatility is low in infrared. We present a fully-automatic living tissue detection method to enable continuous monitoring of pulse rate and oxygen saturation during sleep. The system is validated on a dataset where various typical sleep scenarios have been simulated. Results show the proposed method to outperform the current state-of-the-art, especially for the estimation of oxygen saturation.
C1 [Vogels, Tom; van Gastel, Mark] Eindhoven Univ Technol, Eindhoven, Netherlands.
   [Wang, Wenjin; de Haan, Gerard] Philips Res, Eindhoven, Netherlands.
RP Vogels, T (corresponding author), Eindhoven Univ Technol, Eindhoven, Netherlands.
EM tommel_vogels@hotmail.com; m.j.h.v.gastel@tue.nl;
   wenjin.wang@philips.com; g.de.haan@philips.com
RI van Gastel, Mark/AAO-9675-2020
CR [Anonymous], 2011, PART REQ BAS SAF ESS
   Bobbia S., 2017, PATTERN RECOGNITION
   . C. on Sleep Disorders Research, 1993, WAK AM NAT SLEEP AL
   Chaichulee S, 2017, IEEE INT CONF AUTOMA, P266, DOI 10.1109/FG.2017.41
   de Haan G, 2014, PHYSIOL MEAS, V35, P1913, DOI 10.1088/0967-3334/35/9/1913
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Gibert G, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P449, DOI 10.1109/AVSS.2013.6636681
   Henriques J. F., 2015, PATTERN ANAL MACHINE
   Idzikowski C., SLEEP POSITION GIVES
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Lempe G., 2013, BILDVERARBEITUNG MED, P99
   Liu H, 2015, LECT NOTES COMPUT SC, V9085, P79, DOI 10.1007/978-3-319-19156-0_9
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Paruthi S., MORBIDITY MORTALITY
   Tsai WH, 1999, AM J RESP CRIT CARE, V159, P43, DOI 10.1164/ajrccm.159.1.9709017
   van Gastel M, 2018, BIOMED OPT EXPRESS, V9, P102, DOI 10.1364/BOE.9.000102
   van Gastel M, 2016, BIOMED OPT EXPRESS, V7, P4941, DOI 10.1364/BOE.7.004941
   van Gastel M, 2016, SCI REP-UK, V6, DOI 10.1038/srep38609
   van Gastel M, 2015, IEEE T BIO-MED ENG, V62, P1425, DOI 10.1109/TBME.2015.2390261
   Van Luijtelaar R., 2011, P AS C COMP VIS, P360
   Viola Paul, 2001, COMP VIS PATT REC 20, V1
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P2781, DOI 10.1109/TBME.2017.2676160
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wang WJ, 2015, IEEE T BIO-MED ENG, V62, P2629, DOI 10.1109/TBME.2015.2438321
   [No title captured]
NR 25
TC 5
Z9 5
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 1430
EP 1438
DI 10.1109/CVPRW.2018.00183
PG 9
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800176
DA 2022-02-10
ER

PT S
AU Deutscher, J
   Isard, M
   MacCormick, J
AF Deutscher, J
   Isard, M
   MacCormick, J
BE Heyden, A
   Sparr, G
   Nielsen, M
   Johansen, P
TI Automatic camera calibration from a single Manhattan image
SO COMPUTER VISION - ECCV 2002, PT IV
SE Lecture Notes in Computer Science
LA English
DT Article; Proceedings Paper
CT 7th European Conference on Computer Vision (ECCV 2002)
CY MAY 28-31, 2002
CL COPENHAGEN, DENMARK
SP IT Univ Copenhagen, Univ Copenhagen, Lund Univ
AB We present a completely automatic method for obtaining the approximate calibration of a camera (alignment to a world frame and focal length) from a single image of an unknown scene, provided only that the scene satisfies a Manhattan world assumption. This assumption states that the imaged scene contains three orthogonal, dominant directions, and is often satisfied by outdoor or indoor views of man-made structures and environments.
   The proposed method combines the calibration likelihood introduced in [5] with a stochastic search algorithm to obtain a MAP estimate of the camera's focal length and alignment. Results on real images of indoor scenes are presented. The calibrations obtained are less accurate than those from standard methods employing a calibration pattern or multiple images. However, the outputs are certainly good enough for common vision tasks such as tracking. Moreover, the results are obtained without any user intervention, from a single image, and without use of a calibration pattern.
C1 Compaq Comp Corp, Syst Res Ctr, Palo Alto, CA 94301 USA.
RP Deutscher, J (corresponding author), Compaq Comp Corp, Syst Res Ctr, 130 Lytton Ave, Palo Alto, CA 94301 USA.
CR Antone ME, 2000, PROC CVPR IEEE, P282, DOI 10.1109/CVPR.2000.854809
   BOUGUET JY, 2001, CAMERA CALIBRATION M
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   CIPOLLA R, 1999, P BRIT MACH VIS C, P382
   Coughlan J.M., 1999, P ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349
   Criminisi A, 2000, INT J COMPUT VISION, V40, P123, DOI 10.1023/A:1026598000963
   Daniilidis K, 1996, PATTERN RECOGN LETT, V17, P1179, DOI 10.1016/0167-8655(96)00073-6
   FAUGERAS O, 1993, 3D COMPUTER VISION
   Hartley R., 2000, MULTIPLE VIEW GEOMET
   Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594
   KANATANI K, 1992, IEEE T ROBOTICS AUTO, V8
   Liebowitz D, 1998, PROC CVPR IEEE, P482, DOI 10.1109/CVPR.1998.698649
   MacLachlan G. J., 1988, MIXTURE MODELS INFER
   MCLEAN GF, 1995, IEEE T PATTERN ANAL, V17, P1090, DOI 10.1109/34.473236
   RAMSHAW L, 2002003 COMP SYST RE
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Watt A, 1992, ADV ANIMATION RENDER
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 18
TC 39
Z9 39
U1 1
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 3-540-43748-7
J9 LECT NOTES COMPUT SC
PY 2002
VL 2353
BP 175
EP 188
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Imaging Science & Photographic Technology
GA BV79J
UT WOS:000180067300012
DA 2022-02-10
ER

PT C
AU Spicer, P
   Bohl, K
   Abramovich, G
   Barhak, J
AF Spicer, Patrick
   Bohl, Kristin
   Abramovich, Gil
   Barhak, Jacob
GP INSTICC
TI Robust calibration of a Reconfigurable Camera Array for Machine Vision
   Inspection (RAMVI) - Using rule-based colour recognition
SO VISAPP 2006: PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON
   COMPUTER VISION THEORY AND APPLICATIONS, VOL 1
LA English
DT Proceedings Paper
CT 1st International Conference on Computer Vision Theory and Applications
CY FEB 25-28, 2006
CL Polytechn Inst Setubal, Business Sch, Setubal, PORTUGAL
SP Inst Syst & Technologies Informat, Control & Commun, Setubal Polytechn Inst
HO Polytechn Inst Setubal, Business Sch
DE machine vision; Reconfigurable Systems; Camera Calibration; Multiple
   Cameras; Colour Recognition
AB This paper describes a Reconfigurable Array for Machine Vision Inspection (RAMVI) that is able to produce spatially-accurate images combining information obtained from several cameras. Automatic camera calibration is essential for minimizing the changeover time required to reconfigure the array. This paper describes an automatic calibration method that uses a colour coded calibration grid (CCG) to determine the field of view of each camera relative to the other cameras. Since colour is integral to the calibration process, robust colour recognition is essential, particularly since several cameras are involved. Hence, a rule-based colour recognition methodology is described. Results are presented demonstrating the effectiveness of this approach under varying lighting conditions.
C1 [Spicer, Patrick; Bohl, Kristin; Abramovich, Gil; Barhak, Jacob] Univ Michigan, Coll Engn, NSF, Engn Res Ctr Reconfigurable Mfg Syst, 2250 GG Brown Bldg,2350 Hayward St, Ann Arbor, MI 48109 USA.
RP Spicer, P (corresponding author), Univ Michigan, Coll Engn, NSF, Engn Res Ctr Reconfigurable Mfg Syst, 2250 GG Brown Bldg,2350 Hayward St, Ann Arbor, MI 48109 USA.
EM pspicer@umich.edu; kebohl@umich.edu; gabramov@umich.edu;
   jbarhak@umich.edu
FU NSFNational Science Foundation (NSF) [EEC-9529125]
FX The authors gratefully acknowledge the financial support of the
   Engineering Research Center for Reconfigurable Manufacturing Systems
   (NSF Grant EEC-9529125) at the University of Michigan and the valuable
   input from the centers industrial sponsors. Special thanks to Nelson
   Woo, Brent Carr, Kyung Han, David Wintermute, and Steve Erskine for
   their aid in system construction and software programming.
CR ABRAMOVICH G, 2005, 2005 CIRP 3 INT C RE
   ANDERSON I, 1974, 1 COURSE COMBINATORI, P92
   [Anonymous], COMMUNICATIONS TOOLB
   Bouguet J. Y., CAMERA CALIBRATION T
   Brown M., 2003, 9 IEEE INT C COMP VI
   CORNEY D, CLUSTERING MATLAB
   Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468
   KOREN Y, 2002, VISION PRINCIPLES IM, P14
   Naemura T, 2002, IEEE COMPUT GRAPH, V22, P66, DOI 10.1109/38.988748
   Shum HY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P953, DOI 10.1109/ICCV.1998.710831
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Vaish V, 2004, PROC CVPR IEEE, P2
   Wilburn B, 2004, PROC CVPR IEEE, P294
   WILBURN B, 2002, P MED PROC 2002 SPIE
   YANG JC, 2002, 13 EUR WORKSH REND
   ZHANG C, 2004, EUR S REND
NR 16
TC 2
Z9 2
U1 0
U2 0
PU INSTICC-INST SYST TECHNOLOGIES INFORMATION CONTROL & COMMUNICATION
PI SETUBAL
PA AVENIDA D MANUEL L, 27A 2 ESQUERDO, SETUBAL, 2910-595, PORTUGAL
BN 972-8865-40-6
PY 2006
BP 131
EP +
PG 2
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BFH74
UT WOS:000241913400021
DA 2022-02-10
ER

PT C
AU Barioni, RR
   Costa, WL
   Neto, JAC
   Figueiredo, LS
   Teichrieb, V
   Quintino, JP
   da Silva, FQB
   Santos, ALM
   Pinho, H
AF Barioni, Ricardo R.
   Costa, Willams L.
   Neto, Jose A. C.
   Figueiredo, Lucas S.
   Teichrieb, Veronica
   Quintino, Jonysberg P.
   da Silva, Fabio Q. B.
   Santos, Andre L. M.
   Pinho, Helder
GP IEEE COMP SOC
TI HuTrain: a Framework for Fast Creation of Real Human Pose Datasets
SO ADJUNCT PROCEEDINGS OF THE 2020 IEEE INTERNATIONAL SYMPOSIUM ON MIXED
   AND AUGMENTED REALITY (ISMAR-ADJUNCT 2020)
LA English
DT Proceedings Paper
CT 19th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY NOV 09-13, 2020
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, VirBELA, Immers Learning Res Network, Liferay, React Real, Qualcomm
DE Motion capture; Camera calibration; Reconstruction; Image processing
AB Image-based body tracking algorithms are useful in several scenarios, such as avatar animations and gesture interaction for VR applications. In the last few years, the best-ranked solutions presented on the state of the art of body tracking (according to the most popular datasets in the field) are intensively based on Convolutional Neural Networks (CNNs) algorithms and use large datasets for training and validation. Although these solutions achieve high precision scores while evaluated with some of these datasets, there are particular tracking challenges (for example, upside-down cases) that are not well-modeled and, therefore, not correctly tracked. Instead of lurking an all-in-one solution for all cases, we propose HuTrain, a framework for creating datasets quickly and easily. HuTrain comprises a series of steps, including automatic camera calibration, refined human pose estimation, and known dataset formats conversion. We show that, with our system, the user can generate human pose datasets, targeting specific tracking challenges for the desired application context, with no need to annotate human pose instances manually.
C1 [Barioni, Ricardo R.; Costa, Willams L.; Neto, Jose A. C.; Figueiredo, Lucas S.; Teichrieb, Veronica] Univ Fed Pernambuco, Voxar Labs, Recife, PE, Brazil.
   [Quintino, Jonysberg P.] Univ Fed Pernambuco, P&D CIn Samsung, Recife, PE, Brazil.
   [da Silva, Fabio Q. B.; Santos, Andre L. M.] Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
   [Pinho, Helder] SiDi, Campinas, SP, Brazil.
RP Barioni, RR (corresponding author), Univ Fed Pernambuco, Voxar Labs, Recife, PE, Brazil.
EM rrb@cin.ufpe.br; wlc2@cin.ufpe.br; jacn@cin.ufpe.br; lsf@cin.ufpe.br;
   vt@cin.ufpe.br; jpq@cin.ufpe.br; fabio@cin.ufpe.br; alms@cin.ufpe.br;
   helder.p@sidi.org.br
OI Teichrieb, Veronica/0000-0003-4685-3634
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Johnson S., 2010, BMVC, DOI [DOI 10.5244/C.24.12, 10.5244/C.24.12]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Tome D., ARXIV PREPRINT ARXIV
NR 9
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
BN 978-1-7281-7675-8
PY 2020
BP 65
EP 66
DI 10.1109/ISMAR-Adjunct51615.2020.00031
PG 2
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Software Engineering; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BS3NJ
UT WOS:000713571300016
DA 2022-02-10
ER

PT J
AU Danaher, MW
   Ward, C
   Zettler, LW
   Covell, CV
AF Danaher, Mark W.
   Ward, Carlton, Jr.
   Zettler, Lawrence W.
   Covell, Charles V., Jr.
TI Pollinia removal and suspected pollination of the endangered ghost
   orchid, Dendrophylax lindenii (Orchidaceae) by various hawk moths
   (Lepidoptera: Sphingidae): another mystery dispelled
SO FLORIDA ENTOMOLOGIST
LA English
DT Article
DE Florida Panther National Wildlife Refuge; Fakahatchee Strand;
   conservation
AB The ghost orchid, Dendrophylax lindenii (Lindl.) Bentham ex Rolfe (Orchidaceae), is a rare, leafless epiphyte restricted to forests in southernmost Florida and western Cuba. The species' appealing floral display, high public profile, and challenging cultivation contribute to its ongoing removal from the wild by unethical collectors. To effectively conserve this and other native orchids that rely on seed for reproduction, a thorough understanding of natural pollination mechanisms is essential. Digital single lens reflex camera traps were used to survey for potential pollinators visiting D. lindenii flowers on the Florida Panther National Wildlife Refuge during the summers of 2016 to 2018. Based on suspected D. lindenii pollinia affixed to photographed moths, we provide visual evidence that D. lindenii is pollinated by at least 2 large hawk moths (Sphingidae) in southern Florida, which include the fig sphinx moth, Pachylia ficus Linnaeus, and pawpaw sphinx moth, Dolba hyloeus Drury (both Lepidoptera: Sphingidae). Species that were documented probing D. lindenii flowers, but lacked pollinia, included the giant sphinx moth (Cocytius antaeus Drury), banded sphinx moth (Eumorpha fasciatus Sulzer), and streaked sphinx moth (Protambulyx strigilis Linnaeus) (all Lepidoptera: Sphingidae). In addition to the aforementioned species of hawk moths (sphinx moths), the seagrape spanworm moth (Ametris nitocris Cramer; Lepidoptera: Geometridae), palamedes swallowtail (Papilio palamedes Drury; Lepidoptera: Papilionidae), monk skipper (Asbolis capucinus Lucas; Lepidoptera: Hesperiidae), Brazilian skipper (Calpodes ethlius Stoll; Lepidoptera: Hesperiidae), and 3 unidentifiable geometrid moths were observed visiting D. lindenii flowers within the study area. During 2017 and 2018, a total of 21 different visits by Lepidoptera were recorded, and the duration of each visit was rarely longer than 1 s. Hawk moth visits were infrequent, but did show some evidence of clustering by species. Measurements of proboscis lengths of the 2 documented pollinators from museum specimens were of sufficient length (50-100 mm) to probe D. lindenii nectar spurs, further lending support to our field observations. Larval food sources of the 2 confirmed pollinators include plant species native to southern Florida, suggesting that these moths are natural pollinators of D. lindenii. Our findings, although preliminary, provide critically needed baseline information that will augment ongoing conservation efforts in southern Florida aimed at the recovery of D. lindenii.
C1 [Danaher, Mark W.] US Fish & Wildlife Serv, Florida Panther Natl Wildlife Refuge, 12085 SR 29 South, Immokalee, FL 34142 USA.
   [Ward, Carlton, Jr.] Florida Wild, 520 East Davis Blvd, Tampa, FL 33606 USA.
   [Zettler, Lawrence W.] Illinois Coll, Dept Biol, 1101 West Coll Ave, Jacksonville, IL 62650 USA.
   [Covell, Charles V., Jr.] Univ Florida, McGuire Ctr Lepidoptera & Biodivers, Florida Museum Nat Hist, Gainesville, FL 32611 USA.
RP Danaher, MW (corresponding author), US Fish & Wildlife Serv, Florida Panther Natl Wildlife Refuge, 12085 SR 29 South, Immokalee, FL 34142 USA.
EM mark_danaher@fws.gov; carltonward@floridawild.com; lwzettle@ic.edu;
   ccovell@flmnh.ufl.edu
CR Brown P.M., 2005, WILD ORCHIDS FLORIDA
   Coile NC, 2003, CONTRIBUTION N DAKOT
   Coopman J, 2019, NATIVE PLANTS J, V19, P100
   Correll DS., 1950, NATIVE ORCHIDS N AM
   Covell Jr CV, 1984, PETERSON FIELD GUIDE
   Dressler R. L, 1981, ORCHIDS NATURAL HIST
   FDEP-Florida Department of Environmental Protection, 2014, FAK STRAND STAT PRES
   Hammer RL., 2002, EVERGLADES WILDFLOWE
   Hoang NH, 2016, ANN BOT, V119, P379
   Langdon KR., 1979, 56 FLOR DEP AGR CONS
   Lind Henirk, 1994, Svensk Botanisk Tidskrift, V88, P185
   Luer C. A., 1972, NATIVE ORCHIDS FLORI
   Mujica EB, 2018, BOT J LINN SOC, V186, P572, DOI 10.1093/botlinnean/box106
   Pailler T., 2019, 7 INT ORCH CONS C 28
   Reese RS., 2010, 20101270 US GEOL SUR
   SADLER JJ, 2011, EUR J ENVIRON SCI, V1, P137
   Sheehan T, 1979, ORCHID GENERA ILLUST
   Sonenshein RS., 2008, 2008 GREAT EV EC RES
   Stewart J., 2006, ANGRAECOID ORCHIDS S
   Stewart S. L., 2008, N AM NATIV ORCHID J, V14, P70
   Swarts ND, 2009, ANN BOT-LONDON, V104, P543, DOI 10.1093/aob/mcp025
   Tuttle J. P., 2007, HAWK MOTHS N AM
   VANDERCINGEL NA, 2007, ORCHID BIOL REV PERS, P201
   Wiegand T, 2013, BIOTROPICA, V45, P441, DOI 10.1111/btp.12025
   Zettler JA, 2012, SOUTHEAST NAT, V11, P127, DOI 10.1656/058.011.0112
   Zettler L.W., 2019, P 22 WORLD ORCH C GU, V2, P136
NR 26
TC 3
Z9 3
U1 9
U2 23
PU FLORIDA ENTOMOLOGICAL SOC
PI LUTZ
PA 16125 E LAKE BURRELL DR, LUTZ, FL 33548 USA
SN 0015-4040
EI 1938-5102
J9 FLA ENTOMOL
JI Fla. Entomol.
PD DEC
PY 2019
VL 102
IS 4
BP 671
EP 683
DI 10.1653/024.102.0401
PG 13
WC Entomology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Entomology
GA KF4ND
UT WOS:000509220000001
OA gold
DA 2022-02-10
ER

PT C
AU Itu, R
   Danescu, R
AF Itu, Razvan
   Danescu, Radu
BE Dubbert, J
   Muller, B
   Meyer, G
TI Machine Learning Based Automatic Extrinsic Calibration of an Onboard
   Monocular Camera for Driving Assistance Applications on Smart Mobile
   Devices
SO ADVANCED MICROSYSTEMS FOR AUTOMOTIVE APPLICATIONS 2018: SMART SYSTEMS
   FOR CLEAN, SAFE AND SHARED ROAD VEHICLES
SE Lecture Notes in Mobility
LA English
DT Proceedings Paper
CT 22nd International Forum on Advanced Microsystems for Automotive
   Applications (AMAA) - Smart Systems for Clean, Safe and Shared Road
   Vehicles
CY SEP 11-12, 2018
CL Berlin, GERMANY
SP European Technol Platform Smart Syst Integrat, VDI VDE Innovat + Technik GmbH, European Commiss
DE Automatic camera calibration; Monocular vision; Convolutional neural
   networks; Smart mobile devices
ID VANISHING POINTS
AB Smart mobile devices can be easily transformed into driving assistance tools or traffic monitoring systems. These devices are placed behind the windshield such that the camera is facing forward to observe the traffic. For the visual information to be useful, the camera must be calibrated, and a proper calibration is laborious and difficult to perform for the average user. In this paper, we propose a calibration technique that requires no input from the user and is able to estimate the extrinsic parameters of the camera: yaw, pitch and roll angles and the height of the camera above the road. The calibration algorithm is based on detecting vehicles using CNN based classifiers, and using statistics about their size and position in the image to estimate the extrinsic parameters via Extended Kalman filters.
C1 [Itu, Razvan; Danescu, Radu] Tech Univ Cluj Napoca, Str Memorandumului 28, Cluj Napoca, Romania.
RP Danescu, R (corresponding author), Tech Univ Cluj Napoca, Str Memorandumului 28, Cluj Napoca, Romania.
EM razvan.itu@cs.utcluj.ro; radu.danescu@cs.utcluj.ro
FU Ministry of Research and Innovation, CNCS - UEFISCDI within PNCDI
   IIIConsiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva
   pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si
   Inovarii (UEFISCDI) [PN-III-P1-1.1-TE-2016-0440]
FX This work was supported by a grant of Ministry of Research and
   Innovation, CNCS - UEFISCDI, project number PN-III-P1-1.1-TE-2016-0440,
   within PNCDI III.
CR Abadi Martin., 2016, ARXIV PREPRINT ARXIV
   Bazin JC, 2012, IEEE INT C INT ROBOT, P4282, DOI 10.1109/IROS.2012.6385802
   Bileschi Stanley, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1457, DOI 10.1109/ICCVW.2009.5457439
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Danescu R, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16101721
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Howard A.G., 2017, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.JAL.2014.11.010
   Itu R, 2017, INT C INTELL COMP CO, P273, DOI 10.1109/ICCP.2017.8117016
   Levinson J, 2013, P ROB SCI SYST, DOI DOI 10.15607/RSS.2013.IX.029
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   MAGEE MJ, 1984, COMPUT VISION GRAPH, V26, P256, DOI 10.1016/0734-189X(84)90188-9
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
NR 13
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 2196-5544
EI 2196-5552
BN 978-3-319-99762-9; 978-3-319-99761-2
J9 LECT N MOBIL
PY 2019
BP 16
EP 28
DI 10.1007/978-3-319-99762-9_2
PG 13
WC Automation & Control Systems; Computer Science, Theory & Methods;
   Engineering, Electrical & Electronic; Remote Sensing; Transportation
   Science & Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Remote
   Sensing; Transportation
GA BP5XP
UT WOS:000558588700002
DA 2022-02-10
ER

PT C
AU Saadatseresht, M
   Samadzadegan, F
   Azizi, A
AF Saadatseresht, M
   Samadzadegan, F
   Azizi, A
GP ieee computer society
TI ANN-based visibility prediction for camera placement in vision metrology
SO 1ST CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS
LA English
DT Proceedings Paper
CT 1st Canadian Conference on Computer and Robot Vision
CY MAY 17-19, 2004
CL Univ Western Ontario, London, CANADA
SP Canadian Soc Image Proc & Pattern Recognit, Int Assoc Pattern Recognit, IEEE Comp Soc
HO Univ Western Ontario
ID DESIGN; SENSOR; SYSTEM
AB Simulated based network design is a powerful technique in vision metrology systems (VMS) to automate camera placement process. Relatively expensive and time-consuming process of building 3D simulated CAD model is the main drawback of this technique which causes to do not be practically attended by users. In this paper, a new visibility prediction analysis using artificial neural network (ANN) is proposed. This flexible and reliable method can be installed in intelligent cameras to predict the visibility of weak points during imaging in order to suggest the best position and situation of next camera station. Our primary tests on a complex network demonstrated high capability of the proposed method.
C1 Univ Tehran, Dept Surveying Engn & Geomat, Tehran 14174, Iran.
RP Saadatseresht, M (corresponding author), Univ Tehran, Dept Surveying Engn & Geomat, Tehran 14174, Iran.
EM msaadat@ut.ac.ir; samadz@ut.ac.ir; aazizi@ut.ac.ir
RI Saadatseresht, Mohammad/B-1894-2018
OI Saadatseresht, Mohammad/0000-0002-7918-3166
CR COWAN CK, 1992, INTELLIGENT ROBOTS C, V11
   Fraser C., 2001, AUSTR SURVEYOR, V46, P5, DOI DOI 10.1080/00050355.2001.10558822
   Fraser C.S., 1996, CLOSE RANGE PHOTOGRA, P371
   Fraser CS, 1998, DIGIT SIGNAL PROCESS, V8, P277, DOI 10.1006/dspr.1998.0321
   FRASER CS, 1984, PHOTOGRAMM ENG REM S, V50, P1115
   Fraser CS, 2000, ISPRS J PHOTOGRAMM, V55, P94, DOI 10.1016/S0924-2716(00)00010-1
   FRASER CS, 2000, ISPRS J PHOTOGRAMMET
   FRASER CS, 2001, 3 INT IM SENS SEM NE
   FRASER CS, 1997, TURK GERM GEOD C BER
   GANCI G, 2001, BOEING LARG SCAL MET
   GANCI G, 1989, REAL TIME IMAGING DY
   *GSI, 2000, MAN V STARS VIS METR, P60
   Hagan M.T., 1996, NEURAL NETWORK DESIG
   HAGAN MT, 1994, IEEE T NEURAL NETWOR, V5, P989, DOI 10.1109/72.329697
   HALL CJ, 1989, SURVEYING LAND INFOR, P127
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   MASON S, 1995, ISPRS J PHOTOGRAMM, V50, P13, DOI 10.1016/0924-2716(95)90117-W
   *MATHW, 2002, MATLAB MAN NEUR NETW
   Nguyen D, 1990, INT JOINT C NEUR NET
   OLAGUE G, 1998, OPTIMAL CAMERA PLACE, P37
   SHORTIS MR, 1998, 21 INT C FIG BRIGHT
   TARABANIS K, 1994, CVGIP-IMAG UNDERSTAN, V59, P340, DOI 10.1006/cviu.1994.1028
   YI SK, 1995, COMPUT VIS IMAGE UND, V61, P122, DOI 10.1006/cviu.1995.1009
NR 23
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
BN 0-7695-2127-4
PY 2004
BP 188
EP 194
DI 10.1109/CCCRV.2004.1301443
PG 7
WC Computer Science, Artificial Intelligence; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BAG13
UT WOS:000222055200026
DA 2022-02-10
ER

PT C
AU Makantasis, K
   Protopapadakis, E
   Doulamis, A
   Grammatikopoulos, L
   Stentoumis, C
AF Makantasis, Konstantinos
   Protopapadakis, Eftychios
   Doulamis, Anastasios
   Grammatikopoulos, Lazaros
   Stentoumis, Christos
BE Fusiello, A
   Murino, V
   Cucchiara, R
TI Monocular Camera Fall Detection System Exploiting 3D Measures: A
   Semi-supervised Learning Approach
SO COMPUTER VISION - ECCV 2012, PT III
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 12th European Conference on Computer Vision (ECCV)
CY OCT 07-13, 2012
CL Florence, ITALY
SP Google, Natl Robot Engn Ctr, Adobe, Microsoft Res, Mitsubishi Elect, Mobileye, Nvidia, Point Grey, Technicolor, Toshiba, Toyota, Datalogic, IBM Res, ST, Univ Studi Firenze, Univ Cambridge, Ente Cassa Risparmio Firenze
DE image motion analysis; semisupervised learning; self calibration; fall
   detection
AB Falls have been reported as the leading cause of injury-related visits to emergency departments and the primary etiology of accidental deaths in elderly. The system presented in this article addresses the fall detection problem through visual cues. The proposed methodology utilize a fast, real-time background subtraction algorithm based on motion information in the scene and capable to operate properly in dynamically changing visual conditions, in order to detect the foreground object and, at the same time, it exploits 3D space's measures, through automatic camera calibration, to increase the robustness of fall detection algorithm which is based on semi-supervised learning. The above system uses a single monocular camera and is characterized by minimal computational cost and memory requirements that make it suitable for real-time large scale implementations.
C1 [Makantasis, Konstantinos; Protopapadakis, Eftychios; Doulamis, Anastasios] Tech Univ Crete, Khania 73100, Greece.
   [Grammatikopoulos, Lazaros] Technol Educ Inst Athens, Athens 12210, Greece.
   [Stentoumis, Christos] Natl Tech Univ Athens, GR-15773 Athens, Greece.
RP Makantasis, K (corresponding author), Tech Univ Crete, Khania 73100, Greece.
EM konst.makantasis@gmail.com; eft.protopapadakis@gmail.com;
   adoulam@cs.ntua.gr; lazaros.pcvg@gmail.com; cstent@mail.ntua.gr
RI Doulamis, Anastasios/AAL-5972-2021; Grammatikopoulos,
   Lazaros/AAY-5691-2021; Protopapadakis, Eftychios/AAP-1371-2021;
   Makantasis, Konstantinos/Q-4475-2018
OI Grammatikopoulos, Lazaros/0000-0002-3858-1352; Protopapadakis,
   Eftychios/0000-0003-3876-0024; Makantasis,
   Konstantinos/0000-0002-0889-2766
CR Bevilacqua A, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P126, DOI 10.1109/ICVGIP.2008.10
   Bianchi F, 2010, IEEE T NEUR SYS REH, V18, P619, DOI 10.1109/TNSRE.2010.2070807
   Debard G., 2011, CAMERA BASED FALL DE
   Diraco G, 2010, DES AUT TEST EUROPE, P1536
   Doulamis N., ACM 3 INT C PERV TEC
   Foroughi H, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P413, DOI 10.1109/ICVGIP.2008.49
   Fu ZM, 2008, IEEE INT SYMP CIRC S, P424, DOI 10.1109/ISCAS.2008.4541445
   Grammatikopoulos L, 2007, ISPRS J PHOTOGRAMM, V62, P64, DOI 10.1016/j.isprsjprs.2007.02.002
   Le T., 2009, BIOM CIRC SYST C BIO, P265
   Nyan MN, 2008, J BIOMECH, V41, P3475, DOI 10.1016/j.jbiomech.2008.08.009
   Qian HM, 2008, 2008 10TH INTERNATIONAL CONFERENCE ON CONTROL AUTOMATION ROBOTICS & VISION: ICARV 2008, VOLS 1-4, P1567, DOI 10.1109/ICARCV.2008.4795758
   Rougier C., 2011, IEEE T CSVT, P611
   Thome N, 2008, IEEE T CIRC SYST VID, V18, P1522, DOI 10.1109/TCSVT.2008.2005606
   Zigel Y, 2009, IEEE T BIO-MED ENG, V56, P2858, DOI 10.1109/TBME.2009.2030171
NR 14
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
BN 978-3-642-33885-4; 978-3-642-33884-7
J9 LECT NOTES COMPUT SC
PY 2012
VL 7585
BP 81
EP 90
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods;
   Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BE3TK
UT WOS:000371261400009
DA 2022-02-10
ER

PT C
AU Lovegrove, S
   Davison, AJ
AF Lovegrove, Steven
   Davison, Andrew J.
BE Daniilidis, K
   Maragos, P
   Paragios, N
TI Real-Time Spherical Mosaicing Using Whole Image Alignment
SO COMPUTER VISION-ECCV 2010, PT III
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 11th European Conference on Computer Vision
CY SEP 05-11, 2010
CL Heraklion, GREECE
SP Inst Natl Rech Informat & Automat, Google, Microsoft Res, Technicolor, Adobe, DynaVox Mayer-Johnson, Eur Res Consortium Informat Math, Gen Elect, IBM, Johnson Controls, Point Grey, Univ Houston, Siemens
DE Real-time tracking; spherical mosaicing; SLAM; auto-calibration
AB When a purely rotating camera observes a general scene, overlapping views are related by a parallax-free warp which can be estimated by direct image alignment methods that iterate to optimise photoconsistency. However, building globally consistent mosaics from video has usually been tackled as an off-line task, while sequential methods suitable for real-time implementation have often suffered from long-term drift. In this paper we present a high performance real-time video mosaicing algorithm based on parallel image alignment via ESM (Efficient Second-order Minimisation) and global optimisation of a map of keyframes over the whole viewsphere. We present real-time results for drift-free camera. rotation tracking and globally consistent spherical mosaicing from a variety of cameras in real scenes, demonstrating high global accuracy and the ability to track very rapid rotation while maintaining solid 30Hz operation. We also show that automatic camera calibration refinement can be straightforwardly built into our framework.
C1 [Lovegrove, Steven; Davison, Andrew J.] Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England.
RP Lovegrove, S (corresponding author), Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England.
EM sl203@doc.ic.ac.uk; ajd@doc.ic.ac.uk
CR Agapito L, 2001, INT J COMPUT VISION, V45, P107, DOI 10.1023/A:1012471930694
   Brown M., 2003, P INT C COMP VIS
   Civera J, 2009, INT J COMPUT VISION, V81, P128, DOI 10.1007/s11263-008-0129-5
   GUTMANN JS, 1999, INT S COMP INT ROB A
   Klein G., 2007, P INT S MIX AUGM REA
   Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733
   Lucas B.D., 1981, P INT JOINT C ART IN
   Malis E., 2004, P IEEE INT C ROB AUT
   Mei C, 2008, IEEE T ROBOT, V24, P1352, DOI 10.1109/TRO.2008.2007941
   Morimoto C., 1997, P IEEE C COMP VIS PA
   Silveira G, 2008, IEEE T ROBOT, V24, P969, DOI 10.1109/TRO.2008.2004829
   STEEDLY D, 2005, P INT C COMP VIS
   Szeliski R., 2006, COMPUTER GRAPHICS VI, V2, P1, DOI DOI 10.1561/0600000009
   SZELISKI R, 1997, ACM T GRAPHICS SIGGR
NR 14
TC 22
Z9 24
U1 0
U2 1
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
BN 978-3-642-15557-4
J9 LECT NOTES COMPUT SC
PY 2010
VL 6313
SI III
BP 73
EP 86
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BTD62
UT WOS:000286578500006
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Kenney, ML
   Belthoff, JR
   Carling, M
   Miller, TA
   Katzner, TE
AF Kenney, Macy L.
   Belthoff, James R.
   Carling, Matthew
   Miller, Tricia A.
   Katzner, Todd E.
TI Spatial and temporal patterns in age structure of Golden Eagles
   wintering in eastern North America
SO JOURNAL OF FIELD ORNITHOLOGY
LA English
DT Article
DE age cohorts; Aquila chrysaetos; camera trap; latitudinal variation;
   wintering behavior
ID AQUILA-CHRYSAETOS; POPULATION-DYNAMICS; CAMERA-TRAP; SPACE USE; SIZE;
   PERFORMANCE; SUMMER; BIRDS; DEER
AB The behavior of wildlife varies seasonally, and that variation can have substantial demographic consequences. This is especially true for long-distance migrants where the use of landscapes varies by season and, sometimes, age cohort. We tested the hypothesis that distributional patterns of Golden Eagles (Aquila chrysaetos) wintering in eastern North America are age-structured (i.e., birds of similar ages winter together) through the analysis of 370,307 images collected by motion-sensitive trail cameras set over bait during the winters of 2012-2013 and 2013-2014. At nine sites with sufficient data for analysis, we documented 145 eagle visits in 2012-2013 and 146 in 2013-2014. We found significant between-year variation in age structure of wintering eastern Golden Eagles, driven largely by annual differences in the proportion of first-winter birds. However, although many other species show spatial structure in wintering behavior, our analysis revealed no latitudinal organization among age cohorts of wintering eastern Golden Eagles. The lack of age-related latitudinal segregation in wintering behavior does not exclude the possibility that these eagles have sex-based or other types of dominance hierarchies that could result in spatial or temporal segregation. Alternatively, other mechanisms such as food availability or habitat structure may determine the distribution and abundance of Golden Eagles in winter.
C1 [Kenney, Macy L.; Belthoff, James R.] Boise State Univ, Raptor Res Ctr, Boise, ID 83725 USA.
   [Kenney, Macy L.; Belthoff, James R.] Boise State Univ, Dept Biol Sci, Boise, ID 83725 USA.
   [Kenney, Macy L.; Carling, Matthew] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
   [Miller, Tricia A.] West Virginia Univ, Div Forestry & Nat Resources, Morgantown, WV 26506 USA.
   [Miller, Tricia A.] Conservat Sci Global, West Cape May, NJ 08204 USA.
   [Katzner, Todd E.] US Geol Survey, Forest & Rangeland Ecosyst Sci Ctr, Boise, ID 83706 USA.
RP Kenney, ML; Belthoff, JR (corresponding author), Boise State Univ, Raptor Res Ctr, Boise, ID 83725 USA.; Kenney, ML; Belthoff, JR (corresponding author), Boise State Univ, Dept Biol Sci, Boise, ID 83725 USA.; Kenney, ML (corresponding author), Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.; Katzner, TE (corresponding author), US Geol Survey, Forest & Rangeland Ecosyst Sci Ctr, Boise, ID 83706 USA.
EM macykenney5@gmail.com; jbeltho@boisestate.edu; tkatzner@usgs.gov
OI Belthoff, James/0000-0002-6051-2353; Miller, Trish/0000-0001-5152-9789
FU Virginia Department of Game and Inland Fisheries through a Federal Aid
   in Wildlife Restoration grant from USFWS, Pennsylvania SWG [T-12,
   T47-R-1]; US DoEUnited States Department of Energy (DOE) [DEEE0003538];
   Charles A. and Anne Morrow Lindbergh Foundation; National Science
   Foundation REU Site AwardNational Science Foundation (NSF) [DBI:
   1263167]; Boise State University's Raptor Research Center; Boise State
   University's Department of Biological Sciences; Boise State University's
   College of Arts and Sciences; Boise State University's Division of
   Research
FX Funding for this work was received from the Virginia Department of Game
   and Inland Fisheries through a Federal Aid in Wildlife Restoration grant
   from USFWS, Pennsylvania SWG grants T-12 and T47-R-1, US DoE grant
   DEEE0003538, Charles A. and Anne Morrow Lindbergh Foundation, and the
   authors' organizations. Jeff Cooper, Michael Lanzone, Kieran O'Malley,
   and many others assisted with numerous phases of this research. At the
   time of this research, MLK and JRB were supported by National Science
   Foundation REU Site Award (DBI: 1263167) to Boise State University, and
   by Boise State University's Raptor Research Center, Department of
   Biological Sciences, College of Arts and Sciences, and Division of
   Research. MLK also received additional logistical support from the
   University of Wyoming Museum of Vertebrates. Any use of trade, product,
   or firm names is for descriptive purposes only and does not imply
   endorsement by the U.S. Government.
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Akesson S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086779
   Bloom Peter H., 2001, North American Bird Bander, V26, P97
   Boulanger JR, 2012, NORTHEAST NAT, V19, P159, DOI 10.1656/045.019.s612
   Calvert AM, 2009, AVIAN CONSERV ECOL, V4
   Czenze ZJ, 2017, OECOLOGIA, V183, P1, DOI 10.1007/s00442-016-3707-1
   Dearing MD, 1997, J MAMMAL, V78, P1156, DOI 10.2307/1383058
   Dennhardt AJ, 2015, BIOL CONSERV, V184, P68, DOI 10.1016/j.biocon.2015.01.003
   Duerr AE, 2015, FUNCT ECOL, V29, P779, DOI 10.1111/1365-2435.12381
   Gill F.B., 2006, ORNITHOLOGY
   Jachowski DS, 2015, WILDLIFE SOC B, V39, P553, DOI 10.1002/wsb.571
   Jimenez J, 2017, SCI REP-UK, V7, DOI 10.1038/srep41036
   Jollie Malcolm, 1947, AUK, V64, P549
   Katzner T, 2015, WILSON J ORNITHOL, V127, P102, DOI 10.1676/14-066.1
   Katzner T, 2012, AUK, V129, P168, DOI 10.1525/auk.2011.11078
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Ketterson E.D., 1983, Current Ornithology, V1, P357
   Kochert MN, 2002, BIRDS N AM
   Komar O, 2005, AUK, V122, P938, DOI 10.1642/0004-8038(2005)122[0938:EOLSSA]2.0.CO;2
   Lazaro J, 2017, CURR BIOL, V27, pR1106, DOI 10.1016/j.cub.2017.08.055
   Luo G, 2019, AVIAN RES, V10, DOI 10.1186/s40657-019-0144-y
   Marra PP, 1998, SCIENCE, V282, P1884, DOI 10.1126/science.282.5395.1884
   Mcintyre CL, 2012, IBIS, V154, P124, DOI 10.1111/j.1474-919X.2011.01181.x
   Mendez D, 2019, IBIS, V161, P867, DOI 10.1111/ibi.12681
   Mettke-Hofmann C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123775
   Miller TA, 2017, CONDOR, V119, P697, DOI 10.1650/CONDOR-16-154.1
   Miller TA, 2016, IBIS, V158, P116, DOI 10.1111/ibi.12331
   Molina David, 2018, Huitzil, V19, P22, DOI 10.28947/hrmo.2018.19.1.308
   Nadjafzadeh M, 2016, IBIS, V158, P1, DOI 10.1111/ibi.12311
   Newton I, 2008, MIGRATION ECOLOGY OF BIRDS, P1
   Norris DR, 2004, P ROY SOC B-BIOL SCI, V271, P59, DOI 10.1098/rspb.2003.2569
   Pande Satish, 2010, Journal of Threatened Taxa, V2, P1214
   Pandolfino ER, 2011, J RAPTOR RES, V45, P236, DOI 10.3356/JRR-10-66.1
   Riley SJ, 2003, ECOSCIENCE, V10, P455, DOI 10.1080/11956860.2003.11682793
   Roncal CM, 2019, J FIELD ORNITHOL, V90, P203, DOI 10.1111/jofo.12299
   Rus AI, 2017, AUK, V134, P485, DOI 10.1642/AUK-16-147.1
   Rushing CS, 2017, ECOLOGY, V98, P2837, DOI 10.1002/ecy.1967
   Saino N, 2017, J ANIM ECOL, V86, P239, DOI 10.1111/1365-2656.12625
   Stewart FEC, 2019, J WILDLIFE MANAGE, V83, P985, DOI 10.1002/jwmg.21657
   Sumasgutner P, 2016, BIRD STUDY, V63, P430, DOI 10.1080/00063657.2016.1214814
   Vukovich M., 2019, J FIELD ORNITHOL, V86, P337
   Ward DH, 2018, J WILDLIFE MANAGE, V82, P362, DOI 10.1002/jwmg.21388
   WATSON J., 2010, GOLDEN EAGLE, V2nd
   Watson JW, 2019, J WILDLIFE MANAGE, V83, P1735, DOI 10.1002/jwmg.21760
   Zar J.H., 2010, BIOSTATISTICAL ANAL, Vfifth
NR 45
TC 0
Z9 0
U1 2
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0273-8570
EI 1557-9263
J9 J FIELD ORNITHOL
JI J. Field Ornithol.
PD MAR
PY 2020
VL 91
IS 1
BP 92
EP 101
DI 10.1111/jofo.12325
EA FEB 2020
PG 10
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA KX4BP
UT WOS:000515834700001
OA Bronze
DA 2022-02-10
ER

PT C
AU Serpa, YR
   Nogueira, MB
   Macedo, PP
   Rodrigues, MAF
AF Serpa, Ygor Reboucas
   Nogueira, Matheus Batista
   Macedo Neto, Pedro Paulo
   Formico Rodrigues, Maria Andreia
BE Rodrigues, N
   Smith, R
   Dias, N
   Vilaca, JL
   Qayumi, K
   Oliveira, E
   Duque, D
TI Evaluating Pose Estimation as a Solution to the Fall Detection Problem
SO 2020 IEEE 8TH INTERNATIONAL CONFERENCE ON SERIOUS GAMES AND APPLICATIONS
   FOR HEALTH (SEGAH 20)
SE IEEE International Conference on Serious Games and Applications for
   Health
LA English
DT Proceedings Paper
CT IEEE 8th International Conference on Serious Games and Applications for
   Health (SeGAH)
CY AUG 12-14, 2020
CL Vancouver, CANADA
SP IEEE
DE fall detection; neural networks; pose estimation; evaluation
AB In this age of evolving technological capabilities, assisted living has proven useful to ease the frailty that comes with aging. Within this context, the detection and prevention of accidents are paramount to ensure a longer life expectancy for the elderly. Over the years, many approaches for fall detection have been proposed, such as ambient sensors, wearable devices, and automated camera monitoring. A recent approach is to use pose estimation software to identify humans and pinpoint the location of their most important joints. This pose information can be later used as features for an effective fall detection system. This scenario begs the question: Can pose estimation methods be as effective as the sensor or other camera-based ones? To answer this question, we analyzed three pose estimation frameworks, totalizing eleven models, paired with a simple neural network classifier. In our experiments, we have obtained competitive results among the state-of-the-art on the UR Fall Detection dataset, a multi-modal fall detection benchmark, comprised of RGB, depth, and acceleration data. More specifically, our best model achieved a sensitivity rate of 94:5% and a specificity rate of 99:9%, in line with the best camera and sensor-based solutions.
C1 [Serpa, Ygor Reboucas; Nogueira, Matheus Batista; Macedo Neto, Pedro Paulo; Formico Rodrigues, Maria Andreia] Univ Fortaleza UNIFOR, Programa Posgrad Informat Aplicada PPGIA, Fortaleza, Ceara, Brazil.
RP Serpa, YR (corresponding author), Univ Fortaleza UNIFOR, Programa Posgrad Informat Aplicada PPGIA, Fortaleza, Ceara, Brazil.
EM yogar.reboucas@gmail.com; matb369@gmail.com; pedroppmm@gmail.com;
   andreia.formico@gmail.com
FU Fundacao Cearense de Apoio ao Desenvolvimento Cientifico e Tecnologico
   (FUNCAP)Fundacao Cearense de Apoio ao Desenvolvimento Cientifico e
   Tecnologico (FUNCAP); University of Fortaleza (UNIFOR)
FX The authors would like to thank Fundacao Cearense de Apoio ao
   Desenvolvimento Cientifico e Tecnologico (FUNCAP) and the University of
   Fortaleza (UNIFOR) for their financial support. Copyright and Reprint
CR Abobakr A, 2018, 2018 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P1
   Adhikari K., 2019, INT J COMPUTER SYSTE, V13, P255
   Andriluka M., IEEE C COMP VIS PATT
   C. Team, 2017, KEY POINT EV METR
   Cai X, 2020, IEEE ACCESS, V8, P44493, DOI 10.1109/ACCESS.2020.2978249
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Dozat T., 2016, P 4 INT C LEARN REPR, P6
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Google, 2019, TENS POS
   Harrou F, 2017, IEEE INSTRU MEAS MAG, V20, P49, DOI 10.1109/MIM.2017.8121952
   Hossin M, 2015, INT J DATA MINING KN, V5, P1, DOI [10.5121/ijdkp.2015.5201, DOI 10.5121/IJDKP.2015.5201]
   Huang ZY, 2018, 2018 4TH INTERNATIONAL CONFERENCE ON UNIVERSAL VILLAGE (IEEE UV 2018)
   Jansi R., 2020, MULTIDIMENSIONAL SYS, P1
   Kasturi S, 2017, INT C CONTR AUTOMAT, P1346, DOI 10.23919/ICCAS.2017.8204202
   Kepski M, 2016, LECT NOTES ARTIF INT, V9648, P414, DOI 10.1007/978-3-319-32034-2_35
   Kwolek B, 2014, COMPUT METH PROG BIO, V117, P489, DOI 10.1016/j.cmpb.2014.09.005
   Li X, 2017, J ELECTR COMPUT ENG, V2017, P1, DOI 10.1155/2017/5232507
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mohd MNH, 2017, IEEE I C SIGNAL IMAG, P407, DOI 10.1109/ICSIPA.2017.8120645
   Nunez-Marcos A, 2017, WIREL COMMUN MOB COM, DOI 10.1155/2017/9474806
   Reboucas Serpa Ygor, 2019, 2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T). Proceedings, P22, DOI 10.1109/SIBGRAPI-T.2019.00008
   Santos GL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071644
   Solbach MD, 2017, IEEE INT CONF COMP V, P1433, DOI 10.1109/ICCVW.2017.170
   Soni PK, 2018, LECT NOTES COMPUT SC, V11278, P220, DOI 10.1007/978-3-030-04021-5_20
   Theodoridis T., 2018, PRECISION MED POWERE, P145, DOI DOI 10.1007/978-981-10-7419-6
   World Health Organization, 2007, WHO GLOB REP FALLP
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 28
TC 0
Z9 0
U1 2
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2330-5649
BN 978-1-7281-9042-6
J9 IEEE INT CONF SERIOU
PY 2020
PG 7
WC Computer Science, Interdisciplinary Applications; Medical Informatics;
   Rehabilitation
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Medical Informatics; Rehabilitation
GA BR0HU
UT WOS:000629061000010
DA 2022-02-10
ER

PT C
AU Ring, L
   Utami, D
   Olafsson, S
   Bickmore, T
AF Ring, Lazlo
   Utami, Dina
   Olafsson, Stefan
   Bickmore, Timothy
BE Traum, D
   Swartout, W
   Khooshabeh, P
   Kopp, S
   Scherer, S
   Leuski, A
TI Increasing Engagement with Virtual Agents Using Automatic Camera Motion
SO INTELLIGENT VIRTUAL AGENTS, IVA 2016
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 16th International Conference on Intelligent Virtual Agents (IVA)
CY SEP 20-23, 2016
CL Los Angeles, CA
SP Alelo, Springer, Univ So Calif, Inst Creat Technologies
DE Relational agent; Cinematography; Natural language understanding
ID EXPRESSION; HUMANS
AB We describe a series of algorithms which automatically control camera position in a virtual environment while a user is engaged in a simulated face-to-face dialog with a single virtual agent. The common objective of the algorithms is to increase user engagement with the interaction. In our work, we describe three different automated camera control systems that: (1) control the camera's position based on topic changes in dialog; (2) use sentiment analysis to control the camera-to-agent distance; and (3) adjust the camera's depth-of-field based on "important" segments of the dialog. Evaluation studies of each method are described. We find that changing camera position based on topic shifts results in significant increases in a self-reported measure of engagement, while the other methods seem to actually decrease user engagement. Interpretations and ramifications of the results are discussed.
C1 [Ring, Lazlo; Utami, Dina; Olafsson, Stefan; Bickmore, Timothy] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
RP Ring, L; Utami, D; Olafsson, S; Bickmore, T (corresponding author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
EM lring@ccs.neu.edu; dinau@ccs.neu.edu; stefanolafs@ccs.neu.edu;
   bickmore@ccs.neu.edu
CR Arijan D., 1976, GRAMMAR FILM LANGUAG
   Battaglino C., 2015, INT8 WORKSH SANT CRU
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   Bickmore T, 2010, APPL ARTIF INTELL, V24, P648, DOI 10.1080/08839514.2010.492259
   Bickmore T, 2009, LECT NOTES ARTIF INT, V5773, P6
   CALAHAN S, 1996, STORYTELLING LIGHTIN
   Canini L, 2011, INT SYMP IMAGE SIG, P253
   Cassell J, 2001, 39TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P106
   Cassell J, 2001, COMP GRAPH, P477
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Collins, 1988, COGNITIVE STRUCTURE
   de Melo C, 2007, LECT NOTES COMPUT SC, V4738, P546
   DeVault D., 2015, AAAI 2015 SPRING S T
   Hymes Del., 1972, DIRECTIONS SOCIOLING
   Lehmann J., 2012, P 20 INT C US MOD AD, P164, DOI [10.1007/978-3-642-31454-4_14, DOI 10.1007/978-3-642-31454-4_14]
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   MARCUS BH, 1993, J SPORT MED PHYS FIT, V33, P83
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Rui Y., 2003, P SIGCHI C HUM FACT, P457
   Schiffrin Deborah, 1987, DISCOURSE MARKERS
   SMITH J, 2000, GRANDCHAIR CONVERSAT
   Tannen D., 1993, FRAMING DISCOURSE
NR 22
TC 5
Z9 5
U1 1
U2 6
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-47665-0; 978-3-319-47664-3
J9 LECT NOTES ARTIF INT
PY 2016
VL 10011
BP 29
EP 39
DI 10.1007/978-3-319-47665-0_3
PG 11
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BG6GQ
UT WOS:000390287300003
DA 2022-02-10
ER

PT C
AU Meingast, M
   Oh, S
   Sastry, S
AF Meingast, Marci
   Oh, Songhwai
   Sastry, Shankar
GP IEEE
TI Automatic camera network localization using object image tracks
SO 2007 IEEE 11TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS 1-6
SE IEEE International Conference on Computer Vision
LA English
DT Proceedings Paper
CT 11th IEEE International Conference on Computer Vision
CY OCT 14-21, 2007
CL Rio de Janeiro, BRAZIL
SP IEEE
ID DATA ASSOCIATION; ALGORITHM
AB Camera networks are being used in more applications as different types of sensor networks are used to instrument large spaces. Here we show a method for localizing the cameras in a camera network to recover the orientation and position up to scale of each camera, even when cameras are wide-baseline or have different photometric properties. Using moving objects in the scene, we use an intra-camera step and an inter-camera step in order to localize. The intra-camera step compares frames from a single camera to build the tracks of the objects in the image plane of the camera. The inter-camera step uses these object image tracks from each camera as features for correspondence between cameras. We demonstrate this idea on both simulated and real data.
C1 [Meingast, Marci; Sastry, Shankar] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Oh, Songhwai] Univ Calif, Sch Engn, Merced, CA USA.
RP Meingast, M (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM marci@eecs.berkeley.edu; songhwai.oh@ucmerced.edu;
   sastry@eecs.berkeley.edu
CR Black J., 2002, P WORKSH MOT VID COM
   COLLINS JB, 1992, IEEE T AERO ELEC SYS, V28, P909, DOI 10.1109/7.256316
   COX IJ, 1994, INT C PATT RECOG, P437, DOI 10.1109/ICPR.1994.576318
   Cucchiara R., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P119, DOI 10.1109/6979.880969
   CUCCHIARA R, 2006, PATTERN RECOGNITION
   DEUTSCHER J, 2000, IEEE C COMP VIS PATT
   FUNIAK S, 2006, 5 INT C INF PROC SEN
   Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594
   KADIR AZT, 2004, ECCV
   Kamijo S., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P108, DOI 10.1109/6979.880968
   Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223
   Langendoen K, 2003, COMPUT NETW, V43, P499, DOI 10.1016/S1389-1286(03)00356-6
   LEE L, 2000, IEEE T PATTERN ANAL
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MANTZEL W, 2004, AS C SIGN SYST COMP
   Masoud O, 2001, IEEE T INTELL TRANSP, V2, P18, DOI 10.1109/6979.911082
   MCCAHILL M, 2004, CCTV SOCIAL CONTROL
   MOORE MT, 2005, USA TODAY       0718
   OH S, 2007, UNPUB IEEE T AUT CON
   Oh S., 2004, P 43 IEEE C DEC CONT
   Okuma K., 2004, EUR C COMP VIS
   POORE A, 1995, PARTITIONING DATA SE, P169
   RAHIMI A, 2004, COMPUTER VISION 0701
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Sinha S., 2004, P IEEE C COMP VIS PA
   SITTLER RW, 1964, IEEE T MIL ELECTRON, VMIL8, P125, DOI 10.1109/TME.1964.4323129
   STEVENSON D, 1995, INT C COMP VIS PAR I
   WOLFF R, 2007, INT J COMPUTER APPL, V29
   YANG Z, 2007, ACM MULTIMEDIA
   Zhao T., 2004, IEEE C COMP VIS PATT
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 31
TC 2
Z9 2
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1550-5499
BN 978-1-4244-1630-1
J9 IEEE I CONF COMP VIS
PY 2007
BP 2980
EP +
PG 4
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA BHP11
UT WOS:000255099302126
DA 2022-02-10
ER

PT C
AU Steele, RM
   Ye, M
   Yang, RG
AF Steele, R. Matt
   Ye, Mao
   Yang, Ruigang
GP IEEE
TI Color Calibration of Multi-Projector Displays through Automatic
   Optimization of Hardware Settings
SO 2009 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPR WORKSHOPS 2009), VOLS 1 AND 2
SE IEEE Conference on Computer Vision and Pattern Recognition
LA English
DT Proceedings Paper
CT IEEE-Computer-Society Conference on Computer Vision and Pattern
   Recognition Workshops
CY JUN 20-25, 2009
CL Miami Beach, FL
SP IEEE Comp Soc
AB We describe a system that performs automatic, camera-based photometric projector calibration by adjusting hardware settings (e.g. brightness, contrast, etc.). The approach has two basic advantages over software-correction methods. First, there is no software interface imposed on graphical programs: all imagery displayed on the projector benefits from the calibration immediately, without render-time overhead or code changes. Secondly,, the approach benefits from the fact that projector hardware settings typically are capable of expanding or shifting color gamuts (e.g. trading off maximum brightness versus darkness of black levels), something that software methods, which only shrink gamuts, cannot do. In practice this means that hardware settings can possibly match colors between projectors while maintaining a larger overall color gamut (e.g. better contrast) than software-only correction can.
   The prototype system is fully automatic. The space of hardware settings is explored by using a computer-controlled universal remote to navigate each projector's menu system. An off-the-shelf camera observes each projector's response curves. A cost function is computed for the curves based on their similarity to each other, as well as intrinsic characteristics, including color balance, black level, gamma, and dynamic range. An approximate optimum is found using a heuristic combinatoric search. Results show significant qualitative improvements in the absolute colors, as well as the color consistency, of the display.
C1 [Steele, R. Matt; Ye, Mao; Yang, Ruigang] Univ Kentucky, Ctr Visualizat & Virtual Environm, Lexington, KY 40507 USA.
RP Steele, RM (corresponding author), Univ Kentucky, Ctr Visualizat & Virtual Environm, Lexington, KY 40507 USA.
OI Yang, Ruigang/0000-0001-5296-6307
CR ASHDOWN M, 2006, PROCAMS
   Debevec P., 1997, ACM SIGGRAPH
   Fujii Kensaku, 2005, CVPR
   *HOM EL, HOM EL TIR 2 1 REM C
   ILIE A, 2005, ICCV
   JUANG R, 2007, CVPR
   SONG P, 2005, PROCAMS
   WALLACE G, 2003, EUR WORKSH VIRT ENV
NR 8
TC 2
Z9 2
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1063-6919
BN 978-1-4244-3994-2
J9 PROC CVPR IEEE
PY 2009
BP 594
EP 599
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BNL85
UT WOS:000274887100089
DA 2022-02-10
ER

PT J
AU Orban, B
   Kabafouako, G
   Morley, R
   Gaugris, CV
   Melville, H
   Gaugris, J
AF Orban, Ben
   Kabafouako, Gerard
   Morley, Robert
   Gaugris, Caroline
   Melville, Haemish
   Gaugris, Jerome
TI Common mammal species inventory utilizing camera trapping in the forests
   of Kouilou Departement, Republic of Congo
SO AFRICAN JOURNAL OF ECOLOGY
LA English
DT Article
DE camera; Congo; forest; mammal; survey; trapping
C1 [Orban, Ben; Kabafouako, Gerard; Morley, Robert; Gaugris, Caroline; Gaugris, Jerome] Flora Fauna & Man Ecol Serv, Tortola, British Virgin Isl.
   [Gaugris, Caroline; Gaugris, Jerome] Univ Witwatersrand, Ctr African Ecol, Sch Anim Plant & Environm Sci, Johannesburg, South Africa.
   [Melville, Haemish] UNISA Univ South Africa, Dept Environm Sci, Pretoria, South Africa.
RP Gaugris, J (corresponding author), Flora Fauna & Man Ecol Serv, Tortola, British Virgin Isl.
EM jeromegaugris@florafaunaman.com
OI Gaugris, Jerome/0000-0002-6606-060X
CR Ancrenaz M., 2012, HDB WILDLIFE MONITOR
   Barnes RFW, 1996, MAMMAL REV, V26, P67, DOI 10.1111/j.1365-2907.1996.tb00147.x
   DesWasseige C., 2014, FOR C BAS STAT FOR 2, P328
   Devers D., 2006, FOR C BAS STAT FOR 2
   Guil F, 2010, EUR J WILDLIFE RES, V56, P633, DOI 10.1007/s10344-009-0353-5
   Gullison T., 2015, GOOD PRACTICES COLLE
   HECKETSWEILER P, 1991, RESERVE CONKOUATI CO
   Hortal J, 2006, J ANIM ECOL, V75, P274, DOI 10.1111/j.1365-2656.2006.01048.x
   IUCN, 2017, IUCN RED LIST THREAT
   Kays Roland, 2011, International Journal of Research and Reviews in Wireless Sensor Networks, V1, P19
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Maisels Fiona, 2007, Primate Conservation, V22, P111, DOI 10.1896/052.022.0110
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Mohd-Azlan J, 2013, RAFFLES B ZOOL, V61, P397
   Mugerwa B, 2013, AFR J ECOL, V51, P21, DOI 10.1111/aje.12004
   O'Brien C., 2009, TERRESTRIAL SMALL MA
   Rostand A. N., 2007, ESTIMATION DENSITE L
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Van Rooyen M, 2016, TROP ECOL, V57, P805
   Walters J., 2010, EVALUATING SUITABILI
NR 22
TC 4
Z9 5
U1 1
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0141-6707
EI 1365-2028
J9 AFR J ECOL
JI Afr. J. Ecol.
PD DEC
PY 2018
VL 56
IS 4
SI SI
BP 750
EP 754
DI 10.1111/aje.12551
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HC1PV
UT WOS:000451574200008
DA 2022-02-10
ER

PT J
AU Wang, L
   Yao, HX
   Cheng, HD
AF Wang, Liang
   Yao, Hongxun
   Cheng, H. D.
TI EFFECTIVE AND AUTOMATIC CALIBRATION USING CONCENTRIC CIRCLES
SO INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Camera calibration; computer vision; concentric circles; planar pattern;
   geometric constraint
ID CAMERA CALIBRATION
AB In this paper, we present an effective, flexible and completely automated camera calibration approach using only one pair of concentric circles. This approach utilizes the characteristics of concentric circles' tangent lines to locate the center of these circles, and finds the geometric constraints for calibration based on the orthogonality formed by a point on the circle and the two intersected points of the circle with the line through the center of the circle. The entire process requires no conic equation fitting and no metric measurement of the test pattern, which is very flexible to implement.
C1 [Wang, Liang; Yao, Hongxun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Heilongjiang Pr, Peoples R China.
   [Cheng, H. D.] Utah State Univ, Dept Comp Sci, Logan, UT 84322 USA.
RP Wang, L (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Heilongjiang Pr, Peoples R China.
EM wangliang@jdl.ac.cn; yhx@vilab.hit.edu.cn; hengda.cheng@usu.edu
CR CHEN Q, 2004, P EUR C COMP VIS, V3, P521
   Faugeras O., 1993, 3 DIMENSIONAL COMPUT
   Fiala M, 2005, PROC CVPR IEEE, P590
   Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658
   Forsyth D. A., 2004, COMPUTER VISION MODE
   FREMONT V, 2002, P INT C ART REAL TEL, P93
   Golub G, 1996, MATRIX COMPUTATION
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Heikkila J, 2000, IEEE T PATTERN ANAL, V22, P1066, DOI 10.1109/34.879788
   Jiang G, 2005, IEEE I CONF COMP VIS, P333
   Kim JS, 2005, IEEE T PATTERN ANAL, V27, P637, DOI 10.1109/TPAMI.2005.80
   KIM JS, 2002, P AS C COMP VIS, V2, P512
   KNIGHT J, 2000, P INT C PATT REC, V1, P1411
   Meng XQ, 2003, PATTERN RECOGN, V36, P1155, DOI 10.1016/S0031-3203(02)00225-X
   Otlu B, 2006, INT J PATTERN RECOGN, V20, P649, DOI 10.1142/S0218001406004946
   Park SW, 2000, REAL-TIME IMAGING, V6, P433, DOI 10.1006/rtim.1999.0199
   Pollefeys M, 2007, COMPUTATIONAL VISION IN NEURAL AND MACHINE SYSTEMS, P85
   Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149
   Semple J. G., 1952, ALGEBRAIC PROJECTIVE
   STRUM P, 2004, P EUR C COMP VIS, V2, P1
   STURM P, 1999, P IEEE C COMP VIS PA, V1, P430
   Triggs B., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P89
   Triggs B, 1997, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.1997.609388
   Trujillo M, 2006, INT J PATTERN RECOGN, V20, P633, DOI 10.1142/S0218001406004922
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Wang GH, 2002, PROC SPIE, V4875, P830, DOI 10.1117/12.477078
   YANG C, 2000, P INT C PATT REC, V1, P1555
   ZHANG Z, 2910 INRIA
   Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   [No title captured]
NR 31
TC 8
Z9 8
U1 0
U2 6
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0218-0014
EI 1793-6381
J9 INT J PATTERN RECOGN
JI Int. J. Pattern Recognit. Artif. Intell.
PD NOV
PY 2008
VL 22
IS 7
BP 1379
EP 1401
DI 10.1142/S0218001408006831
PG 23
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 396NM
UT WOS:000262598400006
DA 2022-02-10
ER

PT C
AU Wang, H
   Chu, P
AF Wang, H
   Chu, P
GP IEEE COMP SOC
TI Voice source localization for automatic camera pointing system in video
   conferencing
SO 1997 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS I - V: VOL I: PLENARY, EXPERT SUMMARIES, SPECIAL,
   AUDIO, UNDERWATER ACOUSTICS, VLSI; VOL II: SPEECH PROCESSING; VOL III:
   SPEECH PROCESSING, DIGITAL SIGNAL PROCESSING; VOL IV: MULTIDIMENSIONAL
   SIGNAL PROCESSING, NEURAL NETWORKS - VOL V: STATISTICAL SIGNAL AND ARRAY
   PROCESSING, APPLICATIONS
LA English
DT Proceedings Paper
CT 1997 IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP 97)
CY APR 21-24, 1997
CL MUNICH, GERMANY
SP IEEE Signal Proc Soc, DPG, GI, ITG, TUM
AB This paper describes the voice source localization algorithm used in the PictureTel automatic camera pointing system (LimeLight(TM), Dynamic Speech Locating Technology). The system uses an array of 46cm wide and 30cm high, which contains 4 microphones, and is mounted on top of the monitor. The three dimensional position of a sound source is calculated from the time delays of 4 pairs of microphones. In time delay estimation, the averaging of signal onsets of each frequency band is combined with phase correlation to reduce the influence of noise and reverberation. With this approach, it is possible to provide reliable three dimensional voice source localization by a small microphone array. Post processing based on a priori knowledge is also introduced to eliminate the influences of reflections from furniture such as tables. Results of speech source localization under real conference room conditions will be given. Some system related issues will also be discussed.
RP Wang, H (corresponding author), PICTURETEL CORP,M-S 635,100 MINUTEMAN RD,ANDOVER,MA 01810, USA.
NR 0
TC 89
Z9 91
U1 1
U2 5
PU I E E E, COMPUTER SOC PRESS
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, LOS ALAMITOS, CA 90720
BN 0-8186-7920-4
J9 INT CONF ACOUST SPEE
PY 1997
BP 187
EP 190
PG 4
WC Acoustics; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Acoustics; Engineering
GA BH95E
UT WOS:A1997BH95E00048
DA 2022-02-10
ER

PT C
AU Santes, M
   Vigueras, JF
AF Santes, Mario
   Flavio Vigueras, Javier
BE Aguirre, AH
   Borja, RM
   Garcia, CAR
TI Automatic Camera Localization, Reconstruction and Segmentation of
   Multi-planar Scenes Using Two Views
SO MICAI 2009: ADVANCES IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 8th Mexican International Conference on Artificial Intelligence
CY NOV 09-13, 2009
CL Guanajuato, MEXICO
SP Mexican Soc Artificial Intelligence, Ctr Invest Matemat
DE computer vision; unsupervised segmentation; camera localization; scene
   reconstruction; two-view geometry
ID CALIBRATION
AB This work addresses two main problems: (i) localization of two cameras observing a 3D scene composed by planar structures; (ii) recovering of the original structure of the scene, i.e. the scene reconstruction and segmentation stages. Although there exist some work intending to deal with these problems, most of them are based on: epipolar geometry, non-linear optimization, or linear systems that do not incorporate geometrical consistency.
   In this paper, we propose an iterative linear algorithm exploiting geometrical and algebraic constraints induced by rigidity and planarity in the scene. Instead of solving a complex multi-linear problem, we solve iteratively several linear problems: coplanar features segmentation, planar projective transferring, epipole computation, and all plane intersections. Linear methods allow our approach to be suitable for real-time localization and 3D reconstruction. Furthermore, our approach does not compute the fundamental matrix; therefore it does not face stability problems commonly associated with explicit epipolar geometry computation.
EM santes@cimat.mx; flavio@cimat.mx
OI Vigueras, Javier/0000-0002-3647-8220
CR Bartoli A., 2000, RR4070 INRIA
   Faugeras O., 1988, RR0856 INRIA
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   MALIS E, 2000, LNCS, V1843, P610
   SIMON G, 2000, P INT S AUGM REAL
   Simon G., 2008, 19 INT C PATT REC IC
   SOLA J, 2007, THESIS LAAS CNRS TOU
   Stephens M, 1988, ALV VIS C, P147, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]
   VIGUERAS JF, 2009, REGISTRATION ITERACT
   Xu G, 2000, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2000.854886
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 11
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-642-05257-6
J9 LECT NOTES ARTIF INT
PY 2009
VL 5845
BP 280
EP 291
PG 12
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BPJ30
UT WOS:000278966100025
DA 2022-02-10
ER

PT C
AU Pinhanez, CS
   Bobick, AF
AF Pinhanez, CS
   Bobick, AF
GP AMER ASSOC ARTIFICIAL INTELLIGENCE
   AMER ASSOC ARTIFICIAL INTELLIGENCE
TI Approximate world models: Incorporating qualitative and linguistic
   information into vision systems
SO PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL
   INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL
   INTELLIGENCE CONFERENCE, VOLS 1 AND 2
LA English
DT Proceedings Paper
CT 13th National Conference on Artificial Intelligence (AAAI 96) / 8th
   Conference on Innovative Applications of Artificial Intelligence (IAAI
   96)
CY AUG 04-08, 1996
CL PORTLAND, OR
SP Amer Assoc Artificial Intelligence
ID REPRESENTATION
AB Approximate world models are coarse descriptions of the elements of a scene, and are intended to be used in the selection and control of vision routines in a vision system. In this paper we present a control architecture in which the approximate models represent the complex relationships among the objects in the world, allowing the vision routines to be situation or context specific. Moreover, because of their reduced accuracy requirements, ap proximate world models can employ qualitative information such as those provided by linguistic descriptions of the scene. The concept is demonstrated in the development of automatic cameras for a TV studio - SmartCams. Results are shown where SmartCams use vision processing of real imagery and information written in the script of a TV show to achieve TV-quality framing.
C1 MIT, Media Lab, Perceptual Comp Grp, Cambridge, MA 02139 USA.
RP Pinhanez, CS (corresponding author), MIT, Media Lab, Perceptual Comp Grp, 20 Ames St, Cambridge, MA 02139 USA.
EM pinhanez@media.mit.edu; bobick@media.mit.edu
RI Pinhanez, Claudio/R-7089-2019
CR ALLEN JF, 1984, ARTIF INTELL, V23, P123, DOI 10.1016/0004-3702(84)90008-0
   BOBICK A, 1995, P ICCV 95 WORKSH CON, P13
   BOBICK AF, 1992, IEEE T PATTERN ANAL, V14, P146, DOI 10.1109/34.121786
   DRAPER BA, 1987, P DARPA IM UND WORKS, P178
   KALITA JK, 1991, THESIS U PENNSYLVANI
   MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020
   NAGEL HH, 1994, ARTIF INTELL REV, V8, P189, DOI 10.1007/BF00849074
   NEWTSON D, 1977, J PERS SOC PSYCHOL, V35, P847, DOI 10.1037/0022-3514.35.12.847
   RIEGER CJ, 1975, CONCEPTUAL INFORMATI, P157
   Schank Roger C., 1975, CONCEPTUAL INFORMATI, P22
   SISKIND JM, 1995, ARTIF INTELL REV, V8, P371, DOI 10.1007/BF00849726
   STRAT TM, 1991, IEEE T PATTERN ANAL, V13, P1050, DOI 10.1109/34.99238
   TARR MJ, 1994, CVGIP-IMAG UNDERSTAN, V60, P65, DOI 10.1006/cviu.1994.1036
   THIBADEAU R, 1986, COGNITIVE SCI, V10, P117, DOI 10.1016/S0364-0213(86)80001-5
   TSOTSOS JK, 1985, COMPUT INTELL, V1, P16
   WILKS Y, 1975, ARTIF INTELL, V6, P53, DOI 10.1016/0004-3702(75)90016-8
NR 16
TC 8
Z9 8
U1 0
U2 0
PU ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI PALO ALTO
PA 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
BN 0-262-51091-X
PY 1996
BP 1116
EP 1123
PG 8
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BN59H
UT WOS:000082323300166
DA 2022-02-10
ER

PT J
AU Xu, BB
   Wang, WS
   Falzon, G
   Kwan, P
   Guo, LF
   Chen, GP
   Tait, A
   Schneider, D
AF Xu, Beibei
   Wang, Wensheng
   Falzon, Greg
   Kwan, Paul
   Guo, Leifeng
   Chen, Guipeng
   Tait, Amy
   Schneider, Derek
TI Automated cattle counting using Mask R-CNN in quadcopter vision system
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
LA English
DT Article
DE Object detection; Deep learning; Remote monitoring; Livestock
   management; Quadcopter vision system
ID OBJECT DETECTION; AERIAL IMAGES; ANIMALS; DETECT
AB The accurate and reliable counting of animals in quadcopter acquired imagery is one of the most promising but challenging tasks in intelligent livestock management in the future. In this paper we demonstrate the application of the cutting-edge instance segmentation framework, Mask R-CNN, in the context of cattle counting in different situations such as extensive production pastures and also in intensive housing such as feedlots. The optimal IoU threshold (0.5) and the full-appearance detection for the algorithm in this study are verified through performance evaluation. Experimental results in this research show the framework's potential to perform reliably in offline quadcopter vision systems with an accuracy of 94% in counting cattle on pastures and 92% in feedlots. Compared with the existing typical competing algorithms, Mask R-CNN outperforms both in the counting accuracy and average precision especially on the datasets with occlusion and overlapping. Our research shows promising steps towards the incorporation of artificial intelligence using quadcopters for enhanced management of animals.
C1 [Xu, Beibei; Wang, Wensheng; Guo, Leifeng] Chinese Acad Agr Sci, Agr Informat Inst, Beijing 100086, Peoples R China.
   [Falzon, Greg; Schneider, Derek] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
   [Falzon, Greg; Schneider, Derek] Univ New England, Precis Agr Res Grp, Armidale, NSW 2351, Australia.
   [Kwan, Paul] Melbourne Inst Technol, Sch Informat Technol & Engn, Melbourne, Vic 3000, Australia.
   [Chen, Guipeng] Jiangxi Acad Agr Sci, Agr Econ & Informat Inst, Nanchang 330200, Jiangxi, Peoples R China.
   [Tait, Amy] Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2351, Australia.
RP Wang, WS (corresponding author), Chinese Acad Agr Sci, Agr Informat Inst, Beijing 100086, Peoples R China.
EM wangwensheng@caas.cn
RI Falzon, Greg A/A-2657-2012
OI Falzon, Greg A/0000-0002-1989-9357; , Beibei/0000-0001-5804-2906
FU Beijing Aokemei Technical Service Company Limited; Fundamental Research
   Funds of Agricultural Information Institute of Chinese Academy of
   Agriculture Sciences, China [JBYW-AII-2019-19]; General Project of
   Jiangxi Province Key Research and Development Plan [20192BBF60053];
   Jiangxi Province Science Foundation for Youths [20192ACBL21023]; Meat
   and Livestock Australia (MLA) (University of New England Animal Ethics)
   [AEC18-308]
FX This research was funded by Beijing Aokemei Technical Service Company
   Limited and also was supported by Fundamental Research Funds of
   Agricultural Information Institute of Chinese Academy of Agriculture
   Sciences, China (JBYW-AII-2019-19), General Project of Jiangxi Province
   Key Research and Development Plan (20192BBF60053) and Jiangxi Province
   Science Foundation for Youths (20192ACBL21023). Imagery of the feedlot
   animals was provided by a University of New England project funded by
   Meat and Livestock Australia (MLA) (University of New England Animal
   Ethics Approval Number AEC18-308) and we are grateful to three private
   farmlands in New England in Australia for their kindly support with data
   collection (University of New England Standard Operating Procedure W14
   Camera Traps and Animal Ethics Approval Number AEC19-009).
CR Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
   Andrew W, 2017, IEEE INT CONF COMP V, P2850, DOI 10.1109/ICCVW.2017.336
   Barbedo JGA, 2018, OUTLOOK AGR, V47, P214, DOI 10.1177/0030727018781876
   Bengio Yoshua, 2009, P 26 ANN INT C MACH, P41, DOI [10.1145/1553374.1553380, DOI 10.1145/1553374.1553380]
   Buric M, 2018, 2018 41ST INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1034, DOI 10.23919/MIPRO.2018.8400189
   Chabot, 2009, SYSTEMATIC EVALUATIO
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chamoso P., 2014, AMBIENT INTELLIGENCE, P71, DOI DOI 10.1007/978-3-319-07596-9_8
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   CHRETIEN L.-P., 2015, INT ARCH PHOTOGRAMM, P241, DOI DOI 10.5194/ISPRSARCHIVES-XL-1-W4-241-2015
   Condon J., 2015, DRONES HOLD PROMISE
   Dalal N., 2021, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Danish M., 2018, THESIS TECHNOLOGICAL, DOI DOI 10.21427/D7S51F
   Descamps S, 2011, BIRD STUDY, V58, P302, DOI 10.1080/00063657.2011.588195
   Dolecheck KA, 2015, J DAIRY SCI, V98, P8723, DOI 10.3168/jds.2015-9645
   Frost AR, 1997, COMPUT ELECTRON AGR, V17, P139, DOI 10.1016/S0168-1699(96)01301-4
   GAO CQ, 2016, NEUROCOMPUTING, V208, P108, DOI DOI 10.1016/J.NEUC0M.2016.01.097
   GIRSHICK R, 2014, CVPR, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gonzalez LF, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010097
   Grenzdorffer G.J., 2013, REMOTE SENSING SPATI, V1, P169, DOI DOI 10.5194/ISPRSARCHIVES-XL-1-W2-169-2013
   Handcock RN, 2009, SENSORS-BASEL, V9, P3586, DOI 10.3390/s90503586
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
   Hollings T, 2018, METHODS ECOL EVOL, V9, P881, DOI 10.1111/2041-210X.12973
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Jail S., 2018, INT J ENG COMPUT SCI, V7, P23908
   Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
   Koski William R., 2009, Aquatic Mammals, V35, P347, DOI 10.1578/AM.35.3.2009.347
   Lee A, 2015, COMP DEEP NEURAL NET
   Lhoest S, 2015, INT ARCH PHOTOGRAMM, V40-3, P355, DOI 10.5194/isprsarchives-XL-3-W3-355-2015
   Li JL, 2007, PROCEEDINGS OF UK-CHINA SPORTS ENGINEERING WORKSHOP, P1
   Liaghat S., 2010, American Journal of Agricultural and Biological Sciences, V5, P50
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
   Marsh J.R., 2008, ASSESSMENT INJECTABL, P1, DOI [10.13031/2013.24845, DOI 10.13031/2013.24845]
   Mejias L., 2013, OCEANS SAN DIEGO 201, DOI [10. 23919/oceans. 2013. 6741088, DOI 10.23919/OCEANS.2013.6741088]
   Neethirajan Suresh, 2017, Sensing and Bio-Sensing Research, V12, P15, DOI 10.1016/j.sbsr.2016.11.004
   Neethirajan S, 2017, BIOSENS BIOELECTRON, V98, P398, DOI 10.1016/j.bios.2017.07.015
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pathare S.P., 2015, DETECTION BLACK BACK
   Qiao Y., 2019, COMPUT ELECTRON AGR, V165, P54
   Radovic M, 2017, J IMAGING, V3, DOI 10.3390/jimaging3020021
   Redmon J., 2018, ARXIV PREPRINT ARXIV
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
   Rivas A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072048
   Ruiz-Garcia L, 2009, SENSORS-BASEL, V9, P4728, DOI 10.3390/s90604728
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sa I, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16081222
   Sadgrove EJ, 2018, COMPUT IND, V98, P183, DOI 10.1016/j.compind.2018.03.014
   Sadgrove EJ, 2017, COMPUT ELECTRON AGR, V139, P204, DOI 10.1016/j.compag.2017.05.017
   Schneider S., 2018, ARXIV180310842
   Sellier N., 2014, AM J AGR SCI TECHNOL, DOI 10.7726/ajast.2014.1008
   Sommer L, 2018, IEEE WINT CONF APPL, P635, DOI 10.1109/WACV.2018.00075
   Stein M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111915
   Sun, 2017, ARXIV171107264
   Tian F. Y., 2013, T CHIN SOC AGR MACHI, V44, P277
   Van Nuffel A, 2015, ANIMALS, V5, P861, DOI 10.3390/ani5030388
   Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
   Viazzi S, 2013, J DAIRY SCI, V96, P257, DOI 10.3168/jds.2012-5806
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   ZHANG W, 1990, APPL OPTICS, V29, P4790, DOI 10.1364/AO.29.004790
   Zhang W., 1988, P ANN C JPN SOC APPL
   Zhou D., 2014, REAL TIME ANIMAL DET
NR 69
TC 31
Z9 32
U1 8
U2 13
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0168-1699
EI 1872-7107
J9 COMPUT ELECTRON AGR
JI Comput. Electron. Agric.
PD APR
PY 2020
VL 171
AR 105300
DI 10.1016/j.compag.2020.105300
PG 12
WC Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Computer Science
GA LC4VS
UT WOS:000525324500013
OA hybrid
DA 2022-02-10
ER

PT C
AU Bullinger, S
   Bodensteiner, C
   Arens, M
AF Bullinger, Sebastian
   Bodensteiner, Christoph
   Arens, Michael
BE Sousa, AA
   Havran, V
   Braz, J
   Bouatouch, K
TI A Photogrammetry-based Framework to Facilitate Image-based Modeling and
   Automatic Camera Tracking
SO GRAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS -
   VOL. 1: GRAPP
LA English
DT Proceedings Paper
CT 16th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP) / 16th
   International Conference on Computer Graphics Theory and Applications
   (GRAPP)
CY FEB 08-10, 2021
CL ELECTR NETWORK
DE Image-based Modeling; Camera Tracking; Photogrammetry; Structure from
   Motion; Multi-view Stereo; Blender
AB We propose a framework that extends Blender to exploit Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks such as sculpting or camera and motion tracking. Applying SfM allows us to determine camera motions without manually defining feature tracks or calibrating the cameras used to capture the image data. With MVS we are able to automatically compute dense scene models, which is not feasible with the built-in tools of Blender. Currently, our framework supports several state-of-the-art SfM and MVS pipelines. The modular system design enables us to integrate further approaches without additional effort. The framework is publicly available as an open source software package.
C1 [Bullinger, Sebastian; Bodensteiner, Christoph; Arens, Michael] Fraunhofer IOSB, Dept Object Recognit, Ettlingen, Germany.
RP Bullinger, S (corresponding author), Fraunhofer IOSB, Dept Object Recognit, Ettlingen, Germany.
CR AliceVision (2020a), MESHR 3D REC SOFTW
   AliceVision (2020b), MESHROOMMAYA MAYA PL
   [Anonymous], 2012, AS C COMP VIS ACCV
   Attenborrow S., 2020, BLENDER PHOTOGRAMMET
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Blender Online Community, 2020, BLEND 3D MOD REND PA
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Cernea D., 2020, OPENMVS MULT STER RE
   Cignoni P., 2008, EUR IT CHAPT C
   Fuhrmann S., 2014, P EUR WORKSH GRAPH C, P11, DOI DOI 10.2312/GCH.20141299
   Fuhrmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601163
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Girardeau-Montaut Daniel, 2020, CLOUDCOMPARE
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Hiestand R, 2020, REGARD3D FREE OPEN S
   Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Jancosek Michal, 2014, Int Sch Res Notices, V2014, P798595, DOI 10.1155/2014/798595
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Langguth Fabian, 2016, P EUR C COMP VIS ECC
   Moulon P, 2012, SCEAUX CASTLE DATASE
   Moulon P., 2013, OPENMVG OPEN MULTIPL
   Schonberger J. L, 2020, COLMAP GEN PURPOSE S
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   SideEffects, 2020, GAM DEV TOOLS HOUD
   Uhik J., 2020, AG PHOT IMP BLEND
   Ummenhofer B., 2017, INT J COMPUT VISION, P1
   Waechter M, 2014, LECT NOTES COMPUT SC, V8693, P836, DOI 10.1007/978-3-319-10602-1_54
   Woo M., 1999, OPENGL PROGRAMMING G
   Wu C., 2011, VISUALSFM VISUAL STR
   Zhou Q.-Y., 2018, ARXIV180109847
NR 32
TC 0
Z9 0
U1 0
U2 0
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-488-6
PY 2021
BP 106
EP 112
DI 10.5220/0010319801060112
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BR6JJ
UT WOS:000661346500010
OA Green Submitted, hybrid
DA 2022-02-10
ER

PT J
AU Wysong, ML
   Iacona, GD
   Valentine, LE
   Morris, K
   Ritchie, EG
AF Wysong, Michael L.
   Iacona, Gwenllian D.
   Valentine, Leonie E.
   Morris, Keith
   Ritchie, Euan G.
TI On the right track: placement of camera traps on roads improves
   detection of predators and shows non-target impacts of feral cat baiting
SO WILDLIFE RESEARCH
LA English
DT Article
DE apex predator; audio lure; dingo (Canis dingo); macropodid;
   mesopredator; occupancy; poison baiting
ID INVASIVE PREDATORS; SAMPLING DESIGN; CAPTURE RATES; FELIS-CATUS;
   CANIS-LUPUS; OCCUPANCY; DINGO; AUSTRALIA; KANGAROOS; ABUNDANCE
AB WR19175_toc.jpg
C1 [Wysong, Michael L.; Valentine, Leonie E.] Univ Western Australia, Sch Biol Sci, 35 Stirling Highway, Crawley, WA 6009, Australia.
   [Iacona, Gwenllian D.] Univ Queensland, Australian Res Council, Sch Biol Sci, Ctr Excellence Environm Decis, St Lucia, Qld 4072, Australia.
   [Morris, Keith] Bentley Delivery Ctr, Dept Biodivers Conservat & Attract, Biodivers & Conservat Sci, Locked Bag 104, Bentley, WA 6983, Australia.
   [Ritchie, Euan G.] Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, 221 Burwood Highway, Burwood, Vic 3125, Australia.
RP Wysong, ML (corresponding author), Univ Western Australia, Sch Biol Sci, 35 Stirling Highway, Crawley, WA 6009, Australia.
EM mlwysong@gmail.com
RI Valentine, Leonie Ellen/G-9963-2012; Wysong, Michael/H-7992-2013;
   Ritchie, Euan/N-1088-2014
OI Valentine, Leonie Ellen/0000-0003-1479-0755; Iacona,
   Gwenllian/0000-0002-7408-3895; Ritchie, Euan/0000-0003-4410-8868
CR Algar D., 2010, Journal of the Royal Society of Western Australia, V93, P133
   Allen L, 1996, WILDLIFE RES, V23, P197, DOI 10.1071/WR9960197
   Anderson K. A., 2002, MODEL SELECTION MULT
   Ballard G, 2020, WILDLIFE RES, V47, P99, DOI 10.1071/WR18188
   Balme GA, 2009, J WILDLIFE MANAGE, V73, P433, DOI 10.2193/2007-368
   Beard J., 1976, VEGETATION MURCHISON
   Bjornstad O. N., 2009, NCF SPATIAL NONPARAM
   Brook L. A, 2013, THESIS
   Brook LA, 2012, J APPL ECOL, V49, P1278, DOI 10.1111/j.1365-2664.2012.02207.x
   Buckmaster T, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0107788
   Burrows ND, 2003, J ARID ENVIRON, V55, P691, DOI 10.1016/S0140-1963(02)00317-8
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   CAUGHLEY G, 1980, AUST WILDLIFE RES, V7, P1, DOI 10.1071/WR9800001
   Christensen Per E. S., 2013, Ecological Management & Restoration, V14, P47, DOI 10.1111/emr.12025
   Colman NJ, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2013.3094
   Comer S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23495-z
   Creel S, 2008, TRENDS ECOL EVOL, V23, P194, DOI 10.1016/j.tree.2007.12.004
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   Denes FV, 2015, METHODS ECOL EVOL, V6, P543, DOI 10.1111/2041-210X.12333
   Doherty TS, 2019, CONSERV LETT, V12, DOI 10.1111/conl.12633
   Doherty TS, 2019, MAMMAL REV, V49, P31, DOI 10.1111/mam.12139
   Doherty TS, 2017, MAMMAL REV, V47, P83, DOI 10.1111/mam.12080
   Doherty TS, 2017, CONSERV LETT, V10, P15, DOI 10.1111/conl.12251
   Doherty TS, 2016, P NATL ACAD SCI USA, V113, P11261, DOI 10.1073/pnas.1602480113
   Doherty TS, 2015, BIOL CONSERV, V190, P60, DOI 10.1016/j.biocon.2015.05.013
   Doherty TS, 2015, ECOL MANAG RESTOR, V16, P124, DOI 10.1111/emr.12158
   Doherty TS, 2015, J BIOGEOGR, V42, P964, DOI 10.1111/jbi.12469
   Dubey JP, 2008, J EUKARYOT MICROBIOL, V55, P467, DOI 10.1111/j.1550-7408.2008.00345.x
   Executive Steering Committee for Australian Vegetation Information, 2003, AUSTR VEG ATTR MAN N
   Fancourt BA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0119303
   Fiske I, 2010, UNMARKED MODELS DATA
   Fleming P., 2001, MANAGING IMPACTS DIN
   Fleming PJS, 2006, AUST J EXP AGR, V46, P753, DOI 10.1071/EA06009
   Gause GF., 2019, STRUGGLE EXISTENCE C
   Geary WL, 2019, J APPL ECOL, V56, P1992, DOI 10.1111/1365-2664.13427
   Greenville AC, 2014, OECOLOGIA, V175, P1349, DOI 10.1007/s00442-014-2977-8
   Gu WD, 2004, BIOL CONSERV, V116, P195, DOI 10.1016/S0006-3207(03)00190-3
   Hamilton N.E.I.L, 2013, CONSERV SCI W AUST, V8, P367
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Hayward MW, 2015, J APPL ECOL, V52, P286, DOI 10.1111/1365-2664.12408
   Heiniger J, 2018, WILDLIFE RES, V45, P518, DOI 10.1071/WR17171
   Hines D.I, 2017, OCCUPANCY ESTIMATION
   Johnston M., 2011, Occasional Papers of the IUCN Species Survival Commission, V42, P182
   Kellner KF, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111436
   Kery M, 2004, BASIC APPL ECOL, V5, P65, DOI 10.1078/1439-1791-00194
   Koch K, 2015, BMC EVOL BIOL, V15, DOI 10.1186/s12862-015-0542-7
   Larrucea ES, 2007, J WILDLIFE MANAGE, V71, P1682, DOI 10.2193/2006-407
   Legge S, 2017, BIOL CONSERV, V206, P293, DOI 10.1016/j.biocon.2016.11.032
   Leo V, 2019, OIKOS, V128, P630, DOI 10.1111/oik.05546
   Letnic M, 2013, OIKOS, V122, P761, DOI 10.1111/j.1600-0706.2012.20425.x
   Letnic M, 2012, BIOL REV, V87, P390, DOI 10.1111/j.1469-185X.2011.00203.x
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2005, ECOLOGY, V86, P1101, DOI 10.1890/04-1060
   MacKenzie DI, 2004, J AGR BIOL ENVIR ST, V9, P300, DOI 10.1198/108571104X3361
   Medina FM, 2011, GLOBAL CHANGE BIOL, V17, P3503, DOI 10.1111/j.1365-2486.2011.02464.x
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Meek PD, 2013, AUST MAMMAL, V35, P123, DOI 10.1071/AM12014
   Milakovic B, 2011, J MAMMAL, V92, P568, DOI 10.1644/10-MAMM-A-040.1
   Moseby KE, 2011, BIOL CONSERV, V144, P2863, DOI 10.1016/j.biocon.2011.08.003
   Moseby KE, 2011, WILDLIFE RES, V38, P338, DOI 10.1071/WR10235
   Moseby Katherine E., 2004, Ecological Management & Restoration, V5, P228, DOI 10.1111/j.1442-8903.2004.209-8.x
   Nimmo DG, 2015, J APPL ECOL, V52, P281, DOI 10.1111/1365-2664.12369
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   Pierpaoli M, 2003, MOL ECOL, V12, P2585, DOI 10.1046/j.1365-294X.2003.01939.x
   Pike JR, 1999, WILDLIFE SOC B, V27, P4
   Pople AR, 2000, WILDLIFE RES, V27, P269, DOI 10.1071/WR99030
   Raiter KG, 2018, BIOL CONSERV, V228, P281, DOI 10.1016/j.biocon.2018.10.011
   Read JL, 2015, WILDLIFE RES, V42, P1, DOI 10.1071/WR14193
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Ritchie EG, 2012, TRENDS ECOL EVOL, V27, P265, DOI 10.1016/j.tree.2012.01.001
   Ritchie EG, 2009, ECOL LETT, V12, P982, DOI 10.1111/j.1461-0248.2009.01347.x
   Rocha DG, 2016, J ZOOL, V300, P205, DOI 10.1111/jzo.12372
   Schmitz OJ, 2000, AM NAT, V155, P141, DOI 10.1086/303311
   Schuette P, 2013, BIOL CONSERV, V158, P301, DOI 10.1016/j.biocon.2012.08.008
   SHORT J, 1983, AUST WILDLIFE RES, V10, P435
   Sollmann R, 2012, MAMM BIOL, V77, P41, DOI 10.1016/j.mambio.2011.06.011
   Spong G, 2002, BEHAV ECOL SOCIOBIOL, V52, P303, DOI 10.1007/s00265-002-0515-x
   Srbek-Araujo AC, 2013, BIOTA NEOTROP, V13, P51, DOI 10.1590/S1676-06032013000200005
   Stephens D.W., 1986, pi
   Stokeld D, 2015, WILDLIFE RES, V42, P642, DOI 10.1071/WR15083
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   THOMSON PC, 1992, WILDLIFE RES, V19, P519, DOI 10.1071/WR9920519
   Tille P. J, 2006, SOIL LANDSCAPES W AU
   TOBLER M. W., 2007, CAMERA BASE VERSION
   Torretta E, 2016, ACTA ETHOL, V19, P123, DOI 10.1007/s10211-015-0231-y
   Towerton AL, 2011, WILDLIFE RES, V38, P208, DOI 10.1071/WR10213
   Wallach AD, 2010, ECOL LETT, V13, P1008, DOI 10.1111/j.1461-0248.2010.01492.x
   Wallach AD, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006861
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wang YW, 2012, WILDLIFE RES, V39, P611, DOI 10.1071/WR11210
   Woinarski J. C., 2014, ACTION PLAN AUSTR MA
   Woinarski J.C., 2019, CATS AUSTR COMPANION
   Wysong ML, 2020, MOV ECOL, V8, DOI 10.1186/s40462-020-00203-z
   Wysong ML, 2019, J MAMMAL, V100, P410, DOI 10.1093/jmammal/gyz040
   Zuur Alain F., 2009, P1
NR 96
TC 8
Z9 8
U1 2
U2 8
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
PY 2020
VL 47
IS 8
BP 557
EP 569
DI 10.1071/WR19175
PG 13
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA OE6NL
UT WOS:000580645200005
DA 2022-02-10
ER

PT J
AU Young, A
   Rogers, P
AF Young, Aaron
   Rogers, Pratt
TI A Review of Digital Transformation in Mining
SO MINING METALLURGY & EXPLORATION
LA English
DT Review
DE Big data; Analytics; Operations technology; Data science; Innovation
ID ARTIFICIAL-INTELLIGENCE; CHALLENGES; MODELS; TECHNOLOGY; INTERNET;
   FUTURE; ISSUES; THINGS
AB Digital transformation (DT) is the process by which entities adapt themselves to modern technology. As digital technology becomes more prevalent (automation, cameras, sensors, touchscreens, artificial intelligence, etc.), there will be increased pressure on companies to leverage it for additional gains. For many companies in the mining industry, this will involve overcoming a steep learning curve. Furthermore, as the long-term success of the mining industry is dependent on a labor force with new skills (data management, analytics, digital literacy, etc.), new curriculum will need to be appended to the mining engineering discipline. This paper considers the mining industry's need for DT. Then, it presents a three-part review of the foundational components of the DT process-ubiquitous data, connectivity, and decision making. Finally, it provides a discussion on the future of DT in mining as well as a research direction, which identifies the need of academia to provide support for mining companies in the form of both research and the education of digitally literate, dynamically capable mining engineers into the coming decades.
C1 [Young, Aaron; Rogers, Pratt] Univ Utah, Min Engn, William Browning Bldg,135 S 1460 E,Room 313, Salt Lake City, UT 84112 USA.
RP Young, A (corresponding author), Univ Utah, Min Engn, William Browning Bldg,135 S 1460 E,Room 313, Salt Lake City, UT 84112 USA.
EM Aaronsyoung@gmail.com
CR Afrapoli AM, 2019, INT J MIN RECLAM ENV, V33, P42, DOI 10.1080/17480930.2017.1336607
   Aggarwal I., 2018, TEAM CREATIVITY COGN
   Anderson R, 2017, INT J MIN SCI TECHNO, V27, P651, DOI 10.1016/j.ijmst.2017.05.020
   [Anonymous], 2017, ANN REP 2017
   [Anonymous], 2018, 2017 ANN REP MET LON
   [Anonymous], 2016, STATE DATA ED 2016
   [Anonymous], 2017, ANN REP
   [Anonymous], 2018, ANN REP
   [Anonymous], 2018, ANN REP
   [Anonymous], 2018, INT REP 2018
   [Anonymous], 2017, MORE TWO THIRDS DATA
   [Anonymous], 2016, REVAMP YOUR BUSINESS
   [Anonymous], 2018, 60 ANN REP 2017 18
   [Anonymous], 2018, ANN REP
   [Anonymous], 2018, CNN WIRE
   [Anonymous], 2003, DICT E BUSINESS
   [Anonymous], 2018, ANN REP 2017
   [Anonymous], 2007, IND ROBOTS PROGRAMMI, P109
   [Anonymous], 2017, ANN REP 2016 17
   [Anonymous], 2012, E MJ ENG MINING J, V213, P84
   Atzori L, 2010, COMPUT NETW, V54, P2787, DOI 10.1016/j.comnet.2010.05.010
   Augusto JC, 2013, HUM-CENT COMPUT INFO, V3, DOI 10.1186/2192-1962-3-12
   Bag D, 2016, BUSINESS ANAL
   Balaba B., 2012, IND INF INDIN 2012 1
   Baldemair R, 2013, IEEE VEH TECHNOL MAG, V8, P24, DOI 10.1109/MVT.2012.2234051
   Bamford T, 2017, INT J MIN RECLAM ENV, V31, P439, DOI 10.1080/17480930.2017.1339170
   Batambuze III E, 2018, PC TECH MAGAZINE
   Beck K., 2001, AGILE MANIFESTO
   Berlioz C-A, 2015, CONVERGENCE DATA ANA
   BizTrends, 2017, BIZTRENDS2017 PEOPLE
   Blome MW, 2020, INT J OCCUP SAF ERGO, V26, P112, DOI 10.1080/10803548.2018.1514135
   Boulter A, 2015, INT J MIN RECLAM ENV, V29, P368, DOI 10.1080/17480930.2015.1086549
   Broekhuizen TLJ, 2018, BUS HORIZONS, V61, P555, DOI 10.1016/j.bushor.2018.03.003
   Business H., 2018, INT DESCR PRED PRESC
   Caldwell Tracey, 2018, Network Security, V2018, P9, DOI 10.1016/S1353-4858(18)30078-3
   Chaulya SK, 2016, SENSING AND MONITORING TECHNOLOGIES FOR MINES AND HAZARDOUS AREAS: MONITORING AND PREDICTION TECHNOLOGIES, P1
   Chen HC, 2012, MIS QUART, V36, P1165
   Chen Y, 2018, BUS HORIZONS, V61, P567, DOI 10.1016/j.bushor.2018.03.006
   Cloud I. M., 2017, KEY MARKETING TRENDS
   Cooke JL., 2014, AGILE PRODUCTIVITY U
   COOPER R, 1971, AM PSYCHOL, V26, P467, DOI 10.1037/h0031539
   Córdova Felisa, 2012, Journal of Technology Management & Innovation, V7, P175, DOI 10.4067/S0718-27242012000100012
   Corporation F-N, 2018, ANN REP
   Corporation N. M, 2017, ANN REP FORM 10 K
   Cory S, 2001, SCIENCE, V293, P2169, DOI 10.1126/science.293.5538.2169
   Cosbey A., 2016, MINING MIRAGE
   Coulson M., 2012, HIST MINING EVENTS T
   Crozier R., 2013, BHP BILLITON OPENS P, V10, P2015
   Cutifani M., 2015, REINVENTING MINING C
   Dasgupta N, 2018, PRACTICAL BIG DATA A
   Desai N., 2016, GLOBALSIGN BLOG
   Dessureault S., 2015, 37 INT S APPL COMP O
   Dessureault S., 2006, CIM B
   Dessureault SD, 2001, INT J SURF MIN RECLA, V15, P6
   Diaz R., 2015, SAG 2015 C P VANC
   Dirican C, 2015, WORLD CONFERENCE ON TECHNOLOGY, INNOVATION AND ENTREPRENEURSHIP, P564, DOI 10.1016/j.sbspro.2015.06.134
   Duffy K., 2015, P FUT MIN C SYDN AUS
   Ebert C, 2018, IEEE SOFTWARE, V35, P16, DOI 10.1109/MS.2018.2801537
   Enji S., 2015, 2015 12 INT C FUZZ S
   Erkayaoglu M., 2015, DATA DRIVEN MINE TO
   Erkayaoglu M, 2019, INT J MIN RECLAM ENV, V33, P409, DOI 10.1080/17480930.2018.1496885
   Evans Dave., 2011, INTERNET THINGS NEXT, DOI DOI 10.1109/IEEESTD.2007.373646
   Fekete J., 2015, THESIS
   Forum WE, 2016, WORLD EC FOR WHIT PA
   Gale M., 2018, LEADER LEADER, V2018, P30
   Gale M, 2017, DIGITAL HELIX TRANSF
   Galindo-Martin M-a, 2018, J BUS RES
   Gandhi P, 2016, HARV BUS REV, V1
   Ganesh K, 2014, MANAGE PROF, P1, DOI 10.1007/978-3-319-05927-3
   Gilchrist A., 2016, IND 4 0 IND INTERNET
   Gilster P., 1997, DIGITAL LITERACY
   Gold B., 2017, MINING MAGAZINE
   Gren L., 2018, STPIS CAISE
   Group F. M, 2017, ANNU REP 2017
   Group L, 2016, 1 DIG DEC BEG DEF DN
   Group N. N, 2018, ANN REP 2017
   Gupta A, 2015, IEEE ACCESS, V3, P1206, DOI 10.1109/ACCESS.2015.2461602
   Heine C., 2013, TEACHING INFORM FLUE
   HICKSON DJ, 1969, ADMIN SCI QUART, V14, P378, DOI 10.2307/2391134
   Hinings B, 2018, INFORM ORGAN-UK, V28, P52, DOI 10.1016/j.infoandorg.2018.02.004
   Hobbs L, 2011, ORACLE 10G DATA WARE
   Holtel S, 2016, PROCEDIA COMPUT SCI, V99, P171, DOI 10.1016/j.procs.2016.09.109
   Hoover H, 1950, DE RE METALLICA
   Huo D, 2019, RES POLICY, V48, P1564, DOI 10.1016/j.respol.2019.03.020
   Iansiti M, 2015, HARV BUS REV
   Ignited M., 2016, SECTOR COMPETITIVENE
   Isokangas E., 2012, PLAT 2012 SAIMM C
   Isson Jean-Paul., 2012, WIN ADV BUSINESS ANA
   Ittoo A, 2016, COMPUT IND, V78, P1, DOI 10.1016/j.compind.2016.01.001
   ITU, 2005, ITU NEWS         NOV, P13
   IYER B, 2015, HARVARD BUSINESS REV
   Jacobs J, 2017, J S AFR I MIN METALL, V117, P636, DOI 10.17159/2411-9717/2017/v117n7a5
   Jane G., 2011, 7 PRINCIPLES SUSTAIN, P37
   Jarrahi MH, 2018, BUS HORIZONS, V61, P577, DOI 10.1016/j.bushor.2018.03.007
   Javier M, 2016, SME ANN M
   Jeschke S, 2017, SP SER WIRELESS TECH, P3, DOI 10.1007/978-3-319-42559-7_1
   Job A., 2017, IRON ORE 2017
   Joyce S, 1998, ENVIRON HEALTH PERSP, V106, pA538, DOI 10.2307/3434253
   Kazakidis V, 2004, MIN TECHNOL, V113, P30, DOI [10.1179/037178404225004274, DOI 10.1179/037178404225004274]
   Kearns D., 2017, MACHINE LEARNING MIN
   Kennedy J., 1994, FIELD EXPERIENCE PRO
   Knobel Michele, 2008, DIGITAL LITERACIES C, V30
   Koller S, 2019, FORENSIC SCI INT, V295, P30, DOI 10.1016/j.forsciint.2018.11.006
   Kova J, 2017, ACTA MECH SLOVACA, V21, P20
   Kowalski-Trakofler KM, 2005, SAFETY SCI, V43, P779, DOI 10.1016/j.ssci.2005.08.014
   Kubacki T, 2014, J GEOPHYS RES-SOL EA, V119, P4876, DOI 10.1002/2014JB011037
   Kuratko D. F., 2016, TECHNOLOGICAL INNOVA
   La Rosa D., P 29 INT S COMP APPL
   Lala A, 2016, AUSIMM B, P46
   Lampshire G, 2016, DATA ANAL PLAYBOOK P
   Lavrin A, 2010, ACTA MONTAN SLOVACA, V15, P225
   Lee J., 2014, MINING METALS INTER
   Li Q.-M., 2018, 2017 3 INT FOR EN EN
   Ltd A., 2018, TECHN CONV ADV MIN E
   Ltd A. A. P, 2018, INT REP 2017
   Ltd H. Z, 2016, ANNU REP 2018
   Ltd I. C, 2018, ANN REP PER END DEC
   Ltd S, 2018, ANN REP 2017
   Ltd T. H., 2016, ANN INF FORM YEAR EN
   Ltd T. R, 2018, ANN REP
   Ltd Y. C. M. C, 2017, 2016 ANN REP
   Ma'Aden, 2017, ANN REP 2017
   Maan B., 2017, BLOCKCHAIN ADOPTION
   Martin A, 2008, DIGITAL LITERACIES C, V30, P151
   Marvel MR, ENCY NEW VENTURE MAN, V2012
   Mauri P, 2018, STPIS CAISE
   McCarthy F, 2008, INNOV-ORGAN MANAG, V10, P257, DOI 10.5172/impp.453.10.2-3.257
   McMahon T, 2008, CHEM ENG PROG, V104, P28
   McNab K.L, 2011, AUTONOMOUS REMOTE OP
   Meyen E, 2015, REM SPEC EDUC, V36, P67, DOI 10.1177/0741932514554103
   Minelli M., 2012, BIG DATA BIG ANAL EM
   Moffat K., 2016, LIFE OF MINE 2016
   Moffat K, 2016, FORESTRY, V89, P477, DOI 10.1093/forestry/cpv044
   Munoz Javier I., 2014, International Journal of Mining and Mineral Engineering, V5, P38, DOI 10.1504/IJMME.2014.058918
   Newell S, 2015, J STRATEGIC INF SYST, V24, P3, DOI 10.1016/j.jsis.2015.02.001
   Newman D, 2017, FORBES
   Niosh U., 2015, WORKPLACE SAFETY HLT
   Notley SR, 2018, APPL PHYSIOL NUTR ME, V43, P869, DOI 10.1139/apnm-2018-0173
   Obile W., 2016, ERICSSON MOBILITY RE
   Ohlhorst F., 2012, BIG DATA ANAL TURNIN
   Orenstein D., COMPUTERWORLD
   Pan X, IFAC P, V2013, P148
   Pires J. N., 2007, IND ROBOTS PROGRAMMI
   PLC A, 2018, ANN REP FIN STAT 201
   PLC A. A, 2018, ANNU REP
   PLC F, 2018, ANN REP ACC 2017
   PLC G, 2018, FORT MET GROUP W PER
   Polyus P, 2018, ANN REP 2017
   Power D., 2018, DATA BASED DECISION
   Preuveneers D., 2016, 2016 12 INT C INT EN
   Prevention C. f. D. C, 2015, MIN FACTS 2015
   Provost F., 2013, DATA SCI BUSINESS WH, V1
   PwC, 2017, PWCS FUT SIGHT SER
   Raab D. M., 2010, MARKETING SYSTEMS ON, P34
   Ramirez R, 2016, IEEE ACCESS, V4, P2216, DOI 10.1109/ACCESS.2016.2544381
   Roberts JJ, 2017, FORTUNE, V176, P49
   Rogers WP, 2019, INT J MIN RECLAM ENV, V33, P286, DOI 10.1080/17480930.2017.1405473
   Rouse M., 2016, OPERATIONAL TECHNOLO
   S. A. S. Q. y. M. d. C, 2018, ANN REP 2017
   S. A. V, 2018, FORM 20 F
   Setia P., 2013, LEVERAGING DIGITAL T, P565
   Sganzerla C, 2016, PROCEDIA ENGINEER, V138, P64, DOI 10.1016/j.proeng.2016.02.057
   Shah M, 2016, BUSINESSWORLD
   SHNEIDERMAN B, 1974, COMMUN ACM, V17, P566, DOI 10.1145/355620.361170
   Singh A, 2018, AD HOC NETW
   Sousa MJ, 2019, FUTURE GENER COMP SY, V91, P327, DOI 10.1016/j.future.2018.08.048
   Souza GC, 2014, BUS HORIZONS, V57, P595, DOI 10.1016/j.bushor.2014.06.004
   SparklingLogic, 2018, PRESCR AN
   Stahl P., 2011, P HUM FACT ERG SOC A
   Strharsky J, 2017, AUSIMM B, V18
   Stubbs E, 2011, VALUE BUSINESS ANAL, V43
   Subashini S, 2011, J NETW COMPUT APPL, V34, P1, DOI 10.1016/j.jnca.2010.07.006
   Sun EJ, 2010, SAFETY SCI, V48, P1490, DOI 10.1016/j.ssci.2010.07.012
   Taylor S., 2017, DATA WORKERS PROVE R
   Thrybom Linus, 2015, IFAC - Papers Online, V48, P222, DOI 10.1016/j.ifacol.2015.08.135
   Trilling B., 2009, LEARNING LIFE OUR TI
   Trist EL, 1951, HUM RELAT, V4, P3, DOI 10.1177/001872675100400101
   Troshani I, 2018, INT J ACCOUNT INF SY, V31, P17, DOI 10.1016/j.accinf.2018.09.002
   Uralkali P, 2018, INTEGRATED REPORT 20
   Vale, 2017, VIT WILL REC VIRT RE
   Valery W., 2011, BALK C
   Vavra C, 2016, CONTROL ENG, P63
   Vial G, 2019, J STRATEGIC INF SYST, V28, P118, DOI 10.1016/j.jsis.2019.01.003
   von Leipzig T, 2017, PROCEDIA MANUF, V8, P517, DOI 10.1016/j.promfg.2017.02.066
   Watson HJ, 2017, MIS Q EXEC
   WeaverK D., 2003, GENDER COMPUTERS UND
   Wilson J., 2014, FINANC TIMES, P4
   Wu XD, 2014, IEEE T KNOWL DATA EN, V26, P97, DOI 10.1109/TKDE.2013.109
   Xoom trainings, 2015, WHY WED TECHN DAT SC
   Yoo Y, 2010, COMMUN ASSOC INF SYS, V27, P637
   Zeng Y., 2017, SIEMENS CUSTOMER MAG
   Zhang P., 2006, FDN HUMAN COMPUTER I, V6, P1
   Zhou C, 2017, Min Eng, V69, P50, DOI 10.19150/me.7919
   2018, ANN REP
NR 194
TC 6
Z9 6
U1 18
U2 73
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2524-3462
EI 2524-3470
J9 MINING METALL EXPLOR
JI Mining Metall. Explor.
PD AUG
PY 2019
VL 36
IS 4
BP 683
EP 699
DI 10.1007/s42461-019-00103-w
PG 17
WC Metallurgy & Metallurgical Engineering; Mining & Mineral Processing
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Metallurgy & Metallurgical Engineering; Mining & Mineral Processing
GA IL9ED
UT WOS:000477587100009
DA 2022-02-10
ER

PT C
AU MCKEE, GT
   XIE, Q
   BROOKS, BG
AF MCKEE, GT
   XIE, Q
   BROOKS, BG
BE Schenker, PS
TI VISION-GUIDED LOCALIZATION FOR AUTOMATED CAMERA CONTROL
SO SENSOR FUSION VII
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
LA English
DT Proceedings Paper
CT Sensor Fusion VII Conference
CY OCT 31-NOV 01, 1994
CL BOSTON, MA
SP SOC PHOTO OPT INSTRUMENTAT ENGINEERS
DE ROBOT LOCALIZATION; TELEOPERATION; REMOTE VIEWING; COMPUTER VISION
C1 UNIV READING,DEPT COMP SCI,READING RG6 2AY,ENGLAND.
NR 0
TC 0
Z9 0
U1 0
U2 0
PU SPIE - INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA PO BOX 10, BELLINGHAM, WA 98227-0010
BN 0-8194-1690-8
J9 P SOC PHOTO-OPT INS
PY 1994
VL 2355
BP 293
EP 300
DI 10.1117/12.189064
PG 8
WC Remote Sensing
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Remote Sensing
GA BB55B
UT WOS:A1994BB55B00028
DA 2022-02-10
ER

PT J
AU Sochor, J
   Juranek, R
   Herout, A
AF Sochor, Jakub
   Juranek, Roman
   Herout, Adam
TI Traffic surveillance camera calibration by 3D model bounding box
   alignment for accurate vehicle speed measurement
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Speed measurement; Camera calibration; Fully automatic; Traffic
   surveillance; Bounding box alignment; Vanishing point detection
AB In this paper, we focus on fully automatic traffic surveillance camera calibration, which we use for speed measurement of passing vehicles. We improve over a recent state-of-the-art camera calibration method for traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference method. The method is based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from arbitrary viewpoints, since it has no constraints on camera placement. We evaluate our method on the recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration method by detection of two vanishing points reduces error by 50% (mean distance ratio error reduced from 0.18 to 0.09) compared to the previous state-of-the-art method. We also show that our scene scale inference method is more precise, outperforming both state-of-the-art automatic calibration method for speed measurement (error reduction by 86 % - 7.98 km/h to 1.10 km/h) and manual calibration (error reduction by 19 % - 1.35 km/h to 1.10 km/h). We also present qualitative results of the proposed automatic camera calibration method on video sequences obtained from real surveillance cameras in various places, and under different lighting conditions (night, dawn, day). (C) 2017 Elsevier Inc. All rights reserved.
C1 [Sochor, Jakub; Juranek, Roman; Herout, Adam] Brno Univ Technol, Fac Informat Technol, Ctr Excellence IT4Innovat, Bozetechova 2, Brno 61266, Czech Republic.
RP Sochor, J (corresponding author), Brno Univ Technol, Fac Informat Technol, Ctr Excellence IT4Innovat, Bozetechova 2, Brno 61266, Czech Republic.
EM isochor@fit.vutbr.cz; ijuranek@fit.vutbr.cz; herout@fit.vutbr.cz
RI Herout, Adam/B-5651-2014
FU Ministry of Education, Youth and Sports of the Czech Republic from the
   National Programme of Sustainability (NPU II); project IT4lnnovations
   excellence in science [LQ1602]
FX This work was supported by The Ministry of Education, Youth and Sports
   of the Czech Republic from the National Programme of Sustainability (NPU
   II); project IT4lnnovations excellence in science - LQ1602.
CR Cathey FW, 2005, 2005 IEEE Intelligent Vehicles Symposium Proceedings, P777
   Chaperon T, 2011, COMPUT VIS IMAGE UND, V115, P576, DOI 10.1016/j.cviu.2010.11.018
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dailey D. J., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P98, DOI 10.1109/6979.880967
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Do V.H., P 2015 12 INT C EL E, P1, DOI [10.1109/ECTICon.2015.7207027, DOI 10.1109/ECTICON.2015.7207027]
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dubska M., 2014, AUTOMATIC CAMERA CAL
   Dubska M, 2015, IEEE T INTELL TRANSP, V16, P1162, DOI 10.1109/TITS.2014.2352854
   Dubska M, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.90
   Fang J., 2016, IEEE INTELLIGENT TRA, P1
   Fung GSK, 2003, OPT ENG, V42, P2967, DOI 10.1117/1.1606458
   Gao Yang, 2016, IEEE C COMP VIS PATT
   Girshick R., 2014, COMPUTER VISION PATT
   Grammatikopoulos L., 2005, P INT S MOD TECHN ED, P332
   He X. C., 2007, IEEE WORKSH APPL COM
   He XC, 2007, OPT ENG, V46, DOI 10.1117/1.2714991
   Hsiao E., 2014, CAR MAKE MODEL RECOG
   Juranek R, 2015, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2015.274
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Krause J., 2015, IEEE C COMP VIS PATT
   Krause J., 2013, 3DRR13 ICCV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan JH, 2014, OPTIK, V125, P289, DOI 10.1016/j.ijleo.2013.06.036
   Li CM, 2009, PROC CVPR IEEE, P218, DOI 10.1109/CVPRW.2009.5206553
   Lin T.-Y., 2015, INT C COMP VIS ICCV
   Lin Y.-L., 2014, JOINTLY OPTIMIZING 3
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Long J., 2015, IEEE C COMP VIS PATT
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luvizon Diogo C., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6563, DOI 10.1109/ICASSP.2014.6854869
   Luvizon D. C., 2016, IEEE T INTELL TRANSP, VPP, P1
   Maduro C, 2008, IEEE IMAGE PROC, P777, DOI 10.1109/ICIP.2008.4711870
   Nurhadiyatna A, 2013, INT C ADV COMP SCI I, P451, DOI 10.1109/ICACSIS.2013.6761617
   Prokaj J., 2009, 3 D MODEL BASED VEHI
   Ren Shaoqing, 2015, ADV NEURAL INFORM PR
   Schoepflin TN, 2003, IEEE T INTELL TRANSP, V4, P90, DOI 10.1109/TITS.2003.821213
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Simon M., 2015, INT C COMP VIS ICCV
   Sina I, 2013, INT C ADV COMP SCI I, P149, DOI 10.1109/ICACSIS.2013.6761567
   Sochor J., 2016, INTELL TRAN IN PRESS
   Sochor J., 2016, IEEE C COMP VIS PATT
   Tomasi C., 1991, INT J COMPUTER VISIO
   Yang J., 2016, IEEE C COMP VIS PATT
   You XH, 2016, NEUROCOMPUTING, V204, P222, DOI 10.1016/j.neucom.2015.09.132
   Yu XG, 2009, COMPUT VIS IMAGE UND, V113, P643, DOI 10.1016/j.cviu.2008.01.006
   Zhang ZX, 2013, IEEE T CIRC SYST VID, V23, P518, DOI 10.1109/TCSVT.2012.2210670
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zheng Y, 2014, IEEE T INTELL TRANSP, V15, P831, DOI 10.1109/TITS.2013.2288353
   Zhou K, 2016, DESTECH TRANS COMP
NR 50
TC 28
Z9 30
U1 0
U2 14
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD AUG
PY 2017
VL 161
BP 87
EP 98
DI 10.1016/j.cviu.2017.05.015
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA FG8WN
UT WOS:000410718600009
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Konda, KR
   Rosani, A
   Conci, N
   De Natale, FGB
AF Konda, Krishna Reddy
   Rosani, Andrea
   Conci, Nicola
   De Natale, Francesco G. B.
BE Agapito, L
   Bronstein, MM
   Rother, C
TI Smart Camera Reconfiguration in Assisted Home Environments for Elderly
   Care
SO COMPUTER VISION - ECCV 2014 WORKSHOPS, PT IV
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 13th European Conference on Computer Vision (ECCV)
CY SEP 06-12, 2014
CL Zurich, SWITZERLAND
DE Elderly care; Real time video analysis; Automatic camera reconfiguration
ID FALL DETECTION; VIDEO
AB Researchers of different fields have been involved in human behavior analysis during the last years. The successful recognition of human activities from video analysis is still a challenging problem. Within this context, applications targeting elderly care are of considerable interest both for public and industrial bodies, especially considering the aging society we are living in. Ambient intelligence (AmI) technologies, intended as the possibility of automatically detecting and reacting to the status of the environment and of the persons, is probably the major enabling factor. AmI technologies require suitable networks of sensors and actuators, as well as adequate processing and communication technologies. In this paper we propose an innovative solution based on a real time analysis of video with application in the field of elderly care. The system performs anomaly detection and proposes the automatic reconfiguration of the camera network for better monitoring of the ongoing event. The developed framework is tested on a publicly available dataset and has also been deployed and evaluated in a real environment.
C1 [Konda, Krishna Reddy; Rosani, Andrea; Conci, Nicola; De Natale, Francesco G. B.] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
RP Rosani, A (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, Via Sommar 9, I-38123 Trento, Italy.
EM andrea.rosani@unitn.it
RI Conci, Nicola/AAH-4671-2020
OI Conci, Nicola/0000-0002-7858-0928
CR Anderson Derek, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P6388
   Chaaraoui AA, 2012, EXPERT SYST APPL, V39, P10873, DOI 10.1016/j.eswa.2012.03.005
   Auvinet E., 2010, DIRO U MONTR TECH RE 1350 DIRO U MONTR
   Borges PVK, 2013, IEEE T CIRC SYST VID, V23, P1993, DOI 10.1109/TCSVT.2013.2270402
   Cardinaux F, 2011, J AMB INTEL SMART EN, V3, P253, DOI 10.3233/AIS-2011-0110
   Chen CH, 2008, IEEE T CIRC SYST VID, V18, P1052, DOI 10.1109/TCSVT.2008.928223
   Cucchiara R, 2005, IEEE T SYST MAN CY A, V35, P42, DOI 10.1109/TSMCA.2004.838501
   Feng W., 2014, SIGNAL IMAGE VIDEO P, P1
   Foroughi Homa, 2008, 2008 11th International Conference on Computer and Information Technology (ICCIT), P219, DOI 10.1109/ICCITECHN.2008.4803020
   Hazelhoff L, 2008, LECT NOTES COMPUT SC, V5259, P298, DOI 10.1007/978-3-540-88458-3_27
   HHI, 2014, H 264 REF DEC H HERT
   KATZ S, 1970, GERONTOLOGIST, V10, P20, DOI 10.1093/geront/10.1_Part_1.20
   Lin CW, 2007, IEEE IC COMP COM NET, P1172
   Micheloni C, 2010, IEEE SIGNAL PROC MAG, V27, P78, DOI 10.1109/MSP.2010.937333
   Mubashir M, 2013, NEUROCOMPUTING, V100, P144, DOI 10.1016/j.neucom.2011.09.037
   MURRAY D, 1994, IEEE T PATTERN ANAL, V16, P449, DOI 10.1109/34.291452
   Nait-Charif H, 2004, INT C PATT RECOG, P323, DOI 10.1109/ICPR.2004.1333768
   Open source multiple contributions O. S., 2014, OPEN SOURCE MULTIPLE
   Quaritsch M, 2007, EURASIP J EMBED SYST, DOI 10.1155/2007/92827
   Rashidi P, 2013, IEEE J BIOMED HEALTH, V17, P579, DOI 10.1109/JBHI.2012.2234129
   Rougier C, 2007, 21ST INTERNATIONAL CONFERENCE ON ADVANCED NETWORKING AND APPLICATIONS WORKSHOPS/SYMPOSIA, VOL 2, PROCEEDINGS, P875, DOI 10.1109/ainaw.2007.181
   Scotti G, 2005, IEE P-VIS IMAGE SIGN, V152, P250, DOI 10.1049/ip-vis:20041302
   van Kasteren TLM, 2010, PERS UBIQUIT COMPUT, V14, P489, DOI 10.1007/s00779-009-0277-9
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yu XG, 2008, 2008 10TH IEEE INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATIONS AND SERVICES, P42, DOI 10.1109/HEALTH.2008.4600107
NR 25
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-16220-1; 978-3-319-16219-5
J9 LECT NOTES COMPUT SC
PY 2015
VL 8928
BP 45
EP 58
DI 10.1007/978-3-319-16220-1_4
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BD5WP
UT WOS:000361842800004
DA 2022-02-10
ER

PT J
AU Micheloni, C
   Foresti, GL
AF Micheloni, Christian
   Foresti, Gian Luca
TI Active Tuning of Intrinsic Camera Parameters
SO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
LA English
DT Article
DE Image enhancement; image processing; image sensors; video signal
   processing; visual system
ID TRACKING
AB In the last years, the research effort of the scientific community to study systems for ambient intelligence has been really strong. Usually, the systems developed so far base their analysis on images acquired by automatic cameras. In this paper, we propose a way to develop new smart systems that are able to actively decide both what to see and how to see it. In particular, the main idea is to tune the acquisition parameters on the basis of what the system desires to acquire. The regulation strategy is based on two camera parameters, focus and iris. It aims to identify an optimal sequence of steps to enhance the acquisition quality of an object of interest. To this end, a hierarchy of neural networks has been employed first to select which parameter must be regulated then to adjust it. The proposed solution can be applied to both static and moving cameras. The results show how the proposed technique can be applied to images acquired by a moving camera with zoom capabilities for surveillance purposes.
   Note to Practitioners-The proposed work presents a new active vision paradigm. While traditional active vision systems control the acquisition only by moving the cameras to keep an object of interest in the field-of-view, the proposed solution extends such a concept by introducing the control of the image creation process. The objective is to keep the objects of interest within the field-of-view and at the same time to acquire it with an optimal quality. This is an interesting feature as it allows to focus the attention only on the areas of interest on which our system is interested in thus improving the performance of the image processing tasks. To achieve such an objective, we propose a neural network hierarchy that is able to find out the optimal configuration of the intrinsic camera parameters (focus and iris) in a fast way. This result is even more important if we think that it can be achieved regardless the dimension and the position of the area of interest inside the image.
C1 [Micheloni, Christian] Univ Udine, Dept Math & Comp Sci DIMI, I-33100 Udine, Italy.
   [Foresti, Gian Luca] Univ Udine, Dept Comp Sci, I-33100 Udine, Italy.
RP Micheloni, C (corresponding author), Univ Udine, Dept Math & Comp Sci DIMI, I-33100 Udine, Italy.
EM christian.micheloni@dimi.uniud.it; gianluca.foresti@dimi.uniud.it
RI Micheloni, Christian/E-5427-2012
OI Micheloni, Christian/0000-0003-4503-7483
FU Italian Ministry of University and Scientific ResearchMinistry of
   Education, Universities and Research (MIUR)
FX This work was supported in part by the Italian Ministry of University
   and Scientific Research within the framework of the project "Dynamic and
   unstructured environments interpretation, virtualization and monitoring
   by an integrated autonomous system of sensors and robots" (2009-2012).
CR Aloimonos Y., 1993, ACTIVE PERCEPTION
   Araki S, 2000, IEICE T INF SYST, VE83D, P1583
   Ben-Arie J, 2002, IEEE T PATTERN ANAL, V24, P1091, DOI 10.1109/TPAMI.2002.1023805
   Collins RT, 2001, P IEEE, V89, P1456, DOI 10.1109/5.959341
   DAVIS YYL, 1997, P DARPA97 IM UND WOR, P19
   DAVISON A, 1999, P 6 BRIT MACH VIS C, P11
   Dockstader SL, 2001, P IEEE, V89, P1441, DOI 10.1109/5.959340
   Feng Q, 2007, INT J COMPUT SCI NET, V7, P31
   Foresti G.L., 2000, MULTIMEDIA VIDEO BAS
   Foresti GL, 2005, PATTERN RECOGN LETT, V26, P2232, DOI 10.1016/j.patrec.2005.03.031
   Foresti GL, 2002, IEEE T NEURAL NETWOR, V13, P1540, DOI 10.1109/TNN.2002.804290
   Gandhi T, 2005, MACH VISION APPL, V16, P85, DOI 10.1007/s00138-004-0168-z
   GROEN FCA, 1985, CYTOMETRY, V6, P81, DOI 10.1002/cyto.990060202
   Irani M, 1998, IEEE T PATTERN ANAL, V20, P577, DOI 10.1109/34.683770
   Kanade T., 1998, P DARPA IM UND WORKS, V1, P3
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2
   Kehtarnavaz N, 2003, REAL-TIME IMAGING, V9, P197, DOI 10.1016/S1077-2014(03)00037-8
   KROTKOV E, 1987, INT J COMPUT VISION, V1, P223
   Krotkov EP., 1989, ACTIVE COMPUTER VISI
   Kumar P, 2005, IEEE T INTELL TRANSP, V6, P43, DOI 10.1109/TITS.2004.838219
   LI F, 2005, P INT C MACH LEARN C, V8, P5001
   Ligthart G., 1982, Proceedings of the 6th International Conference on Pattern Recognition, P597
   Micheloni C, 2006, J VIS COMMUN IMAGE R, V17, P589, DOI 10.1016/j.jvcir.2005.08.002
   Murino V, 1996, IEEE T SYST MAN CY B, V26, P1, DOI 10.1109/3477.484434
   MURRAY D, 1994, IEEE T PATTERN ANAL, V16, P449, DOI 10.1109/34.291452
   Ng KC, 2001, IEEE INT CONF ROBOT, P2791, DOI 10.1109/ROBOT.2001.933045
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   REGAZZONI CS, 2001, P IEEE OCT, V89
   Russo F, 2005, IEEE T INSTRUM MEAS, V54, P1600, DOI 10.1109/TIM.2005.851084
   SUBBARAO M, 1993, OPT ENG, V32, P2824, DOI 10.1117/12.147706
   SUBBARAO M, 1995, P SOC PHOTO-OPT INS, V2598, P89, DOI 10.1117/12.220891
   Zhou J, 2007, IEEE T VEH TECHNOL, V56, P51, DOI 10.1109/TVT.2006.883735
NR 32
TC 3
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-5955
EI 1558-3783
J9 IEEE T AUTOM SCI ENG
JI IEEE Trans. Autom. Sci. Eng.
PD OCT
PY 2009
VL 6
IS 4
BP 577
EP 587
DI 10.1109/TASE.2009.2017735
PG 11
WC Automation & Control Systems
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems
GA 497DQ
UT WOS:000270035600002
DA 2022-02-10
ER

PT J
AU Gibson, S
   Hubbold, RJ
   Cook, J
   Howard, TLJ
AF Gibson, S
   Hubbold, RJ
   Cook, J
   Howard, TLJ
TI Interactive reconstruction of virtual environments from video sequences
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article; Proceedings Paper
CT 3rd International Conference on VR and its Application in Industry
CY 2002
CL PEOPLES R CHINA
DE virtual reality; computer vision; model building; camera calibration;
   structure and motion; texture
AB There are many real-world applications of Virtual Reality requiring the construction of complex and accurate three-dimensional models that represent real environments. In this paper, we describe a rapid and robust semi-automatic system that allows such environments to be quickly and easily built from video sequences captured with standard consumer-level digital cameras. The system combines an automatic camera calibration algorithm with an interactive model-building phase, followed by automatic extraction and synthesis of surface textures from frames of the video sequence. The capabilities of the system are illustrated using a variety of example reconstructions. (C) 2003 Elsevier Science Ltd. All rights reserved.
C1 Univ Manchester, Adv Interfaces Grp, Dept Comp Sci, Manchester M13 9PL, Lancs, England.
RP Gibson, S (corresponding author), Univ Manchester, Adv Interfaces Grp, Dept Comp Sci, Oxford Rd, Manchester M13 9PL, Lancs, England.
EM sg@cs.man.ac.uk; roger@cs.-man.ac.uk; cookj@cs.man.ac.uk;
   toby@cs.man.ac.uk
CR *3RDTECH, DELT 3D SCEN DIG
   BEARDSLEY PA, 1996, LNCS, V1065, P683
   BECKER S, 1995, SPIE S EL IM SCI TEC
   Bougnoux S., 1997, P IEEE INT C COMP VI
   Cipolla R., 1998, Proceedings of IAPR Workshop on Machine Vision Applications, P559
   CIPOLLA R, 1999, P BRIT MACH VIS C, V2, P382
   Debevec P.E., 1996, P SIGGRAPH 96, P11, DOI DOI 10.1145/237170.237191
   FITZGIBBON AW, 1998, P EUR C COMP VIS, P311
   GIBSON S, 2001, P EUR 2001 MANCH UK, V19
   GIBSON S, 2002, ACM IEEE INT S MIX A
   Gibson S., 2000, P ACM S VIRT REAL SO
   HAKIM SE, 2000, INT ARCH PHOTOGRAMME, V33, P122
   HARTLEY R, 1995, P C COMP AN IM PATT
   Hartley R., 2000, MULTIPLE VIEW GEOMET
   HARTLEY RI, 1994, LNCS SERIES, V825, P237
   HEIGL B, 1999, LECT NOTES COMPUTER
   HOWARD TLJ, 2000, P SPIE IS T, V3960
   JIN H, 2001, P INT C COMP VIS VAN
   MURTA A, 1998, P EUR LEEDS UK, P169
   Pollefeys M, 1999, INT J COMPUT VISION, V32, P7, DOI 10.1023/A:1008109111715
   POLLEFEYS M, 2001, P C OPT 3 D MEAS TEC, V5, P251
   POULIN P, 1998, P EUR WORKSH END VIE
   Rowan TH., 1990, FUNCTIONAL STABILITY
   SMITH SM, 1992, P 3 BRIT MACH VIS C, P139
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   TRIGGS W, 1997, P IEEE C COMP VIS PA, P609
   TRIGGS W, 2000, LECT NOTES COMPUTER, V1883
   Yu YZ, 2001, IEEE T VIS COMPUT GR, V7, P351, DOI 10.1109/2945.965349
   [No title captured]
NR 29
TC 26
Z9 27
U1 0
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2003
VL 27
IS 2
BP 293
EP 301
DI 10.1016/S0097-8493(02)00285-6
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 659QJ
UT WOS:000181788300013
DA 2022-02-10
ER

PT C
AU Bastos, R
   Dias, MS
AF Bastos, Rafael
   Dias, Miguel Sales
BE Skala, V
TI Automatic Camera Pose Initialization, using Scale, Rotation and
   Luminance Invariant Natural Feature Tracking
SO JOURNAL OF WSCG, 2008
SE Journal of WSCG
LA English
DT Proceedings Paper
CT 16th International Conference in Central Europe on Computer Graphics,
   Visualization and Computer Vision
CY FEB 04-07, 2008
CL Univ W Bohemia, Plzen, CZECH REPUBLIC
HO Univ W Bohemia
DE Camera Pose Initialization; Feature Detection and Tracking; Augmented
   Reality; Texture Tracking; scale invariant; rotation invariant;
   luminance invariant
AB The solution to the camera registration and tracking problem serves Augmented Reality, in order to provide an enhancement to the user's cognitive perception of the real world and his/her situational awareness. By analyzing the five most representative tracking and feature detection techniques, we have concluded that the Camera Pose Initialization (CPI) problem, a relevant sub-problem in the overall camera tracking problem, is still far from being solved using straightforward and non-intrusive methods. The assessed techniques often use user inputs (i.e. mouse clicking) or auxiliary artifacts (i.e. fiducial markers) to solve the CPI problem. This paper presents a novel approach to real-time scale, rotation and luminance invariant natural feature tracking, in order to solve the CPI problem using totally automatic procedures. The technique is applicable for the case of planar objects with arbitrary topologies and natural textures, and can be used in Augmented Reality. We also present a heuristic method for feature clustering, which has revealed to be efficient and reliable. The presented work uses this novel feature detection technique as a baseline for a real-time and robust planar texture tracking algorithm, which combines optical flow, backprojection and template matching techniques. The paper presents also performance and precision results of the proposed technique.
C1 [Bastos, Rafael] ADETTI ISCTE, Ave Forcas Armadas,Edificio ISCTE, P-1500 Lisbon, Portugal.
   [Dias, Miguel Sales] Microsoft ISCTE, P-1500 Lisbon, Portugal.
RP Bastos, R (corresponding author), ADETTI ISCTE, Ave Forcas Armadas,Edificio ISCTE, P-1500 Lisbon, Portugal.
EM Rafael.Afonso.Bastos@gmail.com; Miguel.Dias@microsoft.com
RI Dias, José Miguel Sales/M-5392-2013
OI Dias, José Miguel Sales/0000-0002-1445-2695
CR Abdel-Aziz YI, 1971, P S CLOS RANG PHOT
   BASTOS R, 2005, ACE TECHNOLOGY
   Bay H., 2006, P 9 ECCV
   Bouguet J.-Y., 1999, PYRAMIDAL IMPLEMENTA
   Brakke K.A., 2004, SURFACE EVOLVER MANU
   BUENAPOSADA JM, 2002, ICPR 02
   CAUDELL TP, 1992, P IEEE HAW INT C SYS
   Chen J, 2006, ICAT 2006: 16TH INTERNATIONAL CONFERENCE ON ARTIFICIAL REALITY AND TELEXISTENCE - WORSHOPS, PROCEEDINGS, P119
   DEMENTHON D, 1991, EUR C COMP VIS
   DIAS JMS, 2004, INTERACCAO
   DINIZ N, 2004, DCC 04 MIT, P19
   FISCHLER MA, 1981, COMMUNICATIONS ASS C
   Golub G.H., 2013, MATRIX COMPUTATIONS
   KATO H, 2003, 2 IEEE INT ART WORKS
   KATO H, 1999, INT WORKSH AUG REAL
   KE Y, 2004, PCA SIFT MORE DISTIN
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MALIK S, 2002, P VIS INT CALG ALB C, P399
   MARCHAND E, 2002, EUROGRAPHICS 02 C P
   SHI J, 1994, IEEE C CVPR
   SIMON G, 2002, ISMAR 02 C P
   YUAN C, 2006, ISVC
   ZHANG Z, 1999, FLEXIBLE CAMERA CALI
NR 23
TC 3
Z9 3
U1 0
U2 4
PU UNION AGENCY SCIENCE PRESS
PI PLZEN
PA NA MAZINACH 9, PLZEN, 322 00, CZECH REPUBLIC
SN 1213-6972
BN 978-80-86943-14-5
J9 JOURNAL WSCG
PY 2008
VL 16
IS 1-3
BP 97
EP +
PG 2
WC Computer Science, Software Engineering; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BIK35
UT WOS:000260375000013
DA 2022-02-10
ER

PT C
AU Han, KM
   DeSouza, GN
AF Han, Kyung Min
   DeSouza, Guilherme N.
BE Zaytoon, J
   Ferrier, JL
   Cetto, JA
   Filipe, J
TI A feature detection algorithm for autonomous camera calibration
SO ICINCO 2007: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON
   INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS, VOL RA-2: ROBOTICS AND
   AUTOMATION, VOL 2
LA English
DT Proceedings Paper
CT 4th International Conference on Informatics in Control, Automation and
   Robotics
CY MAY 09-12, 2007
CL Angers, FRANCE
SP Inst Syst & Technol Informat, Control & Commun, Univ Angers, Int Federat Automat Control, GDR MACS, CNRS, EEA, Assoc Advancement Artificial Intelligence
DE autonomous camera calibration; automatic feature detection; line and
   corner detection
AB This paper presents an adaptive and robust algorithm for automatic corner detection. Ordinary camera calibration methods require that a set of feature points - usually, corner points of a chessboard type of pattern - be presented to the camera in a controlled manner. On the other hand, the proposed approach automatically locates the feature points even in the presence of cluttered background, change in illumination, arbitrary poses of the pattern, etc. As the results demonstrate, the proposed technique is much more appropriate to automatic camera calibration than other existing methods.
C1 [Han, Kyung Min; DeSouza, Guilherme N.] Univ Missouri, Columbia, MO USA.
RP Han, KM (corresponding author), Univ Missouri, Columbia, MO USA.
EM Khx8d@mizzou.edu; desouzag@missouri.edu
CR Baker P, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P134, DOI 10.1109/OMNVIS.2000.853820
   CHEN KW, 2005, P 2 IEEE WORKSH VS P
   Hough P. V. C., 1962, METHODS MEANS RECOGN, Patent No. 3069654
   HUANG Z, 2006, P 2002 IEEE INT C SY, V4
   KIM JS, 2001, P IEEE INT C INT ROB
   ROGER Y, 1987, IEEE T ROBOTICS AUTO, V3
   Stephens M, 1988, ALV VIS C, P147, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]
   WENG J, 1992, IEEE T PATTERN ANAL, V14
   YAMAZOE H, 2006, ICPR HONG KONG AUG 2
   ZHANG Z, 1998, FLEXIBLE NEW TECHNIQ
NR 10
TC 1
Z9 1
U1 0
U2 1
PU INSTICC-INST SYST TECHNOLOGIES INFORMATION CONTROL & COMMUNICATION
PI SETUBAL
PA AVENIDA D MANUEL L, 27A 2 ESQUERDO, SETUBAL, 2910-595, PORTUGAL
BN 978-972-8865-83-2
PY 2007
BP 286
EP 291
PG 6
WC Automation & Control Systems; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Robotics
GA BHF19
UT WOS:000252643500044
DA 2022-02-10
ER

PT C
AU Gomez, JR
   Guerrero, JJ
   Herrero-Jaraba, E
AF Gomez, Jorge Raul
   Guerrero, Jose J.
   Herrero-Jaraba, Elias
BA Filipe, J
BF Filipe, J
BE Cetto, JA
   Ferrier, JL
TI Visual tracking on the ground - A comparative analysis
SO ICINCO 2008: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON
   INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS, VOL RA-1: ROBOTICS AND
   AUTOMATION, VOL 1
LA English
DT Proceedings Paper
CT 5th International Conference on Informatics in Control, Automation and
   Robotics
CY MAY 11-15, 2008
CL Funchal, PORTUGAL
SP Inst Syst & Technol Informat, Control & Commun, Univ Madeira, IEEE Syst, Man & Cybernet Soc, Int Federat Automat Control, Assoc Advancement Artificial Intelligence
DE tracking on the ground; Kalman Filter; homography
AB Tracking is an important field in visual surveillance systems. Trackers have been applied traditionally in the image, but a new concept of tracking has been used gradually, applying the tracking on the ground map of the surrounding area. The purpose of this article is to compare both alternatives and prove that this new usage makes possible to obtain a higher performance and a minimization of the projective effects. Moreover, it provides the concept of multi-camera as a new tool for mobile object tracking in surveillance scenes, because a common reference system can be defined without increasing complexity. An automatic camera re-calibration procedure is also proposed, which avoids some practical limitations of the approach.
C1 [Gomez, Jorge Raul; Guerrero, Jose J.; Herrero-Jaraba, Elias] Univ Zaragoza, Aragon Inst Engn Res, Zaragoza, Spain.
RP Gomez, JR (corresponding author), Univ Zaragoza, Aragon Inst Engn Res, Maria de Luna 1, Zaragoza, Spain.
RI Guerrero, Jose J/K-5435-2014
OI Guerrero, Jose J/0000-0001-5209-2267
CR BARSHALOM T, 1988, TRACKING DATA ASS
   CRIMINISI A, 1997, IEEE T PATTERN ANAL
   Durucan E, 2001, P IEEE, V89, P1368, DOI 10.1109/5.959336
   Guerrero JJ, 2003, LECT NOTES COMPUT SC, V2652, P297
   Hartley R., 2000, MULTIPLE VIEW GEOMET
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   Herrero-Jaraba E, 2003, PATTERN RECOGN LETT, V24, P2079, DOI 10.1016/S0167-8655(03)00045-X
   Isler V, 2005, COMPUT VIS IMAGE UND, V100, P225, DOI 10.1016/j.cviu.2004.10.008
   Kalman R.E., 1960, THE J, V82, P35, DOI 10.1115/1.3662552
   LEE L, 2000, IEEE T PATTERN ANAL, V22
   Moscheni F, 1998, IEEE T PATTERN ANAL, V20, P897, DOI 10.1109/34.713358
   MUNDY J, 1992, GEOMETRIC INVARIANCE
   Rousseeuw P., 1987, ROBUST REGRESSION OU
   Tissainayagam P, 2001, PATTERN RECOGN, V34, P641, DOI 10.1016/S0031-3203(00)00019-4
NR 14
TC 0
Z9 0
U1 0
U2 0
PU INSTICC-INST SYST TECHNOLOGIES INFORMATION CONTROL & COMMUNICATION
PI SETUBAL
PA AVENIDA D MANUEL L, 27A 2 ESQUERDO, SETUBAL, 2910-595, PORTUGAL
BN 978-989-8111-31-9
PY 2008
BP 45
EP 52
PG 8
WC Automation & Control Systems; Computer Science, Cybernetics; Computer
   Science, Theory & Methods; Engineering, Electrical & Electronic;
   Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BIE32
UT WOS:000258900100008
DA 2022-02-10
ER

PT C
AU Alsadik, B
   Remondino, F
   Menna, F
   Gerke, M
   Vosselman, G
AF Alsadik, Bashar
   Remondino, Fabio
   Menna, Fabio
   Gerke, Markus
   Vosselman, George
BE Boehm, J
   Remondino, F
   Kersten, T
   Fuse, T
   GonzalezAguilera, D
TI Robust extraction of image correspondences exploiting the image scene
   geometry and approximate camera orientation
SO 3D-ARCH 2013 - 3D VIRTUAL RECONSTRUCTION AND VISUALIZATION OF COMPLEX
   ARCHITECTURES
SE International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences
LA English
DT Proceedings Paper
CT Conference on 3D Virtual Reconstruction and Visualization of Complex
   Architectures (3D-ARCH)
CY FEB 25-26, 2013
CL Trento, ITALY
SP Int Soc Photogrammetry & Remote Sensing
DE IBM; Bundle adjustment; SIFT; 3D image; spanning tree
AB Image-based modeling techniques are an important tool for producing 3D models in a practical and cost effective manner. Accurate image-based models can be created as long as one can retrieve precise image calibration and orientation information which is nowadays performed automatically in computer vision and photogrammetry. The first step for orientation is to have sufficient correspondences across the captured images. Keypoint descriptors like SIFT or SURF are a successful approach for finding these correspondences. The extraction of precise image correspondences is crucial for the subsequent image orientation and image matching steps. Indeed there are still many challenges especially with wide-baseline image configuration. After the extraction of a sufficient and reliable set of image correspondences, a bundle adjustment is used to retrieve the image orientation parameters.
   In this paper, a brief description of our previous work on automatic camera network design is initially reported. This semi-automatic procedure results in wide-baseline high resolution images covering an object of interest, and including approximations of image orientations, a rough 3D object geometry and a matching matrix indicating for each image its matching mates. The main part of this paper will describe the subsequent image matching where the pre-knowledge on the image orientations and the pre-created rough 3D model of the study object is exploited. Ultimately the matching information retrieved during that step will be used for a precise bundle block adjustment.
   Since we defined the initial image orientation in the design of the network, we can compute the matching matrix prior to image matching of high resolution images. For each image involved in several pairs that is defined in the matching matrix, we detect the corners or keypoints and then transform them into the matching images by using the designed orientation and initial 3D model. Moreover, a window is defined for each corner and its initial correspondence in the matching images. A SIFT or SURF matching is implemented between every matching window to find the homologous points. This is followed by Least Square Matching LSM to refine the correspondences for a sub-pixel localization and to avoid inaccurate matches. Image matching is followed by a bundle adjustment to orient the images automatically to finally have a sparse 3D model. We used the commercial software Photomodeler Scanner 2010 for implementing the bundle adjustment since it reports a number of accuracy indices which are necessary for the evaluation purposes. The experimental test of comparing the automated image matching of four pre-designed streopairs shows that our approach can provide a high accuracy and effective orientation when compared to the results of commercial and open source software which does not exploit the pre-knowledge about the scene.
C1 [Alsadik, Bashar; Gerke, Markus; Vosselman, George] Univ Twente, ITC Fac, EOS Dept, NL-7500 AE Enschede, Netherlands.
   [Remondino, Fabio; Menna, Fabio] Bruno Kessler Fdn FBK, Opt Metrol Unit 3D, Trento, Italy.
   [Alsadik, Bashar] Univ Baghdad, Coll Engn, Surveying Dept, Baghdad, Iraq.
RP Alsadik, B (corresponding author), Univ Twente, ITC Fac, EOS Dept, POB 217, NL-7500 AE Enschede, Netherlands.
EM alsadik@itc.nl; remondino@fbk.eu; fmenna@fbk.eu; gerke@itc.nl;
   vosselman@itc.nl
RI Remondino, Fabio/C-5503-2018; Alsadik, Bashar/E-2052-2018; Vosselman,
   George/D-3985-2009; Gerke, Markus/A-8791-2012
OI Remondino, Fabio/0000-0001-6097-5342; Alsadik,
   Bashar/0000-0002-9425-9325; Vosselman, George/0000-0001-8813-8028;
   Gerke, Markus/0000-0002-2221-6182; menna, fabio/0000-0002-5365-8813
CR ALSADIK B, 2012, ISPRS ANN PHOTOGRAMM, V3, P7
   Alsadik B., 2013, J CULTURAL IN PRESS
   Barazzetti L, 2010, PHOTOGRAMM REC, V25, P356, DOI 10.1111/j.1477-9730.2010.00599.x
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Cho W., 1992, 418 OH STAT U
   Fraser C.S., 1996, CLOSE RANGE PHOTOGRA, P256
   Gruen A.W., 1985, S AFRICAN J PHOTOGRA, V14, P176
   Kosecka J., 2010, COMPUTER VISION IMAG, V100, P274
   Lourakis M., 2004, DESIGN IMPLEMENTATIO
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MASON S, 1995, ISPRS J PHOTOGRAMM, V50, P13, DOI 10.1016/0924-2716(95)90117-W
   Nister D., 2001, AUTOMATIC DENSE RECO
   PhotoModeler, 2009, PHOTOMODELER QUICK S
   Photoscan A., 2011, AGISOFT STEREOSCAN
   Pierrot-Deseilligny M., 2012, MICMAC SOFTWARE AUTO
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Snavely N., 2010, BUNDLER STRUCTURE MO
   Snavely N., 2008, P COMP VIS PATT REC
   Troisi, 2012, LECT NOTES COMPUTER, V7616, P40, DOI [10.1007/978-3-642-34234-9_5, DOI 10.1007/978-3-642-34234-9_5]
   Wu C. C., 2012, VISUALSFM VISUAL STR
   YANG YB, 1995, ARTIF INTELL, V78, P121, DOI 10.1016/0004-3702(95)00028-3
NR 22
TC 10
Z9 10
U1 0
U2 1
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PY 2013
VL 40-5-W1
BP 1
EP 7
PG 7
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Physical Geography; Remote Sensing; Imaging Science & Photographic
   Technology
GA BD1RO
UT WOS:000358300800001
DA 2022-02-10
ER

PT J
AU Zhang, M
   Hu, XY
   Yao, J
   Zhao, LK
   Li, JC
   Gong, JY
AF Zhang, Mi
   Hu, Xiangyun
   Yao, Jian
   Zhao, Like
   Li, Jiancheng
   Gong, Jianya
TI Line-Based Geometric Consensus Rectification and Calibration From Single
   Distorted Manhattan Image
SO IEEE ACCESS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 07-12, 2015
CL Boston, MA
SP IEEE
DE Calibration; Cameras; Parameter estimation; Estimation; Image
   segmentation; Deep learning; Three-dimensional displays; Manhattan
   image; line detection; geometric consensus rectification; camera
   calibration; single image undistortion
ID AUTOMATIC CAMERA CALIBRATION; OMNIDIRECTIONAL CAMERAS; SPHERE
AB Recent advances in single image rectification and intrinsic calibration has been addressed by employing line information on the distorted image. The core issues of this technique are the separation of rectification and calibration procedures, and the suffering of geometric nonconformity. In this work, we propose a novel Geometric Consensus Rectification and Calibration algorithm, which we refer to as GCRC framework. We show how the geometric consensus rectification and calibration can be performed in a unified framework and solve the above issues. The proposed GCRC not only guarantees the geometrical consensus on the rectified images, but allows us to perform the robust intrinsic parameters estimation with the grouped circular arcs. Through grouping by voting in a unified framework, the geometric consensus rectification and calibration are robustly conducted on single distorted Manhattan images. Experiments on a number of distorted images, including the simulated YorkUrbanDB dataset, Panoramic Fisheye dataset, checkerboard image, and Internet images, demonstrate that the GCRC significantly improve the performance of geometrically consensus rectification and intrinsic parameters estimation. In particular, the GCRC shows relatively small variations with a different number of lines, which outperforms various previous approaches.
C1 [Zhang, Mi; Hu, Xiangyun; Yao, Jian; Gong, Jianya] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
   [Zhang, Mi; Li, Jiancheng] Wuhan Univ, Sch Geodesy & Geomat, Wuhan 430072, Peoples R China.
   [Zhao, Like] Henan Univ Technol, Coll Informat Sci & Engn, Zhengzhou 450001, Henan, Peoples R China.
RP Hu, XY (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
EM huxy@whu.edu.cn
FU Open Research Fund of State Key Laboratory of Information Engineering in
   Surveying, Mapping and Remote Sensing, Wuhan University [18R01]; China
   Postdoctoral Science FoundationChina Postdoctoral Science Foundation
   [2018M642915]; National Key Research and Development Program of China
   [2016YFB0501403]; National Natural Science Foundation of ChinaNational
   Natural Science Foundation of China (NSFC) [41771363, 41901265]
FX This work was supported in part by the Open Research Fund of State Key
   Laboratory of Information Engineering in Surveying, Mapping and Remote
   Sensing, Wuhan University, under Project 18R01, in part by the China
   Postdoctoral Science Foundation under Project 2018M642915, in part by
   the National Key Research and Development Program of China under Project
   2016YFB0501403, and in part by the National Natural Science Foundation
   of China under Project 41771363 and Project 41901265.
CR Agrawal A, 2013, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2013.184
   Ahn SJ, 2001, PATTERN RECOGN, V34, P2283, DOI 10.1016/S0031-3203(00)00152-7
   Antunes M., 2017, P IEEE C COMP VIS PA, P4288
   Antunes M, 2013, PROC CVPR IEEE, P1336, DOI 10.1109/CVPR.2013.176
   Barreto J.P, 2004, THESIS
   Barreto JP, 2006, COMPUT VIS IMAGE UND, V101, P151, DOI 10.1016/j.cviu.2005.07.002
   Barreto JP, 2005, IEEE T PATTERN ANAL, V27, P1327, DOI 10.1109/TPAMI.2005.163
   BASU A, 1995, PATTERN RECOGN LETT, V16, P433, DOI 10.1016/0167-8655(94)00115-J
   Benligiray B, 2016, EUR SIGNAL PR CONF, P938, DOI 10.1109/EUSIPCO.2016.7760386
   Bermudez-Cameo J, 2015, INT J COMPUT VISION, V114, P16, DOI 10.1007/s11263-014-0792-7
   Blott G., 2018, P EUR C COMP VIS ECC, P0
   Bogdan O., 2018, P 15 ACM SIGGRAPH EU, P6
   Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2
   Chang H, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010063
   Courbon J, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1689
   De Villiers J. P., 2008, P SOC PHOTO-OPT INS, V7266
   Deng ZD, 2018, IEEE T INTELL TRANSP, V19, P1485, DOI 10.1109/TITS.2017.2723902
   Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15
   Deutscher J, 2002, LECT NOTES COMPUT SC, V2353, P175
   Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269
   Drap P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060807
   Eichenseer A, 2016, INT CONF ACOUST SPEE, P1541, DOI 10.1109/ICASSP.2016.7471935
   Fitzgibbon Andrew W, 2001, COMP VIS PATT REC 20, V1, pI
   Fleer D, 2017, ROBOT AUTON SYST, V89, P51, DOI 10.1016/j.robot.2016.12.001
   Grammatikopoulos L., 2013, INT ARCH PHOTOGRAM R, V36, P1
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   Hartley R, 2007, IEEE T PATTERN ANAL, V29, P1309, DOI 10.1109/TPAMI.2007.1147
   Hold-Geoffroy Y, 2018, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR.2018.00250
   Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807
   Lazic N., 2011, THESIS
   Lee H, 2012, PROC CVPR IEEE, P877, DOI 10.1109/CVPR.2012.6247761
   Martinez-Finkelshtein A, 2006, INT MATH RES NOTICES, V2006, DOI 10.1155/IMRN/2006/91426
   Mei C., 2007, THESIS
   Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084
   Melo R, 2013, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2013.72
   PIOTRASCHKE M, 2016, PROC CVPR IEEE, P3418, DOI DOI 10.1109/CVPR.2016.372
   Pritts J, 2018, PROC CVPR IEEE, P1993, DOI 10.1109/CVPR.2018.00213
   Puig L, 2012, COMPUT VIS IMAGE UND, V116, P120, DOI 10.1016/j.cviu.2011.08.003
   Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847
   Saez A, 2018, IEEE INT VEH SYM, P1039, DOI 10.1109/IVS.2018.8500456
   Simon G., 2018, P EUR C COMP VIS ECC, P318
   Su D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18061885
   Tateno Keisuke, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11220), P732, DOI 10.1007/978-3-030-01270-0_43
   Tulsiani S, 2017, P IEEE C COMP VIS PA, P2626
   Urban S, 2015, ISPRS J PHOTOGRAMM, V108, P72, DOI 10.1016/j.isprsjprs.2015.06.005
   Vanschoren J., 2018, ARXIV181003548
   Vasconcelos F, 2018, IEEE T PATTERN ANAL, V40, P791, DOI 10.1109/TPAMI.2017.2699648
   Wildenauer H., 2013, P BRIT MACH VIS C BM, V1, P2
   Workman S., 2016, ARXIV160402129
   Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22
   Yin XQ, 2018, LECT NOTES COMPUT SC, V11214, P475, DOI 10.1007/978-3-030-01249-6_29
   Ying XG, 2004, LECT NOTES COMPUT SC, V3021, P442
   Ying XH, 2004, INT C PATT RECOG, P839, DOI 10.1109/ICPR.2004.1333903
   Ying XH, 2004, IEEE T PATTERN ANAL, V26, P1260, DOI 10.1109/TPAMI.2004.79
   Ying XH, 2008, INT J COMPUT VISION, V78, P89, DOI 10.1007/s11263-007-0082-8
   Ying XH, 2013, IEEE T PATTERN ANAL, V35, P1206, DOI 10.1109/TPAMI.2012.195
   Zhai MH, 2016, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2016.610
   Zhang GP, 2019, APPL OPTICS, V58, P1467, DOI 10.1364/AO.58.001467
   Zhang M, 2015, PROC CVPR IEEE, P4137, DOI 10.1109/CVPR.2015.7299041
   Zhao RQ, 2018, IEEE T PATTERN ANAL, V40, P3059, DOI 10.1109/TPAMI.2017.2772922
   Zhao Y, 2018, ADV MULTIMED, V2018, DOI 10.1155/2018/6182953
   Zhou LJ, 2018, BOUND VALUE PROBL, DOI 10.1186/s13661-018-1110-z
   Zhu Z, 2018, PROCEEDINGS OF 2018 INTERNATIONAL CONFERENCE ON INFORMATION SYSTEMS AND COMPUTER AIDED EDUCATION (ICISCAE 2018), P416, DOI 10.1109/ICISCAE.2018.8666916
NR 63
TC 1
Z9 1
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2019
VL 7
BP 156400
EP 156412
DI 10.1109/ACCESS.2019.2947177
PG 13
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Telecommunications
GA JN8TY
UT WOS:000497165400087
OA gold
DA 2022-02-10
ER

PT J
AU Allen, ML
   Inagaki, A
   Ward, MP
AF Allen, Maximilian L.
   Inagaki, Akino
   Ward, Michael P.
TI CANNIBALISM IN RAPTORS: A REVIEW
SO JOURNAL OF RAPTOR RESEARCH
LA English
DT Review
DE camera trap; cannibalism; competition; died filicide; infanticide;
   scavenging; siblicide
ID PARENTAL INFANTICIDE; BROOD REDUCTION; TYTO-ALBA; OWLS; SIBLICIDE;
   KESTRELS
AB Feeding strategies, including cannibalism (in which an individual eats a member of the same species), are an important aspect of predator ecology. Cannibalism comprises five forms in raptors: siblicide, filicide, non-parental infanticide, conspecific strife, and conspecific scavenging. Cannibalism by raptors has been documented opportunistically for over a century, but it is unknown how frequent or widespread the behavior is. We performed the first systematic literature review and meta-analyses of the studies documenting filicide, non-parental infanticide, conspecific strife, and conspecitic scavenging by raptors. We found 29 reports of these types of cannibalism; we did not review reports of siblicide due to the high frequency of the behavior, making it nearly ubiquitous among raptors. Filicide had nearly twice as many reports (n = 11, 37.9%) as any other type of cannibalism. Most reports were direct observations (n= 23, 79.3%), and nearly half the reports came from North America (n=14, 48.3%) and approximately a third from Europe (n= 10, 34.5%). The 29 reports involved 25 raptor species front four families, with those from Accipitridae most common (n= 19, 65.5%). Cannibalism in raptors varies but most involves nestlings, which are easier to kill than adults, possibly because brood reduction can help the stronger young survive. Documented reports of cannibalism are increasing, possibly due to recent technological advancements that have increased our ability to document cannibalism and other ecological processes. Nevertheless, we encourage future reports of cannibalism from under-represented locations and for taxa that are less well studied.
C1 [Allen, Maximilian L.; Ward, Michael P.] Univ Illinois, Illinois Nat Hist Survey, 1816 South Oak St, Champaign, IL 61820 USA.
   [Inagaki, Akino] Tokyo Univ Agr & Technol, Grad Sch Apiculture, 3-5-8 Saiwai Cho, Fuchu, Tokyo 1838509, Japan.
   [Ward, Michael P.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 South Goodwin, Urbana, IL 61801 USA.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 South Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
FU University of Illinois; Illinois Natural History Survey
FX We thank the Illinois Natural History Survey and the University of
   Illinois for their support. We thank D. E. Varland and an anonymous
   reviewer for their thoughtful comments on previous versions that greatly
   improved this report.
CR Allen ML, 2019, J RAPTOR RES, V53, P410, DOI 10.3356/0892-1016-53.4.410
   Anderson A, 2015, J RAPTOR RES, V49, P498, DOI 10.3356/rapt-49-04-498-500.1
   Arroyo BE, 1997, J RAPTOR RES, V31, P390
   BECHARD MJ, 1983, WILSON BULL, V95, P233
   BORTOLOTTI GR, 1991, CAN J ZOOL, V69, P1447, DOI 10.1139/z91-205
   Camina A., 2003, VULTURE NEWS, V47, P25
   Caro J, 2014, J RAPTOR RES, V48, P292, DOI 10.3356/JRR-13-87.1
   Clements J.F., 2018, EBIRD CLEMENTS CHECK
   CLEVENGER GA, 1974, AUK, V91, P639
   Coffin L. V. B., 1906, BIRD LORE, V8, P68
   de Lecea FM, 2011, ARDEA, V99, P240, DOI 10.5253/078.099.0216
   FISHER B M, 1975, Canadian Field-Naturalist, V89, P71
   FORBES LS, 1991, BEHAV ECOL SOCIOBIOL, V29, P189, DOI 10.1007/BF00166400
   Franke A, 2013, ARCTIC, V66, P226
   Hadjikyriakou TG, 2016, J RAPTOR RES, V50, P220, DOI 10.3356/0892-1016-50.2.220
   Hollingsworth Julie, 2017, Australian Field Ornithology, V34, P129
   HOLTHUIJZEN A M A, 1987, Journal of Raptor Research, V21, P32
   Inagaki A, 2020, ECOL EVOL, V10, P1223, DOI 10.1002/ece3.5976
   INGRAM COLLINGWOOD, 1959, AUK, V76, P218
   INGRAM COLLINGWOOD, 1962, AUK, V79, P715
   JONES A M, 1990, Journal of Raptor Research, V24, P28
   Kang Seung Gu, 2018, [KOREAN JOURNAL OF ENVIRONMENT AND ECOLOG, 한국환경생태학회지], V32, P256, DOI 10.13047/KJEE.2018.32.3.256
   Kornan M, 2011, J RAPTOR RES, V45, P95
   Krofel Miha, 2011, Acrocephalus, V32, P45, DOI 10.2478/v10100-011-0003-3
   LENTON GM, 1984, IBIS, V126, P551, DOI 10.1111/j.1474-919X.1984.tb02080.x
   Lewis SB, 2017, J RAPTOR RES, V51, P476, DOI 10.3356/JRR-17-22.1
   LYONS D, 1982, ARDEA, V70, P217
   Margalida A, 2004, IBIS, V146, P386, DOI 10.1111/j.1474-919X.2004.00261.x
   Markham AC, 2007, J RAPTOR RES, V41, P41, DOI 10.3356/0892-1016(2007)41[41:DOIACI]2.0.CO;2
   MILLARD JB, 1978, WILSON BULL, V90, P449
   Miller SJ, 2015, J RAPTOR RES, V49, P152, DOI 10.3356/0892-1016-49.2.152
   Mori Devvratsinh, 2017, Indian Birds, V13, P111
   NEGRO JJ, 1992, J RAPTOR RES, V26, P225
   Newton I, 2010, POPULATION ECOLOGY R
   Newton I., 1998, POPULATION LIMITATIO
   PILZ WR, 1978, AUK, V95, P584
   Rana Gargi, 2003, Journal of the Bombay Natural History Society, V100, P116
   Redondo T, 2019, ECOL EVOL, V9, P9185, DOI 10.1002/ece3.5466
   Robinson T. S., 1954, Wilson Bulletin, V66, P72
   Sebastian-Gonzalez E, 2019, GLOBAL CHANGE BIOL, V25, P3005, DOI 10.1111/gcb.14708
   SHEFFIELD SR, 1994, J RAPTOR RES, V28, P119
   Steen R, 2016, J RAPTOR RES, V50, P217, DOI 10.3356/0892-1016-50.2.217
   STEFFEN JF, 1977, AUK, V94, P593
   Temple Dick, 2008, British Birds, V101, P687
   TRENBERTH KE, 1983, B AM METEOROL SOC, V64, P1276, DOI 10.1175/1520-0477(1983)064<1276:WATS>2.0.CO;2
   Webster A, 1999, EMU, V99, P80, DOI 10.1071/MU99009D
   Wilson EE, 2011, TRENDS ECOL EVOL, V26, P129, DOI 10.1016/j.tree.2010.12.011
   Woodford JE, 2008, J RAPTOR RES, V42, P79, DOI 10.3356/JRR-07-44.1
NR 48
TC 2
Z9 4
U1 3
U2 7
PU RAPTOR RESEARCH FOUNDATION INC
PI HASTINGS
PA 14377 117TH STREET SOUTH, HASTINGS, MN 55033 USA
SN 0892-1016
EI 2162-4569
J9 J RAPTOR RES
JI J. Raptor Res.
PD DEC
PY 2020
VL 54
IS 4
BP 424
EP 430
DI 10.3356/0892-1016-54.4.424
PG 7
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA PN9FZ
UT WOS:000604780200008
DA 2022-02-10
ER

PT C
AU Pastarmov, Y
AF Pastarmov, Yulian
BE Rachev, B
   Smrikarov, A
TI System for Automatic Camera Calibration Robust Against Blur and Lighting
   Conditions Changes
SO COMPUTER SYSTEMS AND TECHNOLOGIES, COMPSYSTECH'16
LA English
DT Proceedings Paper
CT 17th International Conference on Computer Systems and Technologies
   (CompSysTech)
CY JUN 23-24, 2016
CL Palermo, ITALY
SP Assoc Comp Machinery, Bulgarian Chapter, Bulgarian Acad Soc Comp Syst & Informat Technologies, Bulgarian Union Automat & Informat, Univ Ruse, QUERBIE Inc, Univ Palermo, Tech Univ Varna
DE Camera Calibration; Intrinsic Camera Models; Monocular and Stereo
   Computer Vision
AB This paper presents a system for precise intrinsic and extrinsic camera calibration for monocular and stereo cameras. The presented approach is based on some well established research in the field and utilizes calibration patterns. The novelty of the presented method is the automatic detection of motion blur, that is common to the process of calibration based on moving the camera or the calibration object, especially under bad lighting conditions. The method allows for sub-pixel re-projection precision for cameras with perspective lenses and can be extended to omnidirectional cameras as well.
C1 [Pastarmov, Yulian] Google, Munich, Germany.
   [Pastarmov, Yulian] St Cyril & St Methodius Univ Veliko Tarnovo, Comp Syst & Technol Dept, Veliko Tarnovo, Bulgaria.
RP Pastarmov, Y (corresponding author), Google, Munich, Germany.; Pastarmov, Y (corresponding author), St Cyril & St Methodius Univ Veliko Tarnovo, Comp Syst & Technol Dept, Veliko Tarnovo, Bulgaria.
EM pastarmovj@google.com
CR Bao Q., 2008, BLUR METRIC IMPLEMEN
   Crete-Roffet F., 2007, P SPIE INT SOC OPTIC
   Datta A., 2009, IEEE 12 ICCV COMP VI
   Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269
   Fischler M. A., 1981, COMMUNICATIONS ACM, V24
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   MathWorks, EST GEOM PAR SINGL C
   Richardson A., 2013, P IEEE RSJ INT C INT
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Yao Y., 2006, EVALUATION SHARPNESS
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 11
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-4182-0
PY 2016
BP 167
EP 174
DI 10.1145/2983468.2983522
PG 8
WC Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BP5WA
UT WOS:000558303700022
OA Bronze
DA 2022-02-10
ER

PT C
AU Gomez-Fernandez, F
   Liu, ZC
   Pardo, A
   Mejail, M
AF Gomez-Fernandez, Francisco
   Liu, Zicheng
   Pardo, Alvaro
   Mejail, Marta
BE BayroCorrochano, E
   Hancock, E
TI Automatic Camera-Screen Localization
SO PROGRESS IN PATTERN RECOGNITION IMAGE ANALYSIS, COMPUTER VISION, AND
   APPLICATIONS, CIARP 2014
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 19th Iberoamerican Congress on Pattern Recognition (CIARP)
CY NOV 01-05, 2014
CL Puerto Vallarta, MEXICO
SP CINVESTAV, Campus Guadalajara, Mexican Assoc Comp Vis, Neural Comp & Robot, Int Assoc Pattern Recognit, Cuban Assoc Pattern Recognit, Chilean Assoc Pattern Recognit, Brazilian Comp Soc, Special Interest Grp, Spanish Assoc Pattern Recognt & Image Anal, Portuguese Assoc Pattern Recognit, INTEL Educ
DE Human-Computer Interaction; Head pose estimation; Screen localization
AB Knowing the location of the TV screen with respect to a camera it is important for many applications. This work addresses this problem in a configuration where there are people looking at the TV and a RGB-D camera facing them, located near the TV screen. We propose a method to automatically estimate the screen location and camera rotation using only people's head pose obtained from a Face Tracking analysis on the RGB-D video. We validated these algorithms on a dataset with groundtruth and obtained very promising results.
C1 [Gomez-Fernandez, Francisco; Mejail, Marta] Univ Buenos Aires, RA-1053 Buenos Aires, DF, Argentina.
   [Pardo, Alvaro] Univ Catolica Uruguay, Montevideo, Uruguay.
   [Liu, Zicheng] Microsoft Res, Washington, DC USA.
RP Gomez-Fernandez, F (corresponding author), Univ Buenos Aires, RA-1053 Buenos Aires, DF, Argentina.
CR Asteriadis S., 2013, INT J COMPUT VISION, V107, P1
   Cai Q, 2010, LECT NOTES COMPUT SC, V6313, P229
   Duda RO, 2012, PATTERN CLASSIFICATI
   Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11
   Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kondori F.A., 2011, WIR COMM SIGN PROC W, P1
   Mora K. A. F., 2012, P IEEE COMP SOC C CO, P25, DOI DOI 10.1109/CVPRW.2012.6239182
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
NR 9
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-12568-8; 978-3-319-12567-1
J9 LECT NOTES COMPUT SC
PY 2014
VL 8827
BP 588
EP 595
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Mathematical & Computational Biology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematical & Computational Biology
GA BB8DA
UT WOS:000346407400072
DA 2022-02-10
ER

PT C
AU Li, XH
   Wang, GY
   Liu, JG
AF Li, Xiuhua
   Wang, Guoyou
   Liu, Jianguo
BE Tian, J
   Ma, J
TI Automatic camera calibration method based on dashed lines
SO MIPPR 2013: REMOTE SENSING IMAGE PROCESSING, GEOGRAPHIC INFORMATION
   SYSTEMS, AND OTHER APPLICATIONS
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT 8th Symposium on Multispectral Image Processing and Pattern Recognition
   (MIPPR) - Remote Sensing Image Processing, Geographic Information
   Systems, and Other Applications
CY OCT 26-27, 2013
CL Wuhan, PEOPLES R CHINA
SP SPIE, Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multi Spectral Informat Proc
DE Camera Calibration; Dashed line; Endpoint; Traffic monitoring; Computer
   vision
AB We present a new method for full-automatic calibration of traffic cameras using the end points on dashed lines. Our approach uses the improved RANSAC method with the help of pixels transverse projection to detect the dashed lines and end points on them. Then combining analysis of the geometric relationship between the camera and road coordinate systems, we construct a road model to fit the end points. Finally using two-dimension calibration method we can convert pixels in image to meters along the ground truth lane. On a large number of experiments exhibiting a variety of conditions, our approach performs well, achieving less than 5% error in measuring test lengths in all cases.
C1 [Li, Xiuhua; Wang, Guoyou; Liu, Jianguo] Huazhong Univ Sci & Technol, Sch Automat, Sci & Technol Multispectral Informat Proc Lab, Wuhan 430074, Peoples R China.
RP Li, XH (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Sci & Technol Multispectral Informat Proc Lab, Wuhan 430074, Peoples R China.
EM gywang@mail.hust.edu.cn
CR [Anonymous], 2011, PANORAMA ACTUAL MED, V35, P30
   Bas EK, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P362, DOI 10.1109/ITSC.1997.660502
   Cathey R, 2004, ITSC 2004: 7TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P865
   Dawson Douglas N., 2013, IEEE INTEL TRANSP SY, P1
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu Y., 2012, ELECT DESIGN ENG, P49
   Fung GSK, 2003, OPT ENG, V42, P2967, DOI 10.1117/1.1606458
   He XC, 2007, OPT ENG, V46, DOI 10.1117/1.2714991
   Kanhere NK, 2008, IEEE T INTELL TRANSP, V9, P148, DOI 10.1109/TITS.2007.911357
   Kanhere NK, 2008, TRANSPORT RES REC, P30, DOI 10.3141/2086-04
   Lai AHS, 2000, IEEE T SYST MAN CY B, V30, P539, DOI 10.1109/3477.865171
   Otsu N., 1997, IEEE T SYST MAN CYB, V9, P62, DOI DOI 10.1109/TSMC.1979.4310076
   Schoepflin Todd N, 2007, 2007 IEEE Intelligent Transportation Systems Conference, P277, DOI 10.1109/ITSC.2007.4357806
   Song KT, 2006, IEEE T SYST MAN CY B, V36, P1091, DOI 10.1109/TSMCB.2006.872271
   Trajkovi'c M., 2002, P AS C COMP VIS, P1
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Wu B.F., 2007, P IEEE INT C SMC OCT, P1717
NR 17
TC 0
Z9 0
U1 0
U2 1
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-0-8194-9806-9
J9 PROC SPIE
PY 2013
VL 8921
AR 892112
DI 10.1117/12.2031362
PG 8
WC Geosciences, Multidisciplinary; Remote Sensing; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Geology; Remote Sensing; Optics
GA BID12
UT WOS:000327588700038
DA 2022-02-10
ER

PT C
AU Ishikawa, R
   Oishi, T
   Ikeuchi, K
AF Ishikawa, Ryoichi
   Oishi, Takeshi
   Ikeuchi, Katsushi
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI LiDAR and Camera Calibration using Motions Estimated by Sensor Fusion
   Odometry
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
LA English
DT Proceedings Paper
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
SP IEEE Robot & Automat Soc, IEEE Ind Elect Soc, Robot Soc Japan, Soc Instrument & Control Engineers, New Technol Fdn, IEEE, Adept MobileRobots, Willow Garage, Aldebaran Robot, Natl Instruments, Reflexxes GmbH, Schunk Intec S L U, Univ Carlos III Madrid, BOSCH, JD COM, Pal Robot, KUKA, Santander, Squirrel AI Learning, Baidu, Generat Robots, KINOVA Robot, Ouster, Univ Pablo Olavide Sevilla, Rapyuta Robot, SICK, TOYOTA, UP, Amazon, ARGO, Built Robot, Disney Res, Easy Mile, Hitachi, Robot, Khalifa Univ, Magazino, MathWorks, New Dexterity, Schunk, nuTonomy, PILZ, Prophesee, Rootnik, Saga Robot, Shadow, Soft Bank Robot, Anyverse, GalTech, Generat Robot, IEEE CAA Journal Automatica Sinica, Sci Robot, AAAS, TERAS
ID RECONSTRUCTION; LASER
AB This paper proposes a targetless and automatic camera-LiDAR calibration method. Our approach extends the hand-eye calibration framework to 2D-3D calibration. The scaled camera motions are accurately calculated using a sensorfusion odometry method. We also clarify the suitable motions for our calibration method.
   Whereas other calibrations require the LiDAR reflectance data and an initial extrinsic parameter, the proposed method requires only the three-dimensional point cloud and the camera image. The effectiveness of the method is demonstrated in experiments using several sensor configurations in indoor and outdoor scenes. Our method achieved higher accuracy than comparable state-of-the-art methods.
C1 [Ishikawa, Ryoichi; Oishi, Takeshi] Univ Tokyo, Inst Ind Sci, Tokyo, Japan.
   [Ikeuchi, Katsushi] Microsoft, Syracuse, NY USA.
RP Ishikawa, R (corresponding author), Univ Tokyo, Inst Ind Sci, Tokyo, Japan.
EM ishikawa@cvl.iis.utokyo.ac.jp; oishi@cvl.iis.utokyo.ac.jp;
   katsuike@microsoft.com
FU social corporate program (Base Technologies for Future Robots) - NIDEC
   corporation; JSPS KAKENHI GrantMinistry of Education, Culture, Sports,
   Science and Technology, Japan (MEXT)Japan Society for the Promotion of
   ScienceGrants-in-Aid for Scientific Research (KAKENHI) [JP16747698,
   JP17923471]; JSPSMinistry of Education, Culture, Sports, Science and
   Technology, Japan (MEXT)Japan Society for the Promotion of Science
   [16J09277]
FX This work was partially supported by the social corporate program (Base
   Technologies for Future Robots) sponsored by NIDEC corporation. This
   work was also supported by JSPS KAKENHI Grant Number JP16747698,
   JP17923471, and JSPS Research Fellow Grant No. 16J09277.
CR Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Banno A, 2010, COMPUT VIS IMAGE UND, V114, P491, DOI 10.1016/j.cviu.2009.12.005
   Bok Y, 2016, ROBOT AUTON SYST, V78, P17, DOI 10.1016/j.robot.2015.12.007
   Bok Y, 2011, INT J COMPUT VISION, V94, P36, DOI 10.1007/s11263-010-0397-8
   Cui TT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010070
   Fassi I, 2005, J ROBOTIC SYST, V22, P497, DOI 10.1002/rob.20082
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Heng L, 2013, IEEE INT C INT ROBOT, P1793, DOI 10.1109/IROS.2013.6696592
   Hol JD, 2010, INT J ROBOT RES, V29, P231, DOI 10.1177/0278364909356812
   Irie Kiyoshi, 2016, 2016 IEEE International Conference on Automation Science and Engineering (CASE), P1340, DOI 10.1109/COASE.2016.7743564
   Ishikawa R, 2016, INT CONF 3D VISION, P620, DOI 10.1109/3DV.2016.70
   Kelly J, 2011, INT J ROBOT RES, V30, P56, DOI 10.1177/0278364910382802
   Kneip L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2969, DOI 10.1109/CVPR.2011.5995464
   Levinson J, 2013, ROBOTICS SCI SYSTEMS, V2
   Lucas B. D., 1981, P 7 INT JOINT C ART, V2, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Nister D, 2006, J FIELD ROBOT, V23, P3, DOI 10.1002/rob.20103
   Oishi T, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P476, DOI 10.1109/3DIM.2005.41
   Pagani A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P375, DOI 10.1109/ICCVW.2011.6130266
   Pandey G, 2015, J FIELD ROBOT, V32, P696, DOI 10.1002/rob.21542
   Rodriguez F Sergio A, 2008, 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2008), P214, DOI 10.1109/MFI.2008.4648067
   SHIU YC, 1989, IEEE T ROBOTIC AUTOM, V5, P16, DOI 10.1109/70.88014
   Taylor Z., 2012, P AUSTR C ROB AUT DE, P3
   Taylor Z, 2016, IEEE T ROBOT, V32, P1215, DOI 10.1109/TRO.2016.2596771
   Taylor Z, 2015, J FIELD ROBOT, V32, P675, DOI 10.1002/rob.21523
   VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930
   Zhang J, 2017, AUTON ROBOT, V41, P31, DOI [10.1007/s10514-015-9525-1, 10.1109/MWSYM.2015.7167049]
   Zhang Q., 2004, IEEE RSJ INT C INT R, P2301, DOI [10.1109/IROS.2004.1389752, DOI 10.1109/IROS.2004.1389752]
   Zheng  B., 2015, INT C 3D VIS
NR 28
TC 21
Z9 22
U1 3
U2 18
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2153-0858
BN 978-1-5386-8094-0
J9 IEEE INT C INT ROBOT
PY 2018
BP 7342
EP 7349
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BM0LT
UT WOS:000458872706097
DA 2022-02-10
ER

PT J
AU Vieira, T
   Bordignon, A
   Peixoto, A
   Tavares, G
   Lopes, H
   Velho, L
   Lewiner, T
AF Vieira, Thales
   Bordignon, Alex
   Peixoto, Adelailson
   Tavares, Geovan
   Lopes, Helio
   Velho, Luiz
   Lewiner, Thomas
TI Learning good views through intelligent galleries
SO COMPUTER GRAPHICS FORUM
LA English
DT Article
DE Artificial Intelligence [I.2.6]: Learning; Analogies; Computer Graphics
   [I.3.6]: Methodology and Techniques; Interaction techniques
AB The definition of a good view of a 3D scene is highly subjective and strongly depends on both the scene content and the 3D application. Usually, camera placement is performed directly by the user, and that task may be laborious. Existing automatic virtual cameras guide the user by optimizing a single rule, e.g. maximizing the visible silhouette or the projected area. However, the use of a static pre-defined rule may fail in respecting the user's subjective understanding of the scene. This work introduces intelligent design galleries, a learning approach for subjective problems such as the camera placement. The interaction of the user with a design gallery teaches a statistical learning machine. The trained machine can then imitate the user, either by pre-selecting good views or by automatically placing the camera. The learning process relies on a Support Vector Machines for classifying views from a collection of descriptors, ranging from 2D image quality to 3D features visibility. Experiments of the automatic camera placement demonstrate that the proposed technique is efficient and handles scenes with occlusion and high depth complexities. This work also includes user validations of the intelligent gallery interface.
C1 [Vieira, Thales; Bordignon, Alex; Tavares, Geovan; Lopes, Helio; Lewiner, Thomas] Matmidia PUC, Rio De Janeiro, Brazil.
   [Peixoto, Adelailson] CPMAT UFAL, Maceio, Brazil.
   [Velho, Luiz] Visgraf IMPA, Rio De Janeiro, Brazil.
RP Vieira, T (corresponding author), Matmidia PUC, Rio De Janeiro, Brazil.
RI Lewiner, Thomas/B-7751-2008; Lopes, Helio C. V./A-2369-2009; Lopes,
   Hélio/AAX-6609-2020; de Almeida Vieira, Thales Miranda/C-7689-2017
OI Lopes, Hélio/0000-0003-4584-1455; de Almeida Vieira, Thales
   Miranda/0000-0001-7775-5258; Lewiner, Thomas/0000-0001-9518-6423
FU CNPqConselho Nacional de Desenvolvimento Cientifico e Tecnologico
   (CNPQ); FAPERJFundacao Carlos Chagas Filho de Amparo a Pesquisa do
   Estado do Rio De Janeiro (FAPERJ); CAPESCoordenacao de Aperfeicoamento
   de Pessoal de Nivel Superior (CAPES)
FX The authors wish to thank Fabiano Petronetto for constructive
   discussions, Eduardo Telles, Debora Lima, Bernardo Ribeiro, Clarissa
   Marques and Afonso Paiva for their help with the interface, and Marcos
   Lage and Rener Castro for help with the coding. This work was supported
   in part by a grant from the CNPq (Universal MCT/CNPq, Productivity
   scholarship, Instituto Nacional de Ciencia e Tecnologia), FAPERJ (Jovem
   Cientista) and CAPES.
CR BARES W, 2000, MULTIMEDIA 00, P177
   Bares W, 2006, LECT NOTES COMPUT SC, V4073, P172
   BARRAL P, 2000, EUROGRAPHICS
   Blanz V, 1999, PERCEPTION, V28, P575, DOI 10.1068/p2897
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Christie M., 2008, COMPUTER GRAPHICS FO, V27, P8
   DRUCKER SM, 1994, GRAPH INTER, P190
   Fleishman S., 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P12, DOI 10.1109/PCCGA.1999.803344
   Gomez F, 2001, J VIS COMMUN IMAGE R, V12, P387, DOI 10.1006/jvci.2001.0488
   Gooch B, 2001, SPRING EUROGRAP, P83
   Hachet M, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P83
   HALPER N, 2000, SMART GRAPHICS
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   LAGA H, 2007, NICOGRAPH
   Lee C., 2005, SIGGRAPH
   MARKS J, 1997, SIGGRAPH 97, P389
   Meyer M., 2003, VISUALIZATION MATH, P35, DOI DOI 10.1007/978-3-662-05105-4_2
   Page DL, 2003, IEEE IMAGE PROC, P229
   PASTOR OEM, 2002, VISIBILITY PREPROCES
   PLEMENOS D, 1996, GRAPHICON
   Podolak J, 2006, ACM T GRAPHIC, V25, P549, DOI 10.1145/1141911.1141923
   Polonsky O, 2005, VISUAL COMPUT, V21, P840, DOI 10.1007/s00371-005-0326-y
   Scholkopf B., 2002, LEARNING KERNELS
   SHILANE P, 2008, T GRAPHICS, V26, P7
   SINGH K, 2004, AFRIGRAPH 04, P41
   SOKOLOV D, 2006, GRAPP
   Sokolov D, 2008, VISUAL COMPUT, V24, P173, DOI 10.1007/s00371-007-0182-z
   STOEV S, 2002, VISUALIZATION
   Vapnik V., 2000, NATURE STAT LEARNING
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Yamauchi H, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P265
   [No title captured]
   [No title captured]
NR 33
TC 25
Z9 29
U1 1
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0167-7055
EI 1467-8659
J9 COMPUT GRAPH FORUM
JI Comput. Graph. Forum
PY 2009
VL 28
IS 2
BP 717
EP 726
DI 10.1111/j.1467-8659.2009.01412.x
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 424DC
UT WOS:000264544700057
DA 2022-02-10
ER

PT J
AU Davis, ML
   Kelly, MJ
   Stauffer, DF
AF Davis, M. L.
   Kelly, M. J.
   Stauffer, D. F.
TI Carnivore co-existence and habitat use in the Mountain Pine Ridge Forest
   Reserve, Belize
SO ANIMAL CONSERVATION
LA English
DT Article
DE camera trapping; carnivore co-existence; interspecific competition;
   jaguar; ocelot; puma
ID JAGUAR PANTHERA-ONCA; OCELOT LEOPARDUS-PARDALIS; PUMA PUMA-CONCOLOR;
   MESOPREDATOR RELEASE; FOOD-HABITS; BEHAVIOR; DENSITY; ECOLOGY;
   COMPETITION; ABUNDANCE
AB To protect and manage an intact neotropical carnivore guild, it is necessary to understand the relative importance of habitat selection and intraguild competition to the ecology of individual species. This study examined habitat use of four carnivores in the Mountain Pine Ridge Forest Reserve, Belize. We calculated photographic trap success (TS) rates for jaguars Panthera onca, pumas Puma concolor, ocelots Leopardus pardalis, grey foxes Urocyon cinereoargenteus, potential prey and humans at 47 remote camera stations spaced along roads and trails within the 139 km2 study site. At each station, we used manual habitat sampling in combination with geographic information systems to estimate habitat characteristics pertaining to vegetation cover. We used negative binomial models to analyse species-specific TS as a response to habitat (including vegetation and landscape variables, prey activity and human activity) and co-predator activity rates. Jaguars [TS=7.56 +/- 1.279 (se) captures per 100 trap-nights (TN)] and grey foxes (31.5 +/- 6.073 captures per 100 TN) were commonly captured by cameras, while pumas (0.66 +/- 0.200 captures per 100 TN) and ocelots (0.55 +/- 0.209 captures per 100 TN) were rare. Model selection via Akaike's information criterion (AIC) revealed that models including habitat variables generally performed better than models including co-predator activity. Felid captures were positively associated with small bird TS and with the width or length of surrounding roads, while fox counts showed few habitat associations. Ocelot activity was positively related to jaguar captures, an effect probably explained by their shared preference for areas with more roads. Pumas were negatively related to human activity and jaguars showed a similar, though non-significant, trend, suggesting that these felids may be sensitive to human disturbance even within protected areas. Results suggest that these predators do not spatially partition habitat and that the jaguar could function as an umbrella species for smaller sympatric carnivores.
C1 [Davis, M. L.; Kelly, M. J.; Stauffer, D. F.] Virginia Tech, Dept Fisheries & Wildlife Sci, Blacksburg, VA USA.
RP Davis, ML (corresponding author), Univ Durham, Dept Biol & Biomed Sci, South Rd, Durham DH1 3LE, England.
EM mldavis13@gmail.com
RI Kelly, Marcella J/B-4891-2011
FU Acorn Alcinda Foundation; Philadelphia Zoo and Virginia Tech
FX This project was made possible by assistance from the Forest Department
   of Belize, J. Meerman, C. Wultsch, A. Passarelli, N. Lambert, J. Monzon
   and J. Vance. Field support was also provided by T. McNamara, N. Bol,
   Blancaneaux Lodge, and G. Headley and M. Headley. The authors would also
   like to thank R. Bagchi, M. Vaughan, S. Prisley, O.T. Lewis, P.A.
   Stephens and M. Tewes who provided invaluable advice at different points
   throughout the development of this paper. Financial support was provided
   by The Acorn Alcinda Foundation, The Philadelphia Zoo and Virginia Tech.
CR Abreu KC, 2008, MAMM BIOL, V73, P407, DOI 10.1016/j.mambio.2007.07.004
   Anderson K. A., 2002, MODEL SELECTION MULT
   Asquith NM, 1997, ECOLOGY, V78, P941, DOI 10.1890/0012-9658(1997)078[0941:DMCCCR]2.0.CO;2
   BISBAL FJ, 1993, STUD NEOTROP FAUNA E, V28, P145, DOI 10.1080/01650529309360899
   Boydston EE, 2003, ANIM CONSERV, V6, P207, DOI 10.1017/S1367943003003263
   Caro TM, 2003, BIOL CONSERV, V110, P67, DOI 10.1016/S0006-3207(02)00177-5
   CRAWSHAW PG, 1991, J ZOOL, V223, P357, DOI 10.1111/j.1469-7998.1991.tb04770.x
   Crooks KR, 1999, NATURE, V400, P563, DOI 10.1038/23028
   DAVIS M, 2009, THESIS VIRGINIA TECH
   Dillon A, 2008, J ZOOL, V275, P391, DOI 10.1111/j.1469-7998.2008.00452.x
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   Donadio E, 2006, AM NAT, V167, P524, DOI 10.1086/501033
   Dunning J. B., 1993, CRC HDB AVIAN BODY M
   EMMONS LH, 1987, BEHAV ECOL SOCIOBIOL, V20, P271, DOI 10.1007/BF00292180
   EVERATT K, 2010, JAGUAR PANTHERA ONCA
   Fedriani JM, 2000, OECOLOGIA, V125, P258, DOI 10.1007/s004420000448
   Gehrt SD, 2003, WILDLIFE SOC B, V31, P836
   Gittleman JL, 2001, CONSERV BIOL SER, V5, P1
   Harmsen BJ, 2009, J MAMMAL, V90, P612, DOI 10.1644/08-MAMM-A-140R.1
   JOHNSON WE, 1994, CAN J ZOOL, V72, P1788, DOI 10.1139/z94-242
   Jones H. L., 2003, BIRDS BELIZE
   Kellman M, 1997, J BIOGEOGR, V24, P23, DOI 10.1111/j.1365-2699.1997.tb00047.x
   KELLMAN M, 1982, J BIOGEOGR, V9, P193, DOI 10.2307/2844663
   Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Kerley LL, 2002, CONSERV BIOL, V16, P97, DOI 10.1046/j.1523-1739.2002.99290.x
   Konecny M.J., 1989, P243
   LUDLOW ME, 1987, NATL GEOGR RES, V3, P447
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   MEERMAN JC, 2007, BIODIVERSITY ENV RES
   Moreno RS, 2006, J MAMMAL, V87, P808, DOI 10.1644/05-MAMM-A-360R2.1
   NOSS RF, 1990, CONSERV BIOL, V4, P355, DOI 10.1111/j.1523-1739.1990.tb00309.x
   NOVARO AJ, 1995, MAMMALIA, V59, P19, DOI 10.1515/mamm.1995.59.1.19
   Palomares F, 1999, AM NAT, V153, P492, DOI 10.1086/303189
   Paviolo A, 2009, J MAMMAL, V90, P926, DOI 10.1644/08-MAMM-A-128.1
   POLLARD JH, 1971, BIOMETRICS, V27, P991, DOI 10.2307/2528833
   RABINOWITZ AR, 1986, J ZOOL, V210, P149, DOI 10.1111/j.1469-7998.1986.tb03627.x
   Reid, 1997, FIELD GUIDE MAMMALS
   Sanderson EW, 2002, CONSERV BIOL, V16, P58, DOI 10.1046/j.1523-1739.2002.00352.x
   SAS Institute Inc, 2002, SAS 9 2
   SAWYER DT, 1994, 48 ANN C SE ASS FISH, P162
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Scognamillo D, 2003, J ZOOL, V259, P269, DOI 10.1017/S0952836902003230
   Shindle DB, 1998, SOUTHWEST NAT, V43, P273
   Sileshi G, 2008, PEDOBIOLOGIA, V52, P1, DOI 10.1016/j.pedobi.2007.11.003
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Sunquist M., 2002, WILD CATS WORLD, DOI 10.1644/1545-1542(2004)0852.0.co;2
   Taber AB, 1997, BIOTROPICA, V29, P204, DOI 10.1111/j.1744-7429.1997.tb00025.x
   Terborgh J, 2006, J ECOL, V94, P253, DOI 10.1111/j.1365-2745.2006.01106.x
   Terborgh J, 2001, SCIENCE, V294, P1923, DOI 10.1126/science.1064397
   Terborgh John, 1990, Vida Silvestre Neotropical, V2, P3
   Tewes M.E., 1986, THESIS U IDAHO MOSCO
   *WORLD WILDL FUND, 2001, CENTR AM PIN OAK FOR
NR 53
TC 71
Z9 76
U1 3
U2 153
PU WILEY-BLACKWELL
PI MALDEN
PA COMMERCE PLACE, 350 MAIN ST, MALDEN 02148, MA USA
SN 1367-9430
J9 ANIM CONSERV
JI Anim. Conserv.
PD FEB
PY 2011
VL 14
IS 1
BP 56
EP 65
DI 10.1111/j.1469-1795.2010.00389.x
PG 10
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA 721MU
UT WOS:000287359800010
DA 2022-02-10
ER

PT C
AU Gelowitz, CM
   Benedicenti, L
AF Gelowitz, CM
   Benedicenti, L
BE Callaos, N
   Duale, A
   Benedicenti, L
   Yeary, M
TI Mobile agent audio source indexing using acoustic source localization
   techniques
SO 6TH WORLD MULTICONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL
   XV, PROCEEDINGS: MOBILE/WIRELESS COMPUTING AND COMMUNICATION SYSTEMS III
LA English
DT Proceedings Paper
CT 6th World Multi-Conference on Systemics, Cybernetics and Informatics
   (SCI 2002)/8th International Conference on Information Systems Analysis
   and Synthesis (ISAS 2002)
CY JUL 14-18, 2002
CL ORLANDO, FL
SP Int Inst Informat & System, World Org System & Cybernet, Ctr Syst Studies, Syst Soc Poland, Soc Appl Syst Res, Slovenian Artificial Intelligence Soc, Simon Bolivar Univ, Polish Syst Soc, Italian Soc System, Int Soc Syst Sci, Int Syst Inst, Int Federat Syst Res, Cybernet & Human Knowing, Journal Second Order Cybernet & Cybersemiot, Blaise Pascal Univ, Engineer Sci Inst, CUST, Univ Las Palmas Gran Canaria, Concurrency & Architecture Grp, Telemat Engn Dept, Tunisian Sci Soc, Acad Non Linear Sci, San Luis Natl Univ, Lab Res Computat Intelligence, Dept Informat, Amer Soc Cybernet (Canada), Wolfram Res Inc, IEEE Comp Soc, Venezuela Chapter, Steacie Inst Molec Sci, Natl Res Council Canada
AB Acoustic Source Localization can determine the position in 3D space that a person is speaking from. If each person in a room is assigned their own mobile agent, the 3D data and audio data that are collected can be cross-referenced by each individual's mobile agent.
   Mobile agents in this architecture could be considered real-time servants of intelligent audio data manipulation. Agents can be responsible for multi-client statistical gathering and audio analysis. The analysis of data can be used to include automatic camera movement and switching, microphone switching, audio search queries to key word conversation and real-time audio scheduling.
   Time delay data can be calculated using an array of microphones and digitally sampling the signal from each microphone simultaneously. Correlation techniques allow for the calculation of time delay estimates with respect to each microphone. From these time delay estimates and the use of Euclidean geometry, positional data can be determined.
C1 Univ Regina, Regina, SK S4S 7H9, Canada.
RP Gelowitz, CM (corresponding author), Univ Regina, Regina, SK S4S 7H9, Canada.
RI Benedicenti, Luigi/X-9412-2019
OI Benedicenti, Luigi/0000-0003-4036-9593
CR Gibbs C, 2000, TEEMA REFERENCE GUID
   RUSH KJ, 1998, THESIS U SASKATOON S
NR 2
TC 0
Z9 0
U1 0
U2 0
PU INT INST INFORMATICS & SYSTEMICS
PI ORLANDO
PA 14269 LORD BARCLAY DR, ORLANDO, FL 32837 USA
PY 2002
BP 395
EP 398
PG 4
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Telecommunications
GA BV43M
UT WOS:000178929200071
DA 2022-02-10
ER

PT J
AU Sadgrove, EJ
   Falzon, G
   Miron, D
   Lamb, DW
AF Sadgrove, Edmund J.
   Falzon, Greg
   Miron, David
   Lamb, David W.
TI The Segmented Colour Feature Extreme Learning Machine: Applications in
   Agricultural Robotics
SO AGRONOMY-BASEL
LA English
DT Article
DE agricultural robotics; computer vision; drone; stationary camera trap;
   ensemble; extreme learning machine; feature mapping; object
   classification
ID OBJECT DETECTION; NEURAL-NETWORK; K-MEANS; LEAVES
AB This study presents the Segmented Colour Feature Extreme Learning Machine (SCF-ELM). The SCF-ELM is inspired by the Extreme Learning Machine (ELM) which is known for its rapid training and inference times. The ELM is therefore an ideal candidate for an ensemble learning algorithm. The Colour Feature Extreme Learning Machine (CF-ELM) is used in this study due to its additional ability to extract colour image features. The SCF-ELM is an ensemble learner that utilizes feature mapping via k-means clustering, a decision matrix and majority voting. It has been evaluated on a range of challenging agricultural object classification scenarios including weed, livestock and machinery detection. SCF-ELM model performance results were excellent both in terms of detection, 90 to 99% accuracy, and also inference times, around 0.01(s) per image. The SCF-ELM was able to compete or improve upon established algorithms in its class, indicating its potential for remote computing applications in agriculture.
C1 [Sadgrove, Edmund J.; Falzon, Greg] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
   [Falzon, Greg] Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA 5042, Australia.
   [Miron, David] Univ New England, Strateg Res Initiat, Armidale, NSW 2351, Australia.
   [Lamb, David W.] Univ New England, Precis Agr Res Grp, Armidale, NSW 2351, Australia.
   [Lamb, David W.] Food Agil Cooperat Res Ctr Ltd, 81 Broadway, Ultimo, NSW 2007, Australia.
RP Sadgrove, EJ (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
EM esadgro2@une.edu.au; greg.falzon@flinders.edu.au; dmiron@une.edu.au;
   dave.lamb@foodagility.com
OI Falzon, Gregory/0000-0002-1989-9357; Lamb, David/0000-0002-2917-2231
FU Australian GovernmentAustralian GovernmentCGIAR; Australian Government
   Research Training Program (RTP) stipend;  [AEC12-042]
FX This research was supported in part by a Cooperative Research Centres
   Project (CRC-P) Grant from the Australian Government. E. Sadgrove was
   supported by an Australian Government Research Training Program (RTP)
   stipend.
CR Al-Tairi ZH, 2014, J INF PROCESS SYST, V10, P283, DOI 10.3745/JIPS.02.0002
   Barata JCA, 2012, BRAZ J PHYS, V42, P146, DOI 10.1007/s13538-011-0052-z
   [Anonymous], 2015, STUD ENC PAR DIG TEL
   Aslan O., 2016, ARXIV160305691
   Bishop JC, 2019, COMPUT ELECTRON AGR, V162, P531, DOI 10.1016/j.compag.2019.04.020
   Cambria E, 2013, IEEE INTELL SYST, V28, P30, DOI 10.1109/MIS.2013.140
   Chand AA, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11030530
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Daga AP, 2020, ALGORITHMS, V13, DOI 10.3390/a13020033
   Ertugrul O.F., 2014, AM J COMPUT SCI ENG, V1, P43
   Glorot X., 2010, PROC 13 INT C ARTIFI, V9, P249, DOI DOI 10.1038/S41593-021-00857-X
   Gomes GSD, 2011, NEURAL COMPUT APPL, V20, P417, DOI 10.1007/s00521-010-0407-3
   Grace BS, 2002, RANGELAND J, V24, P313, DOI 10.1071/RJ02018
   Gonzalez-Gonzalez MG, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11051002
   Harase S, 2019, MONTE CARLO METHODS, V25, P61, DOI 10.1515/mcma-2019-2029
   Hashemi A, 2022, INT J MACH LEARN CYB, V13, P49, DOI 10.1007/s13042-021-01347-z
   Hsu D., 2020, ARXIVMATHST200910670
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Jiantao Xu, 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1490
   Juntao Wang, 2011, 2011 IEEE 3rd International Conference on Communication Software and Networks (ICCSN 2011), P44, DOI 10.1109/ICCSN.2011.6014384
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kim Y, 2005, MANAGE SCI, V51, P264, DOI 10.1287/mnsc.1040.0296
   Liberti L, 2014, SIAM REV, V56, P3, DOI 10.1137/120875909
   Liu N, 2010, IEEE SIGNAL PROC LET, V17, P754, DOI 10.1109/LSP.2010.2053356
   Malinen MI, 2014, LECT NOTES COMPUT SC, V8621, P32, DOI 10.1007/978-3-662-44415-3_4
   Netlib.org, LAPACKE C INT LAPACK
   Palumbo M, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11071353
   Rahman M., 2019, PROCEEDINGS, V36, P154, DOI [10.3390/proceedings2019036154, DOI 10.3390/PROCEEDINGS2019036154]
   Rod Z.P., 2000, P IEEE C COMP VIS PA
   Sadgrove EJ, 2018, COMPUT IND, V98, P183, DOI 10.1016/j.compind.2018.03.014
   Sadgrove EJ, 2017, COMPUT ELECTRON AGR, V139, P204, DOI 10.1016/j.compag.2017.05.017
   Sheela KG, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/425740
   van Schaik, 2015, P ELM 2014, V1, P41, DOI DOI 10.1007/978-3-319-14063-6_
   Wang X, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010051
   Wang YY, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10051897
   Wood A, 2013, ANZ J SURG, V83, P206, DOI 10.1111/ans.12106
   Zhang L., 2015, THEORY ALGORITHMS AP
   Zhang MH, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13214342
   Zhou G., 2005, C GEOSC REM SENS S P, V3
NR 40
TC 0
Z9 0
U1 1
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2073-4395
J9 AGRONOMY-BASEL
JI Agronomy-Basel
PD NOV
PY 2021
VL 11
IS 11
AR 2290
DI 10.3390/agronomy11112290
PG 16
WC Agronomy; Plant Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Plant Sciences
GA XI0HM
UT WOS:000725804300001
DA 2022-02-10
ER

PT C
AU Yin, HP
   Jiao, XG
   Luo, XK
   Yi, C
AF Yin, Hongpeng
   Jiao, Xuguo
   Luo, Xianke
   Yi, Chai
GP IEEE
TI Sift-based Camera Tamper Detection For Video Surveillance
SO 2013 25TH CHINESE CONTROL AND DECISION CONFERENCE (CCDC)
SE Chinese Control and Decision Conference
LA English
DT Proceedings Paper
CT 25th Chinese Control and Decision Conference (CCDC)
CY MAY 25-27, 2013
CL Guiyang, PEOPLES R CHINA
SP IEEE, NE Univ, IEEE Ind Elect Chapter, IEEE Harbin Sect Control Syst Soc Chapter, Guizhou Univ, IEEE Control Syst Soc, Syst Engn Soc China, Chinese Assoc Artificial Intelligence, Chinese Assoc Automat, Tech Comm Control Theory, Chinese Assoc Aeronaut, Automat Control Soc, Chinese Assoc Syst Simulat, Simulat Methods & Modeling Soc, Intelligent Control & Management Soc
DE video surveillance; camera tamper; SIFT algorithm; covered camera
   detection; moved camera detection
AB Keeping the camera long time proper functioning without tamper is the fundamentally requirement of a video surveillance system. Traditional camera tamper detection is applied by surveillance system operators. It's large human resource consuming and inefficiency. In this paper, a SIFT-based automatic camera tamper detection algorithm for video surveillance is proposed. When camera tamper occurred, the real-time frame will be large changed. Therefore, a Sift feature based decision function is employed to detect camera tamper. The threshold is carefully chosen to reduce false alarms. Several experiments are conducted to demonstrate the effectiveness and robust of the proposed method.
C1 [Yin, Hongpeng; Jiao, Xuguo; Luo, Xianke; Yi, Chai] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Yin, HP (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM yinhongpeng@gmail.com
CR Aksay A, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P558, DOI 10.1109/AVSS.2007.4425371
   Gil-Jimenez P, 2007, LECT NOTES COMPUT SC, V4528, P222
   Grabner M, 2006, LECT NOTES COMPUT SC, V3851, P918
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kim S., 2006, C COMP VIS PATT REC, P193
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Ribnick E., 2006, IEEE INT C VID SIGN
   Saglam Ali, 2006, IEEE INT C VID SIGN, P430
   Wang XH, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION AND LOGISTICS, VOLS 1-6, P843, DOI 10.1109/ICAL.2008.4636267
NR 10
TC 13
Z9 13
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1948-9439
BN 978-1-4673-5532-2; 978-1-4673-5533-9
J9 CHIN CONT DECIS CONF
PY 2013
BP 665
EP 668
PG 4
WC Automation & Control Systems
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems
GA BHY11
UT WOS:000326977300124
DA 2022-02-10
ER

PT C
AU Ren, YH
   Wang, Y
   Tang, Q
   Jiang, HJ
   Lu, WL
AF Ren, Yanhao
   Wang, Yi
   Tang, Qi
   Jiang, Haijun
   Lu, Wenlian
GP IEEE
TI On a videoing control system based on object detection and tracking
SO 2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
LA English
DT Proceedings Paper
CT IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 24-JAN 24, 2020-2021
CL ELECTR NETWORK
SP IEEE, RSJ
AB In this paper, we propose a camera control system towards occasionally videoing preassigned objects. Based on the technique of real-time visual detection and tracking, using the Kalman filter and re-identification (ReID), we propose continuous composition of lens, based on the atomic rules of shots, and give the trajectory planning of the camera, to generate the PID controller to the pan-tilt. By both simulation and emulation by frame-wise cropping of video clips, we illustrate the efficiency of this method. Based on this model, we design and produce an AI automatic camera for lively photography and clip videoing.
C1 [Ren, Yanhao; Lu, Wenlian] Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.
   [Ren, Yanhao; Lu, Wenlian] Fudan Univ, Shanghai Ctr Math Sci, Shanghai, Peoples R China.
   [Wang, Yi; Tang, Qi; Jiang, Haijun] Fantasy Power Shanghai Culture Commun Co Ltd, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Key Lab Math Nonlinear Sci, Minist Educ, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Shanghai Key Lab Contemporary Appl Math, Shanghai, Peoples R China.
RP Ren, YH (corresponding author), Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.; Ren, YH (corresponding author), Fudan Univ, Shanghai Ctr Math Sci, Shanghai, Peoples R China.
EM 18110840015@fudan.edu.cn
FU Shanghai Municipal Science and Technology Major Project [2018SHZDZX01]
FX This work was supported by the Shanghai Municipal Science and Technology
   Major Project under Grant 2018SHZDZX01 and ZJLab.
CR Arijon D., 1976, GRAMMAR FILM LANGUAG
   Assa J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409068
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Chen L, 2018, IEEE INT CON MULTI
   Christianson DB, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P148
   Farhadi A., 2018, COMPUT VISION PATTER, P1804
   Feng Weitao, 2019, ARXIV190106129
   Joubert N., 2016, ARXIV161001691
   Joubert N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818106
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Levinson J., 2013, ROBOT SCI SYST
   Li Bo, 2018, CVPR
   Li-Wei He, 1996, Computer Graphics Proceedings. SIGGRAPH '96, P217
   Lino C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766965
   Liu W., 2016, EUROPEAN C COMPUTER, P21, DOI DOI 10.1007/978-3-319-46448-0_2
   REDMON J, 2016, PROC CVPR IEEE, P779, DOI DOI 10.1109/CVPR.2016.91
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Rhodes C, 1997, PHYS REV E, V56, P2398, DOI 10.1103/PhysRevE.56.2398
   Dinh T, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3786, DOI 10.1109/IROS.2009.5353915
   Wang N., NIPS
   Xie F, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275078
   Yang CG, 2013, IEEE T CYBERNETICS, V43, P24, DOI 10.1109/TSMCB.2012.2198813
NR 23
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2153-0858
BN 978-1-7281-6212-6
J9 IEEE INT C INT ROBOT
PY 2020
BP 8271
EP 8278
DI 10.1109/IROS45743.2020.9341721
PG 8
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BS4YL
UT WOS:000724145802074
DA 2022-02-10
ER

PT J
AU de la Escalera, A
   Armingol, JM
   Pech, JL
   Gomez, JJ
AF de la Escalera, Arturo
   Maria Armingol, Jose
   Luis Pech, Jose
   Julian Gomez, Jose
TI Automatic Detection of a Calibration Pattern for Automatic Camera
   Calibration
SO REVISTA IBEROAMERICANA DE AUTOMATICA E INFORMATICA INDUSTRIAL
LA Spanish
DT Article
DE Error analysis; camera calibration; image distortion; pattern
   recognition; computer vision
AB The number of applications that need to calibrate cameras is increasing. Present methods can calculate camera parameter in a semi-automatic way. Because of that full-automatic methods are being researched in order to save user's time and effort. The algorithm proposed in this article uses a pattern similar to a chessboard. It is automatically found in every image without previous information of the number of files or columns. To do this, a joint analysis of the line set that form the pattern is found through the Hough transform, corners and projective invariants. Some examples and a comparison with other methods are shown. Copyright (C) 2010 CEA.
C1 [de la Escalera, Arturo; Maria Armingol, Jose] Univ Carlos III Madrid, Grp Sistemas Inteligentes, Madrid 28911, Spain.
   [Luis Pech, Jose; Julian Gomez, Jose] Solex Vis Artificial SL, Madrid 28830, Spain.
RP de la Escalera, A (corresponding author), Univ Carlos III Madrid, Grp Sistemas Inteligentes, Ave Univ 30, Madrid 28911, Spain.
EM escalera@ing.uc3m.es; armingol@ing.uc3m.es; joseluis@solexvision.com;
   josejulian@solexvision.com
RI de la Escalera, Arturo/K-1251-2014; Armingol, José Mª/K-6816-2014; de la
   Escalera, Arturo/P-1799-2019
OI de la Escalera, Arturo/0000-0002-2618-857X; Armingol, José
   Mª/0000-0002-3353-9956; de la Escalera, Arturo/0000-0002-2618-857X
CR Ahn SJ, 2001, INT J PATTERN RECOGN, V15, P905, DOI 10.1142/S0218001401001222
   Bouguet J. Y., CAMERA CALIBRATION T
   Bradski G., 2008, LEARNING OPEN CV COM
   DOUSKOS V, 2008, REMOTE SENSING SPATI, V37, P21
   DOUSKOS V, 2007, MEAS TECH, V1, P132
   Fiala M, 2008, MACH VISION APPL, V19, P209, DOI 10.1007/s00138-007-0093-z
   FORBES K, 2002, P 13 ANN S AFRICAN W
   GRAMMATIKOPOULO.L, 2006, REMOTE SENSING SPA 5, P36
   Harris C., 1988, PROC 4 ALVEY VISION, P147, DOI DOI 10.5244/C.2.23
   Matas J, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P877, DOI 10.1109/ICIP.1997.648106
   Ronda JI, 2008, J MATH IMAGING VIS, V32, P193, DOI 10.1007/s10851-008-0095-0
   SHU C, 2003, NRC46497ERB1104 I IN
   TRUCCO E, 1998, INTRODUCTORY TECHNIQ
   TSAI RY, 1987, J ROBOTICS AUTOMATIO, V3, P323
   *VISION, VISION COM VID NUEV
   Wang ZS, 2007, APPL MATH COMPUT, V185, P894, DOI 10.1016/j.amc.2006.05.210
   Yu CS, 2006, OPT ENG, V45, DOI 10.1117/1.2352738
   Zhang ZY, 2004, IEEE T PATTERN ANAL, V26, P892, DOI 10.1109/TPAMI.2004.21
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 19
TC 1
Z9 1
U1 0
U2 3
PU COMITE ESPANOL AUTOMATICA CEA
PI VALENCIA
PA C VERA 14, APDO 22012, VALENCIA, E-46071, SPAIN
SN 1697-7912
J9 REV IBEROAM AUTOM IN
JI Rev. Iberoam. Autom. Inform. Ind.
PD OCT
PY 2010
VL 7
IS 4
BP 83
EP +
DI 10.4995/RIAI.2010.04.11
PG 13
WC Automation & Control Systems; Robotics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems; Robotics
GA 673YE
UT WOS:000283699300009
OA gold, Green Published
DA 2022-02-10
ER

PT C
AU Shabalina, K
   Sagitov, A
   Svinin, M
   Magid, E
AF Shabalina, Ksenia
   Sagitov, Artur
   Svinin, Mikhail
   Magid, Evgeni
BE Ronzhin, A
   Rigoll, G
   Meshcheryakov, R
TI Comparing Fiducial Markers Performance for a Task of a Humanoid Robot
   Self-calibration of Manipulators: A Pilot Experimental Study
SO INTERACTIVE COLLABORATIVE ROBOTICS, ICR 2018
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 3rd International Conference on Interactive Collaborative Robotics (ICR)
CY SEP 18-22, 2018
CL Leipzig, GERMANY
DE ARTag; AprilTag; CALTag; Fiducial marker systems; AR-601M; Humanoid
   robot; Experimental comparison
AB This paper presents our pilot study of experiments automation with a real robot in order to compare performance of different fiducial marker systems, which could be used in automated camera calibration process. We used Russian humanoid robot AR-601M and automated it's manipulators for performing joint rotations. This paper is an extension of our previous work on ARTag, AprilTag and CALTag marker comparison in laboratory settings with large-sized markers that had showed significant superiority of CALTag system over the competitors. This time the markers were scaled down and placed on AR-601M humanoid's palms. We automated experiments of marker rotations, analyzed the results and compared them with the previously obtained results of manual experiments with large-sized markers. The new automated pilot experiments, which were performed both in pure laboratory conditions and pseudo field environments, demonstrated significant differences with previously obtained manual experimental results: AprilTag marker system demonstrated the best performance with a success rate of 97,3% in the pseudo field environment, while ARTag was the most successful in the laboratory conditions.
C1 [Shabalina, Ksenia; Sagitov, Artur; Magid, Evgeni] Kazan Fed Univ, Lab Intelligent Robot Syst, Kazan 420008, Russia.
   [Svinin, Mikhail] Ritsumeikan Univ, Coll Informat Sci & Engn, Robot Dynam & Control Lab, Noji Higashi 1-1-1, Kusatsu 5258577, Japan.
RP Magid, E (corresponding author), Kazan Fed Univ, Lab Intelligent Robot Syst, Kazan 420008, Russia.
EM ks.shabalina@it.kfu.ru; sagitov@it.kfu.ru; svinin@fc.ritsumei.ac.jp;
   magid@it.kfu.ru
RI Magid, Evgeni/B-9697-2014; Shabalina, Ksenia/F-9962-2018
OI Magid, Evgeni/0000-0001-7316-5664; Shabalina,
   Ksenia/0000-0003-4537-9467; Svinin, Mikhail/0000-0003-2459-2250
FU Russian Foundation for Basic Research (RFBR)Russian Foundation for Basic
   Research (RFBR) [18-58-45017]
FX This work was partially supported by the Russian Foundation for Basic
   Research (RFBR) project ID 18-58-45017. Part of the work was performed
   according to the Russian Government Program of Competitive Growth of
   Kazan Federal University.
CR Atcheson B., 2010, P VIS MOD VIS WORKSH, V10, P41
   Degol J, 2017, IEEE I CONF COMP VIS, P1481, DOI 10.1109/ICCV.2017.164
   Fiala M., 2005, 2005 IEEE International Workshop on Haptic Audio Visual Environments and thier Applications (IEEE Cat. No.05EX1164C)
   Fiala M., 2004, NATL RES COUNCIL PUB, V47419, P1
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Higashino S., 2016, ACM SIGGRAPH 2016 PO, P38, DOI [10.1145/2945078.2945116, DOI 10.1145/2945078.2945116]
   Hirzer M., 2008, ICGTR0805
   Kato H., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P85, DOI 10.1109/IWAR.1999.803809
   Krajnik T., 2013, 2013 16 INT C ADV RO, P1, DOI [10.1109/icar.2013.6766520, DOI 10.1109/ICAR.2013.6766520]
   Magid E, 2018, SMART INNOV SYST TEC, V74, P200, DOI 10.1007/978-3-319-59394-4_20
   Sagitov A, 2017, MATEC WEB CONF, V113, DOI 10.1051/matecconf/201711302006
   Sagitov A, 2017, 2017 INTERNATIONAL CONFERENCE ON MECHANICAL, SYSTEM AND CONTROL ENGINEERING (ICMSC), P377, DOI 10.1109/ICMSC.2017.7959505
   Uchiyama H, 2011, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2011.5759433
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 14
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-99582-3; 978-3-319-99581-6
J9 LECT NOTES ARTIF INT
PY 2018
VL 11097
BP 249
EP 258
DI 10.1007/978-3-319-99582-3_26
PG 10
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BS3LD
UT WOS:000712324300026
DA 2022-02-10
ER

PT J
AU Belghith, K
   Kabanza, F
   Bellefeuille, P
   Hartman, L
AF Belghith, Khaled
   Kabanza, Froduald
   Bellefeuille, Philipe
   Hartman, Leo
TI Automated camera planning to film robot operations
SO ARTIFICIAL INTELLIGENCE REVIEW
LA English
DT Article
DE Planning; Camera planning; Path planning; Animation generation; Robotic
   manipulations
AB Automatic 3D animation generation techniques are becoming increasingly popular in different areas related to computer graphics such as video games and animated movies. They help automate the filmmaking process even by non professionals without or with minimal intervention of animators and computer graphics programmers. Based on specified cinematographic principles and filming rules, they plan the sequence of virtual cameras that the best render a 3D scene. In this paper, we present an approach for automatic movie generation using linear temporal logic to express these filming and cinematography rules. We consider the filming of a 3D scene as a sequence of shots satisfying given filming rules, conveying constraints on the desirable configuration (position, orientation, and zoom) of virtual cameras. The selection of camera configurations at different points in time is understood as a camera plan, which is computed using a temporal-logic based planning system (TLPlan) to obtain a 3D movie. The camera planner is used within an automated planning application for generating 3D tasks demonstrations involving a teleoperated robot arm on the the International Space Station (ISS). A typical task demonstration involves moving the robot arm from one configuration to another. The main challenge is to automatically plan the configurations of virtual cameras to film the arm in a manner that conveys the best awareness of the robot trajectory to the user. The robot trajectory is generated using a path-planner. The camera planner is then invoked to find a sequence of configurations of virtual cameras to film the trajectory.
C1 [Belghith, Khaled; Kabanza, Froduald; Bellefeuille, Philipe] Univ Sherbrooke, Sherbrooke, PQ J1K 2R1, Canada.
   [Hartman, Leo] Canadian Space Agcy, John H Chapman Space Ctr, Longueuil, PQ J3Y 8Y9, Canada.
RP Belghith, K (corresponding author), Univ Sherbrooke, 2500 Boul Univ, Sherbrooke, PQ J1K 2R1, Canada.
EM khaled.belghith@usherbrooke.ca; kabanza@usherbrooke.ca;
   philipe.bellefeuille@usherbrooke.ca; leo.hartman@asc-csa.gc.ca
FU Natural Sciences and Engineering Research Council (NSERC) of
   CanadaNatural Sciences and Engineering Research Council of Canada
   (NSERC)
FX The work presented herein was supported by the Natural Sciences and
   Engineering Research Council (NSERC) of Canada.
CR Arijon D., 1976, GRAMMAR FILM LANGUAG
   Bacchus F, 2000, ARTIF INTELL, V116, P123, DOI 10.1016/S0004-3702(99)00071-5
   Bares W. H., 1998, IUI '98. 1998 International Conference on Intelligent User Interfaces, P81
   Bares WH, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P1101
   Belghith K, 2006, IEEE INT CONF ROBOT, P2372, DOI 10.1109/ROBOT.2006.1642057
   Benhamou F., 2004, ACM Transactions on Computational Logic, V5, P732, DOI 10.1145/1024922.1024927
   BLINN J, 1988, IEEE COMPUT GRAPH, V8, P76, DOI 10.1109/38.7751
   Christianson DB, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P148
   Drucker S.M., 1992, S INT 3D GRAPH NEW Y, P67
   Friedman D, 2006, EXPERT SYST APPL, V30, P694, DOI 10.1016/j.eswa.2005.07.027
   Friedman D, 2004, FR ART INT, V110, P256
   Halper N, 2001, J COMPUT GRAPH FORUM, V20
   HALPER N, 2000, P 2000 AAAI SPRING S, P92
   Jardillier F, 1988, J COMPUT GRAPH FORUM, V17, P175
   Jhala Arnav, 2005, P 20 NAT C ART INT A, V5, P307
   Kabanza F., 2008, ICAPS, P164
   Kabanza F, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1729
   Languenou E, 2002, P 8 INT C PRINC PRAC, P618
   Larsen E., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3719, DOI 10.1109/ROBOT.2000.845311
   Lucas C, 1985, DIRECTING FILM TELEV
   Nieuwenhuisen D, 2004, IEEE INT CONF ROBOT, P3870, DOI 10.1109/ROBOT.2004.1308871
   Tomlinon B., 2000, Proceedings of the Fourth International Conference on Autonomous Agents, P317, DOI 10.1145/336595.337513
NR 22
TC 1
Z9 1
U1 2
U2 9
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0269-2821
EI 1573-7462
J9 ARTIF INTELL REV
JI Artif. Intell. Rev.
PD APR
PY 2012
VL 37
IS 4
BP 313
EP 330
DI 10.1007/s10462-011-9233-y
PG 18
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 909DN
UT WOS:000301542600004
DA 2022-02-10
ER

PT J
AU Mikac, KM
   Knipler, ML
   Gracanin, A
   Newbery, MS
AF Mikac, Katarina M.
   Knipler, Monica L.
   Gracanin, Ana
   Newbery, Madeline S.
TI Ground dwelling mammal response to fire: A case study from Monga
   National Park after the 2019/2020 Clyde Mountain fire
SO AUSTRAL ECOLOGY
LA English
DT Article; Early Access
DE camera trapping; fire intensity; ground dwelling mammals; spotted-tailed
   quoll; wildfire
AB Ground dwelling mammal communities are documented six months before and after the Clyde Mountain Wildfire of 2019/2020 in Monga National Park. Across eight sites before fire, approximately 12 ground dwelling mammal species were recorded. Survey effort post-fire increased to 40 sites, spanning three fire severity classes (low, moderate and extreme), revealed 16 ground dwelling mammal species. Species consist of small, medium and large native (one threatened species) and introduced mammals, though consistent with previous findings of ground dwelling mammal diversity in the area. Overall a greater number of species were found in low, compared to moderate and severe fire severity classes. Recovery and detection of mammals occurred in a shorter time period, again, in sites that experienced low, followed by moderate and extreme fire severity.
C1 [Mikac, Katarina M.; Knipler, Monica L.; Gracanin, Ana; Newbery, Madeline S.] Univ Wollongong, Sch Earth Atmospher & Life Sci, Ctr Sustainable Ecosyst Solut, Fac Sci Med & Hlth, Northfields Ave, Wollongong, NSW 2522, Australia.
RP Mikac, KM (corresponding author), Univ Wollongong, Sch Earth Atmospher & Life Sci, Ctr Sustainable Ecosyst Solut, Fac Sci Med & Hlth, Northfields Ave, Wollongong, NSW 2522, Australia.
EM kmikac@uow.edu.au
FU University of Wollongong's Centre for Sustainable Ecosystem Solutions
   and Faculty of Science, Medicine and Health- Bushfire recovery grant;
   Browning Trail Cameras; Crowdfunding via GoFundMe
FX This research was undertaken on Walbanga Country, Yuin Nation. We pay
   our respect to all Walbanga and Yuin Elders and Community past and
   present. We also thank Chris Howard, NPWS; Joslyn and Nick, Friends of
   the Forest; Val Plumwood Trust and Natasha Fiijn; and NSW Forestry
   Corporation. This work was supported by funding from University of
   Wollongong's Centre for Sustainable Ecosystem Solutions and Faculty of
   Science, Medicine and Health- Bushfire recovery grant; Crowdfunding via
   GoFundMe; and Browning Trail Cameras.
CR [Anonymous], 2020, FIRE EXTENT SEVERITY
   Arthur AD, 2012, AUSTRAL ECOL, V37, P958, DOI 10.1111/j.1442-9993.2011.02355.x
   Catling P.C., 1991, P353
   CATLING PC, 1995, WILDLIFE RES, V22, P271, DOI 10.1071/WR9950271
   Catling PC, 2001, WILDLIFE RES, V28, P555, DOI 10.1071/WR00041
   DPIE, 2020, BION VEG CLASS
   Jones SKC, 2019, AUST MAMMAL, V41, P283, DOI 10.1071/AM18028
   NSW Rural Fire Service Department of Planning Industry and Environment (DPIE), 2020, SUPP FIR MAN FIR EXT
NR 8
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1442-9985
EI 1442-9993
J9 AUSTRAL ECOL
JI Austral Ecol.
DI 10.1111/aec.13109
EA SEP 2021
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA UM1XU
UT WOS:000693132300001
DA 2022-02-10
ER

PT J
AU Olson, LO
   Allen, ML
AF Olson, Lucas O.
   Allen, Maximilian L.
TI A Leucisitic Fisher (Pekania pennanti) and the Prevalence of Leucism in
   Wild Carnivores
SO AMERICAN MIDLAND NATURALIST
LA English
DT Article
ID ALBINISM
AB Animal coloration has adaptive roles for communication, concealment, sexual selection, and physiological function. Genetic mutations sometimes cause abnormal coloration such as leucism, in which an animal appears partially or entirely white, except for exposed soft skin tissue. Here we document a leucistic fisher (Pekania pennant:). Fisher fur normally ranges from deep brown to black, but the function of the pelt color is not understood. The literature on the occurrence of leucism includes 33 other records of leucism among carnivores. Reporting cases of rare coloration in the wild helps to understand the distribution, prevalence, and significance of abnormal colors.
C1 [Olson, Lucas O.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Allen, Maximilian L.] Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
RP Olson, LO (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM loolson@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Department of Forest and Wildlife Ecology at UW-Madison; Illinois
   Natural History Survey
FX We thank Lee Ecker for sharing the leucistic fisher photo and
   information on the camera trap placement, as well as Roger Powell and an
   anonymous reviewer for their comments on earlier versions of the
   manuscript. We also thank the Department of Forest and Wildlife Ecology
   at UW-Madison for supporting this research and the Illinois Natural
   History Survey for funding.
CR Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Bensch S, 2000, HEREDITAS, V133, P167, DOI 10.1111/j.1601-5223.2000.t01-1-00167.x
   Camargo I, 2014, WEST N AM NATURALIST, V74, P366, DOI 10.3398/064.074.0301
   Caro T, 2005, BIOSCIENCE, V55, P125, DOI 10.1641/0006-3568(2005)055[0125:TASOCI]2.0.CO;2
   Arriaga-Flores JC, 2016, SOUTHWEST NAT, V61, P63
   COOKE F, 1987, AVIAN GENETICS POPUL
   Ellegren H, 1997, NATURE, V389, P593, DOI 10.1038/39303
   Ferns PN, 2003, ETHOLOGY, V109, P521, DOI 10.1046/j.1439-0310.2003.00894.x
   Goad EH, 2014, BIOL CONSERV, V176, P172, DOI 10.1016/j.biocon.2014.05.016
   Harrington Fred H., 2003, P66
   Hunter JS, 2009, BEHAV ECOL, V20, P1315, DOI 10.1093/beheco/arp144
   JEHL JR, 1985, CONDOR, V87, P439, DOI 10.2307/1367236
   KAYS R, 2017, CANDID CREATURES CAM
   Kirby R, 2018, WILDLIFE BIOL, DOI 10.2981/wlb.00334
   LAWRENCE E., 1989, HENDERSONS DICT BIOL
   Lewis JC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0032726
   Moller AP, 2001, EVOLUTION, V55, P2097
   OWEN M, 1992, IBIS, V134, P22, DOI 10.1111/j.1474-919X.1992.tb07224.x
   Powell R.A., 1993, FISHER LIFE HIST ECO
   Rosel P., 2009, ENCY MARINE MAMMALS
   SAGE BRYAN L., 1962, BRIT BIRDS, V55, P201
   Talamoni S, 2017, BIOTA NEOTROP, V17, DOI [10.1590/1676-0611-BN-2017-0328, 10.1590/1676-0611-bn-2017-0328]
   Wengert GM, 2014, J WILDLIFE MANAGE, V78, P603, DOI 10.1002/jwmg.698
NR 23
TC 3
Z9 3
U1 0
U2 5
PU AMER MIDLAND NATURALIST
PI NOTRE DAME
PA UNIV NOTRE DAME, BOX 369, ROOM 295 GLSC, NOTRE DAME, IN 46556 USA
SN 0003-0031
EI 1938-4238
J9 AM MIDL NAT
JI Am. Midl. Nat.
PD JAN
PY 2019
VL 181
IS 1
BP 133
EP 138
DI 10.1674/0003-0031-181.1.133
PG 6
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA HJ5TS
UT WOS:000457245700013
DA 2022-02-10
ER

PT J
AU Worder, H
AF Worder, H
TI Starting materials, production processes, and measures of quality
   assurance in the manufacture of containers made of pharmaceutical glass
   - An overview on the state of engineering with regard to raw materials,
   manufacturing processes, coating, glass types, new procedures and also
   quality assurance and validation
SO PHARMAZEUTISCHE INDUSTRIE
LA German
DT Article
AB The high resistance of glass to pharmaceutical products is of importance for the selection of appropriate containers. Especially for small-volume parenterals, but also for higer volumes of up to 1000 ml glass is the preferred packaging material.
   The developments of the last years have lead to improvements with regard to size accuracy and tolerance limits, weight of moulded glass items and the optical appearance of the respective glass containers. A significantly higher overall quality level and, thus, a better performance in the mechanical filling process have been achieved by the use of fully automated camera systems for controlling the outer dimensions of glass containers.
   The following paper describes the state of engineering of production, standardisation and quality development considering the differences between moulded and tubular glass. The special modes of operation for the manufacture of both types point at various opportunities for a wide-range use of the two different glass types for the packaging of pharmaceutical products.
C1 HAWE Packing Consulting, Bunde Westf, Germany.
RP Worder, H (corresponding author), Lange Wand 27, D-32557 Bunde Westf, Germany.
EM H.Woerder@t-online.de
CR Auterhoff G, 2003, PHARM IND, V65, P893
   DOEGE T, 2000, FEHLERBEWERTUNGSLIST, V14
   RIMKUS FR, 2000, FEHLERBEWERTUNGSLIST, V19
NR 3
TC 0
Z9 0
U1 1
U2 2
PU ECV-EDITIO CANTOR VERLAG MEDIZIN NATURWISSENSCHAFTEN
PI AULENDORF
PA BANDELSTOCKWEG 20, POSTFACH 1255, D-88322 AULENDORF, GERMANY
SN 0031-711X
J9 PHARM IND
JI Pharm. Ind.
PY 2003
VL 65
IS 9A
BP 923
EP 934
PG 12
WC Pharmacology & Pharmacy
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Pharmacology & Pharmacy
GA 726AG
UT WOS:000185574600006
DA 2022-02-10
ER

PT C
AU Sitara, K
   Mehtre, BM
AF Sitara, K.
   Mehtre, B. M.
BE Thampi, SM
   Bandyopadhyay, S
   Krishnan, S
   Li, KC
   Mosin, S
   Ma, M
TI Real-Time Automatic Camera Sabotage Detection for Surveillance Systems
SO ADVANCES IN SIGNAL PROCESSING AND INTELLIGENT RECOGNITION SYSTEMS
   (SIRS-2015)
SE Advances in Intelligent Systems and Computing
LA English
DT Proceedings Paper
CT 2nd International Symposium on Signal Processing and Intelligent
   Recognition Systems (SIRS)
CY DEC 16-19, 2015
CL Trivandrum, INDIA
SP Indian Inst Informat Technol & Management, ACM, Trivandrum Chapter, Int Neural Networks Soc India, Reg Chapter, TECHNOPARK, IUPRAI
DE Video surveillance; Camera tampering; Camera sabotage; Camera occlusion;
   Real-time video processing; Camera defocusing; Camera displacement
AB Video surveillance is very common for security monitoring of premises and sensitive installations. Criminals tamper the surveillance camera settings so that their (criminal) activities in the scene are not recorded properly, thereby making the captured video frames useless. Various camera tampering/sabotage include - changing the normal view of the camera by turning the camera away from the scene, obstructing the camera lens by placing some objects in front of the camera or spraying paint on it and defocusing the camera lens by changing the camera focus settings, spraying water or some viscous fluid on it. Manual monitoring of the surveillance systems have many limitations - human fatigue, lack of continuous monitoring, etc. Hence, real-time automated analysis and detection of suspicious events have gained importance. In this paper, we propose an efficient algorithm for camera tamper detection based on background modeling, edge details, foreground object size and its movement. In our testing or experimental setup, the results are encouraging with high precision and lowfalse alarm rate. As the proposed method can process 320x240 resolution videos at 60-70 frames/sec, it can be implemented for real-time applications.
C1 [Sitara, K.; Mehtre, B. M.] IDRBT, Hyderabad 500057, Andhra Pradesh, India.
   [Sitara, K.] Univ Hyderabad, Sch Comp Sci & Informat Sci SCIS, Hyderabad 500046, Andhra Pradesh, India.
RP Sitara, K (corresponding author), IDRBT, Hyderabad 500057, Andhra Pradesh, India.
EM ksitara@idrbt.ac.in; bmmehtre@idrbt.ac.in
RI K, Sitara/AAW-2986-2020
OI K, Sitara/0000-0001-8242-8593
CR Aksay A, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P558, DOI 10.1109/AVSS.2007.4425371
   Ellwart D, 2012, LECT NOTES COMPUT SC, V7053, P45, DOI 10.1007/978-3-642-25261-7_4
   Gil-Jimenez P, 2007, LECT NOTES COMPUT SC, V4528, P222
   Hati KK, 2013, IEEE SIGNAL PROC LET, V20, P759, DOI 10.1109/LSP.2013.2263800
   Huang DY, 2014, J VIS COMMUN IMAGE R, V25, P1865, DOI 10.1016/j.jvcir.2014.09.007
   i-Lids, 2007, I LIDS DATASET AVSS
   Kryjak T., 2012, 2012 C DES ARCH SIGN
   Lin D.-T., 2012, 8 INT C INT INF HID
   Ribnick E., P IEEE INT C 2006 VI, P10, DOI [10.1109/ AVSS.2006.94., DOI 10.1109/AVSS.2006.94]
   Saglam A, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P430, DOI 10.1109/AVSS.2009.29
   Tsesmelis T, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P57, DOI 10.1109/AVSS.2013.6636616
   Tung C.-L., 2012, 2012 INT C MACH LEAR, V5
   Yin HP, 2013, CHIN CONT DECIS CONF, P665
   Yuan-Kai Wang, 2011, Proceedings of the 2011 International Conference on Machine Learning and Cybernetics (ICMLC 2011), P1520, DOI 10.1109/ICMLC.2011.6017032
NR 14
TC 2
Z9 2
U1 1
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 2194-5357
EI 2194-5365
BN 978-3-319-28658-7; 978-3-319-28656-3
J9 ADV INTELL SYST
PY 2016
VL 425
BP 75
EP 84
DI 10.1007/978-3-319-28658-7_7
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BF7ZC
UT WOS:000384639900007
DA 2022-02-10
ER

PT J
AU Izo, T
   Grimson, WEL
AF Izo, Tomas
   Grimson, W. Eric L.
TI Simultaneous pose recovery and camera registration from multiple views
   of a walking person
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE post-estimation; exemplar-based motion modelling; automatic camera
   calibration; model-based motion segmentation
AB We present an algorithm to estimate the body pose of a walking person given synchronized video input from multiple uncalibrated cameras. We construct an appearance model of human walking motion by generating examples from the space of body poses and camera locations, and clustering them using expectation-maximization. Given a segmented input video sequence, we find the closest matching appearance cluster for each silhouette and use the sequence of matched clusters to extrapolate the position of the camera with respect to the person's direction of motion. For each frame, the matching cluster also provides an estimate of the walking phase. We combine these estimates from all views and find the most likely sequence of walking poses using a cyclical, feed-forward hidden Markov model. Our algorithm requires no manual initialization and no prior knowledge about the locations of the cameras. (c) 2007 Elsevier B.V. All rights reserved.
C1 MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
RP Izo, T (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM tomas@csail.mit.edu; welg@csail.mit.edu
CR BERGLER C, 1998, P IEEE C COMP VIS PA
   Bilmes J. A., 1998, ICSITR97021
   CHAM T, 1999, P IEEE C COMP VIS PA
   COHEN I, 2001, 5 WORLD MULT SYST CY
   Duda R.O., 2001, PATTERN CLASSIFICATI, P544
   GRAUMAN K, 2003, INT C COMP VIS NIC F
   HOWE N, 1999, NEURAL INFORMATION P
   JOHANSSON G, 1975, SCI AM, V232, P76, DOI 10.1038/scientificamerican0675-76
   LEE L, 2003, INT C COMP VIS NIC F
   MIGDAL J, 2003, THESIS MIT
   MIKIC I, 2001, P IEEE C COMP VIS PA
   MORRIS D, 1998, P IEEE C COMP VIS PA
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   ROSALES R, 2000, P IEEE C COMP VIS PA
   Rosales R, 2000, IEEE WORKSH HUM MOT
   Stauffer C., 1999, P IEEE C COMP VIS PA
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010
   ZHAO T, 2001, P IEEE C COMP VIS PA
NR 18
TC 3
Z9 3
U1 0
U2 2
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR 1
PY 2007
VL 25
IS 3
BP 342
EP 351
DI 10.1016/j.imavis.2005.10.003
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA 125NY
UT WOS:000243450100010
DA 2022-02-10
ER

PT C
AU Hulens, D
   Goedeme, T
   Rumes, T
AF Hulens, Dries
   Goedeme, Toon
   Rumes, Tom
GP IEEE
TI Autonomous lecture recording with a PTZ camera while complying with
   cinematographic rules
SO 2014 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION (CRV)
LA English
DT Proceedings Paper
CT 11th Canadian Conference on Computer and Robot Vision (CRV)
CY MAY 07-09, 2014
CL Montreal, CANADA
SP Canadian Image Proc & Pattern Recognit Soc
DE automatic camera system; cinematographic rules; computer vision;
   tracking; real-time
AB Nowadays, many lectures and presentations are recorded and broadcasted for teleteaching applications. When no human camera crew is present, the most obvious choice is for static cameras. In order to enhance the viewing experience, more advanced systems automatically track and steer the camera towards the lecturer. In this paper we propose an even more advanced system that tracks the lecturer while taking cinematographic rules into account. On top of that, the lecturer can be filmed in different types of shots. Our system is able to detect and track the position of the lecturer, even with non-static backgrounds and in difficult illumination. We developed an action axis determination system, needed to apply cinematographic rules and to steer the Pan-Tilt-Zoom (PTZ) camera towards the lecturer.
C1 [Hulens, Dries; Goedeme, Toon] Katholieke Univ Leuven, EAVISE, St Katelijne Waver, Belgium.
   [Rumes, Tom] Thomas More, PRO media lab, Mechelen, Belgium.
RP Hulens, D (corresponding author), Katholieke Univ Leuven, EAVISE, St Katelijne Waver, Belgium.
EM dries.hulens@kuleuven.be; toon.goedeme@kuleuven.be;
   tom.rumes@thomasmore.be
OI Goedeme, Toon/0000-0002-7477-8961
CR Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Chaumette F, 2006, IEEE ROBOT AUTOM MAG, V13, P82, DOI 10.1109/MRA.2006.250573
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   De Smedt F, 2013, IEEE COMPUT SOC CONF, P622, DOI 10.1109/CVPRW.2013.94
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Geys T., 2004, 5 WORKSH OMN VIS CAM, P17
   Han-Ping Chou, 2010, 2010 International Conference on System Science and Engineering (ICSSE 2010), P167, DOI 10.1109/ICSSE.2010.5551811
   Lampi F, 2008, IEEE MULTIMEDIA, V15, P58, DOI 10.1109/MMUL.2008.43
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Qiong Liu, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P442
   Viola P., 2001, CVPR
   Welch G., 1995, INTRO KALMAN FILTER
   Winkler MB, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P471, DOI 10.1109/ISM.2012.96
NR 13
TC 8
Z9 8
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4799-4338-8
PY 2014
BP 371
EP 377
DI 10.1109/CRV.2014.57
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BB2CN
UT WOS:000341646300051
OA Green Accepted
DA 2022-02-10
ER

PT J
AU Yu, XG
   Jiang, NJ
   Cheong, LF
   Leong, HW
   Yan, X
AF Yu, Xinguo
   Jiang, Nianjuan
   Cheong, Loong-Fah
   Leong, Hon Wai
   Yan, Xin
TI Automatic camera calibration of broadcast tennis video with applications
   to 3D virtual content insertion and ball detection and tracking
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Sports video; Camera calibration; Group-wise data analysis; Hough-like
   search; Virtual content insertion; Ball detection and tracking
ID MODELS
AB This paper presents an original algorithm to automatically acquire accurate camera calibration from broadcast tennis video (BTV) as well as demonstrates two of its many applications. Accurate camera calibration from BTV is challenging because the frame-data of BTV is often heavily distorted and full of errors, resulting in wildly fluctuating camera parameters. To meet this challenge, we propose a frame grouping technique, which is based on the observation that many frames in BTV possess the same camera viewpoint. Leveraging on this fact, our algorithm groups frames according to the camera viewpoints. We then perform a group-wise data analysis to obtain a more stable estimate of the camera parameters. Recognizing the fact that some of these parameters do vary somewhat even if they have similar camera viewpoint, we further employ a Hough-like search to tune such parameters, minimizing the reprojection disparity. This two-tiered process gains stability in the estimates of the camera parameters, and yet ensures good match between the model and the reprojected camera view via the tuning step. To demonstrate the utility of such stable calibration, we apply the camera matrix acquired to two applications: (a) 3D virtual content insertion: and (b) tennis-ball detection and tracking. The experimental results show that our algorithm is able to acquire accurate camera matrix and the two applications have very good performances. (C) 2008 Elsevier Inc. All rights reserved.
C1 [Yu, Xinguo; Yan, Xin] Inst Infocomm Res, Singapore 119613, Singapore.
   [Jiang, Nianjuan; Cheong, Loong-Fah] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117543, Singapore.
   [Leong, Hon Wai] Natl Univ Singapore, Sch Comp, Singapore 117590, Singapore.
RP Yu, XG (corresponding author), Inst Infocomm Res, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
EM xinguo@i2r.a-star.edu.sg; go700213@nus.edu.sg; eleclf@nus.edu.sg;
   leonghw@comp.nus.edu.sg; yanxin@i2r.a-star.edu.sg
CR AHMED MT, 1999, IEEE P 7 IEEE INT C, V1, P463
   Batista J, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P709, DOI 10.1109/ICCV.1998.710795
   Farin D, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P482, DOI 10.1109/ICME.2005.1521465
   Fusiello A, 2004, IEEE T PATTERN ANAL, V26, P1633, DOI 10.1109/TPAMI.2004.125
   Gibson S, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P37, DOI 10.1109/ISMAR.2002.1115068
   GONZALEZ JI, CI WORKSH AG FUS SPA
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hemayed EE, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P351, DOI 10.1109/AVSS.2003.1217942
   ILLINGWORTH J, 1987, IEEE T PATTERN ANAL, V9, P690, DOI 10.1109/TPAMI.1987.4767964
   INAMOTO N, 2004, ACM SIGCHI, P42
   Ji Q, 2001, IEEE T SYST MAN CY A, V31, P120, DOI 10.1109/3468.911369
   Kim H, 2001, PATTERN ANAL APPL, V4, P9, DOI 10.1007/s100440170020
   LAI JZC, 1993, IMAGE VISION COMPUT, V11, P656
   LI Q, 2004, T ENG COMPUT TECHNOL, V1, P482
   Lourakis MIA, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P569, DOI 10.1109/CGI.2004.1309266
   LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043
   REID I, 1996, EUR C COMP VIS, P647
   Robert L, 1996, COMPUT VIS IMAGE UND, V63, P314, DOI 10.1006/cviu.1996.0021
   SHIMAWAKI T, 2006, ECCV WORKSH COMP VIS, P26
   Sturm P.F., 1999, P 1999 IEEE COMP SOC, V1, P432, DOI DOI 10.1109/CVPR.1999.786974
   Sudhir G, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P81, DOI 10.1109/CAIVD.1998.646036
   Tian TY, 1997, IEEE T PATTERN ANAL, V19, P1178, DOI 10.1109/34.625131
   TRIGGS B, 2000, LECT NOTES COMPUTER, V1883, P298, DOI DOI 10.1007/3-540-44480-7
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   WENG JY, 1992, IEEE T PATTERN ANAL, V14, P965, DOI 10.1109/34.159901
   XU M, 2003, ICME, V11, P281
   YU X, 2006, ACM MM, P619
   YU X, 2007, ACCURATE STABLE CAME, V111, P93
   Yu XG, 2004, IEEE IMAGE PROC, P1049
   Yu XG, 2006, IEEE T MULTIMEDIA, V8, P1164, DOI 10.1109/TMM.2006.884621
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 31
TC 24
Z9 25
U1 0
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD MAY
PY 2009
VL 113
IS 5
BP 643
EP 652
DI 10.1016/j.cviu.2008.01.006
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA 433SX
UT WOS:000265227200007
DA 2022-02-10
ER

PT C
AU Burelli, P
   Preuss, M
AF Burelli, Paolo
   Preuss, Mike
BE EsparciaAlcazar, AI
TI Automatic Camera Control: A Dynamic Multi-Objective Perspective
SO APPLICATIONS OF EVOLUTIONARY COMPUTATION
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 17th European Conference on Applications of Evolutionary Computation
   (EvpApplications)
CY APR 23-25, 2014
CL Granada, SPAIN
SP Univ Granada, Free Software Off, Granada Excellence Network Innovat Lab, Edinburgh Napier Univ, Inst Informat & Digital Innovat, World Federat Soft Comp
ID SELECTION
AB Automatically generating computer animations is a challenging and complex problem with applications in games and film production. In this paper, we investigate how to translate a shot list for a virtual scene into a series of virtual camera configurations - i.e automatically controlling the virtual camera. We approach this problem by modelling it as a dynamic multi-objective optimisation problem and show how this metaphor allows a much richer expressiveness than a classical single objective approach. Finally, we showcase the application of a multi-objective evolutionary algorithm to generate a shot for a sample game replay and we analyse the results.
C1 [Burelli, Paolo] Aalborg Univ, Dept Architecture Design & Media Technol, Copenhagen, Denmark.
   [Preuss, Mike] Univ Munster, European Res Ctr Informat Syst, D-48149 Munster, Germany.
RP Preuss, M (corresponding author), Univ Munster, European Res Ctr Informat Syst, D-48149 Munster, Germany.
EM pabu@create.aau.dk; mike.preuss@uni-muenster.de
RI Burelli, Paolo/AAM-5327-2020
OI Burelli, Paolo/0000-0003-2804-9028
CR Bares W., 2000, Proceedings ACM Multimedia 2000, P177, DOI 10.1145/354384.354463
   Beume N, 2007, EUR J OPER RES, V181, P1653, DOI 10.1016/j.ejor.2006.08.008
   Beume N, 2009, LECT NOTES COMPUT SC, V5467, P21, DOI 10.1007/978-3-642-01020-0_7
   Bourne O, 2008, CONSTRAINTS, V13, P180, DOI 10.1007/s10601-007-9026-8
   Burelli P., 2012, THESIS IT U COPENHAG
   Burelli P, 2009, AAAI C ART INT INT D
   Burelli P, 2008, LECT NOTES COMPUT SC, V5166, P130, DOI 10.1007/978-3-540-85412-8_12
   Cheong Y.G., 2008, AAAI C ART INT INT D, P167
   Christie M, 2008, COMPUT GRAPH FORUM, V27, P2197, DOI 10.1111/j.1467-8659.2008.01181.x
   Deb R., 2002, IEEE T EVOLUTIONARY, V6
   Dominguez M, 2011, AAAI C ART INT INT D
   DRUCKER SM, 1994, GRAPH INTER, P190
   Jardillier F, 1998, COMPUT GRAPH FORUM, V17, pC175
   Lowood, 2008, J MEDIA PRACTICE, V7, P25, DOI DOI 10.1386/JMPR.7.1.25/1
   Olivier P., 1999, ART INT SIM BEH
   PHILLIPS CB, 1992, P 1992 S INT 3D GRAP, P71
   Pickering J, 2002, THESIS U YORK
   PONTRIAGIN LS, 1962, MATH THEORY OPTIMAL
   SARKER R, 2002, INT SER OPER RES MAN, V48, P177
   Togelius J, 2013, GENET PROGRAM EVOL M, V14, P245, DOI 10.1007/s10710-012-9174-5
   Van Veldhuizen David A., 1998, LAT BREAK PAP GEN PR
   Ware C., 1990, Computer Graphics, V24, P175
NR 22
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-662-45523-4; 978-3-662-45522-7
J9 LECT NOTES COMPUT SC
PY 2014
VL 8602
BP 361
EP 373
DI 10.1007/978-3-662-45523-4_30
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BC7HA
UT WOS:000354874300030
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Ji, JJ
   Krishnan, S
   Patel, V
   Fer, D
   Goldberg, K
AF Ji, Jessica J.
   Krishnan, Sanjay
   Patel, Vatsal
   Fer, Danyal
   Goldberg, Ken
BE Reveliotis, S
   Cappelleri, D
   Dimarogonas, DV
   Dotoli, M
   Fanti, MP
   Li, J
   Lucena, V
   Seatu, C
   Xie, X
   Zhu, K
TI Learning 2D Surgical Camera Motion From Demonstrations
SO 2018 IEEE 14TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND
   ENGINEERING (CASE)
SE IEEE International Conference on Automation Science and Engineering
LA English
DT Proceedings Paper
CT 14th IEEE International Conference on Automation Science and Engineering
   (IEEE CASE)
CY AUG 20-24, 2018
CL Tech Univ Munich, Munich, GERMANY
SP IEEE
HO Tech Univ Munich
DE Surgical Robotics; Active Perception; Viewpoint Selection
AB Automating camera movement during robotassisted surgery has the potential to reduce burden on surgeons and remove the need to manually move the camera. An important sub-problem is automatic viewpoint selection, proposing camera poses that focus on important anatomical features. We use the 6 DoF Stewart Platform Research Kit (SPRK) to move the environment with a fixed endoscope, as a dual to moving the endoscope itself, to study camera motion in surgical robotics. To provide demonstrations, we link the platform's control directly to the da Vinci Research Kit (dVRK) master control system and allow control of the platform using the same pedals and tools as a clinical movable endoscope. We propose a probabilistic model that identifies image features that "dwell" close to the camera's focal point in expert demonstrations. Our experiments consider a surgical debridement scenario on silicone phantoms with inclusions of varying color and shape. We evaluate the extent to which the system correctly segments candidate debridement targets (box accuracy) and correctly ranks those targets (rank accuracy). For debridement of a single uniquely colored inclusion, the box accuracy is 80% and the rank accuracy is 100% after 100 training data points. For debridement of multiple inclusions of the same color, the box accuracy is 70.8% and the rank accuracy is 100% after 100 training data points. For debridement of inclusions of a particular shape, the box accuracy is 70.5% and the rank accuracy is 90% after 100 training data points. A demonstration video is available at: https://vimeo.com/260362958
C1 [Ji, Jessica J.; Krishnan, Sanjay; Patel, Vatsal; Goldberg, Ken] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Fer, Danyal] UC San Francisco East Bay, Oakland, CA USA.
RP Ji, JJ (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
FU NSF National Robotics Initiative Award [1734633]; Intuitive Surgical;
   National Science Foundation, via the National Robotics Initiative (NRI)
   [IIS 1637789, IIS 1637759, IIS 1637444]
FX This research was performed at the AUTOLab at UC Berkeley in affiliation
   with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent
   Secure Execution (RISE) Lab, the CITRIS "People and Robots" (CPAR)
   Initiative, by the Scalable Collaborative Human-Robot Learning (SCHooL)
   Project, NSF National Robotics Initiative Award 1734633, and in
   affiliation with UC Berkeley's Center for Automation and Learning for
   Medical Robotics (Cal-MR). The authors were supported in part by
   donations from Siemens, Google, Honda, Intel, Comcast, Cisco, Autodesk,
   Amazon Robotics, Toyota Research Institute, ABB, Samsung, Knapp,
   Loccioni, and by a major equipment grant from Intuitive Surgical and by
   generous donations from Andy Chou and Susan and Deepak Lim. The da Vinci
   Research Kit is supported by the National Science Foundation, via the
   National Robotics Initiative (NRI), as part of the collaborative
   research project "Software Framework for Research in Semi-Autonomous
   Teleoperation" between The Johns Hopkins University (IIS 1637789),
   Worcester Polytechnic Institute (IIS 1637759), and the University of
   Washington (IIS 1637444). Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the author(s)
   and do not necessarily reflect the views of the sponsors. We thank our
   colleagues who provided helpful feedback and suggestions, in particular
   Brijen Thananjeyan, Carolyn Chen, Jeff Mahler, Daniel Seita, Matthew
   Matl.
CR Ali  S., 2008, STUDIES HLTH TECHNOL, V132
   Allaf  M., 1998, SURG ENDOSCOPY, V12
   Arai  F., 2000, INT C ROB AUT, V1
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Canny J., 1987, READINGS COMPUTER VI
   Casals  A., 1996, ROB AUT ICRA 1996 IE
   Chen SY, 2011, INT J ROBOT RES, V30, P1343, DOI 10.1177/0278364911410755
   Chen  X., 2000, 22 STANF U COMP GRAP, V2
   Chitwood Jr W. R., 2001, ANN SURG, V234
   Christie M., 2008, COMPUTER GRAPHICS FO, V27, P8
   Deinzer  F., 2003, INT C COMP AN IM PAT
   Eslamian S., AUTONOMOUS CAMERA SY
   Gilbert  J., 2009, ANN ROYAL COLL SURGE, V91
   He L. W., 1996, C COMP GRAPH INT TEC
   Kavoussi L. R., 1995, J UROLOGY, V154
   Kazanzides  P., 2014, OPEN SOURCE RES KIT
   Leifman  G., 2012, C COMP VIS PATT REC
   Liang  J., 2017, C AUT SCI ENG
   Mahler  J., 2014, C AUT SCI ENG
   Motai  Y., 2008, IEEE T IND ELECT, V55
   Muhler  K., 2007, EUROVIS
   Mylonas G. P., 2006, COMPUTER AIDED SURG, V11
   Pandya A, 2014, ROBOTICS, V3, DOI 10.3390/robotics3030310
   Partin A. W., 1995, J AM COLL SURG, V181
   Patel  V., 2018, INT S MED ROB ISMR
   Ren Shaoqing, 2015, ADV NEURAL INFORM PR
   Roch P. J., 2018, SURG ENDOSCOPY, V32
   Sakane  S., 1991, ROB AUT ICRA 1991 IE
   Seita  D., 2018, INT C ROB AUT
   Tong I, 2015, IEEE INT C INT ROBOT, P2043, DOI 10.1109/IROS.2015.7353648
   Triggs  B., 1995, ROB AUT ICRA 1995 IE, V2
   Vazquez P. -P., 2001, VMV, V1
   Ware  C., 1990, SIGGRAPH COMPUTER GR, V24
   Weede  O., 2011, ROB AUT ICRA 2011 IE
   Wilson  M., 2010, SURG ENDOSCOPY, V24
NR 36
TC 7
Z9 7
U1 1
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2161-8070
BN 978-1-5386-3593-3
J9 IEEE INT CON AUTO SC
PY 2018
BP 35
EP 42
PG 8
WC Automation & Control Systems; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Engineering
GA BM1TD
UT WOS:000460536600006
DA 2022-02-10
ER

PT J
AU Begin, PN
   Tanabe, Y
   Kumagai, M
   Culley, AI
   Paquette, M
   Sarrazin, D
   Uchida, M
   Vincent, WF
AF Begin, Paschale N.
   Tanabe, Yukiko
   Kumagai, Michio
   Culley, Alexander, I
   Paquette, Michel
   Sarrazin, Denis
   Uchida, Masaki
   Vincent, Warwick F.
TI Extreme warming and regime shift toward amplified variability in a far
   northern lake
SO LIMNOLOGY AND OCEANOGRAPHY
LA English
DT Article
ID SEA-ICE; ECOSYSTEMS; HEMISPHERE; RESPONSES; ISLAND; SNOW
AB Mean annual air temperatures in the High Arctic are rising rapidly, with extreme warming events becoming increasingly common. Little is known, however, about the consequences of such events on the ice-capped lakes that occur abundantly across this region. Here, we compared 2 years of high-frequency monitoring data in Ward Hunt Lake in the Canadian High Arctic. One of the years included a period of anomalously warm conditions that allowed us to address the question of how loss of multi-year ice cover affects the limnological properties of polar lakes. A mooring installed at the deepest point of the lake (9.7 m) recorded temperature, oxygen, chlorophylla(Chla) fluorescence, and underwater irradiance from July 2016 to July 2018, and an automated camera documented changes in ice cover. The complete loss of ice cover in summer 2016 resulted in full wind exposure and complete mixing of the water column. This mixing caused ventilation of lake water heat to the atmosphere and 4 degrees C lower water temperatures than under ice-covered conditions. There were also high values of Chlafluorescence, elevated turbidity levels and large oxygen fluctuations throughout fall and winter. During the subsequent summer, the lake retained its ice cover and the water column remained stratified, with lower Chlafluorescence and anoxic bottom waters. Extreme warming events are likely to shift polar lakes that were formerly capped by continuous thick ice to a regime of irregular ice loss and unstable limnological conditions that vary greatly from year to year.
C1 [Begin, Paschale N.; Culley, Alexander, I; Paquette, Michel; Sarrazin, Denis; Vincent, Warwick F.] CEN, Quebec City, PQ, Canada.
   [Begin, Paschale N.; Culley, Alexander, I; Paquette, Michel; Sarrazin, Denis; Vincent, Warwick F.] Takuvik Joint Int Lab, Quebec City, PQ, Canada.
   [Begin, Paschale N.; Vincent, Warwick F.] Univ Laval, Dept Biol, Quebec City, PQ, Canada.
   [Tanabe, Yukiko; Uchida, Masaki] Natl Inst Polar Res, Tachikawa, Tokyo, Japan.
   [Tanabe, Yukiko; Uchida, Masaki] Grad Univ Adv Studies, SOKENDAI, Hayama, Kanagawa, Japan.
   [Kumagai, Michio] Ritsumeikan Univ, Kyoto, Japan.
   [Culley, Alexander, I] Univ Laval, Dept Biochim Microbiol & Bioinformat, Quebec City, PQ, Canada.
   [Paquette, Michel] Queens Univ, Dept Geog & Planning, Kingston, ON, Canada.
RP Begin, PN; Vincent, WF (corresponding author), CEN, Quebec City, PQ, Canada.; Begin, PN; Vincent, WF (corresponding author), Takuvik Joint Int Lab, Quebec City, PQ, Canada.; Begin, PN; Vincent, WF (corresponding author), Univ Laval, Dept Biol, Quebec City, PQ, Canada.
EM pnbegin@gmail.com; warwick.vincent@bio.ulaval.ca
FU Ministry of Education, Culture, Sports, Science and Technology,
   JapanMinistry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT); NEIGE (Northern Ellesmere Island in the Global
   Environment) - Sentinel North (Canada First Research Excellence Fund);
   ArcticNet (Network of Centres of Excellence, Canada); Centre d'etudes
   nordiques (CEN); Fonds de Recherche du Quebec Nature et Technologies
   (FRQNT); Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC); Northern Scientific Training Program (NSTP)
FX This research is a contribution to the projects ArCS (Arctic Challenge
   for Sustainability) supported by the Ministry of Education, Culture,
   Sports, Science and Technology, Japan, and NEIGE (Northern Ellesmere
   Island in the Global Environment) supported by Sentinel North (Canada
   First Research Excellence Fund), ArcticNet (Network of Centres of
   Excellence, Canada), Centre d'etudes nordiques (CEN), Fonds de Recherche
   du Quebec Nature et Technologies (FRQNT), the Natural Sciences and
   Engineering Research Council of Canada (NSERC) and the Northern
   Scientific Training Program (NSTP), with logistic support from the Polar
   Continental Shelf Program (PCSP) and Parks Canada. This work is also a
   contribution to the International Arctic Science Committee (IASC)
   project T-MOSAiC (Terrestrial Multidisciplinary distributed
   Observatories for the Study of Arctic Connections). The authors thank
   Myriam Labbe, Jerome Comte, and Nicolas Bochaton for their help with
   fieldwork, Raoul-Marie Couture for advice on oxygen analysis, and two
   anonymous reviewers for their insightful comments and suggestions that
   improved the manuscript.
CR Babb DG, 2019, J GEOPHYS RES-OCEANS, V124, P6575, DOI 10.1029/2019JC015053
   Belzile C, 2001, CAN J FISH AQUAT SCI, V58, P2405, DOI 10.1139/cjfas-58-12-2405
   Bieniek PA, 2017, INT J CLIMATOL, V37, P208, DOI 10.1002/joc.4994
   Bonilla S, 2005, J PHYCOL, V41, P1120, DOI 10.1111/j.1529-8817.2005.00154.x
   CEN, 2020, NORDICANA, pD1, DOI [10.5885/44985SL-8F203FD3ACCD4138, DOI 10.5885/44985SL-8F203FD3ACCD4138]
   Charvet S, 2012, POLAR BIOL, V35, P733, DOI 10.1007/s00300-011-1118-7
   Cortes A, 2020, LIMNOL OCEANOGR, V65, P260, DOI 10.1002/lno.11296
   CRAIG H, 1992, SCIENCE, V255, P318, DOI 10.1126/science.11539819
   Deshpande BN, 2015, LIMNOL OCEANOGR, V60, P1656, DOI 10.1002/lno.10126
   Doran PT, 1996, LIMNOL OCEANOGR, V41, P839, DOI 10.4319/lo.1996.41.5.0839
   Du JY, 2017, CRYOSPHERE, V11, P47, DOI 10.5194/tc-11-47-2017
   Fountain AG, 2016, BIOSCIENCE, V66, P848, DOI 10.1093/biosci/biw110
   GOSSELIN M, 1985, CAN J FISH AQUAT SCI, V42, P999, DOI 10.1139/f85-125
   Hampton SE, 2017, ECOL LETT, V20, P98, DOI 10.1111/ele.12699
   Huot Y, 2010, DEVEL APPL PHYCOL, V4, P31, DOI 10.1007/978-90-481-9268-7_3
   KALFF J, 1974, J FISH RES BOARD CAN, V31, P621, DOI 10.1139/f74-094
   Kalff J., 2003, LIMNOLOGY INLAND WAT
   Kirillin G, 2012, AQUAT SCI, V74, P659, DOI 10.1007/s00027-012-0279-y
   Landy JC, 2015, GEOPHYS RES LETT, V42, P10714, DOI 10.1002/2015GL066712
   Lehnherr I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03685-z
   Ludlam SD, 1996, J PALEOLIMNOL, V16, P111
   Macias-Fauria M, 2018, BIOL LETTERS, V14, DOI 10.1098/rsbl.2017.0702
   Mackowiak CL, 1997, ACTA HORTIC, P19
   Matveev A, 2019, J GEOPHYS RES-BIOGEO, V124, P3521, DOI 10.1029/2019JG005078
   McGrath D, 2013, GEOPHYS RES LETT, V40, P2091, DOI 10.1002/grl.50456
   Meredith M., 2019, IPCC SPECIAL REPORT
   Mioduszewski JR, 2019, CRYOSPHERE, V13, P113, DOI 10.5194/tc-13-113-2019
   Mohit V, 2017, NPJ BIOFILMS MICROBI, V3, DOI 10.1038/s41522-017-0024-3
   Moore CM, 2006, LIMNOL OCEANOGR, V51, P936, DOI 10.4319/lo.2006.51.2.0936
   Mudryk LR, 2018, CRYOSPHERE, V12, P1157, DOI 10.5194/tc-12-1157-2018
   NEIGE, 2020, NORDICANA D, pD74, DOI [10.5885/45648CE-1A9AB63DFF91440B, DOI 10.5885/45648CE-1A9AB63DFF91440B]
   Obryk MK, 2019, J GEOPHYS RES-EARTH, V124, P686, DOI 10.1029/2018JF004756
   Overland J, 2019, POLAR SCI, V21, P6, DOI 10.1016/j.polar.2018.11.008
   Paquette M, 2017, ARCT SCI, V3, P334, DOI 10.1139/as-2016-0014
   Paquette M, 2015, GEOPHYS RES LETT, V42, P1433, DOI 10.1002/2014GL062960
   Paterson AM, 2008, CAN J FISH AQUAT SCI, V65, P846, DOI 10.1139/F08-022
   Pernica P, 2017, INLAND WATERS, V7, P138, DOI 10.1080/20442041.2017.1296627
   SCHINDLE.DW, 1974, J FISH RES BOARD CAN, V31, P585, DOI 10.1139/f74-092
   Schmidt NM, 2019, PLOS BIOL, V17, DOI 10.1371/journal.pbio.3000392
   Schuur EAG, 2015, NATURE, V520, P171, DOI 10.1038/nature14338
   Serreze MC, 2019, ANN NY ACAD SCI, V1436, P36, DOI 10.1111/nyas.13856
   Sharma S, 2019, NAT CLIM CHANGE, V9, P227, DOI 10.1038/s41558-018-0393-5
   Staehr PA, 2010, LIMNOL OCEANOGR-METH, V8, P628, DOI 10.4319/lom.2010.8.0628
   Tanabe Y, 2008, POLAR BIOL, V31, P199, DOI 10.1007/s00300-007-0347-2
   Vincent AC, 2008, J GEOPHYS RES-OCEANS, V113, DOI 10.1029/2007JC004360
   Vincent WF, 2020, PALGRAVE HANDBOOK OF ARCTIC POLICY AND POLITICS, P507, DOI 10.1007/978-3-030-20557-7_31
   Vincent WF, 2011, ECOSCIENCE, V18, P236, DOI 10.2980/18-3-3448
   Vopel K, 2006, LIMNOL OCEANOGR, V51, P1801, DOI 10.4319/lo.2006.51.4.1801
   WHARTON RA, 1986, LIMNOL OCEANOGR, V31, P437, DOI 10.4319/lo.1986.31.2.0437
   Wrona FJ, 2016, J GEOPHYS RES-BIOGEO, V121, P650, DOI 10.1002/2015JG003133
NR 50
TC 7
Z9 8
U1 3
U2 23
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0024-3590
EI 1939-5590
J9 LIMNOL OCEANOGR
JI Limnol. Oceanogr.
PD JAN
PY 2021
VL 66
SU 1
SI SI
BP S17
EP S29
DI 10.1002/lno.11546
EA JUL 2020
PG 13
WC Limnology; Oceanography
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Marine & Freshwater Biology; Oceanography
GA QN5FR
UT WOS:000551563700001
OA hybrid, Green Accepted
DA 2022-02-10
ER

PT J
AU Avrin, AC
   Pekins, CE
   Sperry, JH
   Wolff, PJ
   Allen, ML
AF Avrin, Alexandra C.
   Pekins, Charles E.
   Sperry, Jinelle H.
   Wolff, Patrick J.
   Allen, Maximilian L.
TI Efficacy of attractants for detecting eastern spotted skunks: an
   experimental approach
SO WILDLIFE BIOLOGY
LA English
DT Article
DE bait; camera trap; carrion; detection; experimental framework; fatty
   acid tablet; large animal carcass; lure; sardines; Spilogale putorius
ID WINTER HABITAT ASSOCIATIONS; SPILOGALE-PUTORIUS; ACTIVITY PATTERNS; SITE
   SELECTION; CARNIVORES; CAPTURE; MOUNTAINS; ARKANSAS; RATES
AB Estimates of abundance and occupancy are essential for wildlife management, particularly for species of conservation concern such as eastern spotted skunks Spilogale putorius. Most studies of eastern spotted skunks rely on limited evidence for best monitoring practices, and while many studies use attractants to increase detections, previous studies have not tested attractants against a control of no attractant to determine their effectiveness. We tested two common attractants (sardines and fatty acid tablets) and one uncommon attractant (wild boar carcasses) against a control of no attractant to determine if any attractant increased detections of eastern spotted skunks or changed their temporal activity. Based on our model, sardines and wild boar carcasses improved detections by three and eight times that of the control, respectively. Further, for every 100 trap nights, we detected eastern spotted skunks 10.67 times with wild boar carcasses, 1.02 times with sardines, 0.53 times with fatty acid tablets and 0.44 times with no attractant. Wild boar carcasses also substantially decreased latency to detection, with skunks detected two times faster than at other attractants and almost three times faster than at the control. Eastern spotted skunks were most active in the early morning before sunrise, and their temporal activity did not vary significantly by attractant. This study is the first to use an experimental framework to test attractants for eastern spotted skunks, and our results showed that choice of attractant matters. Large animal carcasses, although rarely used, may be most effective for detecting eastern spotted skunks, while fatty acid tablets were no different than the control, and we recommend against their use in future studies. Monitoring plans should incorporate our results as increasing detections is essential to understanding the abundance, range and demographics of eastern spotted skunks.
C1 [Avrin, Alexandra C.; Sperry, Jinelle H.; Wolff, Patrick J.; Allen, Maximilian L.] Univ Illinois, Dept Nat Resources & Environm Sci, Urbana, IL 61801 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
   [Pekins, Charles E.] Us Army Garrison, Ft Hood Nat Resources Management Branch, Ft Hood, TX USA.
   [Sperry, Jinelle H.; Wolff, Patrick J.] US Army Corps Engineers, Construct Engn Res Lab, Champaign, IL USA.
RP Avrin, AC (corresponding author), Univ Illinois, Dept Nat Resources & Environm Sci, Urbana, IL 61801 USA.
EM aavrin@illinois.edu
FU Fort Hood Natural Resources Management Branch; US Army Engineer Research
   and Development CenterUnited States Department of Defense; Illinois
   Natural History Survey
FX We thank Jess Daley for help with data collection, Michael Ward and
   Kevin Cagle for their support, and Summer LaRose for reviews that
   substantially improved our paper. We thank the Fort Hood Natural
   Resources Management Branch, US Army Engineer Research and Development
   Center and the Illinois Natural History Survey for logistical support
   and funding.
CR Allen ML, 2020, BIODIVERS CONSERV, V29, P3591, DOI 10.1007/s10531-020-02039-w
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Allen Maximilian L., 2013, Canadian Field-Naturalist, V127, P64
   BAILEY TN, 1971, AM MIDL NAT, V85, P196, DOI 10.2307/2423922
   Benson IW, 2019, SOUTHEAST NAT, V18, P165, DOI 10.1656/058.018.0111
   Boulerice JT, 2017, AM MIDL NAT, V178, P17, DOI 10.1674/0003-0031-178.1.17
   Briffa M, 2007, FUNCT ECOL, V21, P627, DOI 10.1111/j.1365-2435.2006.01188.x
   Diggins CA, 2015, NORTHEAST NAT, V22, pN6, DOI 10.1656/045.022.0211
   Dytham C., 2011, CHOOSING USING STAT, V3rd ed.
   Eckrich GH, 1999, STUD AVIAN BIOL, P267
   Eng RYY, 2019, J MAMMAL, V100, P1295, DOI 10.1093/jmammal/gyz075
   Eng RYY, 2019, J WILDLIFE MANAGE, V83, P1244, DOI 10.1002/jwmg.21684
   ESRI, 2011, ARCGIS DESKT REL 10
   ESSCSG (Eastern Spotted Skunk Cooperative Study Group), 2020, E SPOTT SKUNK CONS P
   Gerber BD, 2012, POPUL ECOL, V54, P43, DOI 10.1007/s10144-011-0276-3
   Gompper M., 2016, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474.en, DOI 10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474, 10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474, DOI 10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474.EN]
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Gompper ME, 2005, ANIM CONSERV, V8, P195, DOI 10.1017/S1367943005001964
   Hackett HM, 2007, AM MIDL NAT, V158, P123, DOI 10.1674/0003-0031(2007)158[123:DROESS]2.0.CO;2
   Harris SN, 2020, J WILDLIFE MANAGE, V84, P127, DOI 10.1002/jwmg.21780
   Hayden TJ, 2000, ECOLOGY AND MANAGEMENT OF COWBIRDS AND THEIR HOSTS, P357
   Heinlein BW, 2020, WILDLIFE RES, V47, P338, DOI 10.1071/WR19117
   Higdon SD, 2020, SOUTHEAST NAT, V19, P74, DOI 10.1656/058.019.0110
   Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
   Kinlaw Al, 1995, Mammalian Species, V511, P1
   Lesmeister DB, 2013, RESTOR ECOL, V21, P267, DOI 10.1111/j.1526-100X.2012.00880.x
   Lesmeister DB, 2009, J WILDLIFE MANAGE, V73, P18, DOI 10.2193/2007-447
   Lombardi JV, 2017, NAT AREA J, V37, P506
   Meredith Mike, 2020, CRAN
   Millspaugh, 2005, WILDLIFE DEMOGRAPHY
   Perry RW, 2018, SOUTHEAST NAT, V17, P298, DOI 10.1656/058.017.0213
   Rasambainarivo F, 2017, ECOHEALTH, V14, P691, DOI 10.1007/s10393-017-1280-7
   Reed AW, 2000, AM MIDL NAT, V144, P133, DOI 10.1674/0003-0031(2000)144[0133:CSOTES]2.0.CO;2
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rocha DG, 2016, J ZOOL, V300, P205, DOI 10.1111/jzo.12372
   Rowcliffe, 2021, PACKAGE ACTIVITY ANI
   Russel, 2020, EMMEANS ESTIMATED MA
   Schlexer Fredrick V., 2008, P263
   Sebastian-Gonzalez E, 2020, ECOGRAPHY, V43, P1143, DOI 10.1111/ecog.05083
   Sprayberry TR, 2018, J MAMMAL, V99, P242, DOI 10.1093/jmammal/gyx168
   Sprayberry TR, 2016, SOUTHEAST NAT, V15, pN53, DOI 10.1656/058.015.0417
   Suarez-Tangil BD, 2017, EUR J WILDLIFE RES, V63, DOI 10.1007/s10344-017-1150-1
   Thorne ED, 2017, J WILDLIFE MANAGE, V81, P1042, DOI 10.1002/jwmg.21282
   Thorne ED, 2017, NORTHEAST NAT, V24, pN1, DOI 10.1656/045.024.0108
   Thornton DH, 2015, WILDLIFE RES, V42, P394, DOI 10.1071/WR15092
   Wilson SB, 2016, SOUTHEAST NAT, V15, P269, DOI 10.1656/058.015.0207
   Zielinski WJ, 1996, ECOL APPL, V6, P1254, DOI 10.2307/2269605
NR 47
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0909-6396
EI 1903-220X
J9 WILDLIFE BIOL
JI Wildlife Biol.
PY 2021
VL 2021
IS 4
DI 10.2981/wlb.00880
PG 9
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA XO1LX
UT WOS:000729955700003
OA gold
DA 2022-02-10
ER

PT J
AU SMIRNOV, VM
   AGREST, AI
AF SMIRNOV, VM
   AGREST, AI
TI CALCULATION OF DEVICE FOR ADJUSTING THE DIAPHRAGM IN AN AUTOMATED CAMERA
   FOR FLASH-LAMP PHOTOGRAPHY
SO SOVIET JOURNAL OF OPTICAL TECHNOLOGY
LA English
DT Article
NR 0
TC 0
Z9 0
U1 0
U2 0
PU OPTICAL SOC AMER
PI WASHINGTON
PA 2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036
SN 0038-5514
J9 SOV J OPT TECHNOL+
PY 1979
VL 46
IS 2
BP 69
EP 72
PG 4
WC Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Optics
GA HM670
UT WOS:A1979HM67000003
DA 2022-02-10
ER

PT J
AU Laufer-Goldshtein, B
   Talmon, R
   Gannot, S
AF Laufer-Goldshtein, Bracha
   Talmon, Ronen
   Gannot, Sharon
TI Data-Driven Multi-Microphone Speaker Localization on Manifolds
SO FOUNDATIONS AND TRENDS IN SIGNAL PROCESSING
LA English
DT Article
ID SOUND SOURCE LOCALIZATION; TIME-DELAY ESTIMATION; DEEP NEURAL-NETWORKS;
   EXPECTATION-MAXIMIZATION ALGORITHM; ACOUSTIC SOURCE LOCALIZATION;
   DIMENSIONALITY REDUCTION; ROBUST LOCALIZATION; SPEECH ENHANCEMENT;
   SUBSPACE TRACKING; DIFFUSION MAPS
AB Speech enhancement is a core problem in audio signal processing with commercial applications in devices as diverse as mobile phones, conference call systems, smart assistants, and hearing aids. An essential component in the design of speech enhancement algorithms is acoustic source localization. Speaker localization is also directly applicable to many other audio related tasks, e.g., automated camera steering, teleconferencing systems, and robot audition.
   From a signal processing perspective, speaker localization is the task of mapping multichannel speech signals to 3-D source coordinates. To obtain viable solutions for this mapping, an accurate description of the source wave propagation captured by the respective acoustic channel is required. In fact, the acoustic channels can be considered as the spatial fingerprints characterizing the positions of each of the sources in a reverberant enclosure. These fingerprints represent complex reflection patterns stemming from the surfaces and objects characterizing the enclosure. Hence, they are usually modelled by a very large number of coefficients, resulting in an intricate high-dimensional representation.
   We claim that in static acoustic environments, despite the high dimensional representation, the difference between acoustic channels can be attributed mainly to changes in the source position. Thus, the true intrinsic dimensionality of the variations of the acoustic channels are significantly smaller than the number of variables commonly used to represent them; that is, the acoustic channels pertain to a low-dimensional manifold that can be inferred from data using nonlinear dimensionality reduction techniques. A comprehensive experimental study carried out in a real-life acoustic environment demonstrates the validity of the proposed manifold-based paradigm.
   Motivated by this result, several high-performance localization and tracking methods were developed by harnessing novel mathematical tools for learning over manifolds, including diffusion maps, semi-supervised learning, optimization in reproducing kernel Hilbert spaces and Gaussian process inference. We present two localization algorithms that were designed for a single microphone array of two microphones. These algorithms were extended to several distributed arrays by merging the information of the different manifolds associated with each array. Tracking a moving source was also addressed by a data-driven propagation model relating movements on the abstract manifold to the actual source displacements. This data-driven propagation model was combined with a classical localization approach, in a hybrid algorithm that ties together the two worlds of classical and data-driven localization, while gaining the benefits of both. We show that the proposed algorithms outperform state-of-the-art localization methods, and obtain high accuracy in challenging noisy and reverberant environments.
C1 [Laufer-Goldshtein, Bracha; Gannot, Sharon] Bar Ilan Univ, Fac Engn, Ramat Gan, Israel.
   [Talmon, Ronen] Technion Israel Inst Technol, Viterbi Fac Elect Engn, IL-32000 Haifa, Israel.
RP Laufer-Goldshtein, B (corresponding author), Bar Ilan Univ, Fac Engn, Ramat Gan, Israel.
EM Bracha.Laufer@biu.ac.il; ronen@ee.technion.ac.il;
   Sharon.Gannot@biu.ac.il
CR Abhayapala TD, 2003, ICT'2003: 10TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS, VOLS I AND II, CONFERENCE PROCEEDINGS, P1617
   Adavanne S, 2018, EUR SIGNAL PR CONF, P1462, DOI 10.23919/EUSIPCO.2018.8553182
   Affes S, 1997, IEEE T SPEECH AUDI P, V5, P425, DOI 10.1109/89.622565
   ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7
   Beard M., 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P542
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M., 2005, P 10 INT WORKSH ART
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Benesty J, 2000, J ACOUST SOC AM, V107, P384, DOI 10.1121/1.428310
   BERARD P, 1994, GEOM FUNCT ANAL, V4, P373, DOI 10.1007/BF01896401
   Berlinet A., 2011, REPRODUCING KERNEL H
   Bertin N. S., 2016, P IEEE INT C AC SPEE
   BLOM HAP, 1988, IEEE T AUTOMAT CONTR, V33, P780, DOI 10.1109/9.1299
   Brandstein MS, 1997, INT CONF ACOUST SPEE, P375, DOI 10.1109/ICASSP.1997.599651
   Brandstein MS, 1997, IEEE T SPEECH AUDI P, V5, P45, DOI 10.1109/89.554268
   Breining C, 1999, IEEE SIGNAL PROC MAG, V16, P42, DOI 10.1109/79.774933
   Brendel A, 2019, 2019 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP 2019), P276, DOI 10.1109/CAMSAP45676.2019.9022522
   Brendel A, 2019, INT CONF ACOUST SPEE, P7898, DOI 10.1109/ICASSP.2019.8683640
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bross A. B., 2020, P 28 EUR SIGN PROC C, P1
   Chakrabarty S., 2019, IEEE J SELECTED TOPI
   Chakrabarty S, 2017, IEEE WORK APPL SIG, P136, DOI 10.1109/WASPAA.2017.8170010
   Cmejla J, 2018, LECT NOTES COMPUT SC, V10891, P270, DOI 10.1007/978-3-319-93764-9_26
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Cox T. F., 2000, MULTIDIMENSIONAL SCA, V2nd
   DALDEGAN N, 1988, SIGNAL PROCESS, V15, P43, DOI 10.1016/0165-1684(88)90027-8
   Datum MS, 1996, J ACOUST SOC AM, V100, P372, DOI 10.1121/1.415854
   Deleforge A., 2012, P IEEE INT WORKSH MA, P1
   Deleforge A, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065714400036
   Deleforge A, 2013, INT CONF ACOUST SPEE, P76, DOI 10.1109/ICASSP.2013.6637612
   DiBiase JH, 2001, DIGITAL SIGNAL PROC, P157
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Dmochowski J, 2008, IEEE T AUDIO SPEECH, V16, P1490, DOI 10.1109/TASL.2008.2005029
   Do H, 2007, INT CONF ACOUST SPEE, P121
   Doclo S, 2003, EURASIP J APPL SIG P, V2003, P1110, DOI 10.1155/S111086570330602X
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Dorfan Y, 2018, IEEE-ACM T AUDIO SPE, V26, P682, DOI 10.1109/TASLP.2017.2788198
   Dorfan Y, 2015, IEEE-ACM T AUDIO SPE, V23, P1692, DOI 10.1109/TASLP.2015.2444654
   Dvorkind TG, 2005, SIGNAL PROCESS, V85, P177, DOI 10.1016/j.sigpro.2004.09.014
   Evers C, 2018, IEEE SIGNAL PROC LET, V25, P1320, DOI 10.1109/LSP.2018.2849579
   Evers C, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P1206, DOI 10.1109/ICDSP.2015.7252071
   FLOYD RW, 1962, COMMUN ACM, V5, P345, DOI 10.1145/367766.368168
   FOURIER BJB, 1822, THEORIE ANAL CHALEUR
   Gannot S, 2001, IEEE T SIGNAL PROCES, V49, P1614, DOI 10.1109/78.934132
   Gannot S., 2006, EURASIP J ADV SIG PR, V2006, P1
   Gannot S, 2017, IEEE-ACM T AUDIO SPE, V25, P692, DOI 10.1109/TASLP.2016.2647702
   Garofolo J.S., 1993, NIST SPEECH DISC, V93, P27403, DOI [10.6028/NIST.IR.4930, DOI 10.6028/NIST.IR.4930]
   Gebru I. D., 2014, P IEEE INT WORKSH MA, P1
   Golan SM, 2010, INT CONF ACOUST SPEE, P201, DOI 10.1109/ICASSP.2010.5496044
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Grondin F, 2019, ROBOT AUTON SYST, V113, P63, DOI 10.1016/j.robot.2019.01.002
   Minh HQ, 2016, J MACH LEARN RES, V17
   Habets E. A. P., 2008, NOISE GENERATORS
   Habets E. A. P., 2008, ROOM IMPULSE RESPONS
   Habets EAP, 2008, J ACOUST SOC AM, V124, P2911, DOI 10.1121/1.2987429
   Habets EAP, 2007, J ACOUST SOC AM, V122, P3464, DOI 10.1121/1.2799929
   Hadad E, 2014, 2014 14TH INTERNATIONAL WORKSHOP ON ACOUSTIC SIGNAL ENHANCEMENT (IWAENC), P313, DOI 10.1109/IWAENC.2014.6954309
   Haddad A, 2014, APPL COMPUT HARMON A, V36, P335, DOI 10.1016/j.acha.2013.05.002
   He WP, 2018, IEEE INT CONF ROBOT, P74
   Hein M., 2005, P 22 INT C MACH LEAR, P289
   Hornstein J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P1170, DOI 10.1109/IROS.2006.281849
   Hu YG, 2020, INT CONF ACOUST SPEE, P571, DOI 10.1109/ICASSP40776.2020.9053656
   Hu YG, 2019, IEEE WORK APPL SIG, P348, DOI 10.1109/WASPAA.2019.8937221
   Huang YT, 2000, INT CONF ACOUST SPEE, P909
   Jensen JR, 2016, INT CONF ACOUST SPEE, P176, DOI 10.1109/ICASSP.2016.7471660
   Jensen JR, 2015, INT CONF ACOUST SPEE, P11, DOI 10.1109/ICASSP.2015.7177922
   Jensen JR, 2013, IEEE T AUDIO SPEECH, V21, P923, DOI 10.1109/TASL.2013.2239290
   Jolliffe I., 2011, INT ENCY STAT SCI, P1094
   Jones PW, 2008, P NATL ACAD SCI USA, V105, P1803, DOI 10.1073/pnas.0710175104
   Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797
   Kitic S., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3087, DOI 10.1109/ICASSP.2014.6854168
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Koldovsky Z, 2015, IEEE-ACM T AUDIO SPE, V23, P1335, DOI 10.1109/TASLP.2015.2425213
   Kounades-Bastian D, 2017, IEEE WORK APPL SIG, P41, DOI 10.1109/WASPAA.2017.8169991
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Kumar L, 2013, 2013 IEEE 5TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP 2013), P304, DOI 10.1109/CAMSAP.2013.6714068
   Kushnir D, 2012, APPL COMPUT HARMON A, V32, P280, DOI 10.1016/j.acha.2011.06.002
   Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184
   Laufer B., 2013, P IEEE WORKSH APPL S, P1
   Laufer-Goldshtein B., 2016, 2016 IEEE INT C SCI, P1
   Laufer-Goldshtein B, 2018, EUR SIGNAL PR CONF, P842, DOI 10.23919/EUSIPCO.2018.8552933
   Laufer-Goldshtein B, 2018, IEEE T SIGNAL PROCES, V66, P6458, DOI 10.1109/TSP.2018.2876349
   Laufer-Goldshtein B, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P71, DOI 10.1109/ICASSP.2018.8462067
   Laufer-Goldshtein B, 2018, IEEE-ACM T AUDIO SPE, V26, P725, DOI 10.1109/TASLP.2018.2790707
   Laufer-Goldshtein B, 2017, IEEE-ACM T AUDIO SPE, V25, P1477, DOI 10.1109/TASLP.2017.2696310
   Laufer-Goldshtein B, 2017, LECT NOTES COMPUT SC, V10169, P59, DOI 10.1007/978-3-319-53547-0_6
   Laufer-Goldshtein B, 2016, IEEE-ACM T AUDIO SPE, V24, P1393, DOI 10.1109/TASLP.2016.2555085
   Laufer-Goldshtein B, 2016, INT CONF ACOUST SPEE, P6335, DOI 10.1109/ICASSP.2016.7472896
   Laufer-Goldshtein B, 2015, LECT NOTES COMPUT SC, V9237, P203, DOI 10.1007/978-3-319-22482-4_23
   Lederman RR, 2018, APPL COMPUT HARMON A, V44, P509, DOI 10.1016/j.acha.2015.09.002
   Levy A, 2011, IEEE T AUDIO SPEECH, V19, P1540, DOI 10.1109/TASL.2010.2093517
   Li B, 2016, INTERSPEECH, P1976, DOI 10.21437/Interspeech.2016-173
   Li XF, 2017, IEEE-ACM T AUDIO SPE, V25, P1997, DOI 10.1109/TASLP.2017.2740001
   Ma N., 2015, INTERSPEECH, V2015, P160
   Ma N, 2017, IEEE-ACM T AUDIO SPE, V25, P2444, DOI 10.1109/TASLP.2017.2750760
   Ma WK, 2006, IEEE T SIGNAL PROCES, V54, P3291, DOI 10.1109/TSP.2006.877658
   MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133
   Madhu N., 2008, P INT WORKSH AC SIGN
   Mandel M. I., 2007, ADV NEURAL INFORM PR, P953
   Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711
   Markovich S, 2009, IEEE T AUDIO SPEECH, V17, P1071, DOI 10.1109/TASL.2009.2016395
   Markovich-Golan S, 2018, EUR SIGNAL PR CONF, P2499, DOI 10.23919/EUSIPCO.2018.8553007
   Markovich-Golan S, 2017, IEEE-ACM T AUDIO SPE, V25, P320, DOI 10.1109/TASLP.2016.2633806
   May T, 2011, IEEE T AUDIO SPEECH, V19, P1, DOI 10.1109/TASL.2010.2042128
   Mercer J, 1909, PHILOS T R SOC LOND, V209, P415, DOI 10.1098/rsta.1909.0016
   Minh H. Q., 2011, P INT C MACH LEARN
   Nadiri O, 2014, IEEE-ACM T AUDIO SPE, V22, P1494, DOI 10.1109/TASLP.2014.2337846
   Nadler B., 2006, ADV NEURAL INFORM PR, P955
   Nadler B, 2006, APPL COMPUT HARMON A, V21, P113, DOI 10.1016/j.acha.2005.07.004
   Nadler Boaz, 2008, PRINCIPAL MANIFOLDS, V58, P238
   Nakadai K., 2002, P IEEE INT C SPOK LA, P193
   Nystrom Evert Johannes, 1929, PRAKTISCHE AUFLOSUNG
   Opochinsky R, 2019, IEEE WORK APPL SIG, P283, DOI 10.1109/WASPAA.2019.8937159
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Perotin L, 2019, IEEE J-STSP, V13, P22, DOI 10.1109/JSTSP.2019.2900164
   PETERSON PM, 1986, J ACOUST SOC AM, V80, P1527, DOI 10.1121/1.394357
   POLACK JD, 1993, APPL ACOUST, V38, P235, DOI 10.1016/0003-682X(93)90054-A
   Rascon C, 2017, ROBOT AUTON SYST, V96, P184, DOI 10.1016/j.robot.2017.07.011
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Reindl K., 2013, P IEEE WORKSH APPL S, P1
   Reuven G, 2008, IEEE T AUDIO SPEECH, V16, P711, DOI 10.1109/TASL.2008.917389
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276
   Rui Y, 2004, INT CONF ACOUST SPEE, P133
   Sainath T. N. R. J., 2016, P IEEE INT C AC SPEE
   Sainath T. N. R. J., 2015, P IEEE AUT SPEECH RE
   Sainath TN, 2016, INTERSPEECH, P1971, DOI 10.21437/Interspeech.2016-92
   Sainath TN, 2017, IEEE-ACM T AUDIO SPE, V25, P965, DOI 10.1109/TASLP.2017.2672401
   Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1
   Sandryhaila A, 2014, IEEE SIGNAL PROC MAG, V31, P80, DOI 10.1109/MSP.2014.2329213
   SCHAU HC, 1987, IEEE T ACOUST SPEECH, V35, P1223, DOI 10.1109/TASSP.1987.1165266
   Scheuing J, 2008, IEEE T AUDIO SPEECH, V16, P1479, DOI 10.1109/TASL.2008.2004533
   SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Schwartz O, 2014, IEEE-ACM T AUDIO SPE, V22, P392, DOI 10.1109/TASLP.2013.2292361
   Shalvi O, 1996, IEEE T SIGNAL PROCES, V44, P2055, DOI 10.1109/78.533725
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Sindhwani V, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1059
   Sindhwani Vikas, 2005, P 22 INT C MACH LEAR, V2005, P74, DOI DOI 10.1145/1102351.1102455
   Singer A, 2008, APPL COMPUT HARMON A, V25, P226, DOI 10.1016/j.acha.2007.11.001
   Stephenne A, 1997, SIGNAL PROCESS, V59, P253, DOI 10.1016/S0165-1684(97)00051-0
   Takeda R, 2017, INT CONF ACOUST SPEE, P2217, DOI 10.1109/ICASSP.2017.7952550
   Takeda R, 2016, IEEE W SP LANG TECH, P603, DOI 10.1109/SLT.2016.7846325
   Takeda R, 2016, INT CONF ACOUST SPEE, P405, DOI 10.1109/ICASSP.2016.7471706
   Talantzis F, 2008, IEEE T SYST MAN CY B, V38, P799, DOI 10.1109/TSMCB.2008.922063
   Talmon R, 2013, P NATL ACAD SCI USA, V110, P12535, DOI 10.1073/pnas.1307298110
   Talmon R, 2009, IEEE T AUDIO SPEECH, V17, P546, DOI 10.1109/TASL.2008.2009576
   Talmon Ronen, 2013, 21 EUR SIGN PROC C E, P1
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Teutsch H, 2005, INT CONF ACOUST SPEE, P89
   Thiergart O, 2014, IEEE-ACM T AUDIO SPE, V22, P2182, DOI 10.1109/TASLP.2014.2363407
   Tian Y, 2015, IEEE-ACM T AUDIO SPE, V23, P1637, DOI 10.1109/TASLP.2015.2442418
   Valin JM, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1228
   Vermaak J, 2001, INT CONF ACOUST SPEE, P3021, DOI 10.1109/ICASSP.2001.940294
   Wabnitz Colette, 2010, Marine Turtle Newsletter, V129, P1
   Wakabayashi M, 2020, IEEE ROBOT AUTOM LET, V5, P782, DOI 10.1109/LRA.2020.2965417
   Wang YC, 2013, 2013 INTERNATIONAL CONFERENCE ON FUZZY THEORY AND ITS APPLICATIONS (IFUZZY 2013), P171, DOI 10.1109/iFuzzy.2013.6825431
   Ward DB, 2003, IEEE T SPEECH AUDI P, V11, P826, DOI 10.1109/TSA.2003.818112
   Weisberg K, 2019, 2019 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP 2019), P286, DOI 10.1109/CAMSAP45676.2019.9022438
   Weisberg K, 2019, INT CONF ACOUST SPEE, P656, DOI 10.1109/ICASSP.2019.8682659
   Weiss R. J., 2017, NEW ERA ROBUST SPEEC
   Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807
   Woodbury M.A., 1950, MEMORANDUM REPORT
   Wu X, 2016, INT CONF ACOUST SPEE, P6320, DOI 10.1109/ICASSP.2016.7472893
   Xiao X, 2015, P IEEE INT C AC SPEE, P76
   Xiao X, 2016, INT CONF ACOUST SPEE, P6330, DOI 10.1109/ICASSP.2016.7472895
   Yalta N, 2017, J ROBOT MECHATRON, V29, P37, DOI 10.20965/jrm.2017.p0037
   Yao K, 2002, INT CONF ACOUST SPEE, P2949
   Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.828896
   Zelnik-Manor L., 2005, ADV NEURAL INFORM PR, P1601
   Zhang QL, 2017, IEEE T SYST MAN CY-S, V47, P2433, DOI 10.1109/TSMC.2016.2523939
   Zhong XH, 2008, INT CONF ACOUST SPEE, P293
   Zhong XH, 2015, SIGNAL PROCESS, V108, P589, DOI 10.1016/j.sigpro.2014.09.031
   Zotkin D, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P20, DOI 10.1109/EVENT.2001.938862
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
NR 178
TC 2
Z9 2
U1 0
U2 3
PU NOW PUBLISHERS INC
PI HANOVER
PA PO BOX 1024, HANOVER, MA 02339 USA
SN 1932-8346
EI 1932-8354
J9 FOUND TRENDS SIGNAL
JI Found. Trends Signal Process.
PY 2020
VL 14
IS 1-2
BP 1
EP 161
DI 10.1561/2000000098
PG 161
WC Engineering, Electrical & Electronic
WE Emerging Sources Citation Index (ESCI)
SC Engineering
GA OE2RV
UT WOS:000580385100001
OA gold
DA 2022-02-10
ER

PT S
AU Pack, ML
   Smith, BL
   Scherer, WT
AF Pack, ML
   Smith, BL
   Scherer, WT
GP TRB
TI Automated camera repositioning technique for video image vehicle
   detection systems - Integrating with freeway closed-circuit television
   systems
SO FREEWAYS, HIGH-OCCUPANCY VEHICLE SYSTEMS, AND TRAFFIC SIGNAL SYSTEMS
   2003: HIGHWAY OPERATIONS, CAPACITY, AND TRAFFIC CONTROL
SE TRANSPORTATION RESEARCH RECORD
LA English
DT Article; Proceedings Paper
CT 82nd Annual Meeting of the Transportation-Research-Board
CY JAN 12-16, 2003
CL WASHINGTON, D.C.
SP Transportat Res Board, US Dept Transportat, Bur Transportat Stat, Fed Aviat Adm, Fed Highway Adm, Fed Motor Carrier Safety Adm, Fed Railroad Adm, Fed Transit Adm, Natl Highway Traff Safety Adm, Res & Special Programs Adm, NASA, USA Corps Engineers, US Coast Guard, US DOE, US EPA
AB Transportation agencies have invested significantly in extensive closed-circuit television (CCTV) systems to monitor freeways in urban areas. While these systems have proven to be very effective in supporting incident management, they do not support the collection of quantitative measures of traffic conditions. Instead, they simply provide images that must be interpreted by trained operators. While there are several video image vehicle detection systems (VIVDS) on the market that have the capability to automatically derive traffic measures from video imagery, these systems require the installation of fixed-position cameras. Thus, they have not been integrated with the existing moveable CCTV cameras. VIVDS camera positioning and calibration challenges were addressed and a prototype machine-vision system was developed that successfully integrated existing moveable CCTV cameras with VIVDS. Results of testing the prototype are presented indicating that when the camera's initial zoom level was kept between x 1 and x 1.5, the camera consistently could be returned to its original position with a repositioning accuracy of less than 0.03degrees to 0.1degrees regardless of the camera's displaced pan, tilt, or zoom settings at the time of repositioning. This level of positional accuracy when combined with a VIVDS resulted in vehicle count errors of less than 1%.
C1 Univ Maryland, Ctr Adv Transportat Technol, College Pk, MD 20742 USA.
   Univ Virginia, Sch Engn & Appl Sci, Dept Syst & Informat Engn, Charlottesville, VA 22904 USA.
RP Pack, ML (corresponding author), Univ Maryland, Ctr Adv Transportat Technol, College Pk, MD 20742 USA.
OI Scherer, William/0000-0002-1069-7898
CR COTTRELL BH, 1994, 94R22 VTRC
   LEE IJ, 2000, P 7 WORLD C INT TRAN
   1997, FHWAPL97018
NR 3
TC 4
Z9 4
U1 1
U2 3
PU TRANSPORTATION RESEARCH BOARD NATL RESEARCH COUNCIL
PI WASHINGTON
PA 500 FIFTH ST, NW, WASHINGTON, DC 20001 USA
SN 0361-1981
BN 0-309-08592-6
J9 TRANSPORT RES REC
PY 2003
IS 1856
BP 25
EP 33
PG 9
WC Engineering, Civil; Transportation Science & Technology
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Transportation
GA BY74T
UT WOS:000189453400004
DA 2022-02-10
ER

PT J
AU Cain, JW
   Morrison, ML
   Bombay, HL
AF Cain, JW
   Morrison, ML
   Bombay, HL
TI Predator activity and nest success of willow flycatchers and yellow
   warblers
SO JOURNAL OF WILDLIFE MANAGEMENT
LA English
DT Article
DE California; Dendroica petechia; Empidonax traillii; montane meadows;
   nest predation; predator activity; Sierra Nevada
ID BROWN-HEADED COWBIRDS; TALLGRASS PRAIRIE; WILDLIFE RESEARCH; ARTIFICIAL
   NESTS; SITE SELECTION; FOREST; EDGE; FRAGMENTATION; BIRDS; PARASITISM
AB Willow flycatchers (Empidonax traillii) and yellow warblers (Dendroica petechia) are riparian-dependent species that have declined throughout much of their former range in California, USA. These declines have been primarily associated with the loss of riparian breeding habitat, increases in brood parasitism, and increases in nest predation. We (1) identified potential nest predators using inactive yellow warbler nests; (2) determined the relationship of meadow wetness, meadow size, and amount of edge to predator activity; (3) determined the association between potential nest predator activity and nest success; and (4) determined how proximity to forest edge and isolated trees was related to nest success. We used automatic cameras to monitor inactive yellow warbler nests baited with zebra finch (Taeniopygia guttata) eggs to identify nest predators. We used track plates (mammalian), point counts (avian), and time-constrained searches (reptilian) to assess the activity of potential nest predators. We photographed short-tailed weasel (Mustela erminea), Douglas squirrel (Tamiasciurus douglasii), lodgepole chipmunk (Tamias speciosus), deer mouse (Peromyscus maniculatus), and unidentified chipmunks (Tamias spp.) depredating yellow warbler nests baited with finch eggs. The amount of meadow covered with water was negatively associated with the activity of chipmunks and Douglas squirrels. Meadow size was negatively associated with Douglas squirrel activity. The amount of edge was positively associated with the activity of Douglas squirrels, chipmunks, Steller's jays (Cyanocitta stelleri), and brown-headed cowbirds (Molothrus ater). Nest predation was the major cause of nest failure in our study. However, only short-tailed weasels, Douglas squirrels, Clark's nutcrackers (Nucifraga columbiana), Steller's jays, Cooper's hawks (Accipiter cooperii), and brown-headed cowbirds had activity indices that were negatively associated with nest success of either species. The distance to isolated trees was associated with willow flycatcher nest success, whereas the distance to both isolated trees and to the forest edge was associated with yellow warbler nest success-nests located closer to isolated trees and the forest edge were more likely to be parasitized and/or depreciated. Our results suggest that flooding portions of meadows may restrict meadow access to forest-edge-associated nest predators.
C1 Calif State Univ Sacramento, Dept Biol Sci, Sacramento, CA 95819 USA.
RP Cain, JW (corresponding author), Univ Arizona, Wildlife & Fisheries Sci Program, Sch Renewable Nat Resources, Tucson, AZ 85721 USA.
EM jwcain@ag.arizona.edu
CR Best Troy L., 1994, Mammalian Species, V478, P1
   BOMBAY HL, 2002, IN PRESS STUDIES AVI
   BURGER LD, 1994, J WILDLIFE MANAGE, V58, P249, DOI 10.2307/3809387
   BURGHAM MCJ, 1989, ANIM BEHAV, V38, P298, DOI 10.1016/S0003-3472(89)80091-0
   *CA DEP FISH GAM, 1992, UNPUB BIRD SPEC SPEC
   *CA DEP FISH GAM, 1999, STAT FED LIST END TH
   CAIN JW, 2001, THESIS CALIFORNIA ST
   CLAY DH, 1984, CALIFORNIA RIPARIAN, P477
   CORN PS, 1990, WILDLIFE HABITAT REL, P1
   Danielson WR, 1996, J FIELD ORNITHOL, V67, P414
   De Santo TL, 2001, J FIELD ORNITHOL, V72, P136, DOI 10.1648/0273-8570-72.1.136
   DeSante David F., 1994, Studies in Avian Biology, V15, P173
   Donovan TM, 1997, ECOLOGY, V78, P2064, DOI 10.1890/0012-9658(1997)078[2064:VILSEE]2.0.CO;2
   *ENV SYST RES I, 2000, ARCVIEW 3 2
   Faaborg John, 1995, P357
   Fagerstone K.A., 1987, P548
   Gannon William L., 1995, Mammalian Species, V502, P1
   GATES JE, 1978, ECOLOGY, V59, P871, DOI 10.2307/1938540
   GOOSSEN JP, 1982, CAN FIELD NAT, V96, P189
   GREENE E., 1998, BIRDS N AM, V343
   Harris SG, 2001, CLASSICAL QUANT GRAV, V18, P27, DOI 10.1088/0264-9381/18/1/303
   HESKE EJ, 1995, J MAMMAL, V76, P562, DOI 10.2307/1382364
   Heske EJ, 2001, WILDLIFE SOC B, V29, P52
   JOHNSON RG, 1990, J WILDLIFE MANAGE, V54, P106, DOI 10.2307/3808909
   Kus BE, 1998, RESTOR ECOL, V6, P75, DOI 10.1046/j.1526-100x.1998.06110.x
   Liebezeit JR, 2002, CONDOR, V104, P507, DOI 10.1650/0010-5422(2002)104[0507:NPNSSA]2.0.CO;2
   Major RE, 1996, IBIS, V138, P298, DOI 10.1111/j.1474-919X.1996.tb04342.x
   Martin TE, 1996, T N AM WILDL NAT RES, P43
   MARTIN TE, 1992, ECOLOGY AND CONSERVATION OF NEOTROPICAL MIGRANT LANDBIRDS, P455
   MARTIN TE, 1993, J FIELD ORNITHOL, V64, P507
   MAYFIELD HAROLD, 1961, WILSON BULL, V73, P255
   MAYFIELD HF, 1975, WILSON BULL, V87, P456
   McCabe R.A., 1991, LITTLE GREEN BIRD EC
   MOLLER AP, 1989, OIKOS, V56, P240, DOI 10.2307/3565342
   Musgrave B. F., 1951, Murrelet, V32, P8, DOI 10.2307/3533996
   *NOAA, 1999, CLIM DAT ANN SUMM CA
   *NOAA, 2000, CLIM DAT ANN SUMM CA
   ORLOFF SG, 1993, CALIF FISH GAME, V79, P45
   Ortega JC, 2000, J FIELD ORNITHOL, V71, P516, DOI 10.1648/0273-8570-71.3.516
   PATON PWC, 1994, CONSERV BIOL, V8, P17, DOI 10.1046/j.1523-1739.1994.08010017.x
   PICMAN J, 1993, AUK, V110, P89
   RATLIFF RD, 1982, PSW60 US FOR SERV
   RATLIFF RD, 1985, PSW84 US FOR SERV
   Robinson Scott K., 1995, P428
   ROBINSON SK, 1995, SCIENCE, V267, P1987, DOI 10.1126/science.267.5206.1987
   ROBINSON SK, 1993, USDA ROCKY, V229, P93
   ROSENFIELD RN, 1993, BIRDS N AM, V75
   SANDERS SD, 1989, US FOR SERV T R PSW, V110, P262
   SAUER JR, 2000, N AM BREEDING BIRD S
   SEDGWICK JA, 1988, CONDOR, V90, P253, DOI 10.2307/1368461
   Sheffield Steven R., 1997, Mammalian Species, V570, P1
   Sieving KE, 1998, ECOLOGY, V79, P2391, DOI 10.1890/0012-9658(1998)079[2391:NPAASD]2.0.CO;2
   *SPSS, 1998, SPSS GRAD PACK 9 0 W
   STEELE MA, 1999, MAMMALIAN SPECIES, V0630, P00001
   Steidl RJ, 1997, J WILDLIFE MANAGE, V61, P270, DOI 10.2307/3802582
   Tewksbury JJ, 1998, ECOLOGY, V79, P2890, DOI 10.1890/0012-9658(1998)079[2890:BPDNDW]2.0.CO;2
   U. S. Fish and Wildlife Service (USFWS), 1995, FED REGISTER, V60, P10694
   UNITT P, 1987, Western Birds, V18, P137
   White JC, 1999, ENVIRON TOXICOL CHEM, V18, P182, DOI 10.1002/etc.5620180212
   Whitfield MJ, 1999, STUD AVIAN BIOL, P260
   Winter M, 2000, CONDOR, V102, P256, DOI 10.1650/0010-5422(2000)102[0256:EFEEOM]2.0.CO;2
   York EC, 2001, WILDLIFE SOC B, V29, P1228
   Zar J.H., 1996, BIOSTATISTICAL ANAL
   Zeedyk B, 1996, USDA ROCKY, V272, P258
   [No title captured]
   [No title captured], DOI DOI 10.2307/3503967
NR 66
TC 32
Z9 34
U1 1
U2 46
PU WILEY-BLACKWELL
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0022-541X
EI 1937-2817
J9 J WILDLIFE MANAGE
JI J. Wildl. Manage.
PD JUL
PY 2003
VL 67
IS 3
BP 600
EP 610
PG 11
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA 703TR
UT WOS:000184297500015
DA 2022-02-10
ER

PT J
AU Stoner, DC
   Wolfe, ML
   Rieth, WR
   Bunnell, KD
   Durham, SL
   Stoner, LL
AF Stoner, David C.
   Wolfe, Michael L.
   Rieth, Wendy R.
   Bunnell, Kevin D.
   Durham, Susan L.
   Stoner, Lisa L.
TI De facto refugia, ecological traps and the biogeography of anthropogenic
   cougar mortality in Utah
SO DIVERSITY AND DISTRIBUTIONS
LA English
DT Article
DE Colorado Plateau; ecological trap; exploitation; Great Basin; Puma
   concolor; range contraction; refuge
ID MOUNTAIN LIONS; CARNIVORE CONSERVATION; PROTECTED AREAS; PUMA-CONCOLOR;
   POPULATION; EXTINCTION; STRATEGIES; DYNAMICS; IMPACT; PREY
AB Aim Modern extirpations within the Carnivora have generally followed the human footprint. The contagion hypothesis predicts that range contractions should occur along gradients in human activity, leaving relict populations in remote areas at range edges. We evaluated this hypothesis for cougars (Puma concolor), a widely distributed and heavily exploited North American carnivore.
   Location Colorado Plateau and Great Basin ecoregions within Utah, USA.
   Methods We examined the spatial distribution of anthropogenic cougar mortality (n = 4217) using indices of remoteness and habitat quality within a GIS/ multiple-regression analytical framework. To identify areas of disproportionately high or low exploitation rates, we used break-points from the literature and local field studies. We defined de facto refugia as watersheds with mean annual harvest rates <= 24% of the predicted population, whereas ecological traps were those watersheds that exceeded this value.
   Results Cougar harvest rates were greater in the core and lower along the periphery of their statewide geographic range. The largest refugia were overrepresented in arid ecoregions with low human population densities, whereas ecological traps were concentrated in areas of low remoteness. Ecological traps were within mean cougar dispersal distances from refugia, highlighting the potential for source-sink dynamics. Patterns of anthropogenic cougar mortality generally followed the predictions of the contagion hypothesis, being spatially correlated with human access in high-quality habitats.
   Main conclusions Low-quality habitats on the range margins are likely to harbour carnivore populations in the event of widespread human-caused declines, and therefore may have greater conservation value than has previously been assumed. Resource managers may consider using the distribution of de facto refugia and ecological traps within a source-sink context to develop conservation strategies for cougars and other wide-ranging, low-density carnivores with high dispersal tendencies.
C1 [Stoner, David C.; Wolfe, Michael L.] Utah State Univ, Dept Wildland Resources, Logan, UT 84322 USA.
   [Stoner, David C.; Wolfe, Michael L.; Durham, Susan L.] Utah State Univ, Ctr Ecol, Logan, UT 84322 USA.
   [Rieth, Wendy R.; Stoner, Lisa L.] Utah State Univ, Dept Wildland Resources, RS GIS Lab, Logan, UT 84322 USA.
   [Bunnell, Kevin D.] Utah Div Wildlife Resources, Salt Lake City, UT 84114 USA.
RP Stoner, DC (corresponding author), Utah State Univ, Dept Wildland Resources, Logan, UT 84322 USA.
EM david.stoner@usu.edu
FU Utah Division of Wildlife Resources through the Federal Aid in Wildlife
   Restoration Program; African Safari Club of Florida; NASA Biodiversity
   and Ecological Forecasting Program (Climate and Biological Response)
   [NNH10ZDA001N]
FX Project Funding came from the Utah Division of Wildlife Resources
   through the Federal Aid in Wildlife Restoration Program, grant no.
   W-65-M. We gratefully acknowledge H. Bernales, K. Hersey, A. Aoude, B.
   Blackwell, B. Bates and T. Becker of the UDWR for logistical and
   technical support. We thank R. Larsen for unpublished camera-trap data
   from the Keg Mountains. The African Safari Club of Florida generously
   provided D. S. with subsidiary funding during the writing phase of the
   project. Additional support was provided by the NASA Biodiversity and
   Ecological Forecasting Program (Climate and Biological Response, grant
   no. NNH10ZDA001N). We thank F. Howe, J. du Toit, J. MacMahon, E. Gese,
   K. Logan and an anonymous reviewer for constructive criticism of the
   original manuscript.
CR Anderson CR, 2005, WILDLIFE SOC B, V33, P179, DOI 10.2193/0091-7648(2005)33[179:EEOPTA]2.0.CO;2
   Andreasen AM, 2012, MOL ECOL, V21, P5689, DOI 10.1111/j.1365-294X.2012.05740.x
   Balme GA, 2010, ANIM CONSERV, V13, P315, DOI 10.1111/j.1469-1795.2009.00342.x
   Balme Guy A., 2010, P341
   Banner R., 2009, RANGELAND RESOURCES
   Basille M, 2009, ECOGRAPHY, V32, P683, DOI 10.1111/j.1600-0587.2009.05712.x
   BECK T, 2005, COUGAR MANAGEMENT GU
   Beier P, 2010, URBAN CARNIVORES ECO
   BROWN JH, 1984, AM NAT, V124, P255, DOI 10.1086/284267
   Carbone C, 2002, SCIENCE, V295, P2273, DOI 10.1126/science.1067994
   Cardillo M, 2005, SCIENCE, V309, P1239, DOI 10.1126/science.1116030
   Caso A, 2008, IUCN RED LIST THREAT
   Channell R, 2000, NATURE, V403, P84, DOI 10.1038/47487
   Choate DM, 2006, WILDLIFE SOC B, V34, P782, DOI 10.2193/0091-7648(2006)34[782:EOCPEI]2.0.CO;2
   Cooley HS, 2009, ECOLOGY, V90, P2913, DOI 10.1890/08-1805.1
   Cougar Discussion Group, 2009, COUG DISC GROUP PUBL
   Davidson AD, 2009, P NATL ACAD SCI USA, V106, P10702, DOI 10.1073/pnas.0901956106
   De Angelo C, 2011, DIVERS DISTRIB, V17, P422, DOI 10.1111/j.1472-4642.2011.00746.x
   Delibes M, 2001, AM NAT, V158, P277, DOI 10.1086/321319
   GILBERT P F, 1970, Journal of Wildlife Management, V34, P15, DOI 10.2307/3799486
   Herfindal I., 2007, J ZOOL, V265, P63
   Jenks, 2011, MANAGING COUGARS N A, P111
   Jung TS, 2005, CAN FIELD NAT, V119, P580, DOI 10.22621/cfn.v119i4.192
   Kerley LL, 2002, CONSERV BIOL, V16, P97, DOI 10.1046/j.1523-1739.2002.99290.x
   Laliberte AS, 2004, BIOSCIENCE, V54, P123, DOI 10.1641/0006-3568(2004)054[0123:RCONAC]2.0.CO;2
   Laundre J, 2003, ANIM CONSERV, V6, P159, DOI 10.1017/S1367943003003202
   Leopold A., 1933, GAME MANAGEMENT
   Lindzey F., 1987, WILD FURBEARER MANAG, P657
   Logan K., 2001, DESERT PUMA
   Lomolino M, 2006, BIOGEOGRAPHY
   Loveridge Andrew J., 2010, P283
   Martin PS, 1999, CONSERV BIOL, V13, P36, DOI 10.1046/j.1523-1739.1999.97417.x
   McClure MF, 2005, EUR J WILDLIFE RES, V51, P170, DOI 10.1007/s10344-005-0086-z
   Minor ES, 2010, CONSERV BIOL, V24, P1549, DOI 10.1111/j.1523-1739.2010.01558.x
   Monteith KL, 2011, ECOSPHERE, V2, DOI 10.1890/ES10-00096.1
   Morrison JC, 2007, J MAMMAL, V88, P1363, DOI 10.1644/06-MAMM-A-124R2.1
   Naidu A, 2011, J FISH WILDL MANAG, V2, P106, DOI 10.3996/042010-JFWM-008
   Naves J, 2003, CONSERV BIOL, V17, P1276, DOI 10.1046/j.1523-1739.2003.02144.x
   Packer C, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005941
   Pettorelli N, 2011, CLIM RES, V46, P15, DOI 10.3354/cr00936
   Pierce BM, 2012, J MAMMAL, V93, P977, DOI 10.1644/12-MAMM-A-014.1
   Pierce BM, 2000, ECOLOGY, V81, P1533
   Rabinowitz A, 2010, BIOL CONSERV, V143, P939, DOI 10.1016/j.biocon.2010.01.002
   Rieth W. R., 2009, THESIS UTAH STATE U
   Robinson HS, 2008, ECOL APPL, V18, P1028, DOI 10.1890/07-0352.1
   Ruth TK, 2011, J WILDLIFE MANAGE, V75, P1381, DOI 10.1002/jwmg.190
   Seager R, 2007, SCIENCE, V316, P1181, DOI 10.1126/science.1139601
   Stein BA, 2008, BIOSCIENCE, V58, P339, DOI 10.1641/B580409
   Stoner D., 2011, THESIS UTAH STATE U
   Stoner DC, 2006, J WILDLIFE MANAGE, V70, P1588, DOI 10.2193/0022-541X(2006)70[1588:CELIUI]2.0.CO;2
   Sweanor LL, 2008, J WILDLIFE MANAGE, V72, P1076, DOI 10.2193/2007-024
   Sweanor LL, 2000, CONSERV BIOL, V14, P798, DOI 10.1046/j.1523-1739.2000.99079.x
   Tolon V, 2012, ECOL APPL, V22, P648, DOI 10.1890/11-0422.1
   Wilson S, 2010, WEST N AM NATURALIST, V70, P238, DOI 10.3398/064.070.0211
   Woodroffe R, 2001, CONSERV BIOL SER, V5, P61
   Woodroffe R, 1998, SCIENCE, V280, P2126, DOI 10.1126/science.280.5372.2126
NR 56
TC 18
Z9 18
U1 0
U2 97
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1366-9516
EI 1472-4642
J9 DIVERS DISTRIB
JI Divers. Distrib.
PD SEP
PY 2013
VL 19
IS 9
BP 1114
EP 1124
DI 10.1111/ddi.12035
PG 11
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA 201EK
UT WOS:000323123300003
DA 2022-02-10
ER

PT C
AU Ellenrieder, MM
   Komoto, H
AF Ellenrieder, MM
   Komoto, H
BE Bouman, CA
   Miller, EL
TI Model-based automatic calculation and evaluation of camera positions for
   industrial machine vision
SO COMPUTATIONAL IMAGING III
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Computational Imaging III
CY JAN 17-18, 2005
CL San Jose, CA
SP Soc Imaging Sci & Technol, SPIE
DE automatic camera positioning; model-based optimal viewpoint selection;
   geometric constraints; visibility map; optimal sensor-feature
   association; inspection; task planning
AB One of die key issues for a successful inspection process is the determination of the necessary number of cameras and their respective positions given a specific inspection task and a geometric model of the inspected work-piece and its surroundings. In the last decades, a number of approaches concerning camera positioning strategies have been proposed. Generally, these approaches define an inspection task in terms of good visibility of certain features on the surface of the inspected objects. However, these approaches neither provide general means to include arbitrary inspection requirements, nor do 14 they minimize the number of required cameras. Others use only hard constraints to determine the area of feasibility for certain task requirements. To overcome these shortcomings, we propose a model-based approach to optimize one or more camera positions by optimizing cost-functions derived from the inspection task. The goal is to use a minimum number of cameras / camera positions to fulfill the inspection task. Feature-visibility is represented using a novel concept: the visibility map. It can be calculated quickly by using a projective approach, consumes little storage memory and allows for quick feature- visibility checks. The system is evaluated on several examples using real inspection tasks from current production processes.
C1 DaimlerChrysler AG, Res & Technol, D-89013 Ulm, Germany.
RP Ellenrieder, MM (corresponding author), DaimlerChrysler AG, Res & Technol, REI AI,POB 2360, D-89013 Ulm, Germany.
EM Marc.Ellenrieder@DaimlerChrysler.Com
RI Komoto, Hitoshi/G-7980-2017
OI Komoto, Hitoshi/0000-0002-4810-1455
CR Boyd S., 2004, CONVEX OPTIMIZATION
   Chen SY, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P2545
   COWAN CK, 1988, IEEE T PATTERN ANAL, V10, P407, DOI 10.1109/34.3905
   Khawaja KW, 1996, IEEE INT CONF ROBOT, P3246, DOI 10.1109/ROBOT.1996.509207
   Kruger L, 2004, PROC SPIE, V5457, P126, DOI 10.1117/12.545396
   Scott WR, 2003, ACM COMPUT SURV, V35, P64, DOI 10.1145/641865.641868
   SINNOTT RW, 1984, SKY TELESCOPE, V68, P159
   Stossel D, 2004, LECT NOTES COMPUT SC, V3175, P528
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P72, DOI 10.1109/70.345939
   TRIGGS B, 1995, P INT S ROB RES MUN
   Trucco E, 1997, IEEE T ROBOTIC AUTOM, V13, P182, DOI 10.1109/70.563641
   Weihua Sheng, 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1157, DOI 10.1109/ROBOT.2000.844755
NR 13
TC 3
Z9 3
U1 0
U2 0
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 0-8194-5647-0
J9 PROC SPIE
PY 2005
VL 5674
BP 467
EP 478
DI 10.1117/12.585427
PG 12
WC Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Imaging Science & Photographic Technology
GA BCD94
UT WOS:000228796600045
DA 2022-02-10
ER

PT J
AU Broker, KCA
   Hansen, RG
   Leonard, KE
   Koski, WR
   Heide-Jorgensen, MP
AF Broker, Koen C. A.
   Hansen, Rikke G.
   Leonard, Kathleen E.
   Koski, William R.
   Heide-Jorgensen, Mads Peter
TI A comparison of image and observer based aerial surveys of narwhal
SO MARINE MAMMAL SCIENCE
LA English
DT Article
DE abundance estimation; aerial surveys; Arctic; Melville Bay; Monodon
   monoceros; narwhal; line-transect survey; strip census; unmanned aerial
   systems; West Greenland
ID UNMANNED AIRCRAFT SYSTEM; MONODON-MONOCEROS; MARINE MAMMALS; ESTIMATING
   ABUNDANCE; MELVILLE BAY; BAFFIN-BAY; VEHICLES; WHALES; WINTER; UAS
AB From 25 to 30 August 2014 a double-observer line-transect survey was conducted over Melville Bay, home to one of two summering populations of narwhal (Monodon monoceros) off West Greenland. A total of 1,932 linear kilometers was surveyed along 33 transects. In addition to using observers, the aircraft was equipped with two oblique cameras to capture a comparable data set. Analysts reviewed the images for narwhal sightings, which were then matched to the observer sightings. The objectives of the study were to determine advantages and disadvantages of the detection capabilities of both methodologies, and to conduct a comparative analysis of population abundance estimates. Correcting for the truncated detection distance of the images (500 m), the image analysts recorded more sightings (62) and a lower mean group size (2.2) compared to aerial observers (36 and 3.5, respectively), resulting in comparable numbers of individuals detected by both platforms (135 vs. 126). The abundance estimate based on the image sightings was 2,536 (CV = 0.51, 95% CI: 1,003-6,406), which was not significantly different from the aerial observers estimate of 2,596 individuals (CV = 0.51; 95% CI: 961-7,008). This study supports the potential of using UAS for marine mammal abundance studies.
C1 [Broker, Koen C. A.] Shell Global Solut Int BV, Kesslerpk 1, NL-2288 GS Rijswijk, Netherlands.
   [Broker, Koen C. A.] Univ Groningen, Groningen Inst Evolutionary Life Sci, POB 11103, NL-9700 CC Groningen, Netherlands.
   [Hansen, Rikke G.; Heide-Jorgensen, Mads Peter] Greenland Inst Nat Resources, Greenland Representat, Strandgade 91,2, DK-1401 Copenhagen K, Denmark.
   [Leonard, Kathleen E.] LGL Alaska Res Associates Inc, 2000 West Int Airport Rd,STE C-1, Anchorage, AK 99502 USA.
   [Koski, William R.] LGL Ltd Environm Res Associates, 22 Fisher St,POB 280, King City, ON L7B 1A6, Canada.
RP Broker, KCA (corresponding author), Shell Global Solut Int BV, Kesslerpk 1, NL-2288 GS Rijswijk, Netherlands.; Broker, KCA (corresponding author), Univ Groningen, Groningen Inst Evolutionary Life Sci, POB 11103, NL-9700 CC Groningen, Netherlands.
EM koen.broker@shell.com
OI Broker, Koen/0000-0002-5965-5517
FU Greenland Bureau of Minerals and Petroleum; Shell Greenland A/S; Shell
   Alaska
FX The aerial survey was funded by the Greenland Bureau of Minerals and
   Petroleum. Shell Greenland A/S and Shell Alaska funded the installation
   and operation of the automated camera system and manual analysis of
   images. We thank Anders Mosbech (DCE) for managing the Baffin Bay
   Environmental Study Program. We are also grateful to Brad Boschetto
   (Shell), Dale Funk and Craig Reiser (LGL Alaska Research) for organizing
   logistics of the camera installation. Per PalsbOll (University of
   Groningen), Kees Camphuysen (Royal Netherlands Institute for Sea
   Research), Louis Brzuzy, Michael Macrander (Shell), and three anonymous
   reviewers provided useful feedback on this document. The excellent
   pilots were provided by Norlandair. Thanks are due to the aerial
   observers Rasmus Due Nielsen, Silje Rekdal Larsen, and Mikkel Sinding,
   and the image analysts Amy Baker, Cynthia Christman, Heather Patterson,
   Lauren Ackein, Lisa Barry, and Matthew O'Dell (LGL Alaska Research) for
   their dedicated efforts.
CR Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
   BAJZAK D, 1990, WILDLIFE SOC B, V18, P125
   Borchers DL, 2006, BIOMETRICS, V62, P372, DOI 10.1111/j.1541-0420.2005.00493.x
   BUCKLAND S, 2015, METH STAT ECOL
   Buckland S.T., 2001, pi
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chabot D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122588
   Chabot D, 2012, WATERBIRDS, V35, P170, DOI 10.1675/063.035.0119
   EBERHARDT LL, 1978, J WILDLIFE MANAGE, V42, P1, DOI 10.2307/3800685
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Finley K.J., 1990, Canadian Bulletin of Fisheries and Aquatic Sciences, V224, P97
   Frouin-Mouy H, 2017, ARCTIC, V70, P59, DOI 10.14430/arctic4632
   Gerrodette T, 2019, MAR MAMMAL SCI, V35, P22, DOI 10.1111/mms.12506
   Goebel ME, 2015, POLAR BIOL, V38, P619, DOI 10.1007/s00300-014-1625-4
   Heide-Jorgensen M. P., 2015, Journal of Cetacean Research and Management, V15, P1
   Heide-Jorgensen MP, 2016, POLAR BIOL, V39, P1605, DOI 10.1007/s00300-015-1885-7
   Heide-Jorgensen MP, 2010, J MAMMAL, V91, P1135, DOI 10.1644/09-MAMM-A-198.1
   Heide-Jorgensen M. P., 2004, MAR MAMMAL SCI, V20, P58, DOI DOI 10.1111/J.1748-7692.2004.TB01154.X
   Heide-JOrgensen M. P., 2013, MONITORING ABUNDANCE
   Heide-JOrgensen M. P., 2014, ABUNDANCE DISTRIBUTI
   Heide-Jorgensen MP, 2013, BIOL CONSERV, V158, P50, DOI 10.1016/j.biocon.2012.08.005
   Heide-Jorgensen MP, 2003, CAN J ZOOL, V81, P1298, DOI 10.1139/Z03-117
   Heide-Jorgensen MP, 2002, POLAR BIOL, V25, P331, DOI 10.1007/s00300-001-0347-6
   Heide-Jorgensen MP, 2002, SCI PUBLICATIONS N A, V4, P191, DOI DOI 10.7557/3.2844
   HEIDEJORGENSEN MP, 1993, CAN J FISH AQUAT SCI, V50, P2323, DOI 10.1139/f93-257
   Henkel Laird A., 2007, Marine Ornithology, V35, P145
   Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
   Hodgson A, 2017, ECOL APPL, V27, P1253, DOI 10.1002/eap.1519
   Innes Stuart, 2002, NAMMCO Scientific Publications, V4, P169
   Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
   Koski WR, 2015, J UNMANNED VEH SYST, V3, P22, DOI 10.1139/juvs-2014-0014
   Koski WR, 2013, J UNMANNED VEH SYST, V1, P25, DOI 10.1139/juvs-2013-0015
   Laake JL, 2004, ADVANCED DISTANCE SAMPLING: ESTIMATING ABUNDANCE OF BIOLOGICAL POPULATIONS, P108
   Lowry MS, 1999, MAR MAMMAL SCI, V15, P143, DOI 10.1111/j.1748-7692.1999.tb00786.x
   Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
   Marques FFC, 2004, ADVANCED DISTANCE SAMPLING: ESTIMATING ABUNDANCE OF BIOLOGICAL POPULATIONS, P31
   Martin SB, 2017, J ACOUST SOC AM, V142, P3331, DOI 10.1121/1.5014049
   Mathews E. A., 1995, P 3 GLAC BAY SCI S U, P254
   Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
   Norton-Griffiths M., 1974, East African Wildlife Journal, V12, P245
   Patterson C, 2016, J UNMANNED VEH SYST, V4, P53, DOI 10.1139/juvs-2015-0014
   Pike D., 2015, 2015034 DFO CSAS
   Ratcliffe N, 2015, J UNMANNED VEH SYST, V3, P95, DOI 10.1139/juvs-2015-0006
   Richardson W.J., 1995, MARINE MAMMALS NOISE
   Scott M.D., 1985, Inter-American Tropical Tuna Commission Bulletin, V18, P381
   Smith CE, 2016, J UNMANNED VEH SYST, V4, P31, DOI 10.1139/juvs-2015-0017
   Stewart R. E., 2013, SCI PUBLICATIONS N A, V9, P95
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Wichmann FA, 2010, J VISION, V10, DOI 10.1167/10.4.6
   Williams R, 2017, ENDANGER SPECIES RES, V34, P149, DOI 10.3354/esr00845
NR 50
TC 10
Z9 10
U1 0
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0824-0469
EI 1748-7692
J9 MAR MAMMAL SCI
JI Mar. Mamm. Sci.
PD OCT
PY 2019
VL 35
IS 4
BP 1253
EP 1279
DI 10.1111/mms.12586
PG 27
WC Marine & Freshwater Biology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Marine & Freshwater Biology; Zoology
GA JH0SS
UT WOS:000492484700003
OA hybrid, Green Published
DA 2022-02-10
ER

EF