FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Kutugata, M
   Baumgardt, J
   Goolsby, JA
   Racelis, AE
AF Kutugata, Matthew
   Baumgardt, Jeremy
   Goolsby, John A.
   Racelis, Alexis E.
TI Automatic Camera-Trap Classification Using Wildlife-Specific Deep
   Learning in Nilgai Management
SO JOURNAL OF FISH AND WILDLIFE MANAGEMENT
LA English
DT Article
DE Boselaphus tragocamelus; camera trap; cattle fever ticks; deep learning;
   nilgai; transfer learning
AB Camera traps provide a low-cost approach to collect data and monitor wildlife across large scales but hand-labeling images at a rate that outpaces accumulation is difficult. Deep learning, a subdiscipline of machine learning and computer science, can address the issue of automatically classifying camera-trap images with a high degree of accuracy. This technique, however, may be less accessible to ecologists or small-scale conservation projects, and has serious limitations. In this study, we trained a simple deep learning model using a dataset of 120,000 images to identify the presence of nilgai Boselaphus tragocamelus, a regionally specific nonnative game animal, in camera-trap images with an overall accuracy of 97%. We trained a second model to identify 20 groups of animals and one group of images without any animals present, labeled as "none,'' with an accuracy of 89%. Lastly, we tested the multigroup model on images collected of similar species, but in the southwestern United States, resulting in significantly lower precision and recall for each group. This study highlights the potential of deep learning for automating camera-trap image processing workflows, provides a brief overview of image-based deep learning, and discusses the often-understated limitations and methodological considerations in the context of wildlife conservation and species monitoring.
C1 [Kutugata, Matthew; Baumgardt, Jeremy] Univ Texas Rio Grande Valley, Sch Earth Environm & Marine Sci, 1201 W Univ Dr, Edinburg, TX 78539 USA.
   [Goolsby, John A.] Texas A&M Univ, Caesar Kleberg Wildlife Res Inst, Kingsville, TX 78363 USA.
   [Racelis, Alexis E.] ARS, USDA, Knipling Bushland US Livestock Insects Res Lab, Cattle Fever Tick Res Lab, Edinburg, TX 78541 USA.
RP Kutugata, M (corresponding author), Univ Texas Rio Grande Valley, Sch Earth Environm & Marine Sci, 1201 W Univ Dr, Edinburg, TX 78539 USA.
FU Integrated Pest Management of Cattle Fever Ticks [3094-32000-042-00-D];
   U.S. Department of Agriculture National Institute of Food and
   AgricultureUnited States Department of Agriculture (USDA)
   [2016-38422-25543]
FX Game camera images and initial processing was supported through
   appropriated research project 3094-32000-042-00-D, Integrated Pest
   Management of Cattle Fever Ticks. This article reports results of
   research only and mention of a proprietary product does not constitute
   an endorsement or recommendation by the U.S. Department of Agriculture
   for its use. U.S. Department of Agriculture is an equal opportunity
   provider and employer. Special thanks to Amelia Berle for data
   management, and research technicians who spent countless hours labeling
   images. Additional thanks to Dr. Rupesh Kariyat and Dr. Christofferson
   for providing access to computing equipment. We would also like to thank
   the journal reviewers and Associate Editor for their commitment to open
   access, which ensures applied conservation science remains accessible to
   all. Matthew Kutugata was supported by U.S. Department of Agriculture
   National Institute of Food and Agriculture Grant 2016-38422-25543.
CR Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Chollet F., 2018, DEEP LEARNING PYTHON
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Foley AM, 2017, PREV VET MED, V146, P166, DOI 10.1016/j.prevetmed.2017.08.002
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Goolsby J.G., 2019, SUBTROPICAL AGR ENV, V70, P1
   Guilford J.P., 1954, PSYCHOMETRIC METHODS, V2d ed
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Ivan JS, 2016, METHODS ECOL EVOL, V7, P499, DOI 10.1111/2041-210X.12503
   Leslie D.M., 2016, INT BORDERLAND CONCE, P136
   Leslie David M. Jr., 2008, Mammalian Species, DOI 10.1644/813.1
   Lohmeyer KH, 2018, J MED ENTOMOL, V55, P515, DOI 10.1093/jme/tjy004
   Azlan JM, 2006, RAFFLES B ZOOL, V54, P469
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Rovero Francesco, 2006, Journal of East African Natural History, V95, P111, DOI 10.2982/0012-8317(2006)95[111:APNGSE]2.0.CO;2
   Schmidly DJ, 2004, MAMMALS TEXAS
   Srbek-Araujo AC, 2005, J TROP ECOL, V21, P121, DOI 10.1017/S0266467404001956
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Toda Y, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/9237136
   Ueda K., 2021, INATURALIST RES GRAD, DOI [10.15468/ab3s5x, DOI 10.15468/AB3S5X]
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
NR 27
TC 0
Z9 0
U1 0
U2 0
PU U S FISH & WILDLIFE SERVICE
PI SHEPHERDSTOWN
PA NATL CONSERVATION TRAINING CENTER, CONSERVATION LIBRARY, 698
   CONSERVATION WAY, SHEPHERDSTOWN, WV 25443 USA
SN 1944-687X
J9 J FISH WILDL MANAG
JI J. Fish Wildl. Manag.
PD DEC
PY 2021
VL 12
IS 2
BP 412
EP 421
DI 10.3996/JFWM-20-076
PG 10
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA XL2ZP
UT WOS:000728017600001
OA gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Tekeli, U
   Bastanlar, Y
AF Tekeli, Ulas
   Bastanlar, Yalin
TI Elimination of useless images from raw camera-trap data
SO TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES
LA English
DT Article
DE Camera-trap; image processing; computer vision; object detection;
   background subtraction; convolutional neural networks; deep learning
ID MANAGEMENT; SOFTWARE
AB Camera-traps are motion triggered cameras that are used to observe animals in nature. The number of images collected from camera-traps has increased significantly with the widening use of camera-traps thanks to advances in digital technology. A great workload is required for wild-life researchers to group and label these images. We propose a system to decrease the amount of time spent by the researchers by eliminating useless images from raw camera-trap data. These images are too bright, too dark, blurred, or they contain no animals To eliminate bright, dark, and blurred images we employ techniques based on image histograms and fast Fourier transform. To eliminate the images without animals, we propose a system combining convolutional neural networks and background subtraction. We experimentally show that the proposed approach keeps 99% of photos with animals while eliminating more than 50% of photos without animals. We also present a software prototype that employs developed algorithms to eliminate useless images.
C1 [Tekeli, Ulas; Bastanlar, Yalin] Izmir Inst Technol, Comp Engn Dept, Izmir, Turkey.
RP Bastanlar, Y (corresponding author), Izmir Inst Technol, Comp Engn Dept, Izmir, Turkey.
EM yalinbastanlar@iyte.edu.tr
RI Bastanlar, Yalin/AAA-7114-2022
OI Tekeli, Ulas/0000-0003-0492-3059
FU Scientific and Technological Research Council of Turkey (TUBITAK)Turkiye
   Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK) [115E918]; NVIDIA
   Corporation
FX This work was supported by the Scientific and Technological Research
   Council of Turkey (TUBITAK) (Grant no. 115E918). We are grateful to
   Republic of Turkey, Ministry of Forest and Water Affairs for sharing the
   camera-trap dataset. We also acknowledge the support of NVIDIA
   Corporation with the donation of the GPU used for this research.
CR Boom BJ, 2014, ECOL INFORM, V23, P83, DOI 10.1016/j.ecoinf.2013.10.006
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Dosselmann RW, 2012, TECHNICAL REPORT
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hernandez-Serna A, 2014, PEERJ, V2, DOI 10.7717/peerj.563
   Islam J, 2017, MACH LEARN HLTH WORK
   Ju C, 2018, J APPL STAT, V45, P2800, DOI 10.1080/02664763.2018.1441383
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Orhan S, 2018, ELECTRON LETT, V54, P424, DOI 10.1049/el.2017.4725
   Pavlovic G, 1992, IEEE T IMAGE PROCESS, V1, P496, DOI 10.1109/83.199919
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Song DZ, 2010, IEEE T IMAGE PROCESS, V19, P2321, DOI 10.1109/TIP.2010.2048151
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tong HH, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P17, DOI 10.1109/ICME.2004.1394114
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 30
TC 1
Z9 1
U1 3
U2 9
PU TUBITAK SCIENTIFIC & TECHNICAL RESEARCH COUNCIL TURKEY
PI ANKARA
PA ATATURK BULVARI NO 221, KAVAKLIDERE, ANKARA, 00000, TURKEY
SN 1300-0632
EI 1303-6203
J9 TURK J ELECTR ENG CO
JI Turk. J. Electr. Eng. Comput. Sci.
PY 2019
VL 27
IS 4
BP 2395
EP 2411
DI 10.3906/elk-1808-130
PG 17
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA IT3HG
UT WOS:000482742800002
OA Bronze, Green Submitted
DA 2022-02-10
ER

PT J
AU Green, SE
   Rees, JP
   Stephens, PA
   Hill, RA
   Giordano, AJ
AF Green, Sian E.
   Rees, Jonathan P.
   Stephens, Philip A.
   Hill, Russell A.
   Giordano, Anthony J.
TI Innovations in Camera Trapping Technology and Approaches: The
   Integration of Citizen Science and Artificial Intelligence
SO ANIMALS
LA English
DT Review
DE camera trapping; citizen science; artificial intelligence; engagement;
   camera traps; public awareness; data processing; conservation technology
ID CONSERVATION; BIODIVERSITY; MOTIVATIONS; SCIENTISTS; BEHAVIOR; NETWORK;
   TRAPS; EXTINCTION; EXPERIENCE; ATTITUDES
AB Simple Summary Camera traps, also known as "game cameras" or "trail cameras", have increasingly been used in wildlife research over the last 20 years. Although early units were bulky and the set-up was complicated, modern camera traps are compact, integrated units able to collect vast digital datasets. Some of the challenges now facing researchers include the time required to view, classify, and sort all of the footage collected, as well as the logistics of establishing and maintaining camera trap sampling arrays across wide geographic areas. One solution to this problem is to enlist or recruit the public for help as 'citizen scientists' collecting and processing data. Artificial Intelligence (AI) is also being used to identify animals in digital photos and video; however, this process is relatively new, and machine-based classifications are not yet fully reliable. By combining citizen science with AI, it should be possible to improve efficiency and increase classification accuracy, while simultaneously maintaining and promoting the benefits associated with public engagement with, and awareness of, wildlife.
   Abstract Camera trapping has become an increasingly reliable and mainstream tool for surveying a diversity of wildlife species. Concurrent with this has been an increasing effort to involve the wider public in the research process, in an approach known as 'citizen science'. To date, millions of people have contributed to research across a wide variety of disciplines as a result. Although their value for public engagement was recognised early on, camera traps were initially ill-suited for citizen science. As camera trap technology has evolved, cameras have become more user-friendly and the enormous quantities of data they now collect has led researchers to seek assistance in classifying footage. This has now made camera trap research a prime candidate for citizen science, as reflected by the large number of camera trap projects now integrating public participation. Researchers are also turning to Artificial Intelligence (AI) to assist with classification of footage. Although this rapidly-advancing field is already proving a useful tool, accuracy is variable and AI does not provide the social and engagement benefits associated with citizen science approaches. We propose, as a solution, more efforts to combine citizen science with AI to improve classification accuracy and efficiency while maintaining public involvement.
C1 [Green, Sian E.; Hill, Russell A.] Univ Durham, Dept Anthropol, Durham DH1 3LE, England.
   [Green, Sian E.; Rees, Jonathan P.; Stephens, Philip A.] Univ Durham, Conservat Ecol Grp, Dept Biosci, Durham DH1 3LE, England.
   [Green, Sian E.; Giordano, Anthony J.] SPECIES, Ventura, CA 93006 USA.
RP Green, SE (corresponding author), Univ Durham, Dept Anthropol, Durham DH1 3LE, England.; Green, SE (corresponding author), Univ Durham, Conservat Ecol Grp, Dept Biosci, Durham DH1 3LE, England.; Green, SE (corresponding author), SPECIES, Ventura, CA 93006 USA.
EM sian.e.green@durham.ac.uk; jonathan.p.rees@durham.ac.uk;
   philip.stephens@durham.ac.uk; r.a.hill@durham.ac.uk;
   speciesl@hotmail.com
RI Stephens, Philip/B-8397-2008; Hill, Russell/D-9113-2013
OI Stephens, Philip/0000-0001-5849-788X; Hill, Russell/0000-0002-7601-5802;
   Green, Sian/0000-0003-0513-8490
FU NERC Iapetus Doctoral Training Partnership; NERCUK Research & Innovation
   (UKRI)Natural Environment Research Council (NERC) [NE/R008485/1]
FX Funding for this research was provided through a NERC Iapetus Doctoral
   Training Partnership and NERC Training Grant number NE/R008485/1.
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ballard HL, 2017, BIOL CONSERV, V208, P65, DOI 10.1016/j.biocon.2016.05.024
   Barrueto M, 2014, ECOSPHERE, V5, DOI 10.1890/ES13-00382.1
   Beery Sara, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11220), P472, DOI 10.1007/978-3-030-01270-0_28
   Bonney R., 2009, PUBLIC PARTICIPATION
   Bowley C, 2019, J COMPUT SCI-NETH, V34, P102, DOI 10.1016/j.jocs.2019.04.010
   Bowser A., 2013, P 1 INT C GAM DES RE, P18, DOI DOI 10.1145/2583008.2583011
   Bratman GN, 2015, LANDSCAPE URBAN PLAN, V138, P41, DOI 10.1016/j.landurbplan.2015.02.005
   Brower M, 2008, HIST PHOTOGR, V32, P169, DOI 10.1080/03087290801895761
   Burgess HK, 2017, BIOL CONSERV, V208, P113, DOI 10.1016/j.biocon.2016.05.014
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Catlin-Groves C.L., 2012, INT J ZOOL, V2012, P1, DOI [10.1155/2012/349630, DOI 10.1155/2012/349630]
   Chandler M, 2017, BIOL CONSERV, V213, P280, DOI 10.1016/j.biocon.2016.09.004
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Cox DTC, 2017, INT J ENV RES PUB HE, V14, DOI 10.3390/ijerph14020172
   Cox DTC, 2017, LANDSCAPE URBAN PLAN, V160, P79, DOI 10.1016/j.landurbplan.2016.12.006
   Curtis V, 2015, SCI COMMUN, V37, P723, DOI 10.1177/1075547015609322
   Dayer AA, 2019, PEOPLE NAT, V1, P138, DOI 10.1002/pan3.17
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Domroese MC, 2017, BIOL CONSERV, V208, P40, DOI 10.1016/j.biocon.2016.08.020
   Dorji S, 2019, BIODIVERS CONSERV, V28, P3277, DOI 10.1007/s10531-019-01821-9
   Eaton DP, 2017, BIOL CONSERV, V208, P29, DOI 10.1016/j.biocon.2016.09.010
   Eitzel M. V., 2017, CITIZ SCI THEORY PRA, V2, P1, DOI [10.5334/cstp.96, DOI 10.5334/CSTP.96]
   Ellwood ER, 2017, BIOL CONSERV, V208, P1, DOI 10.1016/j.biocon.2016.10.014
   Engemann K, 2019, P NATL ACAD SCI USA, V116, P5188, DOI 10.1073/pnas.1807504116
   Evans C, 2005, CONSERV BIOL, V19, P589, DOI 10.1111/j.1523-1739.2005.00s01.x
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Forrester TD, 2017, BIOL CONSERV, V208, P98, DOI 10.1016/j.biocon.2016.06.025
   Ganzevoort W, 2017, BIODIVERS CONSERV, V26, P2821, DOI 10.1007/s10531-017-1391-z
   Genovart M, 2013, BIOL CONSERV, V159, P484, DOI 10.1016/j.biocon.2012.10.028
   Geoghegan H., 2016, UNDERSTANDING MOTIVA
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Griffiths Mike, 1993, Tropical Biodiversity, V1, P131
   Haklay M, 2013, CROWDSOURCING GEOGRA, P105, DOI DOI 10.1007/978-94-007-4587-2_7
   Haywood BK, 2016, CONSERV BIOL, V30, P476, DOI 10.1111/cobi.12702
   Hobbs SJ, 2012, J NAT CONSERV, V20, P364, DOI 10.1016/j.jnc.2012.08.002
   Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
   Jones M., 2013, IMPACT CITIZEN SCI A
   Kammerle JL, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0207545
   Kalan AK, 2019, CURR BIOL, V29, P1211, DOI 10.1016/j.cub.2019.02.024
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Koivuniemi M, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214269
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Lewandowski EJ, 2017, BIOL CONSERV, V208, P106, DOI 10.1016/j.biocon.2015.07.029
   Lintott CJ, 2008, MON NOT R ASTRON SOC, V389, P1179, DOI 10.1111/j.1365-2966.2008.13689.x
   MacKenzie CM, 2017, BIOL CONSERV, V208, P121, DOI 10.1016/j.biocon.2016.07.027
   Masters Karen, 2016, J SCI COMMUNICATION, P1, DOI DOI 10.22323/2.15030207
   McKie R., GUARDIAN
   Meek PD, 2012, WILDLIFE RES, V39, P649, DOI 10.1071/WR12138
   Meek PD, 2015, AUST MAMMAL, V37, P1, DOI 10.1071/AM14021
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien L, 2010, VOLUNTAS, V21, P525, DOI 10.1007/s11266-010-9149-1
   OED, 2019, CIT N ADJ
   Orta-Martinez M, 2018, ENVIRON RES, V160, P514, DOI 10.1016/j.envres.2017.10.009
   Parsons AW, 2018, ELIFE, V7, DOI 10.7554/eLife.38012
   Paxton AB, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2687
   Pierce J., 2013, P 2013 C COMP SUPP C, P1463, DOI [10.1145/2441776.2441941, DOI 10.1145/2441776.2441941]
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Rotman D., 2014, ICONFERENCE 2014 P, P110, DOI DOI 10.9776/14054
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2017, SCI TOTAL ENVIRON, V574, P914, DOI 10.1016/j.scitotenv.2016.09.146
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rovero Francesco, 2010, Abc Taxa, V8, P100
   Roy H. E., 2012, UNDERSTANDING CITIZE
   Schuttler SG, 2019, BIOSCIENCE, V69, P69, DOI 10.1093/biosci/biy141
   Schuttler SG, 2018, FRONT ECOL ENVIRON, V16, P405, DOI 10.1002/fee.1826
   Scotson L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185336
   SEYDACK AHW, 1984, S AFR J WILDL RES, V14, P10
   Shirk JL, 2012, ECOL SOC, V17, DOI 10.5751/ES-04705-170229
   Soga M, 2016, FRONT ECOL ENVIRON, V14, P94, DOI 10.1002/fee.1225
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tanner D., 2010, Human Dimensions of Wildlife, V15, P418, DOI 10.1080/10871209.2010.503236
   Thapa K, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216504
   Tinati R, 2017, COMPUT HUM BEHAV, V73, P527, DOI 10.1016/j.chb.2016.12.074
   Toomey AH, 2013, HUM ECOL REV, V20, P50
   Trnovszky T, 2017, ADV ELECTR ELECTRON, V15, P517, DOI 10.15598/aeee.v15i3.2202
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Vernes K, 2014, AUST MAMMAL, V36, P128, DOI 10.1071/AM13037
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wearn OR, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181748
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Welbourne DJ, 2016, REMOTE SENS ECOL CON, V2, P77, DOI 10.1002/rse2.20
   Welbourne DJ, 2015, WILDLIFE RES, V42, P414, DOI 10.1071/WR15054
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Williams ST, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.161090
NR 92
TC 12
Z9 13
U1 10
U2 19
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD JAN
PY 2020
VL 10
IS 1
AR 132
DI 10.3390/ani10010132
PG 16
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Agriculture; Veterinary Sciences; Zoology
GA KO2FZ
UT WOS:000515364400132
PM 31947586
OA Green Accepted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Janzen, M
   Visser, K
   Visscher, D
   MacLeod, I
   Vujnovic, D
   Vujnovic, K
AF Janzen, Michael
   Visser, Kaitlyn
   Visscher, Darcy
   MacLeod, Ian
   Vujnovic, Dragomir
   Vujnovic, Ksenija
TI Semi-automated camera trap image processing for the detection of
   ungulate fence crossing events
SO ENVIRONMENTAL MONITORING AND ASSESSMENT
LA English
DT Article
DE Camera trap; Computer vision; Monitoring; Remote camera; Image
   processing
ID DENSITY-ESTIMATION; IDENTIFICATION
AB Remote cameras are an increasingly important tool for ecological research. While remote camera traps collect field data with minimal human attention, the images they collect require post-processing and characterization before it can be ecologically and statistically analyzed, requiring the input of substantial time and money from researchers. The need for post-processing is due, in part, to a high incidence of non-target images. We developed a stand-alone semi-automated computer program to aid in image processing, categorization, and data reduction by employing background subtraction and histogram rules. Unlike previous work that uses video as input, our program uses still camera trap images. The program was developed for an ungulate fence crossing project and tested against an image dataset which had been previously processed by a human operator. Our program placed images into categories representing the confidence of a particular sequence of images containing a fence crossing event. This resulted in a reduction of 54.8% of images that required further human operator characterization while retaining 72.6% of the known fence crossing events. This program can provide researchers using remote camera data the ability to reduce the time and cost required for image post-processing and characterization. Further, we discuss how this procedure might be generalized to situations not specifically related to animal use of linear features.
C1 [Janzen, Michael; Visser, Kaitlyn; Visscher, Darcy; MacLeod, Ian] Kings Univ, Edmonton, AB, Canada.
   [Vujnovic, Dragomir; Vujnovic, Ksenija] Alberta Pk, Edmonton, AB, Canada.
RP Janzen, M (corresponding author), Kings Univ, Edmonton, AB, Canada.
EM Michael.Janzen@kingsu.ca; Darcy.Visscher@kingsu.ca
FU King's University; King's Centre for Visualization in Science; Alberta
   Parks
FX We thank Alberta Parks for funding for this project and for helping with
   field setup and data collection and the King's University and the King's
   Centre for Visualization in Science for funding and support. This work,
   in part, comes from the undergraduate research projects of K. Visser and
   I. MacLeod. We also thank Dr. Jose Alexander Elvir and an anonymous
   reviewer for their comments.
CR Anderson CJR, 2010, J MAMMAL, V91, P1350, DOI 10.1644/09-MAMM-A-425.1
   Ardekani R, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-61
   Bubnicki J. W., 2016, METHODS ECOLOGY EVOL
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Forrester Tavis, 2013, P 98 ESA ANN CONV 20
   Goehner K, 2015, P IEEE INT C E-SCI, P187, DOI 10.1109/eScience.2015.10
   Gonzalez R.C., 2007, DIGITAL IMAGE PROCES
   Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
   Jhala YV, 2009, BIODIVERS CONSERV, V18, P3383, DOI 10.1007/s10531-009-9648-9
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Osterrieder SK, 2015, J MAMMAL, V96, P988, DOI 10.1093/jmammal/gyv102
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Siren APK, 2016, DIVERSITY-BASEL, V8, DOI 10.3390/d8010003
   Spampinato C, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2015-1
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Visscher DR, 2017, WILDLIFE SOC B, V41, P162, DOI 10.1002/wsb.741
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 21
TC 3
Z9 3
U1 0
U2 11
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0167-6369
EI 1573-2959
J9 ENVIRON MONIT ASSESS
JI Environ. Monit. Assess.
PD OCT
PY 2017
VL 189
IS 10
AR 527
DI 10.1007/s10661-017-6206-x
PG 13
WC Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA FK2ZW
UT WOS:000413354400015
PM 28956203
DA 2022-02-10
ER

PT J
AU Shepley, A
   Falzon, G
   Meek, P
   Kwan, P
AF Shepley, Andrew
   Falzon, Greg
   Meek, Paul
   Kwan, Paul
TI Automated location invariant animal detection in camera trap images
   using publicly available data sources
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal identification; artificial intelligence; camera trap images;
   camera trapping; deep convolutional neural networks; deep learning;
   infusion; location invariance; wildlife ecology; wildlife monitoring
AB A time-consuming challenge faced by camera trap practitioners is the extraction of meaningful data from images to inform ecological management. An increasingly popular solution is automated image classification software. However, most solutions are not sufficiently robust to be deployed on a large scale due to lack of location invariance when transferring models between sites. This prevents optimal use of ecological data resulting in significant expenditure of time and resources to annotate and retrain deep learning models.
   We present a method ecologists can use to develop optimized location invariant camera trap object detectors by (a) evaluating publicly available image datasets characterized by high intradataset variability in training deep learning models for camera trap object detection and (b) using small subsets of camera trap images to optimize models for high accuracy domain-specific applications.
   We collected and annotated three datasets of images of striped hyena, rhinoceros, and pigs, from the image-sharing websites FlickR and iNaturalist (FiN), to train three object detection models. We compared the performance of these models to that of three models trained on the Wildlife Conservation Society and Camera CATalogue datasets, when tested on out-of-sample Snapshot Serengeti datasets. We then increased FiN model robustness by infusing small subsets of camera trap images into training.
   In all experiments, the mean Average Precision (mAP) of the FiN trained models was significantly higher (82.33%-88.59%) than that achieved by the models trained only on camera trap datasets (38.5%-66.74%). Infusion further improved mAP by 1.78%-32.08%.
   Ecologists can use FiN images for training deep learning object detection solutions for camera trap image processing to develop location invariant, robust, out-of-the-box software. Models can be further optimized by infusion of 5%-10% camera trap images into training data. This would allow AI technologies to be deployed on a large scale in ecological applications. Datasets and code related to this study are open source and available on this repository: .
C1 [Shepley, Andrew] Univ New England, Sch Sci & Technol, Armidale, NSW, Australia.
   [Falzon, Greg] Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA, Australia.
   [Meek, Paul] NSW Dept Primary Ind, Vertebrate Pest Res Unit, Coffs Harbour, NSW, Australia.
   [Meek, Paul] Univ New England, Sch Environm & Rural Sci, Armidale, NSW, Australia.
   [Kwan, Paul] Melbourne Inst Technol, Sch IT & Engn, Melbourne, Vic, Australia.
RP Shepley, A (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW, Australia.
EM asheple2@une.edu.au
OI Falzon, Gregory/0000-0002-1989-9357; Shepley, Andrew/0000-0001-7511-4967
FU University of New England; Australian Department of Agriculture and
   Water ResourcesAustralian Government; NSW Department of Primary
   Industries; NSW Environmental Trust; Centre for Invasive Animals
   Solutions
FX Centre for Invasive Animals Solutions; University of New England;
   Australian Department of Agriculture and Water Resources; NSW Department
   of Primary Industries; NSW Environmental Trust
CR Aradhya H.V.R., 2018, 2018 INT C COMM SIGN
   Beery S, 2018, RECOGNITION TERRA IN
   Christensen JH, 2018, 2018 IEEE/OES AUTONOMOUS UNDERWATER VEHICLE WORKSHOP (AUV)
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Clune, 2017, P NATL ACAD SCI USA, V115
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Gibb R, 2019, METHODS ECOL EVOL, V10, P169, DOI 10.1111/2041-210X.13101
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Kellenberger B, 2017, JOINT URB REMOTE SEN
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Maurice, 2019, SURV STAT PANG CAM T
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28
   Rodin CD, 2018, IEEE IJCNN
   Rovero F., 2016, CAMERA TRAPPING WILD
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Singh P., 2020, 2020 IEEE SW S IM AN
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tambe M., 2020, 2020 IEEE WINT C APP
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   Vedaldi, 2017, LEARNING MULTIPLE VI, P506
   Wang G., 2017, 2017 IEEE WINT C APP
   Wang X., 2019, 2019 IEEE CVF C COMP
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wearn OR, 2017, WWF CONSERVATION TEC, V1
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xu BB, 2020, INT J REMOTE SENS, V41, P8121, DOI 10.1080/01431161.2020.1734245
   Yang XY, 2019, IEEE INT CONF COMP V, P255, DOI 10.1109/ICCVW.2019.00034
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zisserman, 2007, DATASET ISSUES OBJEC, V4170, P29
NR 48
TC 1
Z9 1
U1 9
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD MAY
PY 2021
VL 11
IS 9
BP 4494
EP 4506
DI 10.1002/ece3.7344
EA MAR 2021
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA RW6CD
UT WOS:000626984400001
PM 33976825
OA Green Published, gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Howe, EJ
   Buckland, ST
   Despres-Einspenner, ML
   Kuhl, HS
AF Howe, Eric J.
   Buckland, Stephen T.
   Despres-Einspenner, Marie-Lyne
   Kuehl, Hjalmar S.
TI Distance sampling with camera traps
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal abundance; camera trapping; density; distance sampling; Maxwell's
   duiker
ID LINE TRANSECT TECHNIQUES; UDZUNGWA MOUNTAINS; ACTIVITY PATTERNS;
   DENSITY; DUNG; POPULATIONS; ABUNDANCE; UNGULATE; RANGE; NEED
AB 1. Reliable estimates of animal density and abundance are essential for effective wildlife conservation and management. Camera trapping has proven efficient for sampling multiple species, but statistical estimators of density from camera trapping data for species that cannot be individually identified are still in development.
   2. We extend point-transect methods for estimating animal density to accommodate data from camera traps, allowing researchers to exploit existing distance sampling theory and software for designing studies and analysing data. We tested it by simulation, and used it to estimate densities of Maxwell's duikers (Philantomba maxwellii) in Tai National Park, Cote d'Ivoire.
   3. Densities estimated from simulated data were unbiased when we assumed animals were not available for detection during long periods of rest. Estimated duiker densities were higher than recent estimates from line transect surveys, which are believed to underestimate densities of forest ungulates.
   4. We expect these methods to provide an effective means to estimate animal density from camera trapping data and to be applicable in a variety of settings.
C1 [Howe, Eric J.; Buckland, Stephen T.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews KY16 9LZ, Fife, Scotland.
   [Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Deutsch Pl 5e, D-04103 Leipzig, Germany.
RP Howe, EJ (corresponding author), Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews KY16 9LZ, Fife, Scotland.
EM ejh20@st-andrews.ac.uk
RI Buckland, Stephen T/A-1998-2012
OI Buckland, Stephen/0000-0002-9939-709X
FU Robert Bosch Foundation; Max Planck SocietyMax Planck SocietyFoundation
   CELLEX; University of St Andrews
FX We thank the Robert Bosch Foundation, the Max Planck Society and the
   University of St Andrews for funding, the Ministere de l'Enseignement
   Superieur et de la Recherche Scientifique and the Ministere de
   l'Environnement et des Eaux et Forets in Cote d'Ivoire for permission to
   conduct field research in Taii National Park, and Dr. Roman Wittig for
   permitting data collection in the area of the Taii Chimpanzee Project.
CR Balestrieri A, 2016, MAMM BIOL, V81, P439, DOI 10.1016/j.mambio.2016.05.005
   Bowkett AE, 2009, CONSERV GENET, V10, P251, DOI 10.1007/s10592-008-9564-7
   Buckland S.T., 2001, pi
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   BUCKLAND ST, 1984, BIOMETRICS, V40, P811, DOI 10.2307/2530926
   Buckland ST., 2004, ADV DISTANCE SAMPLIN
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Cruz P, 2014, MAMM BIOL, V79, P376, DOI 10.1016/j.mambio.2014.06.003
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   Denes FV, 2015, METHODS ECOL EVOL, V6, P543, DOI 10.1111/2041-210X.12333
   Despres-Einspenner M. -L., 2017, AM J PRIMATOLOGY
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Howe E. J., 2017, DRYAD DIGITAL REPOSI
   Jathanna D, 2003, J ZOOL, V261, P285, DOI 10.1017/S0952836903004278
   KOSTER SH, 1988, AFR J ECOL, V26, P117, DOI 10.1111/j.1365-2028.1988.tb00962.x
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Laake JL, 2011, J AGR BIOL ENVIR ST, V16, P389, DOI 10.1007/s13253-011-0059-5
   Le Saout S, 2014, WILDLIFE BIOL, V20, P122, DOI 10.2981/wlb.13048
   Lynam AJ, 2013, RAFFLES B ZOOL, V61, P407
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   Marini F, 2009, EUR J WILDLIFE RES, V55, P107, DOI 10.1007/s10344-008-0222-7
   Marques TA, 2010, BIOMETRICS, V66, P1247, DOI 10.1111/j.1541-0420.2009.01381.x
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Marshall AR, 2008, AM J PRIMATOL, V70, P452, DOI 10.1002/ajp.20516
   N'Goran P. K., 2006, QUELQUES RESULTATS P
   Newing H, 2001, BIODIVERS CONSERV, V10, P99, DOI 10.1023/A:1016671524034
   Newing HS, 1994, THESIS
   Plumptre AJ, 2000, J APPL ECOL, V37, P356, DOI 10.1046/j.1365-2664.2000.00499.x
   Rovero F, 2004, TROP ZOOL, V17, P267, DOI 10.1080/03946975.2004.10531208
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   Sollmann R, 2013, BIOL CONSERV, V159, P405, DOI 10.1016/j.biocon.2012.12.025
   Sunarto, 2013, RAFFLES B ZOOL, P21
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Todd AF, 2008, INT J PRIMATOL, V29, P549, DOI 10.1007/s10764-008-9247-8
   Wearn OR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077598
   Zero VH, 2013, ORYX, V47, P410, DOI 10.1017/S0030605312000324
NR 47
TC 72
Z9 73
U1 12
U2 82
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD NOV
PY 2017
VL 8
IS 11
BP 1558
EP 1565
DI 10.1111/2041-210X.12790
PG 8
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA FM1AK
UT WOS:000414701900019
OA Green Accepted, Bronze
DA 2022-02-10
ER

PT C
AU Zualkernan, IA
   Dhou, S
   Judas, J
   Sajun, AR
   Gomez, BR
   Hussain, LA
   Sakhnini, D
AF Zualkernan, Imran A.
   Dhou, Salam
   Judas, Jacky
   Sajun, Ali Reza
   Gomez, Brylle Ryan
   Hussain, Lana Alhaj
   Sakhnini, Dara
GP IEEE
TI Towards an IoT-based Deep Learning Architecture for Camera Trap Image
   Classification
SO 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF
   THINGS (GCAIOT)
LA English
DT Proceedings Paper
CT IEEE Global Conference on Artificial Intelligence and Internet of Things
   (GCAIoT)
CY DEC 12-16, 2020
CL ELECTR NETWORK
SP IEEE
DE deep learning; transfer learning; convolutional neural networks; animal
   classification; camera trap; wildlife monitoring; edge computing;
   TensorFlow lite; raspberry pi; IoT
AB Maintaining biodiversity is a key component of the United Nations (UN) "Life on Land" sustainability goal. Remote camera traps monitoring animals' movements support research in biodiversity. However, images from these camera traps are currently labeled manually resulting in high processing costs and long delays. This paper proposes an IoT-based system that leverages deep learning and edge computing to automatically label camera trap images and transmit this information to scientists in a timely manner. Inception-V3, MobileNet-V2, ResNet-18, and DenseNet-121 were trained on data consisting of 33,984 images taken during day and night with 6 animal classes. Inception-V3 yielded the highest macro average Fl-score of 0.93 and an accuracy of 94%. An IoT-based system was developed that directly captures images from a commercial camera trap, does the inference on the edge using a Raspberry Pi (RPi), and sends the classification results back to a cloud database system. A mobile App is used to monitor the camera images classified on camera traps in real-time. The RPi could easily sustain a rate of processing 1 image every 2 seconds with an average latency of 1.8 second/image. After capture and pre-processing, each inference took an average of 0.2 Millisecond/image on a RPi Model 4B.
C1 [Zualkernan, Imran A.; Dhou, Salam; Sajun, Ali Reza; Gomez, Brylle Ryan; Hussain, Lana Alhaj; Sakhnini, Dara] Amer Univ Sharjah, Comp Sci & Engn, Sharjah, U Arab Emirates.
   [Judas, Jacky] Emirates Nat WWF, Conservat Unit, Duai, U Arab Emirates.
RP Zualkernan, IA (corresponding author), Amer Univ Sharjah, Comp Sci & Engn, Sharjah, U Arab Emirates.
EM izualkernan@aus.edu; sdhou@aus.edu; jjudas@enwwf.ae; b00068908@aus.edu;
   b00067871@aus.edu; g00071496@aus.edu; g00068368@aus.edu
RI Zualkernan, Imran/B-6994-2018
OI Zualkernan, Imran/0000-0002-1048-5633; Sajun, Ali
   Reza/0000-0003-1270-3005
CR Al Balushi T, 2019, IEEE INTL CONF IND I, P1035, DOI 10.1109/INDIN41052.2019.8972063
   Allken V, 2019, ICES J MAR SCI, V76, P342, DOI 10.1093/icesjms/fsy147
   [Anonymous], 2018, LIVING PLANET REPORT
   Ayanzadeh A., 2018, MODIFIED DEEP NEURAL, DOI [10.20944/preprints201812.0232.v1, DOI 10.20944/PREPRINTS201812.0232.V1]
   Ayoub W, 2019, IEEE COMMUN SURV TUT, V21, P1561, DOI 10.1109/COMST.2018.2877382
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Curtin BH, 2019, 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P82, DOI 10.1109/UEMCON47517.2019.8993061
   Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
   Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN)
   Haupt J., 2018, LARGE SCALE PLANT CL
   Mac Aodha O, 2019, IEEE I CONF COMP VIS, P9595, DOI 10.1109/ICCV.2019.00969
   Mathur A., 2019, REAL TIME WILDLIFE D
   Monburinon N, 2019, PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (INCIT), P294, DOI 10.1109/INCIT.2019.8912138
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Popat Param, 2019, Information and Communication Technology for Intelligent Systems. Proceedings of ICTIS 2018. Smart Innovation, Systems and Technologies (SIST 106), P319, DOI 10.1007/978-981-13-1742-2_31
   Shi W., 2018, DOG BREED IDENTIFICA
   Sreedevi C. K., 2019, 3447740 SSRN
   Tan M., 2019, ARXIV190511946 CS ST
   Teto J. Kamdem, 2018, THESIS
   Thomassen S., 2017, EMBEDDED ANALYTICS A
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Wang H, 2019, IEEE INT CONF INDUST, P1796, DOI 10.1109/ICIT.2019.8755153
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Zimmerman G., 2019, WYSS CAMPAIGN NA SEP
NR 26
TC 3
Z9 3
U1 2
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-8420-3
PY 2020
BP 111
EP 116
DI 10.1109/GCAIOT51063.2020.9345858
PG 6
WC Computer Science, Artificial Intelligence; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BR9FX
UT WOS:000675459100019
DA 2022-02-10
ER

PT J
AU Thomas, ML
   Baker, L
   Beattie, JR
   Baker, AM
AF Thomas, Morgan L.
   Baker, Lynn
   Beattie, James R.
   Baker, Andrew M.
TI Determining the efficacy of camera traps, live capture traps, and
   detection dogs for locating cryptic small mammal species
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE Antechinus arktos; black-tailed dusky antechinus; camera trapping;
   effectiveness; live trapping
ID ESTIMATING SITE OCCUPANCY; ANTECHINUS-ARKTOS; DENSITY; POPULATIONS;
   PERFORMANCE; COMPETITION; EXTINCTION; MARSUPIALS; ABUNDANCE; REPTILES
AB Metal box (e.g., Elliott, Sherman) traps and remote cameras are two of the most commonly employed methods presently used to survey terrestrial mammals. However, their relative efficacy at accurately detecting cryptic small mammals has not been adequately assessed. The present study therefore compared the effectiveness of metal box (Elliott) traps and vertically oriented, close range, white flash camera traps in detecting small mammals occurring in the Scenic Rim of eastern Australia. We also conducted a preliminary survey to determine effectiveness of a conservation detection dog (CDD) for identifying presence of a threatened carnivorous marsupial, Antechinus arktos, in present-day and historical locations, using camera traps to corroborate detections. 200 Elliott traps and 20 white flash camera traps were set for four deployments per method, across a site where the target small mammals, including A. arktos, are known to occur. Camera traps produced higher detection probabilities than Elliott traps for all four species. Thus, vertically mounted white flash cameras were preferable for detecting the presence of cryptic small mammals in our survey. The CDD, which had been trained to detect A. arktos scat, indicated in total 31 times when deployed in the field survey area, with subsequent camera trap deployments specifically corroborating A. arktos presence at 100% (3) indication locations. Importantly, the dog indicated twice within Border Ranges National Park, where historical (1980s-1990s) specimen-based records indicate the species was present, but extensive Elliott and camera trapping over the last 5-10 years have resulted in zero A. arktos captures. Camera traps subsequently corroborated A. arktos presence at these sites. This demonstrates that detection dogs can be a highly effective means of locating threatened, cryptic species, especially when traditional methods are unable to detect low-density mammal populations.
C1 [Thomas, Morgan L.; Baker, Andrew M.] Queensland Univ Technol, Sci & Engn Fac, Sch Earth Environm & Biol Sci, Brisbane, Qld, Australia.
   [Baker, Lynn] Canines Wildlife, Brierfield, NSW, Australia.
   [Beattie, James R.] Australian Natl Univ, Res Sch Astron & Astrophys, Canberra, ACT, Australia.
   [Baker, Andrew M.] Queensland Museum, Biodivers Program, South Brisbane, Qld, Australia.
RP Thomas, ML (corresponding author), Queensland Univ Technol, Sci & Engn Fac, Sch Earth Environm & Biol Sci, Brisbane, Qld, Australia.
EM m44.thomas@connect.qut.edu.au
RI Beattie, James/AAU-6049-2020
OI Beattie, James/0000-0001-9199-7771; Baker, Andrew/0000-0001-8825-1522;
   Baker, Lynn/0000-0002-2803-761X
FU Saving Our Species (SOS; NSW Environment and Heritage); Fitzroy Basin
   Association; Burnett Mary Regional Group
FX The current project was generously supported by funding from Saving Our
   Species (SOS; NSW Environment and Heritage). Funding for trapping
   equipment used in the study was furnished by the Fitzroy Basin
   Association and the Burnett Mary Regional Group. Thanks to Aila Keto
   (Australian Rainforest Conservation Society) for her support and
   accommodation throughout the study period and the School of Earth,
   Environmental and Biological Sciences (EEBS) at the Queensland
   University of Technology (QUT), who provided access to field equipment
   and vehicles. An enormous thank you to Steve Austin (professional
   detection dog trainer), Canines for Wildlife, Brad Nesbitt (backup
   handler), and Bunya the dog for their time, assistance, and support
   before, during and after detection dog field surveys. Finally, we would
   like to thank Kate Moffatt, Emma Gray, Dimity Ball, Matthew Neill, Jack
   Nesbitt, Isaac Towers, Ellie Frederiksen, Oscar Lehman, and Artie Ziff,
   who all graciously volunteered their time to help with certain
   components of field work, quantitative methods, or provided other
   contributions to the study.
CR Akenson JJ, 2001, URSUS-SERIES, V12, P203
   Arnett EB, 2006, WILDLIFE SOC B, V34, P1440, DOI 10.2193/0091-7648(2006)34[1440:APEOTU]2.0.CO;2
   Baker AM, 2014, ZOOTAXA, V 3765, P101, DOI 10.11646/zootaxa.3765.2.1
   Barnosky AD, 2011, NATURE, V471, P51, DOI 10.1038/nature09678
   Bilney RJ, 2014, AUSTRAL ECOL, V39, P875, DOI 10.1111/aec.12145
   Browne CM, 2015, J VET BEHAV, V10, P496, DOI 10.1016/j.jveb.2015.08.002
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Clare J, 2017, ECOL APPL, V27, P2031, DOI 10.1002/eap.1587
   Cristescu RH, 2015, SCI REP-UK, V5, DOI 10.1038/srep08349
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   DICKMAN CR, 1991, OECOLOGIA, V85, P464, DOI 10.1007/BF00323757
   DICKMAN CR, 1986, OECOLOGIA, V70, P536, DOI 10.1007/BF00379900
   Diggins CA, 2016, WILDLIFE SOC B, V40, P654, DOI 10.1002/wsb.715
   Duggan JM, 2011, J WILDLIFE MANAGE, V75, P1209, DOI 10.1002/jwmg.150
   Fisher JT, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161055
   Fisher JT, 2014, J WILDLIFE MANAGE, V78, P1087, DOI 10.1002/jwmg.750
   Garden JG, 2007, WILDLIFE RES, V34, P218, DOI 10.1071/WR06111
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Gonzalez-Esteban J, 2004, EUR J WILDLIFE RES, V50, P33, DOI 10.1007/s10344-003-0031-y
   Gray EL, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181592
   Gray EL, 2017, MAMMAL RES, V62, P47, DOI 10.1007/s13364-016-0281-1
   Gray EL, 2016, AUST J ZOOL, V64, P249, DOI 10.1071/ZO16044
   Griffiths M., 1994, POPULATION DENSITY S, P93
   Harrison RL, 2006, WILDLIFE SOC B, V34, P548, DOI 10.2193/0091-7648(2006)34[548:ACOSMF]2.0.CO;2
   Hunter RJ, 2003, WORLD HERITAGE ASS N
   Hurt A., 2009, CANINE ERGONOMICS, P175
   Jones L, 1996, 1996 ICHEME RESEARCH EVENT - SECOND EUROPEAN CONFERENCE FOR YOUNG RESEARCHERS IN CHEMICAL ENGINEERING, VOLS 1 AND 2, P115
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Leigh KA, 2015, METHODS ECOL EVOL, V6, P745, DOI 10.1111/2041-210X.12374
   Leung LKP, 1999, WILDLIFE RES, V26, P287, DOI 10.1071/WR96042
   Long RA, 2007, J WILDLIFE MANAGE, V71, P2018, DOI 10.2193/2006-292
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McCallum ML, 2015, BIODIVERS CONSERV, V24, P2497, DOI 10.1007/s10531-015-0940-6
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Meek PD, 2012, WILDLIFE RES, V39, P649, DOI 10.1071/WR12138
   Meek PD, 2016, AUST MAMMAL, V38, P44, DOI 10.1071/AM15016
   Nichols JD, 2008, J APPL ECOL, V45, P1321, DOI 10.1111/j.1365-2664.2008.01509.x
   Paull DJ, 2012, WILDLIFE RES, V39, P546, DOI 10.1071/WR12034
   Peterson LM, 1998, WILDLIFE SOC B, V26, P592
   READ DG, 1988, AUST WILDLIFE RES, V15, P139
   Reed SE, 2011, J WILDLIFE MANAGE, V75, P243, DOI 10.1002/jwmg.8
   Reindl-Thompson SA, 2006, WILDLIFE SOC B, V34, P1435, DOI 10.2193/0091-7648(2006)34[1435:EOSDID]2.0.CO;2
   Rendall AR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086592
   Rovero F, 2017, BIODIVERS CONSERV, V26, P1103, DOI 10.1007/s10531-016-1288-2
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Stanley TR, 2005, J WILDLIFE MANAGE, V69, P874, DOI 10.2193/0022-541X(2005)069[0874:ESOAAU]2.0.CO;2
   Sweitzer RA, 2000, J WILDLIFE MANAGE, V64, P531, DOI 10.2307/3803251
   Tasker Elizabeth M., 2002, Australian Mammalogy, V23, P77
   Thompson GG, 2007, WILDLIFE RES, V34, P491, DOI 10.1071/WR06081
   Trolle M, 2008, BIOTROPICA, V40, P211, DOI 10.1111/j.1744-7429.2007.00350.x
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Van Dyck S., 2008, MAMMALS AUSTR
   Vine SJ, 2009, WILDLIFE RES, V36, P436, DOI 10.1071/WR08069
   Weerakoon MK, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P307
   Wiewel AS, 2007, J MAMMAL, V88, P250, DOI 10.1644/06-MAMM-A-098R1.1
   Woinarski JCZ, 2015, P NATL ACAD SCI USA, V112, P4531, DOI 10.1073/pnas.1417301112
   WOOD D H, 1970, Australian Journal of Zoology, V18, P185, DOI 10.1071/ZO9700185
   WOOD D H, 1971, Australian Journal of Zoology, V19, P371, DOI 10.1071/ZO9710371
NR 61
TC 4
Z9 7
U1 4
U2 27
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD JAN
PY 2020
VL 10
IS 2
BP 1054
EP 1068
DI 10.1002/ece3.5972
EA JAN 2020
PG 15
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA KG3BK
UT WOS:000506025400001
PM 32015864
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Despres-Einspenner, ML
   Howe, EJ
   Drapeau, P
   Kuhl, HS
AF Despres-Einspenner, Marie-Lyne
   Howe, Eric J.
   Drapeau, Pierre
   Kuehl, Hjalmar S.
TI An empirical evaluation of camera trapping and spatially explicit
   capture-recapture models for estimating chimpanzee density
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE camera trapping; chimpanzee; density; monitoring; spatially explicit
   capture-recapture; survey design
ID TAI-NATIONAL-PARK; POPULATION-SIZE; FOREST; RATES; ABUNDANCE; SELECTION;
   PATTERNS; WILDLIFE; DECLINE
AB Empirical validations of survey methods for estimating animal densities are rare, despite the fact that only an application to a population of known density can demonstrate their reliability under field conditions and constraints. Here, we present a field validation of camera trapping in combination with spatially explicit capture-recapture (SECR) methods for enumerating chimpanzee populations. We used 83 camera traps to sample a habituated community of western chimpanzees (Pan troglodytes verus) of known community and territory size in Tai National Park, Ivory Coast, and estimated community size and density using spatially explicit capture-recapture models. We aimed to: (1) validate camera trapping as a means to collect capture-recapture data for chimpanzees; (2) validate SECR methods to estimate chimpanzee density from camera trap data; (3) compare the efficacy of targeting locations frequently visited by chimpanzees versus deploying cameras according to a systematic design; (4) evaluate the performance of SECR estimators with reduced sampling effort; and (5) identify sources of heterogeneity in detection probabilities. Ten months of camera trapping provided abundant capture-recapture data. All weaned individuals were detected, most of them multiple times, at both an array of targeted locations, and a systematic grid of cameras positioned randomly within the study area, though detection probabilities were higher at targeted locations. SECR abundance estimates were accurate and precise, and analyses of subsets of the data indicated that the majority of individuals in a community could be detected with as few as five traps deployed within their territory. Our results highlight the potential of camera trapping for cost-effective monitoring of chimpanzee populations.
C1 [Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Despres-Einspenner, Marie-Lyne; Drapeau, Pierre] Univ Quebec Montreal, Dept Sci Biol, Ctr Forest Res, Montreal, PQ, Canada.
   [Howe, Eric J.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP Despres-Einspenner, ML (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM marie_despres@eva.mpg.de
FU Centre for Forest Research-Fonds de Recherche Quebec Nature et
   Technologies International internship program
FX The authors thank the Max Planck Society in Germany, Robert Bosch
   Foundation, Centre Suisse de Recherches Scientifiques, Ministere de
   l'Enseignement Superieur et de la Recherche Scientifique and Ministere
   de l'Environnement et des Eaux et Forets in Cote d'Ivoire, as well as
   the TaiChimpanzee Project for the possibility of conducting field
   research in Cote d'Ivoire. This study was conducted with the financial
   support of the Centre for Forest Research-Fonds de Recherche Quebec
   Nature et Technologies International internship program (M.Sc.
   scholarship to Despres-Einspenner). We thank Appollinaire Gnahe Djirian,
   Oulai Landry, Frederic Yehanon Oulai, Anna Preis, and Liran Samuni for
   providing the tracking data, and Frederic Yehanon Oulai and Liran Samuni
   for the invaluable help they provided with identifying the chimpanzees
   in the videos. At last, we thank Stephen Buckland, David Borchers,
   Marina Cords, and three anonymous reviewers for providing insightful
   comments on earlier versions of the manuscript. Field protocols, data
   collection, and data analysis complied with animal care regulations and
   applicable national laws in Germany and Ivory Coast.
CR Anderson DP, 2005, BIOTROPICA, V37, P631, DOI 10.1111/j.1744-7429.2005.00080.x
   ANDERSON DR, 1994, ECOLOGY, V75, P1780, DOI 10.2307/1939637
   Balcomb SR, 2000, AM J PRIMATOL, V51, P197, DOI 10.1002/1098-2345(200007)51:3&lt;197::AID-AJP4&gt;3.0.CO;2-C
   Baldwin P.J., 1982, International Journal of Primatology, V3, P367, DOI 10.1007/BF02693739
   Basabose AK, 2005, INT J PRIMATOL, V26, P33, DOI 10.1007/s10764-005-0722-1
   Beyer HL, 2004, HAWTHS ANAL TOOLS AR
   Boesch C., 2000, CHIMPANZEES TAI FORE, P328
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers D, 2012, J ORNITHOL, V152, pS435, DOI 10.1007/s10336-010-0583-z
   Buckland S.T., 2001, INTRO DISTANCE SAMPL, P448
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campbell G, 2008, CURR BIOL, V18, pR903, DOI 10.1016/j.cub.2008.08.015
   CHAPMAN CA, 1993, AM J PRIMATOL, V31, P263, DOI 10.1002/ajp.1350310403
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Creel S, 2003, MOL ECOL, V12, P2003, DOI 10.1046/j.1365-294X.2003.01868.x
   Doran D, 1997, INT J PRIMATOL, V18, P183, DOI 10.1023/A:1026368518431
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford MG, 2013, METHODS ECOL EVOL, V4, P629, DOI 10.1111/2041-210X.12049
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Efford MG, 2011, ECOLOGY, V92, P2202, DOI 10.1890/11-0332.1
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Efford MG, 2009, ECOLOGY, V90, P2676, DOI 10.1890/08-1735.1
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Goodall J., 1988, SHADOW MAN, P297
   Granjon AC, 2017, J WILDLIFE MANAGE, V81, P279, DOI 10.1002/jwmg.21190
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Herbinger I, 2001, INT J PRIMATOL, V22, P143, DOI 10.1023/A:1005663212997
   Hill K, 2001, J HUM EVOL, V40, P437, DOI 10.1006/jhev.2001.0469
   HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kuhl H, 2008, BEST PRACTICE GUIDEL, P32
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lehmann J, 2004, BEHAV ECOL SOCIOBIOL, V56, P207, DOI 10.1007/s00265-004-0781-x
   Link WA, 2010, BIOMETRICS, V66, P178, DOI 10.1111/j.1541-0420.2009.01244.x
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Mathewson PD, 2008, ECOL APPL, V18, P208, DOI 10.1890/07-0385.1
   McCarthy MS, 2015, BMC ECOL, V15, DOI 10.1186/s12898-015-0052-x
   McLennan MR, 2010, AM J PRIMATOL, V72, P907, DOI 10.1002/ajp.20839
   Mills LS, 2000, ECOL APPL, V10, P283, DOI 10.2307/2641002
   Moore DL, 2014, AM J PRIMATOL, V76, P335, DOI 10.1002/ajp.22237
   Mugerwa B, 2013, AFR J ECOL, V51, P21, DOI 10.1111/aje.12004
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   O'Connell A. F., 2010, CAMERA TRAPS ANIMAL, P271
   Obbard ME, 2010, J APPL ECOL, V47, P76, DOI 10.1111/j.1365-2664.2009.01758.x
   Otis D. L., 1978, WILDLIFE MONOGR, V62, P3, DOI DOI 10.2307/3830650
   Pledger S, 2000, BIOMETRICS, V56, P434, DOI 10.1111/j.0006-341X.2000.00434.x
   R Core Team, 2015, R LANG ENV STAT COMP
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Royle JA, 2008, ECOLOGY, V89, P2281, DOI 10.1890/07-0601.1
   Spehar SN, 2015, BIOL CONSERV, V191, P185, DOI 10.1016/j.biocon.2015.06.013
   Treves A, 2010, BIOL CONSERV, V143, P521, DOI 10.1016/j.biocon.2009.11.025
   Walsh PD, 2003, NATURE, V422, P611, DOI 10.1038/nature01566
   Whittington J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134446
   Wich SA, 2008, ORYX, V42, P329, DOI 10.1017/S003060530800197X
   WORTON BJ, 1989, ECOLOGY, V70, P164, DOI 10.2307/1938423
   Wrangham R. W., 1979, J REPROD FERTILITY S, V28, P13
NR 60
TC 30
Z9 30
U1 2
U2 46
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD JUL
PY 2017
VL 79
IS 7
SI SI
AR e22647
DI 10.1002/ajp.22647
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA EY4CO
UT WOS:000403924400007
PM 28267880
DA 2022-02-10
ER

PT C
AU Evans, BC
   Tucker, A
   Wearn, OR
   Carbone, C
AF Evans, Benjamin C.
   Tucker, Allan
   Wearn, Oliver R.
   Carbone, Chris
BE Koprinska, I
   Kamp, M
   Appice, A
   Loglisci, C
   Antonie, L
   Zimmermann, A
   Guidotti, R
   Ozgobek, O
TI Reasoning About Neural Network Activations: An Application in Spatial
   Animal Behaviour from Camera Trap Classifications
SO ECML PKDD 2020 WORKSHOPS
SE Communications in Computer and Information Science
LA English
DT Proceedings Paper
CT European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECML PKDD)
CY SEP 14-18, 2020
CL ELECTR NETWORK
SP Fraunhofer IAIS, ASML, F Secure, Roche, Amazon, Science, EURA NOVA, Google, NEC, Internet & Data Lab, KNIME, Qualcomm, AI Res, imec, FWO, Ghent Univ, Springer, Visitgent, gentcongres, AI Growth
DE Animal behavior; Convolutional Neural Networks; Bayesian networks;
   Activation based reasoning
AB Camera traps are a vital tool for ecologists to enable them to monitor wildlife over large areas in order to determine population changes, habitat, and behaviour. As a result, camera-trap datasets are rapidly growing in size. Recent advancements in Artificial Neural Networks (ANN) have emerged in image recognition and detection tasks which are now being applied to automate camera-trap labelling. An ANN designed for species detection will output a set of activations, representing the observation of a particular species (an individual class) at a particular location and time and are often used as a way to calculate population sizes in different regions. Here we go one step further and explore how we can combine ANNs with probabilistic graphical models to reason about animal behaviour using the ANN outputs over different geographical locations. By using the output activations from ANNs as data along with the trap's associated spatial coordinates, we build spatial Bayesian networks to explore species behaviours (how they move and distribute themselves) and interactions (how they distribute in relation to other species). This combination of probabilistic reasoning and deep learning offers many advantages for large camera trap projects as well as potential for other remote sensing datasets that require automated labelling.
C1 [Evans, Benjamin C.; Tucker, Allan] Brunel Univ London, Uxbridge UB8 3PH, Middx, England.
   [Wearn, Oliver R.; Carbone, Chris] Zool Soc London, Inst Zool, London NW1 4RY, England.
RP Evans, BC (corresponding author), Brunel Univ London, Uxbridge UB8 3PH, Middx, England.
EM Benjamin.Evans@brunel.ac.uk; Allan.Tucker@brunel.ac.uk;
   Oliver.Wearn@gmail.com; Chris.Carbone@ioz.ac.uk
OI Carbone, Chris/0000-0002-9253-3765; Wearn, Oliver/0000-0001-8258-3534
FU NERC (The Natural Environment Research Council)
FX Benjamin C. Evans work is funded by NERC (The Natural Environment
   Research Council).
CR Beery S., 2019, ARXIV190706772
   Devlin J., 2019, BERT PRETRAINING DEE, DOI DOI 10.18653/V1/N19-1423
   Franco C, 2016, ENVIRON MODELL SOFTW, V80, P132, DOI 10.1016/j.envsoft.2016.02.029
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Kavukcuoglu K., 2016, ARXIV160903499, DOI DOI 10.1109/ICASSP.2009.4960364
   Maldonado AD, 2019, ENVIRON MODELL SOFTW, V118, P281, DOI 10.1016/j.envsoft.2019.04.011
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Spirtes P., 1993, CAUSATION PREDICTION, P238
   Trifonova N, 2015, ECOL INFORM, V30, P142, DOI 10.1016/j.ecoinf.2015.10.003
   Uusitalo L, 2007, ECOL MODEL, V203, P312, DOI 10.1016/j.ecolmodel.2006.11.033
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
NR 12
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 1865-0929
EI 1865-0937
BN 978-3-030-65965-3; 978-3-030-65964-6
J9 COMM COM INF SC
PY 2020
VL 1323
BP 26
EP 37
DI 10.1007/978-3-030-65965-3_2
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Computer
   Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS4YI
UT WOS:000724139600002
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Giraldo-Zuluaga, JH
   Salazar, A
   Gomez, A
   Diaz-Pulido, A
AF Giraldo-Zuluaga, Jhony-Heriberto
   Salazar, Augusto
   Gomez, Alexander
   Diaz-Pulido, Angelica
GP IEEE
TI Recognition of Mammal Genera on Camera-Trap Images using Multi-Layer
   Robust Principal Component Analysis and Mixture Neural Networks
SO 2017 IEEE 29TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE (ICTAI 2017)
SE Proceedings-International Conference on Tools With Artificial
   Intelligence
LA English
DT Proceedings Paper
CT 29th Annual IEEE International Conference on Tools with Artificial
   Intelligence (ICTAI)
CY NOV 06-08, 2017
CL Boston, MA
SP IEEE, IEEE Comp Soc
DE Camera-trap; mammal recognition; Convolutional Neural Networks;
   Multi-Layer Robust Principal Component Analysis; Least Absolute
   Shrinkage and Selection Operator
ID CLASSIFICATION
AB The segmentation and classification of animals from camera-trap images is a difficult task due to the conditions under which the images are taken. This work presents a method for recognizing mammal genera from camera-trap images. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA) for segmenting, Convolutional Neural Networks (CNNs) for extracting features, Least Absolute Shrinkage and Selection Operator (LASSO) for selecting features, and Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for classifying mammal genera present in the Colombian forest. Our classification method mixes the features of several CNNs. We evaluated our method with the camera-trap images from the Instituto de Investigacion de Recursos Biologicos Alexander von Humboldt. We obtained an accuracy of 92.65% classifying 8 mammal genera and a False Positive (FP) class, using automatic-segmented images. On the other hand, we reached 90.32% of accuracy classifying 10 mammal genera, using ground-truth images only. Unlike all previous works, we confront the animal segmentation and genera classification on the camera-trap framework. This method shows a new approach toward a fully-automatic detection of animals from camera-trap images.
C1 [Giraldo-Zuluaga, Jhony-Heriberto; Salazar, Augusto; Gomez, Alexander] Univ Antioquia, Grp Invest SISTEMIC, Medellin, Colombia.
   [Diaz-Pulido, Angelica] Inst Invest Recursos Biol Alexander von Humboldt, Bogota, Colombia.
RP Giraldo-Zuluaga, JH (corresponding author), Univ Antioquia, Grp Invest SISTEMIC, Medellin, Colombia.
EM robilas@montclair.edu; augusto.salazar@udea.edu.co; alviurlex@gmail.com;
   adiaz@humboldt.org.co
RI Zuluaga, Jhony Heriberto Giraldo/O-7502-2019
OI Zuluaga, Jhony Heriberto Giraldo/0000-0002-0039-1270
FU Colombian National Fund for Science, Technology and Innovation,
   Francisco Jose de Caldas - COLCIENCIAS (Colombia) [111571451061]
FX This work was supported by the Colombian National Fund for Science,
   Technology and Innovation, Francisco Jose de Caldas - COLCIENCIAS
   (Colombia). Project No. 111571451061.
CR Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Giraldo-Zuluaga J.-H., 2017, ARXIV170108180
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kumar YHS, 2015, PROCEDIA COMPUT SCI, V45, P336, DOI 10.1016/j.procs.2015.03.156
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Santoro A., 2016, ARXIV160506065
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zahzah, 2015, ROBUST LOW RANK SPAR
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 22
TC 7
Z9 7
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1082-3409
BN 978-1-5386-3876-7
J9 PROC INT C TOOLS ART
PY 2017
BP 53
EP 60
DI 10.1109/ICTAI.2017.00020
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BK3OG
UT WOS:000435294700009
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Despres-Einspenner, ML
   Samuni, L
   Mundry, R
   Lemoine, S
   Preis, A
   Wittig, RM
   Boesch, C
   Kuhl, HS
AF McCarthy, Maureen S.
   Despres-Einspenner, Marie-Lyne
   Samuni, Liran
   Mundry, Roger
   Lemoine, Sylvain
   Preis, Anna
   Wittig, Roman M.
   Boesch, Christophe
   Kuehl, Hjalmar S.
TI An assessment of the efficacy of camera traps for studying demographic
   composition and variation in chimpanzees (Pan troglodytes)
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE biomonitoring; camera trap; demography; Pan troglodytes; party size;
   seasonal variation
ID MAMMAL COMMUNITIES; TOOL USE; FOREST; WILD; POPULATION; ABUNDANCE;
   BEHAVIOR; DENSITY; CONSEQUENCES; PREDATION
AB Demographic factors can strongly influence patterns of behavioral variation in animal societies. Traditionally, these factors are measured using longitudinal observation of habituated social groups, particularly in social animals like primates. Alternatively, noninvasive biomonitoring methods such as camera trapping can allow researchers to assess species occupancy, estimate population abundance, and study rare behaviors. However, measures of fine-scale demographic variation, such as those related to age and sex structure or subgrouping patterns, pose a greater challenge. Here, we compare demographic data collected from a community of habituated chimpanzees (Pan troglodytes verus) in the Tai Forest using two methods: camera trap videos and observational data from long-term records. By matching data on party size, seasonal variation in party size, measures of demographic composition, and changes over the study period from both sources, we compared the accuracy of camera trap records and long-term data to assess whether camera trap data could be used to assess such variables in populations of unhabituated chimpanzees. When compared to observational data, camera trap data tended to underestimate measures of party size, but revealed similar patterns of seasonal variation as well as similar community demographic composition (age/sex proportions) and dynamics (particularly emigration and deaths) during the study period. Our findings highlight the potential and limitations of camera trap surveys for estimating fine-scale demographic composition and variation in primates. Continuing development of field and statistical methods will further improve the usability of camera traps for demographic studies.
C1 [McCarthy, Maureen S.; Despres-Einspenner, Marie-Lyne; Samuni, Liran; Lemoine, Sylvain; Preis, Anna; Wittig, Roman M.; Boesch, Christophe; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Mundry, Roger] Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
   [Samuni, Liran; Wittig, Roman M.] CSRS, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP McCarthy, MS (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM maureen_mc@eva.mpg.de
OI Lemoine, Sylvain/0000-0001-9853-5246
FU Fonds de Recherche Quebec-Nature et Technologies; Robert Bosch Stiftung;
   Max-Planck-GesellschaftMax Planck Society
FX Fonds de Recherche Quebec-Nature et Technologies; Robert Bosch Stiftung;
   Max-Planck-Gesellschaft
CR Aars J, 2000, AM NAT, V155, P252, DOI 10.1086/303317
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   ALTMANN J, 1974, BEHAVIOUR, V49, P227, DOI 10.1163/156853974X00534
   Anderson DP, 2002, BEHAVIOURAL DIVERSITY IN CHIMPANZEES AND BONOBOS, P90, DOI 10.1017/CBO9780511606397.010
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D., 2014, J STAT SOFTW, V1406, P5823, DOI DOI 10.18637/jss.v067.i01
   Bluff LA, 2010, P ROY SOC B-BIOL SCI, V277, P1377, DOI 10.1098/rspb.2009.1953
   Boesch C, 2000, CHIMPANZEES TAI FORE
   Boyer-Ontl KM, 2014, INT J PRIMATOL, V35, P881, DOI 10.1007/s10764-014-9783-3
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Butchart SHM, 2010, SCIENCE, V328, P1164, DOI 10.1126/science.1187512
   Clutton-Brock T, 2010, TRENDS ECOL EVOL, V25, P562, DOI 10.1016/j.tree.2010.08.002
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Doran D, 1997, INT J PRIMATOL, V18, P183, DOI 10.1023/A:1026368518431
   Fedigan LM, 2010, AM J PRIMATOL, V72, P754, DOI 10.1002/ajp.20814
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   GOLDIZEN AW, 1988, TRENDS ECOL EVOL, V3, P36, DOI 10.1016/0169-5347(88)90045-6
   Goodall J., 1986, CHIMPANZEES GOMBE PA
   Gruen L, 2013, ILAR J, V54, P24, DOI 10.1093/ilar/ilt016
   Hanya G, 2006, PRIMATES, V47, P275, DOI 10.1007/s10329-005-0176-2
   Harmsen BJ, 2011, POPUL ECOL, V53, P253, DOI 10.1007/s10144-010-0211-z
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Kahlenberg SM, 2008, INT J PRIMATOL, V29, P931, DOI 10.1007/s10764-008-9276-3
   Kalan AK, 2015, ECOL INDIC, V54, P217, DOI 10.1016/j.ecolind.2015.02.023
   Kanamori T, 2017, PRIMATES, V58, P225, DOI 10.1007/s10329-016-0584-5
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
   Langergraber KE, 2017, P NATL ACAD SCI USA, V114, P7337, DOI 10.1073/pnas.1701582114
   Langergraber KE, 2014, AM J PRIMATOL, V76, P640, DOI 10.1002/ajp.22258
   LEIMGRUBER P, 1994, J WILDLIFE MANAGE, V58, P254, DOI 10.2307/3809388
   Markham AC, 2015, P NATL ACAD SCI USA, V112, P14882, DOI 10.1073/pnas.1517794112
   Matsumoto-Oda A, 1998, INT J PRIMATOL, V19, P999, DOI 10.1023/A:1020322203166
   McCarthy MS, 2015, BMC ECOL, V15, DOI 10.1186/s12898-015-0052-x
   Meyer NFV, 2015, J NAT CONSERV, V26, P28, DOI 10.1016/j.jnc.2015.04.003
   Mitani JC, 2006, PRIMATES, V47, P6, DOI 10.1007/s10329-005-0139-7
   Mitani JC, 2002, BEHAVIOURAL DIVERSITY IN CHIMPANZEES AND BONOBOS, P102, DOI 10.1017/CBO9780511606397.011
   Musgrave S, 2016, SCI REP-UK, V6, DOI 10.1038/srep34783
   Nishie H, 2018, AM J PHYS ANTHROPOL, V165, P194, DOI 10.1002/ajpa.23327
   Poulsen JR, 2004, INT J PRIMATOL, V25, P285, DOI 10.1023/B:IJOP.0000019153.50161.58
   R Core Team, 2017, R LANG ENV STAT COMP
   Rovero F, 2017, BIODIVERS CONSERV, V26, P1103, DOI 10.1007/s10531-016-1288-2
   Schielzeth H, 2009, BEHAV ECOL, V20, P416, DOI 10.1093/beheco/arn145
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   Siegel S., 1988, NONPARAMETRIC STAT B, V2nd ed
   Sirianni G, 2018, ANIM COGN, V21, P109, DOI 10.1007/s10071-017-1144-0
   STANFORD CB, 1994, BEHAVIOUR, V131, P1, DOI 10.1163/156853994X00181
   Stumpf Rebecca M., 2011, PRIMATES PERSPECTIVE, P340
   Tan CL, 2013, PRIMATES, V54, P1, DOI 10.1007/s10329-012-0318-2
   Torralvo K, 2017, PRIMATES, V58, P279, DOI 10.1007/s10329-017-0603-1
   Treves A, 2010, BIOL CONSERV, V143, P521, DOI 10.1016/j.biocon.2009.11.025
   VanderWaal KL, 2009, ANIM BEHAV, V77, P949, DOI 10.1016/j.anbehav.2008.12.028
   WATTS DP, 1989, ETHOLOGY, V81, P1
   Wegge P, 2004, ANIM CONSERV, V7, P251, DOI 10.1017/S1367943004001441
   Williamson Elizabeth A., 2003, P25
   Wittig R. M., 2017, ENCY ANIMAL COGNITIO, P1, DOI DOI 10.1007/978-3-319-47829-6_1564-1
NR 56
TC 9
Z9 10
U1 1
U2 28
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD SEP
PY 2018
VL 80
IS 9
AR e22904
DI 10.1002/ajp.22904
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Zoology
GA GW0KC
UT WOS:000446552900003
PM 30088683
DA 2022-02-10
ER

PT J
AU Cove, MV
   Kays, R
   Bontrager, H
   Bresnan, C
   Lasky, M
   Frerichs, T
   Klann, R
   Lee, TE
   Crockett, SC
   Crupi, AP
   Weiss, KCB
   Rowe, H
   Sprague, T
   Schipper, J
   Tellez, C
   Lepczyk, CA
   Fantle-Lepczyk, JE
   Lapoint, S
   Williamson, J
   Fisher-Reid, MC
   King, SM
   Bebko, AJ
   Chrysafis, P
   Jensen, AJ
   Jachowski, DS
   Sands, J
   MacCombie, KA
   Herrera, DJ
   van der Merwe, M
   Knowles, TW
   Horan, RV
   Rentz, MS
   Brandt, LSE
   Nagy, C
   Barton, BT
   Thompson, WC
   Maher, SP
   Darracq, AK
   Hess, G
   Parsons, AW
   Wells, B
   Roemer, GW
   Hernandez, CJ
   Gompper, ME
   Webb, SL
   Vanek, JP
   Lafferty, DJR
   Bergquist, AM
   Hubbard, T
   Forrester, T
   Clark, D
   Cincotta, C
   Favreau, J
   Facka, AN
   Halbur, M
   Hammerich, S
   Gray, M
   Rega-Brodsky, CC
   Durbin, C
   Flaherty, EA
   Brooke, JM
   Coster, SS
   Lathrop, RG
   Russell, K
   Bogan, DA
   Cliche, R
   Shamon, H
   Hawkins, MTR
   Marks, SB
   Lonsinger, RC
   O'Mara, MT
   Compton, JA
   Fowler, M
   Barthelmess, EL
   Andy, KE
   Belant, JL
   Beyer, DE
   Kautz, TM
   Scognamillo, DG
   Schalk, CM
   Leslie, MS
   Nasrallah, SL
   Ellison, CN
   Ruthven, C
   Fritts, S
   Tleimat, J
   Gay, M
   Whittier, CA
   Neiswenter, SA
   Pelletier, R
   DeGregorio, BA
   Kuprewicz, EK
   Davis, ML
   Dykstra, A
   Mason, DS
   Baruzzi, C
   Lashley, MA
   Risch, DR
   Price, MR
   Allen, ML
   Whipple, LS
   Sperry, JH
   Hagen, RH
   Mortelliti, A
   Evans, BE
   Studds, CE
   Siren, APK
   Kilborn, J
   Sutherland, C
   Warren, P
   Fuller, T
   Harris, NC
   Carter, NH
   Trout, E
   Zimova, M
   Giery, ST
   Iannarilli, F
   Higdon, SD
   Revord, RS
   Hansen, CP
   Millspaugh, JJ
   Zorn, A
   Benson, JF
   Wehr, NH
   Solberg, JN
   Gerber, BD
   Burr, JC
   Sevin, J
   Green, AM
   Sekercioglu, CH
   Pendergast, M
   Barnick, KA
   Edelman, AJ
   Wasdin, JR
   Romero, A
   O'Neill, BJ
   Schmitz, N
   Alston, JM
   Kuhn, KM
   Lesmeister, DB
   Linnell, MA
   Appel, CL
   Rota, C
   Stenglein, JL
   Anhalt-Depies, C
   Nelson, C
   Long, RA
   Jaspers, KJ
   Remine, KR
   Jordan, MJ
   Davis, D
   Hernandez-Yanez, H
   Zhao, JY
   McShea, AJ
AF Cove, Michael V.
   Kays, Roland
   Bontrager, Helen
   Bresnan, Claire
   Lasky, Monica
   Frerichs, Taylor
   Klann, Renee
   Lee, Thomas E., Jr.
   Crockett, Seth C.
   Crupi, Anthony P.
   Weiss, Katherine C. B.
   Rowe, Helen
   Sprague, Tiffany
   Schipper, Jan
   Tellez, Chelsey
   Lepczyk, Christopher A.
   Fantle-Lepczyk, Jean E.
   Lapoint, Scott
   Williamson, Jacque
   Fisher-Reid, M. Caitlin
   King, Sean M.
   Bebko, Alexandra J.
   Chrysafis, Petros
   Jensen, Alex J.
   Jachowski, David S.
   Sands, Joshua
   MacCombie, Kelly Anne
   Herrera, Daniel J.
   van der Merwe, Marius
   Knowles, Travis W.
   Horan, Robert V., III
   Rentz, Michael S.
   Brandt, LaRoy S. E.
   Nagy, Christopher
   Barton, Brandon T.
   Thompson, Weston C.
   Maher, Sean P.
   Darracq, Andrea K.
   Hess, George
   Parsons, Arielle W.
   Wells, Brenna
   Roemer, Gary W.
   Hernandez, Cristian J.
   Gompper, Matthew E.
   Webb, Stephen L.
   Vanek, John P.
   Lafferty, Diana J. R.
   Bergquist, Amelia M.
   Hubbard, Tru
   Forrester, Tavis
   Clark, Darren
   Cincotta, Connor
   Favreau, Jorie
   Facka, Aaron N.
   Halbur, Michelle
   Hammerich, Steven
   Gray, Morgan
   Rega-Brodsky, Christine C.
   Durbin, Caleb
   Flaherty, Elizabeth A.
   Brooke, Jarred M.
   Coster, Stephanie S.
   Lathrop, Richard G.
   Russell, Katarina
   Bogan, Daniel A.
   Cliche, Rachel
   Shamon, Hila
   Hawkins, Melissa T. R.
   Marks, Sharyn B.
   Lonsinger, Robert C.
   O'Mara, M. Teague
   Compton, Justin A.
   Fowler, Melinda
   Barthelmess, Erika L.
   Andy, Katherine E.
   Belant, Jerrold L.
   Beyer, Dean E., Jr.
   Kautz, Todd M.
   Scognamillo, Daniel G.
   Schalk, Christopher M.
   Leslie, Matthew S.
   Nasrallah, Sophie L.
   Ellison, Caroline N.
   Ruthven, Chip
   Fritts, Sarah
   Tleimat, Jaquelyn
   Gay, Mandy
   Whittier, Christopher A.
   Neiswenter, Sean A.
   Pelletier, Robert
   DeGregorio, Brett A.
   Kuprewicz, Erin K.
   Davis, Miranda L.
   Dykstra, Adrienne
   Mason, David S.
   Baruzzi, Carolina
   Lashley, Marcus A.
   Risch, Derek R.
   Price, Melissa R.
   Allen, Maximilian L.
   Whipple, Laura S.
   Sperry, Jinelle H.
   Hagen, Robert H.
   Mortelliti, Alessio
   Evans, Bryn E.
   Studds, Colin E.
   Siren, Alexej P. K.
   Kilborn, Jillian
   Sutherland, Chris
   Warren, Paige
   Fuller, Todd
   Harris, Nyeema C.
   Carter, Neil H.
   Trout, Edward
   Zimova, Marketa
   Giery, Sean T.
   Iannarilli, Fabiola
   Higdon, Summer D.
   Revord, Ronald S.
   Hansen, Christopher P.
   Millspaugh, Joshua J.
   Zorn, Adam
   Benson, John F.
   Wehr, Nathaniel H.
   Solberg, Jaylin N.
   Gerber, Brian D.
   Burr, Jessica C.
   Sevin, Jennifer
   Green, Austin M.
   Sekercioglu, Cagan H.
   Pendergast, Mary
   Barnick, Kelsey A.
   Edelman, Andrew J.
   Wasdin, Joanne R.
   Romero, Andrea
   O'Neill, Brian J.
   Schmitz, Noel
   Alston, Jesse M.
   Kuhn, Kellie M.
   Lesmeister, Damon B.
   Linnell, Mark A.
   Appel, Cara L.
   Rota, Christopher
   Stenglein, Jennifer L.
   Anhalt-Depies, Christine
   Nelson, Carrie
   Long, Robert A.
   Jaspers, Kodi Jo
   Remine, Kathryn R.
   Jordan, Mark J.
   Davis, Daniel
   Hernandez-Yanez, Haydee
   Zhao, Jennifer Y.
   McShea, Andwilliam J.
TI SNAPSHOT USA 2019: a coordinated national camera trap survey of the
   United States
SO ECOLOGY
LA English
DT Article; Data Paper
DE biodiversity; biogeography; camera traps; carnivora; Cetartiodactyla;
   Cingulata; Didelphimorphia; Lagomorpha; mammals; occupancy modeling;
   Rodentia; species distribution modeling
AB With the accelerating pace of global change, it is imperative that we obtain rapid inventories of the status and distribution of wildlife for ecological inferences and conservation planning. To address this challenge, we launched the SNAPSHOT USA project, a collaborative survey of terrestrial wildlife populations using camera traps across the United States. For our first annual survey, we compiled data across all 50 states during a 14-week period (17 August-24 November of 2019). We sampled wildlife at 1,509 camera trap sites from 110 camera trap arrays covering 12 different ecoregions across four development zones. This effort resulted in 166,036 unique detections of 83 species of mammals and 17 species of birds. All images were processed through the Smithsonian's eMammal camera trap data repository and included an expert review phase to ensure taxonomic accuracy of data, resulting in each picture being reviewed at least twice. The results represent a timely and standardized camera trap survey of the United States. All of the 2019 survey data are made available herein. We are currently repeating surveys in fall 2020, opening up the opportunity to other institutions and cooperators to expand coverage of all the urban-wild gradients and ecophysiographic regions of the country. Future data will be available as the database is updated at eMammal.si.edu/snapshot-usa, as will future data paper submissions. These data will be useful for local and macroecological research including the examination of community assembly, effects of environmental and anthropogenic landscape variables, effects of fragmentation and extinction debt dynamics, as well as species-specific population dynamics and conservation action plans. There are no copyright restrictions; please cite this paper when using the data for publication.
C1 [Cove, Michael V.; Bontrager, Helen; Bresnan, Claire; Frerichs, Taylor; Klann, Renee] Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
   [Kays, Roland] North Carolina Museum Nat Sci, Raleigh, NC 27601 USA.
   [Kays, Roland; Lasky, Monica] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
   [Lee, Thomas E., Jr.; Crockett, Seth C.] Abilene Christian Univ, Dept Biol, Abilene, TX 79601 USA.
   [Crupi, Anthony P.] Alaska Dept Fish & Game, Div Wildlife Conservat, Douglas, AK 99824 USA.
   [Weiss, Katherine C. B.; Tellez, Chelsey] Arizona State Univ, Tempe, AZ 85281 USA.
   [Weiss, Katherine C. B.; Schipper, Jan; Tellez, Chelsey] Arizona Ctr Nat Conservat Phoenix Zoo, Field Conservat Res Dept, 455 N Galvin Pkwy, Phoenix, AZ 85008 USA.
   [Rowe, Helen; Sprague, Tiffany] McDowell Sonoran Conservancy, 7729 East Greenway Rd,Suite 100, Scottsdale, AZ 85260 USA.
   [Lepczyk, Christopher A.; Fantle-Lepczyk, Jean E.] Auburn Univ, Sch Forestry & Wildlife Sci, Auburn, AL 36849 USA.
   [Lapoint, Scott] Black Rock Forest, 65 Reservoir Rd, Carnwall, NY 12518 USA.
   [Williamson, Jacque] Brandywine Zoo Delaware State Parks, Dept Educ & Conservat, Wilmington, DC USA.
   [Fisher-Reid, M. Caitlin; King, Sean M.; Bebko, Alexandra J.] Bridgewater State Univ, Dept Biol Sci, Bridgewater, MA 02325 USA.
   [Chrysafis, Petros] Calif State Univ Fresno, Fresno, CA 93740 USA.
   [Jensen, Alex J.; Jachowski, David S.] Clemson Univ, Dept Forestry & Environm Conservat, Clemson, SC 29631 USA.
   [Sands, Joshua; MacCombie, Kelly Anne] Crocodile Lake Natl Wildlife Refuge, Key Largo, FL 33037 USA.
   [Herrera, Daniel J.] DC Cat Count Humane Rescue Alliance, Washington, DC 20011 USA.
   [van der Merwe, Marius] Dixie State Univ, Dept Biol Sci, St George, UT 84770 USA.
   [Knowles, Travis W.] Francis Marion Univ, Dept Biol, Florence, SC 29502 USA.
   [Horan, Robert V., III] Georgia Dept Nat Resources, Wildlife Resources Div, Brunswick, GA 31520 USA.
   [Rentz, Michael S.] Iowa State Univ, Nat Resource Ecol & Management, Ames, IA 50011 USA.
   [Brandt, LaRoy S. E.] Lincoln Mem Univ, Cumberland Mt Res Ctr, Harrogate, TN 37752 USA.
   [Nagy, Christopher] Mianus River Gorge, Bedford, NY 10506 USA.
   [Barton, Brandon T.; Thompson, Weston C.] Mississippi State Univ, Dept Biol Sci, Mississippi State, MS 39762 USA.
   [Maher, Sean P.] Missouri State Univ, Dept Biol, Springfield, MO 65897 USA.
   [Darracq, Andrea K.] Murray State Univ, Dept Biol, Murray, KY 42071 USA.
   [Hess, George; Parsons, Arielle W.; Wells, Brenna] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
   [Roemer, Gary W.; Hernandez, Cristian J.; Gompper, Matthew E.] New Mexico State Univ, Dept Fish Wildlife & Conservat Ecol, Las Cruces, NM 88003 USA.
   [Webb, Stephen L.] Noble Res Inst LLC, 2510 Sam Noble Pkwy, Ardmore, OK 73401 USA.
   [Vanek, John P.] Northern Illinois Univ, Dept Biol Sci, De Kalb, IL 60115 USA.
   [Lafferty, Diana J. R.; Bergquist, Amelia M.; Hubbard, Tru] Northern Michigan Univ, Dept Biol, Wildlife Ecol & Conservat Sci Lab, Marqeutte, MI 49855 USA.
   [Forrester, Tavis; Clark, Darren] Oregon Dept Fish & Wildlife, La Grande, OR 97850 USA.
   [Cincotta, Connor; Favreau, Jorie] Paul Smiths Coll, Paul Smiths, NY 12970 USA.
   [Facka, Aaron N.] Penn Game Commiss, Harrisburg, PA 17110 USA.
   [Halbur, Michelle; Hammerich, Steven; Gray, Morgan] Pepperwood Fdn, 2130 Pepperwood Preserve Rd, Santa Rosa, CA 95404 USA.
   [Rega-Brodsky, Christine C.; Durbin, Caleb] Pittsburg State Univ, Dept Biol, 1701 S Broadway, Pittsburg, KS 66762 USA.
   [Flaherty, Elizabeth A.; Brooke, Jarred M.] Purdue Univ, Dept Forestry & Nat Resources, W Lafayette, IN 47907 USA.
   [Coster, Stephanie S.] Randolph Macon Coll, Dept Biol, Ashland, VA 23005 USA.
   [Lathrop, Richard G.; Russell, Katarina] Rutgers State Univ, Dept Ecol Evolut & Nat Resources, New Brunswick, NJ 08901 USA.
   [Bogan, Daniel A.] Siena Coll, Dept Environm Studies & Sci, 515 Loudon Rd, New York, NY 12211 USA.
   [Cliche, Rachel] Silvio O Conte Natl Fish & Wildlife Refuge, Brunswick, VT 05905 USA.
   [Shamon, Hila] Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
   [Hawkins, Melissa T. R.] Natl Museum Nat Hist, Smithsonian Inst, Div Mammals, Dept Vertebrate Zool, Washington, DC 20560 USA.
   [Hawkins, Melissa T. R.; Marks, Sharyn B.] Humboldt State Univ, Dept Biol, 1 Harpst St, Arcata, CA 95521 USA.
   [Lonsinger, Robert C.] South Dakota State Univ, Dept Nat Resource Management, 1390 Coll Ave, Brookings, SD 57007 USA.
   [O'Mara, M. Teague] Southeastern Louisiana Univ, Dept Biol Sci, 808 N Pine St, Hammond, LA 70402 USA.
   [Compton, Justin A.; Fowler, Melinda] Springfield Coll, Dept Biol & Chem, Springfield, MA 01109 USA.
   [Barthelmess, Erika L.] St Lawrence Univ, Biol Dept, Canton, NY 13617 USA.
   [Barthelmess, Erika L.; Andy, Katherine E.] St Lawrence Univ, Nat North Program, Canton, NY 13617 USA.
   [Belant, Jerrold L.; Kautz, Todd M.] SUNY Coll Environm Sci & Forestry, Global Wildlife Conservat Ctr, Syracuse, NY 13210 USA.
   [Beyer, Dean E., Jr.] Michigan Dept Nat Resources, Wildlife Div, Lansing, MI 48909 USA.
   [Scognamillo, Daniel G.; Schalk, Christopher M.] Stephen F Austin State Univ, Arthur Temple Coll Forestry & Agr, Nacogdoches, TX 75962 USA.
   [Leslie, Matthew S.; Nasrallah, Sophie L.] Swarthmore Coll, Dept Biol, 500 Coll Ave, Swarthmore, PA 19081 USA.
   [Ellison, Caroline N.; Ruthven, Chip] Texas Parks & Wildlife Dept, Paducah, TX 79248 USA.
   [Fritts, Sarah; Tleimat, Jaquelyn; Gay, Mandy] Texas State Univ, Dept Biol, San Marcos, TX 78666 USA.
   [Whittier, Christopher A.] Tufts Univ, Cummings Sch Vet Med, Tufts Ctr Conservat Med, North Grafton, MA 01536 USA.
   [Neiswenter, Sean A.; Pelletier, Robert] Univ Nevada, Sch Life Sci, 4505 Maryland Pkwy, Las Vegas, NV 89154 USA.
   [DeGregorio, Brett A.] Univ Arkansas, US Geol Survey Fish & Wildlife Cooperat Res Unit, Fayetteville, AR 72701 USA.
   [Kuprewicz, Erin K.; Davis, Miranda L.] Univ Connecticut, Dept Ecol & Evolutionary Biol, Storrs, CT 06269 USA.
   [Dykstra, Adrienne; Mason, David S.] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.
   [Baruzzi, Carolina] Univ Florida, Sch Forest Resources & Conservat, Gainesville, FL 32611 USA.
   [Lashley, Marcus A.] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL 32611 USA.
   [Risch, Derek R.; Price, Melissa R.] Univ Hawaii Manoa, Dept Nat Resources & Environm Management, Honolulu, HI 96822 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Allen, Maximilian L.; Whipple, Laura S.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin Ave, Urbana, IL 61801 USA.
   [Sperry, Jinelle H.] Engn Res & Dev Ctr, 2902 Newmark Dr, Champaign, IL 61826 USA.
   [Hagen, Robert H.] Univ Kansas, Environm Studies Program, Lawrence, KS 66045 USA.
   [Mortelliti, Alessio; Evans, Bryn E.] Univ Maine, Dept Wildlife Fisheries & Conservat Biol, 5755 Nutting Hall, Orono, ME 04469 USA.
   [Studds, Colin E.] Univ Maryland Baltimore Cty, Dept Geog & Environm Syst, Baltimore, MD 21250 USA.
   [Siren, Alexej P. K.] Univ Massachusetts, Dept Environm Conservat, Amherst, MA 01003 USA.
   [Kilborn, Jillian] New Hampshire Fish & Game Dept, Concord, NH 03301 USA.
   [Sutherland, Chris; Warren, Paige; Fuller, Todd] Univ Massachusetts, Dept Environm Conservat, Amherst, MA 01003 USA.
   [Harris, Nyeema C.] Univ Michigan, Appl Wildlife Ecol Lab, Ecol & Evolutionary Biol, Ann Arbor, MI 48109 USA.
   [Carter, Neil H.] Univ Michigan, Sch Environm & Sustainabil, Ann Arbor, MI 48109 USA.
   [Trout, Edward] Boise State Univ, Human Environm Syst, Boise, ID 83725 USA.
   [Zimova, Marketa] Univ Michigan, Sch Environm & Sustainabil, Ann Arbor, MI 48109 USA.
   [Giery, Sean T.] Penn State Univ, Eberly Coll Sci, Dept Biol, University Pk, PA 16802 USA.
   [Iannarilli, Fabiola] Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, St Paul, MN 55108 USA.
   [Higdon, Summer D.; Revord, Ronald S.] Univ Missouri, Ctr Agroforestry, 302 Anheuser Busch Nat Resources Bldg, Columbia, MO 65211 USA.
   [Hansen, Christopher P.; Millspaugh, Joshua J.] Univ Montana, WA Franke Coll Forestry & Conservat, Wildlife Biol Program, Missoula, MT 59812 USA.
   [Zorn, Adam] Univ Mt Union, Huston Brumbaugh Nat Ctr, Alliance, OH 44601 USA.
   [Benson, John F.; Wehr, Nathaniel H.] Univ Nebraska, Sch Nat Resources, Lincoln, NE 68583 USA.
   [Solberg, Jaylin N.] Univ North Dakota, Dept Biol, 10 Cornell St,Stop 9019, Grand Forks, ND 58202 USA.
   [Gerber, Brian D.; Burr, Jessica C.] Univ Rhode Isl, Dept Nat Resources Sci, Kingston, RI 02881 USA.
   [Sevin, Jennifer] Univ Richmond, Dept Biol, Richmond, VA 23173 USA.
   [Green, Austin M.] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Sekercioglu, Cagan H.] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Sekercioglu, Cagan H.] Koc Univ, Coll Sci, Istanbul, Sariyer, Turkey.
   [Pendergast, Mary] Wild Utah Project, Salt Lake City, UT 84101 USA.
   [Barnick, Kelsey A.] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Edelman, Andrew J.; Wasdin, Joanne R.] Univ West Georgia, Dept Biol, Carrollton, GA 30118 USA.
   [Romero, Andrea; O'Neill, Brian J.; Schmitz, Noel] Univ Wisconsin Whitewater, Dept Geog Geol & Environm Studies, Dept Biol Sci, Whitewater, WI 53190 USA.
   [Alston, Jesse M.] Univ Wyoming, Dept Zool & Physiol, Program Ecol, Laramie, WY 82071 USA.
   [Kuhn, Kellie M.] US Air Force Acad, Deparment Biol, USAFA, Colorado Springs, CO 80840 USA.
   [Lesmeister, Damon B.; Linnell, Mark A.] USDA Forest Serv, Pacific Northwest Res Stn, Portland, OR 97204 USA.
   [Appel, Cara L.] Oregon State Univ, Dept Fisheries & Wildlife, Corvallis, OR 97331 USA.
   [Rota, Christopher] West Virginia Univ, Div Forestry & Nat Resources, Morgantown, WV 26506 USA.
   [Stenglein, Jennifer L.; Anhalt-Depies, Christine] Wisconsin Dept Nat Resources, Off Appl Sci, Madison, WI 53707 USA.
   [Nelson, Carrie] US Forest Serv, Chequamegon Nicolet Natl Forest, Glidden, WI 54527 USA.
   [Long, Robert A.; Jaspers, Kodi Jo; Remine, Kathryn R.] Woodland Pk Zoo, Seattle, WA 98103 USA.
   [Jordan, Mark J.] Seattle Univ, Dept Biol, Seattle, WA 98122 USA.
   [Davis, Daniel] Smithsonian Inst, Off Chief Informat Officer, Washington, DC 20024 USA.
   [Hernandez-Yanez, Haydee; Zhao, Jennifer Y.; McShea, Andwilliam J.] Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
RP Cove, MV (corresponding author), Smithsonian Conservat Biol Inst, Front Royal, VA 22630 USA.
EM mvcove@ncsu.edu
RI Whittier, Christopher/AAX-2530-2021; Rega-Brodsky,
   Christine/AAN-7397-2021; Alston, Jesse/AAU-8218-2020; Edelman, Andrew
   J/C-2810-2012; O'Mara, M. Teague/J-6090-2012; Studds, Colin/C-3701-2012
OI Whittier, Christopher/0000-0001-9626-6513; Rega-Brodsky,
   Christine/0000-0002-3483-1465; Alston, Jesse/0000-0001-5309-7625; Kays,
   Roland/0000-0002-2947-6665; Mason, David/0000-0001-8456-5700;
   Sutherland, Chris/0000-0003-2073-1751; McShea,
   William/0000-0002-8102-0200; Hawkins, Melissa/0000-0001-8929-1593;
   Baruzzi, Carolina/0000-0003-1796-9355; King, Scott/0000-0001-6821-9234;
   O'Mara, M. Teague/0000-0002-6951-1648; Fisher-Reid, M.
   Caitlin/0000-0003-1587-7086; Crupi, Anthony/0000-0003-2788-6238;
   Lesmeister, Damon/0000-0003-1102-0122; Williamson,
   Jacque/0000-0002-5626-8706; Studds, Colin/0000-0001-5715-1692; Cove,
   Michael/0000-0001-5691-0634; Clark, Darren/0000-0002-8738-9826; Whipple,
   Laura/0000-0003-0736-9946; Sekercioglu, Cagan H./0000-0003-3193-0377;
   Coster, Stephanie/0000-0002-5170-4548; Lasky,
   Monica/0000-0002-9567-4643; Weiss, Katherine C. B./0000-0002-0034-2460;
   Bontrager, Helen/0000-0002-9457-2119; Iannarilli,
   Fabiola/0000-0002-7018-3557
FU Smithsonian InstitutionSmithsonian Institution Funding Source: Medline
NR 0
TC 1
Z9 1
U1 8
U2 11
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0012-9658
EI 1939-9170
J9 ECOLOGY
JI Ecology
PD JUN
PY 2021
VL 102
IS 6
AR e03353
DI 10.1002/ecy.3353
PG 2
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SN7SX
UT WOS:000658488800021
PM 33793977
OA Green Published, Bronze
DA 2022-02-10
ER

PT J
AU Norouzzadeh, MS
   Morris, D
   Beery, S
   Joshi, N
   Jojic, N
   Clune, J
AF Norouzzadeh, Mohammad Sadegh
   Morris, Dan
   Beery, Sara
   Joshi, Neel
   Jojic, Nebojsa
   Clune, Jeff
TI A deep active learning system for species identification and counting in
   camera trap images
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE active learning; camera trap images; computer vision; deep learning;
   deep neural networks
AB A typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical conservation questions may be answered too slowly to support decision-making. Recent studies demonstrated the potential for computer vision to dramatically increase efficiency in image-based biodiversity surveys; however, the literature has focused on projects with a large set of labelled training images, and hence many projects with a smaller set of labelled images cannot benefit from existing machine learning techniques. Furthermore, even sizable projects have struggled to adopt computer vision methods because classification models overfit to specific image backgrounds (i.e. camera locations).
   In this paper, we combine the power of machine intelligence and human intelligence via a novel active learning system to minimize the manual work required to train a computer vision model. Furthermore, we utilize object detection models and transfer learning to prevent overfitting to camera locations. To our knowledge, this is the first work to apply an active learning approach to camera trap images.
   Our proposed scheme can match state-of-the-art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labelling effort by over 99.5%. Our trained models are also less dependent on background pixels, since they operate only on cropped regions around animals.
   The proposed active deep learning scheme can significantly reduce the manual labour required to extract information from camera trap images. Automation of information extraction will not only benefit existing camera trap projects, but can also catalyse the deployment of larger camera trap arrays.
C1 [Norouzzadeh, Mohammad Sadegh; Morris, Dan; Beery, Sara] Microsoft AI Earth, Redmond, WA 98052 USA.
   [Norouzzadeh, Mohammad Sadegh; Clune, Jeff] Univ Wyoming, Comp Sci Dept, Laramie, WY 82071 USA.
   [Beery, Sara] CALTECH, Comp Sci Dept, Pasadena, CA USA.
   [Joshi, Neel; Jojic, Nebojsa] Microsoft Res, Redmond, WA USA.
   [Clune, Jeff] OpenAI, San Francisco, CA USA.
RP Morris, D (corresponding author), Microsoft AI Earth, Redmond, WA 98052 USA.
EM dan@microsoft.com
OI Norouzzadeh, Mohammad Sadegh/0000-0002-2983-9374
CR AI for Earth Microsoft, 2018, DET MOD
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   Beery S., 2018, P EUR C COMP VIS
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cho K., 2014, ARXIV14061078
   Dasgupta S., 2008, P INT C MACH LEARN, P208
   Elith J, 2010, METHODS ECOL EVOL, V1, P330, DOI 10.1111/j.2041-210X.2010.00036.x
   eMammal, EMAMMAL PROJ
   Forrester Tavis, 2013, P 98 ESA ANN CONV 20
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guo YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P823
   Hagan M.T., 1996, NEURAL NETWORK DESIG, V20
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hecht-Nielsen R., 1989, INT JOINT C NEUR NET
   Hermans A., 2017, ARXIV ARXIV170307737
   Hinton G., 2012, COURSERA NEURAL NETW, DOI DOI 10.1007/BF00992698
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Koch G, 2015, RADICAL PHILOS, P28
   Krizhevsky A., 2012, 2012 ADV NEUR INF PR
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3
   Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Miao Z., 2019, SCI REPORTS, V90
   Mohri M., 2012, FDN MACHINE LEARNING
   Norouzzadeh M. S., 2020, NOROUZZADEH ET AL ME, DOI [10.5281/zenodo.4052020, DOI 10.5281/ZENODO.4052020]
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sener O., 2017, ARXIV170800489
   Settles B., 2008, C EMP METH NAT LANG, P1070
   Settles Burr, 2009, TECHNICAL REPORT
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Southwood T., 2009, ECOLOGICAL METHODS
   Sutskever I., 2014, NIPS, P3104
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tikhonov G, 2017, METHODS ECOL EVOL, V8, P443, DOI 10.1111/2041-210X.12723
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xu Z, 2003, LECT NOTES COMPUT SC, V2633, P393
   Yosinski J., 2014, 2014 ADV NEURAL INFO
NR 45
TC 9
Z9 9
U1 5
U2 21
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JAN
PY 2021
VL 12
IS 1
BP 150
EP 161
DI 10.1111/2041-210X.13504
EA NOV 2020
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA PU3OB
UT WOS:000590688700001
OA Green Submitted
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Despres-Einspenner, ML
   Farine, DR
   Samuni, L
   Angedakin, S
   Arandjelovic, M
   Boesch, C
   Dieguez, P
   Havercamp, K
   Knight, A
   Langergraber, KE
   Wittig, RM
   Kuhl, HS
AF McCarthy, Maureen S.
   Despres-Einspenner, Marie-Lyne
   Farine, Damien R.
   Samuni, Liran
   Angedakin, Samuel
   Arandjelovic, Mimi
   Boesch, Christophe
   Dieguez, Paula
   Havercamp, Kristin
   Knight, Alex
   Langergraber, Kevin E.
   Wittig, Roman M.
   Kuehl, Hjalmar S.
TI Camera traps provide a robust alternative to direct observations for
   constructing social networks of wild chimpanzees
SO ANIMAL BEHAVIOUR
LA English
DT Article
DE association patterns; biomonitoring; camera trap; chimpanzee;
   fission-fusion; Pan troglodytes; social network analysis
ID FISSION-FUSION DYNAMICS; COMMUNITY STRUCTURE; PAN-TROGLODYTES;
   TRANSMISSION; ORGANIZATION; POPULATION; BEHAVIOR; SOCIOECOLOGY;
   UNCERTAINTY; TOOLS
AB Social network analysis provides valuable opportunities to quantify the nature of social relationships in animal societies including aspects of group structure, dynamics and behaviour transmission. Remote monitoring approaches such as camera trapping offer rich data sets from groups and species that are difficult to observe, yet the robustness of these data for constructing social networks remains unexplored. Here we compared networks of party association based on camera traps with those based on direct observations over the same 9-month sampling period in a group of habituated western chimpanzees, Pan troglodytes verus. Networks based on camera traps and direct observations were both stable with sufficient sampling, and had very similar structures, patterns of sex assortment and individual network positions. However, camera trap data led to lower estimates of group density and dyadic association strengths, and slightly higher modularity, illustrating the limitations raised by differences in data collection methods for network comparisons. We then constructed a social network using camera trap data from unhabituated eastern chimpanzees, Rt. schweinfurthii, demonstrating the feasibility of this approach in the absence of extensive prior knowledge of the study subjects. Further, differences between the eastern and western chimpanzee social networks followed expected patterns based on recognized social differences, illustrating the promise of this approach for detecting within-species social variation. Although long-term behavioural observations will continue to provide rich data for many species, camera traps offer a powerful alternative to gain information on social group dynamics in elusive or unhabituated animals, as well as to conduct systematic multisite comparative studies. (C) 2019 The Association for the Study of Animal Behaviour. Published by Elsevier Ltd. All rights reserved.
C1 [McCarthy, Maureen S.; Despres-Einspenner, Marie-Lyne; Samuni, Liran; Angedakin, Samuel; Arandjelovic, Mimi; Boesch, Christophe; Dieguez, Paula; Wittig, Roman M.; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Farine, Damien R.] Max Planck Inst Anim Behav, Dept Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Konstanz, Ctr Adv Study Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Konstanz, Dept Biol, Constance, Germany.
   [Farine, Damien R.] Univ Oxford, Edward Grey Inst Field Ornithol, Dept Zool, Oxford, England.
   [Samuni, Liran; Wittig, Roman M.] CSRS, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Havercamp, Kristin] Kyoto Univ, Wildlife Res Ctr, Kyoto, Japan.
   [Knight, Alex] Univ Auckland, Sch Biol Sci, Auckland, New Zealand.
   [Langergraber, Kevin E.] Arizona State Univ, Sch Human Evolut & Social Change, Tempe, AZ USA.
   [Langergraber, Kevin E.] Arizona State Univ, Inst Human Origins, Tempe, AZ USA.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Halle, Germany.
RP McCarthy, MS (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM maureen_mc@eva.mpg.de
RI Farine, Damien R./Y-2454-2019
OI Farine, Damien R./0000-0003-2208-7613; , Kristin/0000-0002-4685-8366
FU Max Planck Society Innovation Fund; Heinz L. Krekeler Foundation; Robert
   Bosch Foundation; Centre Suisse de Recherches Scientifiques; Centre for
   Forest ResearcheFonds de Recherche Quebec Nature et Technologies
   International internship program; DFG Centre for Excellence 2117 'Centre
   for the Advanced Study of Collective Behaviour'German Research
   Foundation (DFG) [422037984]
FX For support, we thank the Max Planck Society Innovation Fund and the
   Heinz L. Krekeler Foundation, the Robert Bosch Foundation, Centre Suisse
   de Recherches Scientifiques, the Centre for Forest ResearcheFonds de
   Recherche Quebec Nature et Technologies International internship
   program, and the DFG Centre for Excellence 2117 'Centre for the Advanced
   Study of Collective Behaviour' (ID: 422037984). For research permission,
   we thank Ministere de l'Enseignement Superieur et de la Recherche
   Scientifique, Ministere de l'Environnement et des Eaux et Forets, the
   Office Ivoirien de Parcs et Reserves in Cote d'Ivoire, Uganda National
   Council for Science and Technology (UNCST) and Uganda Wildlife Authority
   (UWA). We thank the Tai Chimpanzee Project (TCP) and the Ngogo
   Chimpanzee Project for the opportunity to conduct research in Cote
   d'Ivoire and Uganda, respectively. We thank Appollinaire Gnahe Djirian,
   Oulai Landry, Frederic Yehanon Oulai and Anna Preis for providing party
   composition data extracted from the long-term database of the TCP, as
   well as Frederic Yehanon Oulai for identifying chimpanzees in the camera
   trap videos. We thank Mizuki Murai for logistical support in PanAf data
   collection and coordination.
CR Alarcon-Nieto G, 2018, METHODS ECOL EVOL, V9, P1536, DOI 10.1111/2041-210X.13005
   Allen J, 2013, SCIENCE, V340, P485, DOI 10.1126/science.1231976
   ALTMANN J, 1974, BEHAVIOUR, V49, P227, DOI 10.1163/156853974X00534
   Aplin LM, 2012, P ROY SOC B-BIOL SCI, V279, P4199, DOI 10.1098/rspb.2012.1591
   Berger-Wolf T. Y, 2017, BLOOMB DAT GOOD EXCH
   Brent LJN, 2011, AM J PRIMATOL, V73, P720, DOI 10.1002/ajp.20949
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Butts, 2008, J STAT SOFTW, V24, P1, DOI DOI 10.18637/JSS.V024.I06
   CAIRNS SJ, 1987, ANIM BEHAV, V35, P1454, DOI 10.1016/S0003-3472(87)80018-0
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Carter KD, 2013, ANIM BEHAV, V85, P385, DOI 10.1016/j.anbehav.2012.11.011
   CHAPMAN CA, 1993, AM J PRIMATOL, V31, P263, DOI 10.1002/ajp.1350310403
   Coles RC, 2012, INT J PRIMATOL, V33, P93, DOI 10.1007/s10764-011-9555-2
   Cross PC, 2005, ANIM BEHAV, V69, P499, DOI 10.1016/j.anbehav.2004.08.006
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Csardi G., 2006, INTERJOURNAL COMPLEX, V1695, P1, DOI DOI 10.3724/SP.J.1087.2009.02191
   Davis GH, 2018, ANIM BEHAV, V141, P29, DOI 10.1016/j.anbehav.2018.04.012
   Dekker D, 2007, PSYCHOMETRIKA, V72, P563, DOI 10.1007/s11336-007-9016-1
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Dorning J, 2016, THESIS
   Elbroch LM, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1701218
   Farine DR, 2015, J ANIM ECOL, V84, P1144, DOI 10.1111/1365-2656.12418
   Farine DR, 2015, ANIM BEHAV, V104, pE1, DOI 10.1016/j.anbehav.2014.11.019
   Farine DR, 2014, ANIM BEHAV, V89, P141, DOI 10.1016/j.anbehav.2014.01.001
   Farine DR, 2013, METHODS ECOL EVOL, V4, P1187, DOI 10.1111/2041-210X.12121
   Farine DR, 2012, ANIM BEHAV, V84, P1271, DOI 10.1016/j.anbehav.2012.08.008
   Foerster S, 2015, ANIM BEHAV, V105, P139, DOI 10.1016/j.anbehav.2015.04.012
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Goldenberg SZ, 2016, CURR BIOL, V26, P75, DOI 10.1016/j.cub.2015.11.005
   Gompper ME, 1996, BEHAV ECOL, V7, P254, DOI 10.1093/beheco/7.3.254
   Goodall J., 1986, CHIMPANZEES GOMBE PA
   Hamede RK, 2009, ECOL LETT, V12, P1147, DOI 10.1111/j.1461-0248.2009.01370.x
   HAWKINS RE, 1970, J WILDLIFE MANAGE, V34, P407, DOI 10.2307/3799027
   Hobaiter C, 2014, PLOS BIOL, V12, DOI 10.1371/journal.pbio.1001960
   Hohmann G, 2002, BEHAVIOURAL DIVERSITY IN CHIMPANZEES AND BONOBOS, P138, DOI 10.1017/CBO9780511606397.014
   Jacoby DMP, 2016, TRENDS ECOL EVOL, V31, P301, DOI 10.1016/j.tree.2016.01.011
   Johnson KVA, 2017, ANIM BEHAV, V128, P21, DOI 10.1016/j.anbehav.2017.04.001
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Kays RW, 2001, J ZOOL, V253, P491, DOI 10.1017/S0952836901000450
   KRACKHARDT D, 1988, SOC NETWORKS, V10, P359, DOI 10.1016/0378-8733(88)90004-4
   Kummer H., 1995, QUEST SACRED BABOON
   Lusseau D, 2008, ANIM BEHAV, V75, P1809, DOI 10.1016/j.anbehav.2007.10.029
   Maldonado-Chaparro AA, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2018.1577
   Mandal D, 2019, THESIS
   McCarthy MS, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22904
   McCreery EK, 2000, BEHAVIOUR, V137, P579, DOI 10.1163/156853900502222
   Metz MC, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017332
   Mielke A, 2017, ROY SOC OPEN SCI, V4, DOI [10.1098/rsos.171296, 10.1098/rsos.172143]
   Murray CM, 2007, ANIM BEHAV, V74, P1795, DOI 10.1016/j.anbehav.2007.03.024
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Nishida T., 1968, Primates, V9, P167, DOI 10.1007/BF01730971
   Ramos-Fernandez G, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2018.0532
   Ren BP, 2012, INT J PRIMATOL, V33, P1096, DOI 10.1007/s10764-012-9586-3
   Robley A., 2010, A RYLAH I ENV RES TE, V201, P1
   Rubenstein DI, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138645
   Ryder TB, 2012, BIOL LETTERS, V8, P917, DOI 10.1098/rsbl.2012.0536
   Sanz C, 2004, AM NAT, V164, P567, DOI 10.1086/424803
   Shizuka D, 2016, ANIM BEHAV, V112, P237, DOI 10.1016/j.anbehav.2015.12.007
   Siegel S., 1988, NONPARAMETRIC STAT B, V2nd ed
   Silk MJ, 2017, BIOSCIENCE, V67, P245, DOI 10.1093/biosci/biw175
   Smith JE, 2008, ANIM BEHAV, V76, P619, DOI 10.1016/j.anbehav.2008.05.001
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   SYMINGTON MM, 1990, INT J PRIMATOL, V11, P47, DOI 10.1007/BF02193695
   Vaidyanathan G, 2011, NATURE, V476, P266, DOI 10.1038/476266a
   van Schaik CP, 1999, PRIMATES, V40, P69, DOI 10.1007/BF02557703
   Weber N, 2013, CURR BIOL, V23, pR915, DOI 10.1016/j.cub.2013.09.011
   Whitehead H., 2008, ANAL ANIMAL SOC QUAN
   Wilson ML, 2014, NATURE, V513, P414, DOI 10.1038/nature13727
   Wittemyer G, 2005, ANIM BEHAV, V69, P1357, DOI 10.1016/j.anbehav.2004.08.018
   Wittiger L, 2013, BEHAV ECOL SOCIOBIOL, V67, P1097, DOI 10.1007/s00265-013-1534-5
   Wursig B., 1990, Reports of the International Whaling Commission Special Issue, P43
   [No title captured]
NR 73
TC 8
Z9 8
U1 1
U2 13
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0003-3472
EI 1095-8282
J9 ANIM BEHAV
JI Anim. Behav.
PD NOV
PY 2019
VL 157
BP 227
EP 238
DI 10.1016/j.anbehav.2019.08.008
PG 12
WC Behavioral Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Behavioral Sciences; Zoology
GA JI9KJ
UT WOS:000493780600024
DA 2022-02-10
ER

PT J
AU Pal, R
   Bhattacharya, T
   Qureshi, Q
   Buckland, ST
   Sathyakumar, S
AF Pal, Ranjana
   Bhattacharya, Tapajit
   Qureshi, Qamar
   Buckland, Stephen T.
   Sathyakumar, Sambandam
TI Using distance sampling with camera traps to estimate the density of
   group-living and solitary mountain ungulates
SO ORYX
LA English
DT Article
DE Bharal; camera trapping; density estimates; musk deer; point transect
   method; subalpine; trans-Himalaya; Upper Bhagirathi basin
ID MOSCHUS-CHRYSOGASTER; DOMESTIC LIVESTOCK; NATIONAL-PARK; RESOURCE USE;
   MUSK DEER; SNOW; ABUNDANCE; UNCIA; DUNG; TIME
AB Throughout the Himalaya, mountain ungulates are threatened by hunting for meat and body parts, habitat loss, and competition with livestock. Accurate population estimates are important for conservation management but most of the available methods to estimate ungulate densities are difficult to implement in mountainous terrain. Here, we tested the efficacy of the recent extension of the point transect method, using camera traps for estimating density of two mountain ungulates: the group-living Himalayan blue sheep or bharal Pseudois nayaur and the solitary Himalayan musk deer Moschus leucogaster. We deployed camera traps in 2017-2018 for the bharal (summer: 21 locations; winter: 25) in the trans-Himalayan region (3,000-5,000 m) and in 2018-2019 for the musk deer (summer: 30 locations; winter: 28) in subalpine habitats (2,500-3,500 m) in the Upper Bhagirathi basin, Uttarakhand, India. Using distance sampling with camera traps, we estimated the bharal population to be 0.51 +/- SE 0.1 individuals/km(2) (CV = 0.31) in summer and 0.64 +/- SE 0.2 individuals/km(2) (CV = 0.37) in winter. For musk deer, the estimated density was 0.4 +/- SE 0.1 individuals/km(2) (CV = 0.34) in summer and 0.1 +/- SE 0.05 individuals/km(2) (CV = 0.48) in winter. The high variability in these estimates is probably a result of the topography of the landscape and the biology of the species. We discuss the potential application of distance sampling with camera traps to estimate the density of mountain ungulates in remote and rugged terrain, and the limitations of this method.
C1 [Pal, Ranjana; Qureshi, Qamar; Sathyakumar, Sambandam] Wildlife Inst India, Dehra Dun 248001, Uttarakhand, India.
   [Bhattacharya, Tapajit] Durgapur Govt Coll, Durgapur, India.
   [Buckland, Stephen T.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
RP Sathyakumar, S (corresponding author), Wildlife Inst India, Dehra Dun 248001, Uttarakhand, India.
EM ssk@wii.gov.in
OI SATHYAKUMAR, SAMBANDAM/0000-0003-2027-4706; Buckland,
   Stephen/0000-0002-9939-709X
FU Department of Science and Technology, Government of IndiaDepartment of
   Science & Technology (India) [DST/SPLICE/CCP/ NMSHE/TF-2/WII/2014[G]];
   Wildlife Institute of India; Miriam Rothschild Travel Bursary Programme
FX This work is part of a project initiated under the National Mission for
   Sustaining the Himalayan Ecosystem (NMSHE) Programme funded by the
   Department of Science and Technology, Government of India (grant no.:
   DST/SPLICE/CCP/ NMSHE/TF-2/WII/2014[G]). The Miriam Rothschild Travel
   Bursary Programme provided funding for a 4-week internship for R. Pal
   with S.T. Buckland at St Andrews University, UK. We thank the Director
   and Dean of the Wildlife Institute of India for their guidance and
   support; D.V.S. Khati, Principal Chief Conservator of Forests and Chief
   Wildlife Warden, Uttarakhand, for granting research permission; Sandeep
   Kumar, Divisional Forest Officer and former Deputy Director, Gangotri
   National Park, and Shrawan Kumar for their support and cooperation; and
   L. Corlatti for reviewing the manuscript.
CR Bagchi S, 2006, J ZOOL, V268, P217, DOI 10.1111/j.1469-7998.2005.00030.x
   Bagchi S, 2010, OECOLOGIA, V164, P1075, DOI 10.1007/s00442-010-1690-5
   Bhardwaj Manish, 2010, Galemys, V22, P545
   Bhattacharya T., 2012, Proceedings of the Zoological Society (Calcutta), V65, P11, DOI 10.1007/s12595-012-0025-4
   Bhattacharya T, 2012, MAMM STUDY, V37, P173, DOI 10.3106/041.037.0302
   Bhattacharya T, 2011, MT RES DEV, V31, P209, DOI 10.1659/MRD-JOURNAL-D-10-00069.1
   Buckland S.T., 2001, pi
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Cavallini Paolo, 1992, Journal of the Bombay Natural History Society, V89, P302
   Chandola S., 2009, THESIS HNB GARHWAL U
   Corlatti L, 2015, POPUL ECOL, V57, P409, DOI 10.1007/s10144-015-0481-6
   Dendup P, 2018, INT J CONSERV SCI, V9, P193
   Forsyth DM, 1997, NEW ZEAL J ECOL, V21, P97
   Green M.J.B., 1985, THESIS U CAMBRIDGE C
   Hofmeester TR, 2017, REMOTE SENS ECOL CON, V3, P81, DOI 10.1002/rse2.25
   Howe EJ, 2019, METHODS ECOL EVOL, V10, P38, DOI 10.1111/2041-210X.13082
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Jarvis A., 2008, CGIAR CSI SRTM 90M D
   Kittur S, 2010, EUR J WILDLIFE RES, V56, P195, DOI 10.1007/s10344-009-0302-3
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kumar A, 2017, CURR SCI INDIA, V113, P1032
   Laing SE, 2003, J APPL ECOL, V40, P1102, DOI 10.1111/j.1365-2664.2003.00861.x
   McCarthy KP, 2008, J WILDLIFE MANAGE, V72, P1826, DOI 10.2193/2008-040
   MCNAUGHTON SJ, 1979, AM NAT, V113, P691, DOI 10.1086/283426
   Mishra C, 2004, J APPL ECOL, V41, P344, DOI 10.1111/j.0021-8901.2004.00885.x
   Namgail T, 2007, ECOL RES, V22, P25, DOI 10.1007/s11284-006-0015-y
   O'Neill H., 2008, THESIS IMPERIAL COLL
   Pal R, 2021, ORYX, V55, P657, DOI 10.1017/S0030605319001352
   Prater S.H., 1980, BOOK INDIAN MAMMALS
   Qamar QUZ, 2008, PAK J ZOOL, V40, P159
   Rodgers WA., 2000, WILDLIFE PROTECTED A
   Rovero F., 2016, CAMERA TRAPPING WILD
   Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Sathyakumar S., 2006, RELEASE CAPTIVE HIMA
   Sathyakumar S., 2013, PROJECT REPORT
   Sathyakumar S., 2002, MOUNTAIN UNGULATES E
   Sathyakumar S., 1994, THESIS SAURASHTRA U
   Sathyakumar S., 2013, MAMMALS S ASIA, V2, P223
   SCHALLER GB, 1988, BIOL CONSERV, V45, P179, DOI 10.1016/0006-3207(88)90138-3
   Singh NJ, 2011, ORYX, V45, P38, DOI 10.1017/S0030605310000839
   Sulkava RT, 2007, ANN ZOOL FENN, V44, P179
   Suryawanshi K.R., 2020, ORYX, V55, P66
   Suryawanshi KR, 2019, POPUL ECOL, V61, P268, DOI 10.1002/1438-390X.1027
   Suryawanshi KR, 2012, OECOLOGIA, V169, P581, DOI 10.1007/s00442-011-2237-0
   Suryawanshi KR, 2010, OECOLOGIA, V162, P453, DOI 10.1007/s00442-009-1467-x
   Takeshita K, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0164345
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Timmins R.J., 2015, IUCN RED LIST THREAT
   Yoccoz NG, 2001, TRENDS ECOL EVOL, V16, P446, DOI 10.1016/S0169-5347(01)02205-4
NR 52
TC 1
Z9 1
U1 2
U2 2
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0030-6053
EI 1365-3008
J9 ORYX
JI Oryx
PD SEP
PY 2021
VL 55
IS 5
BP 668
EP 676
AR PII S003060532000071X
DI 10.1017/S003060532000071X
PG 9
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA UL6JI
UT WOS:000692754900017
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Willi, M
   Pitman, RT
   Cardoso, AW
   Locke, C
   Swanson, A
   Boyer, A
   Veldthuis, M
   Fortson, L
AF Willi, Marco
   Pitman, Ross T.
   Cardoso, Anabelle W.
   Locke, Christina
   Swanson, Alexandra
   Boyer, Amy
   Veldthuis, Marten
   Fortson, Lucy
TI Identifying animal species in camera trap images using deep learning and
   citizen science
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal identification; camera trap; citizen science; convolutional
   neural networks; deep learning; machine learning
AB Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2% and 98.0%, whereas accuracies for identifying specific species were between 88.7% and 92.7%. Transferring information from CNNs trained on large datasets ("transfer-learning") was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.
C1 [Willi, Marco; Fortson, Lucy] Univ Minnesota, Sch Phys & Astron, Minneapolis, MN 55455 USA.
   [Pitman, Ross T.] Panthera, New York, NY USA.
   [Pitman, Ross T.] Univ Cape Town, Inst Communities & Wildlife Africa, Dept Biol Sci, Cape Town, South Africa.
   [Cardoso, Anabelle W.] Univ Oxford, Sch Geog & Environm, Oxford, England.
   [Locke, Christina] Wisconsin Dept Nat Resources, Off Appl Sci, Madison, WI USA.
   [Locke, Christina; Swanson, Alexandra; Veldthuis, Marten] Univ Oxford, Dept Astrophys, Oxford, England.
   [Boyer, Amy] Adler Planetarium, Chicago, IL USA.
RP Willi, M (corresponding author), Univ Minnesota, Sch Phys & Astron, Minneapolis, MN 55455 USA.
EM will5448@umn.edu
OI Willi, Marco/0000-0002-0041-396X; Cardoso, Anabelle
   Williamson/0000-0002-4327-7259
FU National Science FoundationNational Science Foundation (NSF) [IIS
   1619177]
FX National Science Foundation, Grant/Award Number: IIS 1619177
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Bowyer A., 2015, THIS IMAGE INTENTION
   Branson S., 2017, CVPR
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Fortson L, 2012, CH CRC DATA MIN KNOW, P213
   Gal Y., 2015, BAYESIAN CONVOLUTION
   Gal Y., 2016, THESIS, P174
   Giraldo-Zuluaga JH, 2017, PROC INT C TOOLS ART, P53, DOI 10.1109/ICTAI.2017.00020
   Glorot X., 2010, PROC 13 INT C ARTIFI, V9, P249, DOI DOI 10.1038/S41593-021-00857-X
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Guo CA, 2017, PR MACH LEARN RES, V70
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Parham J., 2016, APPL COMP VIS WORKSH, P1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Simpson E., 2013, DECISION MAKING IMPE, P1, DOI [DOI 10.1007/978-3-642-36406-8_1, DOI 10.1007/978-3-642-36406-8]
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zevin M., 2016, CLASSICAL QUANTUM GR, V34, P1
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 28
TC 80
Z9 82
U1 12
U2 58
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JAN
PY 2019
VL 10
IS 1
BP 80
EP 91
DI 10.1111/2041-210X.13099
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HK2NU
UT WOS:000457750600008
OA Bronze
DA 2022-02-10
ER

PT C
AU Schneider, S
   Taylor, GW
   Kremer, SC
AF Schneider, Stefan
   Taylor, Graham W.
   Kremer, Stefan C.
GP IEEE
TI Deep Learning Object Detection Methods for Ecological Camera Trap Data
SO 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV)
LA English
DT Proceedings Paper
CT 15th Conference on Computer and Robot Vision (CRV)
CY MAY 08-11, 2018
CL Toronto, CANADA
SP Canadian Image Proc & Pattern Recognit Soc, Assoc Canadienne Traitement Images Reconnaissance Formes, MDA, Modiface, NextAI, SPORTLOGiQ, StradigiAI, York Univ Vis Sci Applicat Program, York Univ Ctr Vis Res, ElementAI, EPSON, Miovision, Trans Plan
ID IDENTIFICATION
AB Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem's animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0% and 76.7% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
   [Taylor, Graham W.] Univ Guelph, Sch Engn, Guelph, ON, Canada.
   [Taylor, Graham W.] Vector Inst Artificial Intelligence, Toronto, ON, Canada.
   [Taylor, Graham W.] Canadian Inst Adv Res, Toronto, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca; gwtaylor@uoguelph.ca; skremer@uoguelph.ca
CR Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   BALFOORT HW, 1992, J PLANKTON RES, V14, P575, DOI 10.1093/plankt/14.4.575
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   CHAO A, 1989, BIOMETRICS, V45, P427, DOI 10.2307/2531487
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Fukushima K., 1979, ELECTR COMMUN JPN, V62, P11
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   GYSEL LESLIE W., 1956, JOUR WILDLIFE MANAGEMENT, V20, P451, DOI 10.2307/3797161
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343
   JEFFRIES HP, 1984, MAR BIOL, V78, P329, DOI 10.1007/BF00393019
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kotsiantis S. B., 2007, SUPERVISED MACHINE L
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Norouzzadeh M. S., 2017, ARXIV170305830V5
   Nowozin S, 2014, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2014.77
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   ROBSON D. S., 1964, TRANS AMER FISH SOC, V93, P215, DOI 10.1577/1548-8659(1964)93[215:SSIPME]2.0.CO;2
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   SIMPSON R, 1991, IEEE CONFERENCE ON NEURAL NETWORKS FOR OCEAN ENGINEERING, P223, DOI 10.1109/ICNN.1991.163354
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 34
TC 30
Z9 30
U1 2
U2 12
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-6481-0
PY 2018
BP 321
EP 328
DI 10.1109/CRV.2018.00052
PG 8
WC Computer Science, Theory & Methods; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BL9NK
UT WOS:000457719100042
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Tabak, MA
   Norouzzadeh, MS
   Wolfson, DW
   Sweeney, SJ
   Vercauteren, KC
   Snow, NP
   Halseth, JM
   Di Salvo, PA
   Lewis, JS
   White, MD
   Teton, B
   Beasley, JC
   Schlichting, PE
   Boughton, RK
   Wight, B
   Newkirk, ES
   Ivan, JS
   Odell, EA
   Brook, RK
   Lukacs, PM
   Moeller, AK
   Mandeville, EG
   Clune, J
   Miller, RS
AF Tabak, Michael A.
   Norouzzadeh, Mohammad S.
   Wolfson, David W.
   Sweeney, Steven J.
   Vercauteren, Kurt C.
   Snow, Nathan P.
   Halseth, Joseph M.
   Di Salvo, Paul A.
   Lewis, Jesse S.
   White, Michael D.
   Teton, Ben
   Beasley, James C.
   Schlichting, Peter E.
   Boughton, Raoul K.
   Wight, Bethany
   Newkirk, Eric S.
   Ivan, Jacob S.
   Odell, Eric A.
   Brook, Ryan K.
   Lukacs, Paul M.
   Moeller, Anna K.
   Mandeville, Elizabeth G.
   Clune, Jeff
   Miller, Ryan S.
TI Machine learning to classify animal species in camera trap images:
   Applications in ecology
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE artificial intelligence; camera trap; convolutional neural network; deep
   neural networks; image classification; machine learning; r package;
   remote sensing
AB Motion-activated cameras ("camera traps") are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or "out-of-distribution" in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82% accuracy and correctly identified 94% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.
C1 [Tabak, Michael A.; Wolfson, David W.; Sweeney, Steven J.; Di Salvo, Paul A.; Miller, Ryan S.] USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO 80526 USA.
   [Tabak, Michael A.; Mandeville, Elizabeth G.] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
   [Norouzzadeh, Mohammad S.; Clune, Jeff] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Vercauteren, Kurt C.; Snow, Nathan P.; Halseth, Joseph M.] USDA, Natl Wildlife Res Ctr, Ft Collins, CO USA.
   [Lewis, Jesse S.] Arizona State Univ, Coll Integrat Sci & Arts, Mesa, AZ USA.
   [White, Michael D.; Teton, Ben] Tejon Ranch Conservancy, Lebec, CA USA.
   [Beasley, James C.; Schlichting, Peter E.] Univ Georgia, Savannah River Ecol Lab, Warnell Sch Forestry & Nat Resources, Aiken, SC USA.
   [Boughton, Raoul K.; Wight, Bethany] Univ Florida, Wildlife Ecol & Conservat, Range Cattle Res & Educ Ctr, Ona, FL USA.
   [Newkirk, Eric S.; Ivan, Jacob S.; Odell, Eric A.] Colorado Pk & Wildlife, Ft Collins, CO USA.
   [Brook, Ryan K.] Univ Saskatchewan, Dept Anim & Poultry Sci, Saskatoon, SK, Canada.
   [Lukacs, Paul M.; Moeller, Anna K.] Univ Montana, WA Franke Coll Forestry & Conservat, Dept Ecosyst & Conservat Sci, Wildlife Biol Program, Missoula, MT 59812 USA.
   [Mandeville, Elizabeth G.] Univ Wyoming, Dept Bot, Laramie, WY 82071 USA.
RP Tabak, MA; Miller, RS (corresponding author), USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO 80526 USA.; Tabak, MA (corresponding author), Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
EM tabakma@gmail.com; ryan.s.miller@aphis.usda.gov
RI Ivan, Jacob S/R-9359-2018; Wolfson, David/AAJ-3485-2020
OI Wolfson, David/0000-0003-1098-9206; Snow, Nathan/0000-0002-5171-6493;
   Tabak, Michael/0000-0002-2986-7885
FU U.S. Department of EnergyUnited States Department of Energy (DOE)
   [DE-EM0004391]; USDA Animal and Plant Health Inspection Service,
   National Wildlife Research Center and Center for Epidemiology and Animal
   Health; Canadian Natural Science and Engineering Research CouncilNatural
   Sciences and Engineering Research Council of Canada (NSERC); University
   of Saskatchewan; Idaho Department of Game and Fish
FX U.S. Department of Energy, Grant/Award Number: DE-EM0004391; USDA Animal
   and Plant Health Inspection Service, National Wildlife Research Center
   and Center for Epidemiology and Animal Health; Colorado Parks and
   Wildlife; Canadian Natural Science and Engineering Research Council;
   University of Saskatchewan; Idaho Department of Game and Fish
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Advanced Research Computing Center, 2012, MOUNT MOR IBM SYST 1
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Rovero F., 2013, HYSTRIX ITALIAN J MA, V24, P585
   Scott AB, 2018, AVIAN DIS, V62, P65, DOI 10.1637/11761-101917-Reg.1
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 17
TC 100
Z9 101
U1 21
U2 86
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD APR
PY 2019
VL 10
IS 4
BP 585
EP 590
DI 10.1111/2041-210X.13120
PG 6
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HR3KR
UT WOS:000463036400013
OA Green Submitted, Green Published, Bronze
DA 2022-02-10
ER

PT J
AU Peres, PHF
   Polverini, MS
   Oliveira, ML
   Duarte, JMB
AF Peres, Pedro Henrique F.
   Polverini, Maxihilian S.
   Oliveira, Marcio L.
   Duarte, Jose Mauricio B.
TI Accessing camera trap survey feasibility for estimating Blastocerus
   dichotomus (Cetartiodactyla, Cervidae) demographic parameters
SO IHERINGIA SERIE ZOOLOGIA
LA English
DT Article
DE Aerial survey; capture-recapture; distance sampling; population
   estimate; sex ratio
ID WHITE-TAILED DEER; INFRARED-TRIGGERED CAMERAS; MARSH DEER; PANTANAL
   WETLAND; HERD COMPOSITION; POPULATION-SIZE; AERIAL SURVEY; DENSITY;
   BRAZIL; ABUNDANCE
AB Demographic information is the basis for evaluating and planning conservation strategies for an endangered species. However, in numerous situations there are methodological or financial limitations to obtain such information for some species. The marsh deer, an endangered Neotropical cervid, is a challenging species to obtain biological information. To help achieve such aims, the study evaluated the applicability of camera traps to obtain demographic information on the marsh deer compared to the traditional aerial census method. Fourteen camera traps were installed for three months on the Capao da Cruz floodplain, in state of Sao Paulo, and ten helicopter flyovers were made along a 13-kilometer trajectory to detect resident marsh deer. In addition to counting deer, the study aimed to identify the sex, age group and individual identification of the antlered males recorded. Population estimates were performed using the capture-mark-recapture method with the camera trap data and by the distance sampling method for aerial observation data. The costs and field efforts expended for both methodologies were calculated and compared. Twenty independent photographic records and 42 sightings were obtained and generated estimates of 0.98 and 1.06 ind/km(2), respectively. In contrast to the aerial census, camera traps allowed us to individually identify branch-antlered males, determine the sex ratio and detect fawns in the population. The cost of camera traps was 78% lower but required 20 times more field effort. Our analysis indicates that camera traps present a superior cost-benefit ratio compared to aerial surveys, since they are more informative, cheaper and offer simpler logistics. Their application extends the possibilities of studying a greater number of populations in a long-term monitoring.
C1 [Peres, Pedro Henrique F.; Polverini, Maxihilian S.; Oliveira, Marcio L.; Duarte, Jose Mauricio B.] Univ Estadual Paulista, UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, FCAV, Via Acesso Paulo Donato Castellane S-N, BR-14884900 Jaboticabal, SP, Brazil.
RP Peres, PHF (corresponding author), Univ Estadual Paulista, UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, FCAV, Via Acesso Paulo Donato Castellane S-N, BR-14884900 Jaboticabal, SP, Brazil.
EM pedrof182@gmail.com
RI Duarte, José Maurício Barbanti/AAG-5149-2019; de Oliveira, Márcio
   L/F-3792-2012
OI Duarte, José Maurício Barbanti/0000-0002-7805-0265; de Oliveira, Márcio
   L/0000-0002-7705-0626; Peres, Pedro Henrique/0000-0002-3158-0963
FU Funbio; FAPESPFundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP)
FX Field work for this study was supported by Funbio in the context of
   Tropical Forest Conservation Act grant and the authors PHFP and MLO
   receives a scholarship from FAPESP. We thank FUNEP and NUPECCE for
   logistical support, the Instituto Florestal de Sao Paulo (COTEC SMA:
   260108-004.556/2013) and the Instituto Chico Mendes de Conservacao da
   Biodiversidade (SISBIO 38267-2) for permission to conduct research in
   the study site. Finally we sincerely thank Jonathan Hill for English
   review of the manuscript.
CR Akaike H., 1985, CELEBRATION STAT ISI, P387, DOI DOI 10.1007/978-1-4613-8560-8_1
   Andriolo A, 2005, BRAZ ARCH BIOL TECHN, V48, P807, DOI 10.1590/S1516-89132005000600017
   Andriolo A, 2013, ZOOLOGIA-CURITIBA, V30, P630, DOI 10.1590/S1984-46702013005000015
   Bender LC, 2006, WILDLIFE SOC B, V34, P1225, DOI 10.2193/0091-7648(2006)34[1225:UOHCAA]2.0.CO;2
   Cabrera A., 1961, CIENCIAS ZOOLOGICAS, V4, P309
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   CAUGHLEY G, 1977, J WILDLIFE MANAGE, V41, P605, DOI 10.2307/3799980
   CAUGHLEY G, 1981, AUST WILDLIFE RES, V8, P1
   Curtis Paul D., 2009, Human-Wildlife Conflicts, V3, P116
   DeYoung CA, 2011, BIOLOGY AND MANAGEMENT OF WHITE-TAILED DEER, P147
   Dougherty SQ, 2012, POPUL ECOL, V54, P357, DOI 10.1007/s10144-012-0311-z
   DUARTE J.M.B., 2012, BIODIVERS BRAS, P3
   Duarte J. M. B., 2016, IUCN RED LIST THREAT
   Duarte JMB, 1996, GUIA IDENTIFICACAO C
   Figueira C. J. M., 2005, Braz. J. Biol., V65, P263, DOI 10.1590/S1519-69842005000200009
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Hewison AJM, 1996, BEHAV ECOL, V7, P461, DOI 10.1093/beheco/7.4.461
   Hofmann R. K., 1976, REV FORESTAL PERU, V6, P1
   Ikeda T, 2013, MAMM STUDY, V38, P29, DOI 10.3106/041.038.0103
   Jacobson HA, 1997, WILDLIFE SOC B, V25, P547
   Junqueira J. F. D., 1940, CHACARAS QUINTAIS, V62, P330
   Karanth K. Ullas, 2004, P229
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   KENNEY AJ, 1998, PROGRAMS ECOLOGICAL
   Koerth BH, 1997, WILDLIFE SOC B, V25, P557
   Laake J.L., 1993, DISTANCE USERS GUIDE
   Mccoy JC, 2011, J WILDLIFE MANAGE, V75, P472, DOI 10.1002/jwmg.54
   Moore M. T., 2013, J SE ASS FISH WILDLI, V1, P127
   MOURAO G, 1995, BIOL CONSERV, V73, P27, DOI 10.1016/0006-3207(94)00097-A
   Mourao G, 2000, BIOL CONSERV, V92, P175, DOI 10.1016/S0006-3207(99)00051-8
   Noss A.J., 2003, Tapir Conservation, V12, P24
   Pease BS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0166689
   Pereira R. J. G., 2010, NEOTROPICAL CERVIDOL
   Pinder L, 1996, BIOL CONSERV, V75, P87, DOI 10.1016/0006-3207(95)00033-X
   PINDER L, 1991, Mammalian Species, P1, DOI 10.2307/3504311
   Piovezan Ubiratan, 2010, P66
   Pohler PS, 2014, WILDLIFE SOC B, V38, P466, DOI 10.1002/wsb.430
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Roberts CW, 2006, J WILDLIFE MANAGE, V70, P263, DOI 10.2193/0022-541X(2006)70[263:COCARS]2.0.CO;2
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   SAO PAULO, 2013, PLANO MANEJO ESTACAO
   Schaller G.B., 1978, Oryx, V14, P345
   Schumacher F. X., 1943, JOUR TENNESSEE ACAD SCI, V18, P228
   Shields AV, 2012, SCI WORLD J, DOI 10.1100/2012/846218
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Tiepolo LM, 2010, IHERINGIA SER ZOOL, V100, P111, DOI 10.1590/S0073-47212010000200004
   Tomas WM, 2001, STUD NEOTROP FAUNA E, V36, P9, DOI 10.1076/snfe.36.1.9.8877
   Tomas WM, 1997, BIOL CONSERVACAO CER, P24
   Trolle M, 2008, BIOTROPICA, V40, P211, DOI 10.1111/j.1744-7429.2007.00350.x
   Trolle M, 2007, BIODIVERS CONSERV, V16, P1197, DOI 10.1007/s10531-006-9105-y
   Watts DE, 2008, J WILDLIFE MANAGE, V72, P360, DOI 10.2193/2007-166
   Webb Stephen L., 2010, International Journal of Ecology, V2010, P1
   Weckel M, 2011, WILDLIFE SOC B, V35, P445, DOI 10.1002/wsb.64
   WILSON KR, 1985, J MAMMAL, V66, P13, DOI 10.2307/1380951
NR 57
TC 4
Z9 4
U1 2
U2 16
PU FUNDACAO ZOOBOTANICA  RIO GRANDE SUL, MUSEU CIENCIAS NATURAIS
PI PORTO ALEGRE
PA CAIXA POSTAL 1188, PORTO ALEGRE, RS 00000, BRAZIL
SN 0073-4721
EI 1678-4766
J9 IHERINGIA SER ZOOL
JI Iheringia Ser. Zool.
PY 2017
VL 107
AR e2017041
DI 10.1590/1678-4766e2017041
PG 8
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA FO1BQ
UT WOS:000416492100002
OA Green Submitted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Wei, WD
   Luo, G
   Ran, JH
   Li, J
AF Wei, Weideng
   Luo, Gai
   Ran, Jianghong
   Li, Jing
TI Zilong: A tool to identify empty images in camera-trap data
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Image classification; Non-machine learning algorithm; Software; Wildlife
   management
ID POPULATIONS; TIGER
AB The use of camera traps to research and monitor wildlife results in a large number of images. Many of the images are the result of a false trigger, resulting in an empty photo. Manually removing empty images is time-intensive and costly. To increase image processing efficiency, we present a non-machine learning algorithm to identify empty images in camera-trap data, and developed freely available software, Zilong. We applied Zilong to 53,598 camera-trap images from 24 sites and compared the results to a CNN-based (Convolutional Neural Network) R package MLWIC (Machine Learning for Wildlife Image Classification). Zilong correctly identified 87% of animal images and correctly identified 85% of empty images, while MLWIC identified 65% and 69%, respectively. Our results suggest that Zilong performed better than MLWIC on identifying empty images. Zilong performed well for most of sites (22/24), with reduced performance identifying empty images when there was vegetation swinging significantly in front of camera (2/24). By using Zilong, wildlife researchers can reduce time and resources required to review camera-trap images.
C1 [Wei, Weideng; Luo, Gai; Ran, Jianghong; Li, Jing] Sichuan Univ, Coll Life Sci, Key Lab Bioresources & Ecoenvironm, Minist Educ, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R China.
RP Li, J (corresponding author), Sichuan Univ, Coll Life Sci, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R China.
EM ljtjf@126.com
CR Bradski G., 2016, LEARNING OPENCV 3 CO
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Coster M, 2001, CEMENT CONCRETE COMP, V23, P133, DOI 10.1016/S0958-9465(00)00058-5
   Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
   Garrote G, 2011, EUR J WILDLIFE RES, V57, P355, DOI 10.1007/s10344-010-0440-7
   Janecka JE, 2011, J MAMMAL, V92, P771, DOI 10.1644/10-MAMM-A-036.1
   Jumeau J, 2017, ECOL EVOL, V7, P7399, DOI 10.1002/ece3.3149
   Li S, 2010, IBIS, V152, P299, DOI 10.1111/j.1474-919X.2009.00989.x
   Luo G, 2019, AVIAN RES, V10, DOI 10.1186/s40657-019-0144-y
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien TG, 2008, BIRD CONSERV INT, V18, pS144, DOI 10.1017/S0959270908000348
   R Core Team, 2018, STATS PACK LANG ENV
   Singh P, 2017, J MAMMAL, V98, P1453, DOI 10.1093/jmammal/gyx104
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak M.A., 2018, METHODS ECOL EVOL, V2018, P1
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Tan CKW, 2017, BIOL CONSERV, V206, P65, DOI 10.1016/j.biocon.2016.12.012
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 22
TC 6
Z9 6
U1 5
U2 13
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD JAN
PY 2020
VL 55
AR 101021
DI 10.1016/j.ecoinf.2019.101021
PG 7
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA KH9RO
UT WOS:000510985900007
DA 2022-02-10
ER

PT J
AU Falzon, G
   Lawson, C
   Cheung, KW
   Vernes, K
   Ballard, GA
   Fleming, PJS
   Glen, AS
   Milne, H
   Mather-Zardain, A
   Meek, PD
AF Falzon, Greg
   Lawson, Christopher
   Cheung, Ka-Wai
   Vernes, Karl
   Ballard, Guy A.
   Fleming, Peter J. S.
   Glen, Alistair S.
   Milne, Heath
   Mather-Zardain, Atalya
   Meek, Paul D.
TI ClassifyMe: A Field-Scouting Software for the Identification of Wildlife
   in Camera Trap Images
SO ANIMALS
LA English
DT Article
DE camera traps; camera trap data management; deep learning; ecological
   software; species recognition; wildlife monitoring
AB Simple Summary Camera trap wildlife surveys can generate vast amounts of imagery. A key problem in the wildlife ecology field is that vast amounts of time is spent reviewing this imagery to identify the species detected. Valuable resources are wasted, and the scale of studies is limited by this review process. The use of computer software capable of extracting false positives, automatically identifying animals detected and sorting imagery could greatly increase efficiency. Artificial intelligence has been demonstrated as an effective option for automatically identifying species from camera trap imagery. Currently available code bases are inaccessible to the majority of users; requiring high-performance computers, advanced software engineering skills and, often, high-bandwidth internet connections to access cloud services. The ClassifyMe software tool is designed to address this gap and provides users the opportunity to utilise state-of-the-art image recognition algorithms without the need for specialised computer programming skills. ClassifyMe is especially designed for field researchers, allowing users to sweep through camera trap imagery using field computers instead of office-based workstations.
   Abstract We present ClassifyMe a software tool for the automated identification of animal species from camera trap images. ClassifyMe is intended to be used by ecologists both in the field and in the office. Users can download a pre-trained model specific to their location of interest and then upload the images from a camera trap to a laptop or workstation. ClassifyMe will identify animals and other objects (e.g., vehicles) in images, provide a report file with the most likely species detections, and automatically sort the images into sub-folders corresponding to these species categories. False Triggers (no visible object present) will also be filtered and sorted. Importantly, the ClassifyMe software operates on the user's local machine (own laptop or workstation)-not via internet connection. This allows users access to state-of-the-art camera trap computer vision software in situ, rather than only in the office. The software also incurs minimal cost on the end-user as there is no need for expensive data uploads to cloud services. Furthermore, processing the images locally on the users' end-device allows them data control and resolves privacy issues surrounding transfer and third-party access to users' datasets.
C1 [Falzon, Greg; Lawson, Christopher; Cheung, Ka-Wai] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
   [Vernes, Karl; Ballard, Guy A.; Fleming, Peter J. S.; Meek, Paul D.] Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2351, Australia.
   [Ballard, Guy A.; Milne, Heath] NSW Dept Primary Ind, Vertebrate Pest Res Unit, Allingham St, Armidale, NSW 2351, Australia.
   [Fleming, Peter J. S.] NSW Dept Primary Ind, Vertebrate Pest Res Unit, 1447 Forest Rd, Orange, NSW 2800, Australia.
   [Glen, Alistair S.] Manaaki Whenua Landcare Res, Private Bag 92170, Auckland 1142, New Zealand.
   [Mather-Zardain, Atalya] IO Design Australia, Armidale, NSW 2350, Australia.
   [Meek, Paul D.] NSW Dept Primary Ind, Vertebrate Pest Res Unit, POB 530, Coffs Harbour, NSW 2450, Australia.
RP Falzon, G (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
EM gfalzon2@une.edu.au; clawso21@une.edu.au; kcheun22@une.edu.au;
   kvernes@une.edu.au; guy.ballard@dpi.nsw.gov.au;
   peter.fleming@dpi.nsw.gov.au; GlenA@landcareresearch.co.nz;
   heath.milne@dpi.nsw.gov.au; io.atalya@gmail.com;
   paul.meek@dpi.nsw.gov.au
RI Falzon, Greg A/A-2657-2012; Vernes, Karl/A-2925-2011
OI Falzon, Greg A/0000-0002-1989-9357; Vernes, Karl/0000-0003-1635-9950;
   Meek, Paul/0000-0002-3792-5723; Ballard, Guy/0000-0002-0287-9720;
   Fleming, Peter/0000-0002-3490-6148
FU Centre for Invasive Species Solutions; Department of Agriculture and
   Water ResourcesAustralian GovernmentDepartment of Agriculture and Water
   Resources; NSW Department of Primary Industries, University of New
   England; Meat and Livestock AustraliaMeat and Livestock Australia;
   Australian Wool InnovationAustralian Wool Innovation
FX This research was funded by the 'Wild Dog Alert' research initiative
   delivered through the Invasive Animals Cooperative Research Centre (now
   Centre for Invasive Species Solutions), with major financial and in kind
   resources provided by the Department of Agriculture and Water Resources
   and NSW Department of Primary Industries, University of New England,
   Meat and Livestock Australia and Australian Wool Innovation.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Adamko P, 2017, GLOBALIZATION AND ITS SOCIO-ECONOMIC CONSEQUENCES, PTS I - VI, P1
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Beery S., 2018, P EUR C COMP VIS ECC, P456
   Bennett EL, 2015, CONSERV BIOL, V29, P54, DOI 10.1111/cobi.12377
   Butler DA, 2013, TORTS LAW J, V20, P235
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Claridge AW, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P205
   Csillik O, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2040039
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Falzon G., 2018, P 31 AUSTR WILDL MAN, P102
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009
   Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010
   Forrester T., 2013, P 98 ANN M EC SOC AM
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Gormley AM, 2011, J APPL ECOL, V48, P25, DOI 10.1111/j.1365-2664.2010.01911.x
   Gowen C, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P61
   Harmsen BJ, 2009, J MAMMAL, V90, P612, DOI 10.1644/08-MAMM-A-140R.1
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Jackson RM, 2006, WILDLIFE SOC B, V34, P772, DOI 10.2193/0091-7648(2006)34[772:ESLPAU]2.0.CO;2
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Khorozyan IG, 2008, INTEGR ZOOL, V3, P322, DOI 10.1111/j.1749-4877.2008.00111.x
   Lindenmayer D, 2017, SCIENCE, V356, P800, DOI 10.1126/science.aan1362
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Liu W, 2016, EUR C COMP VIS, V21, P37, DOI DOI 10.1007/978-3-319-46448-0_2
   Meek P., 2014, CAMERA TRAPPING WILD
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Meek P.D., 2015, CAMERA TRAPPING WILD
   Meek P.D., 2016, CAMERA TRAPPING WILD, P219
   Meek Paul D., 2020, Australian Zoologist, V40, P392, DOI 10.7882/AZ.2019.035
   Meek PD, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P349
   Meek PD, 2012, AUST MAMMAL, V34, P223, DOI 10.1071/AM11032
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Ramachandran P, 2018, METHODS ECOL EVOL, V9, P785, DOI 10.1111/2041-210X.12892
   Ramsey David S.L., 2015, Journal of Wildlife Management, V79, P491
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28
   Rovero F., 2016, CAMERA TRAPPING WILD
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Trolle M, 2003, J MAMMAL, V84, P607, DOI 10.1644/1545-1542(2003)084<0607:EOODIT>2.0.CO;2
   Valan M, 2019, SYST BIOL, V68, P876, DOI 10.1093/sysbio/syz014
   Vernes K, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P215
   Vernes Karl, 2015, Cat News, V62, P18
   Vernes K, 2014, AUST MAMMAL, V36, P128, DOI 10.1071/AM13037
   Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang J, 2016, ARTIF INTELL REV, V46, P543, DOI 10.1007/s10462-016-9491-9
   Zhang X, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124308
   Zolanvari, 2018, J OPEN SOURCE SOFTW, V3, P729, DOI [10.21105/joss.00729, DOI 10.21105/JOSS.00729]
NR 65
TC 16
Z9 17
U1 6
U2 20
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD JAN
PY 2020
VL 10
IS 1
AR 58
DI 10.3390/ani10010058
PG 16
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Veterinary Sciences; Zoology
GA KO2FZ
UT WOS:000515364400058
PM 31892236
OA Green Published, gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Schneider, S
   Greenberg, S
   Taylor, GW
   Kremer, SC
AF Schneider, Stefan
   Greenberg, Saul
   Taylor, Graham W.
   Kremer, Stefan C.
TI Three critical factors affecting automated image species recognition
   performance for camera traps
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera traps; computer vision; convolutional networks; deep learning;
   density estimation; monitoring; population dynamics; species
   classification
ID K-FOLD; REIDENTIFICATION; IDENTIFICATION; CAPABILITIES
AB Ecological camera traps are increasingly used by wildlife biologists to unobtrusively monitor an ecosystems animal population. However, manual inspection of the images produced is expensive, laborious, and time-consuming. The success of deep learning systems using camera trap images has been previously explored in preliminary stages. These studies, however, are lacking in their practicality. They are primarily focused on extremely large datasets, often millions of images, and there is little to no focus on performance when tasked with species identification in new locations not seen during training. Our goal was to test the capabilities of deep learning systems trained on camera trap images using modestly sized training data, compare performance when considering unseen background locations, and quantify the gradient of lower bound performance to provide a guideline of data requirements in correspondence to performance expectations. We use a dataset provided by Parks Canada containing 47,279 images collected from 36 unique geographic locations across multiple environments. Images represent 55 animal species and human activity with high-class imbalance. We trained, tested, and compared the capabilities of six deep learning computer vision networks using transfer learning and image augmentation: DenseNet201, Inception-ResNet-V3, InceptionV3, NASNetMobile, MobileNetV2, and Xception. We compare overall performance on "trained" locations where DenseNet201 performed best with 95.6% top-1 accuracy showing promise for deep learning methods for smaller scale research efforts. Using trained locations, classifications with <500 images had low and highly variable recall of 0.750 +/- 0.329, while classifications with over 1,000 images had a high and stable recall of 0.971 +/- 0.0137. Models tasked with classifying species from untrained locations were less accurate, with DenseNet201 performing best with 68.7% top-1 accuracy. Finally, we provide an open repository where ecologists can insert their image data to train and test custom species detection models for their desired ecological domain.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
   [Greenberg, Saul] Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
   [Taylor, Graham W.] Univ Guelph, Sch Engn, Vector Inst Artificial Intelligence, Guelph, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca
OI Greenberg, Saul/0000-0003-0174-9665; Kremer, Stefan
   C./0000-0002-3667-4379; Schneider, Stefan/0000-0002-6903-6605
CR Amodei D, 2016, PR MACH LEARN RES, V48
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   BALFOORT HW, 1992, J PLANKTON RES, V14, P575, DOI 10.1093/plankt/14.4.575
   Beery S., 2019, ARXIV191203538
   Beery S., 2019, ARXIV190405986
   Bengio Y, 2004, J MACH LEARN RES, V5, P1089
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   CHAO A, 1989, BIOMETRICS, V45, P427, DOI 10.2307/2531487
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Csurka Gabriela, 2017, ADV COMPUTER VISION
   Deb D., 2018, ARXIV180408790
   Eraslan G, 2019, NAT REV GENET, V20, P389, DOI 10.1038/s41576-019-0122-6
   Fukushima K., 1979, ELECTR COMMUN JPN, V62, P11
   Goldberg AB., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [DOI 10.2200/S00196ED1V01Y200906AIM006, 10.2200/S00196ED1V01Y200906AIM006]
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goutte C, 2005, LECT NOTES COMPUT SC, V3408, P345
   Greenberg S, 2019, ECOL EVOL, V9, P13706, DOI 10.1002/ece3.5767
   GYSEL LESLIE W., 1956, JOUR WILDLIFE MANAGEMENT, V20, P451, DOI 10.2307/3797161
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Howard A.G., 2013, SOME IMPROVEMENTS ON
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343
   JEFFRIES HP, 1984, MAR BIOL, V78, P329, DOI 10.1007/BF00393019
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Norouzzadeh M. S., 2019, ARXIV191009716
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   ROBSON D. S., 1964, TRANS AMER FISH SOC, V93, P215, DOI 10.1577/1548-8659(1964)93[215:SSIPME]2.0.CO;2
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   SIMPSON R, 1991, IEEE CONFERENCE ON NEURAL NETWORKS FOR OCEAN ENGINEERING, P223, DOI 10.1109/ICNN.1991.163354
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
NR 49
TC 19
Z9 19
U1 2
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD APR
PY 2020
VL 10
IS 7
BP 3503
EP 3517
DI 10.1002/ece3.6147
PG 15
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA LB1TE
UT WOS:000524417200029
PM 32274005
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Cloyed, CS
   Cappelli, LR
   Tilson, DA
   Crawford, JA
   Dell, AI
AF Cloyed, Carl S.
   Cappelli, Laura R.
   Tilson, David A.
   Crawford, John A.
   Dell, Anthony, I
TI Using Camera Traps to Assess Mammal and Bird Assemblages in a Midwestern
   Forest
SO JOURNAL OF FISH AND WILDLIFE MANAGEMENT
LA English
DT Article
DE diversity; edge effects; species composition; biomass; riparian;
   agriculture
ID ARMADILLO DASYPUS-NOVEMCINCTUS; WHITE-TAILED DEER; SCIURUS-CAROLINENSIS;
   HABITAT USE; SQUIRRELS; COYOTE; FRAGMENTATION; POPULATION; CARNIVORES;
   PREDATORS
AB Ecologists are increasing the use of remote technologies in their research, as these methods are less labor intensive than traditional methods and oftentimes minimize the number of human errors. Camera traps can be used to remotely measure abundance and community composition and offer the potential to measure some phenotypic traits, such as body size. We designed a camera-trap setup that enabled us to capture images of both large and small animals and used our camera-trap design to investigate the community composition of mammals and birds and to estimate the biomass of mammals along two transects in a conservation reserve in Missouri. One transect ran from the edge of an agricultural field to an upland forest, and the other transect ran from the edge of a wetland to an upland forest. Over the 4.5-wk study, our cameras recorded 2,245 images that comprised 483 individuals of 16 species of mammals and birds. Coyotes Canis latrans and nine-banded armadillos Dasypus novemcinctus were unique to the riparian transect, as were several bird species. Fewer species used the forest immediately adjacent to the agricultural field, but more species used the forest immediately adjacent to the wetland. Biomass estimates from our camera-trap images were similar to those of published accounts. This is the first study to use camera traps to successfully estimate biomass. We showed that the value and utility of camera traps in wildlife studies and monitoring can be expanded by 1) using multiple cameras at different heights from the ground so as to capture different-sized animals and 2) obtaining phenotypic information of the captured animals.
C1 [Cloyed, Carl S.; Crawford, John A.; Dell, Anthony, I] Natl Great Rivers Res & Educ Ctr, East Alton, IL 62024 USA.
   [Cloyed, Carl S.; Dell, Anthony, I] Washington Univ, Dept Biol, Campus Box 1137, St Louis, MO 63130 USA.
   [Cloyed, Carl S.] Dauphin Isl Sea Lab, Dauphin Isl, AL 36528 USA.
   [Cappelli, Laura R.] SUNY Coll Environm Sci & Forestry, Syracuse, NY 13210 USA.
   [Tilson, David A.] Virginia Polytech Inst & State Univ, Dept Nat Resources & Environm, Blacksburg, VA 24061 USA.
RP Cloyed, CS (corresponding author), Natl Great Rivers Res & Educ Ctr, East Alton, IL 62024 USA.; Cloyed, CS (corresponding author), Washington Univ, Dept Biol, Campus Box 1137, St Louis, MO 63130 USA.; Cloyed, CS (corresponding author), Dauphin Isl Sea Lab, Dauphin Isl, AL 36528 USA.
EM ccloyed@disl.org
OI Cappelli, Laura/0000-0003-3159-2777
FU National Great Rivers Research and Education Center (NGRREC)
   [NGRREC-IP2015-05, NGRREC-IP2015-06]
FX We thank Lindenwood University for access to the field sites. We thank
   the National Great Rivers Research and Education Center (NGRREC) for
   providing undergraduate internships for L.R.C. (NGRREC-IP2015-05) and
   D.A.T. (NGRREC-IP2015-06).
CR Andren H, 1997, OIKOS, V80, P193, DOI 10.2307/3546534
   Barea-Azcon JM, 2007, BIODIVERS CONSERV, V16, P1213, DOI 10.1007/s10531-006-9114-x
   Cattet MRL, 2002, CAN J ZOOL, V80, P1156, DOI 10.1139/Z02-103
   CLARK WR, 1989, J WILDLIFE MANAGE, V53, P982, DOI 10.2307/3809599
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   DEMERS MN, 1995, CONSERV BIOL, V9, P1159, DOI 10.1046/j.1523-1739.1995.9051148.x-i1
   Elliott AG, 2006, WILDLIFE SOC B, V34, P485, DOI 10.2193/0091-7648(2006)34[485:SMRTSA]2.0.CO;2
   Gehring TM, 2003, BIOL CONSERV, V109, P283, DOI 10.1016/S0006-3207(02)00156-8
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Gonzales EK, 2005, CAN FIELD NAT, V119, P343, DOI 10.22621/cfn.v119i3.143
   Hargis CD, 1999, J APPL ECOL, V36, P157, DOI 10.1046/j.1365-2664.1999.00377.x
   Hilderbrand GV, 2000, J WILDLIFE MANAGE, V64, P178, DOI 10.2307/3802988
   Humber JM, 2011, BIOL INVASIONS, V13, P2361, DOI 10.1007/s10530-011-0048-1
   HUMPHREY SR, 1974, BIOSCIENCE, V24, P457, DOI 10.2307/1296853
   Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Keller LF, 2002, TRENDS ECOL EVOL, V17, P230, DOI 10.1016/S0169-5347(02)02489-8
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Lambin EF, 2001, GLOBAL ENVIRON CHANG, V11, P261, DOI 10.1016/S0959-3780(01)00007-3
   Lee JC, 2009, SOUTHEAST NAT, V8, P157, DOI 10.1656/058.008.0114
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Lingle S, 2002, ECOLOGY, V83, P2037
   Lingle S, 2001, ETHOLOGY, V107, P125, DOI 10.1046/j.1439-0310.2001.00647.x
   McCleery RA, 2007, J WILDLIFE MANAGE, V71, P1149, DOI 10.2193/2006-282
   McDonough CM, 2000, AM MIDL NAT, V144, P139, DOI 10.1674/0003-0031(2000)144[0139:SOONBA]2.0.CO;2
   MILLS LS, 1995, CONSERV BIOL, V9, P395, DOI 10.1046/j.1523-1739.1995.9020395.x
   Moorcroft PR, 2006, P ROY SOC B-BIOL SCI, V273, P1651, DOI 10.1098/rspb.2005.3439
   MURCIA C, 1995, TRENDS ECOL EVOL, V10, P58, DOI 10.1016/S0169-5347(00)88977-6
   Nixon Charles M., 1994, Transactions of the Illinois State Academy of Science, V87, P187
   Pierce RA, 2011, ECOLOGY MANAGEMENT W, P1
   PROTHERO J, 1992, AM J PHYSIOL, V262, pR492, DOI 10.1152/ajpregu.1992.262.3.R492
   R Core Development Team, 2016, R LANG ENV STAT COMP
   Reighard Steven L., 2004, Proceedings of the South Dakota Academy of Science, V83, P47
   Roberts CW, 2006, J WILDLIFE MANAGE, V70, P263, DOI 10.2193/0022-541X(2006)70[263:COCARS]2.0.CO;2
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Smith JK, 2012, AUST MAMMAL, V34, P196, DOI 10.1071/AM11034
   Soisalo MK, 2006, BIOL CONSERV, V129, P487, DOI 10.1016/j.biocon.2005.11.023
   Spritzer MD, 2002, AM MIDL NAT, V148, P271, DOI 10.1674/0003-0031(2002)148[0271:DMUASA]2.0.CO;2
   Srbek-Araujo AC, 2005, J TROP ECOL, V21, P121, DOI 10.1017/S0266467404001956
   Stein B, 2000, PRECIOUS HERITAGE
   Stephens HC, 2013, AUSTRAL ECOL, V38, P568, DOI 10.1111/aec.12001
   Stephens RB, 2014, AM MIDL NAT, V171, P139, DOI 10.1674/0003-0031-171.1.139
   Taulman JF, 1996, J BIOGEOGR, V23, P635, DOI 10.1111/j.1365-2699.1996.tb00024.x
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Vas E, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2014.0754
   Way JG, 2005, CAN FIELD NAT, V119, P139, DOI 10.22621/cfn.v119i1.98
   WIGGERS EP, 1986, J WILDLIFE MANAGE, V50, P129, DOI 10.2307/3801502
NR 51
TC 2
Z9 2
U1 4
U2 34
PU U S FISH & WILDLIFE SERVICE
PI SHEPHERDSTOWN
PA NATL CONSERVATION TRAINING CENTER, CONSERVATION LIBRARY, 698
   CONSERVATION WAY, SHEPHERDSTOWN, WV 25443 USA
SN 1944-687X
J9 J FISH WILDL MANAG
JI J. Fish Wildl. Manag.
PD DEC
PY 2018
VL 9
IS 2
BP 485
EP 495
DI 10.3996/122017-JFWM-103
PG 11
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA HE1PX
UT WOS:000453046200012
OA gold
DA 2022-02-10
ER

PT J
AU Rheingantz, ML
   Leuchtenberger, C
   Zucco, CA
   Fernandez, FAS
AF Rheingantz, Marcelo Lopes
   Leuchtenberger, Caroline
   Zucco, Carlos Andre
   Fernandez, Fernando A. S.
TI Differences in activity patterns of the Neotropical otter Lontra
   longicaudis between rivers of two Brazilian ecoregions
SO JOURNAL OF TROPICAL ECOLOGY
LA English
DT Article
DE activity; camera trap; circular kernel; Neotropical otter; niche shift
ID ATLANTIC FOREST; CAMERA TRAPS; ECOLOGY
AB Circadian use of time is an important, but often neglected, part of an animal's niche. We compared the activity patterns of the Neotropical otter Lontra longicaudis in two different areas in Brazil using camera traps placed at the entrance of holts. We obtained 58 independent photos in the Atlantic Forest (273 camera trap-days) and 46 photos in Pantanal (300 camera trap-days). We observed different kernel density probabilities on these two areas (45.6% and 14.1% overlap between the 95% and 50% density isopleths respectively). We observed the plasticity in Neotropical otter activity behaviour with different activity patterns in the two areas. In the Pantanal, the Neotropical otter selected daylight (Ivlev = 0.23) and avoided night (Ivlev = -0.44), while in the Atlantic Forest it selected dawn (Ivlev = 0.24) and night (Ivlev = 0.14), avoiding daylight (Ivlev = -0.33). We believe that this pattern can be due to human activity or shifts in prey activity.
C1 [Rheingantz, Marcelo Lopes; Zucco, Carlos Andre; Fernandez, Fernando A. S.] Univ Fed Rio de Janeiro, Inst Biol, Dept Ecol, Lab Ecol & Conservacao Populacoes,Ilha Fundao,CCS, Ave Brigadeiro Trompowski S-N,Bloco A,Sala A2-102, BR-21941590 Rio De Janeiro, RJ, Brazil.
   [Rheingantz, Marcelo Lopes; Zucco, Carlos Andre; Fernandez, Fernando A. S.] Univ Fed Rio de Janeiro, PPGE, Inst Biol, Dept Ecol,CCS,Ilha Fundao, Ave Brigadeiro Trompowski S-N,Bloco A, BR-21941590 Rio de Janeiro, RJ, Brazil.
   [Leuchtenberger, Caroline] Embrapa Pantanal, Lab Vida Selvagem, 21 Setembro 1880, BR-79320900 Corumba, MS, Brazil.
RP Rheingantz, ML (corresponding author), Univ Fed Rio de Janeiro, Inst Biol, Dept Ecol, Lab Ecol & Conservacao Populacoes,Ilha Fundao,CCS, Ave Brigadeiro Trompowski S-N,Bloco A,Sala A2-102, BR-21941590 Rio De Janeiro, RJ, Brazil.; Rheingantz, ML (corresponding author), Univ Fed Rio de Janeiro, PPGE, Inst Biol, Dept Ecol,CCS,Ilha Fundao, Ave Brigadeiro Trompowski S-N,Bloco A, BR-21941590 Rio de Janeiro, RJ, Brazil.
EM mlrheingantz@gmail.com
RI Fernandez, Fernando/K-2412-2012; Rheingantz, Marcelo Lopes/F-3934-2014
OI Rheingantz, Marcelo Lopes/0000-0003-2521-3654
FU Brazilian Agricultural Research Corporation (Embrapa-Pantanal); Barranco
   Alto Farm; Reserva Botanica Aguas Claras; Universidade Federal do Rio de
   Janeiro; Fundacao Grupo Boticario; CNPqConselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPQ); CNPqscholarshipConselho
   Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)
FX We acknowledge the Brazilian Agricultural Research Corporation
   (Embrapa-Pantanal) and Barranco Alto Farm for financial and logistic
   support in the Pantanal biome. We also acknowledge Reserva Botanica
   Aguas Claras, Universidade Federal do Rio de Janeiro, Fundacao Grupo
   Boticario and CNPq for financial and logistical support in the Atlantic
   Forest biome. CL was recipient of CNPqscholarship. LucasLeuzinger helped
   us with camera trap survey at the Pantanal area. We also acknowledge
   Bernardo Araujo for the English review.
CR Britski HA, 1999, PEIXES PANTANAL MANU
   Di Bitetti MS, 2010, ACTA OECOL, V36, P403, DOI 10.1016/j.actao.2010.04.001
   Galliez M, 2009, J MAMMAL, V90, P93, DOI 10.1644/07-MAMM-A-397.1
   Gerber BD, 2012, J MAMMAL, V93, P667, DOI 10.1644/11-MAMM-A-265.1
   Gomez H, 2005, STUD NEOTROP FAUNA E, V40, P91, DOI 10.1080/01650520500129638
   Ivlev VS, 1961, EXPT ECOLOGY FEEDING
   Junk WJ, 2005, ECOL ENG, V24, P391, DOI 10.1016/j.ecoleng.2004.11.012
   Kruuk H., 2006, OTTERS ECOLOGY BEHAV
   Lerone L, 2015, WILDLIFE SOC B, V39, P193, DOI 10.1002/wsb.508
   Leuchtenberger C, 2014, ETHOL ECOL EVOL, V26, P19, DOI 10.1080/03949370.2013.821673
   LODE T, 1995, ETHOLOGY, V100, P295
   McClennen N, 2001, AM MIDL NAT, V146, P27, DOI 10.1674/0003-0031(2001)146[0027:TEOSAA]2.0.CO;2
   Metzger JP, 2009, BIOL CONSERV, V142, P1138, DOI 10.1016/j.biocon.2008.10.012
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Oliveira-Santos LGR, 2013, ANIM BEHAV, V85, P269, DOI 10.1016/j.anbehav.2012.09.033
   PICKLES R., 2011, IUCN OTTER SPECIALIS, V28, P39
   Rheingantz Marcelo Lopes, 2012, IUCN Otter Specialist Group Bulletin, V29, P80
   RILEY S. P., 2008, CONSERV BIOL, V17, P566
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Silva RE, 2014, J NAT HIST, V48, P465, DOI 10.1080/00222933.2013.800607
   Zhou Q, 2007, INT J PRIMATOL, V28, P657, DOI 10.1007/s10764-007-9144-6
NR 23
TC 14
Z9 15
U1 3
U2 44
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0266-4674
EI 1469-7831
J9 J TROP ECOL
JI J. Trop. Ecol.
PD MAR
PY 2016
VL 32
BP 170
EP 174
DI 10.1017/S0266467416000079
PN 2
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA DH1CR
UT WOS:000372522500010
DA 2022-02-10
ER

PT J
AU Adam, M
   Tomasek, P
   Lehejcek, J
   Trojan, J
   Junek, T
AF Adam, Matyas
   Tomasek, Pavel
   Lehejcek, Jiri
   Trojan, Jakub
   Junek, Tomas
TI The Role of Citizen Science and Deep Learning in Camera Trapping
SO SUSTAINABILITY
LA English
DT Article
DE artificial intelligence; crowdsourcing; environmental monitoring;
   conceptual framework; wildlife
ID FUTURE; MANAGEMENT; SOFTWARE
AB Camera traps are increasingly one of the fundamental pillars of environmental monitoring and management. Even outside the scientific community, thousands of camera traps in the hands of citizens may offer valuable data on terrestrial vertebrate fauna, bycatch data in particular, when guided according to already employed standards. This provides a promising setting for Citizen Science initiatives. Here, we suggest a possible pathway for isolated observations to be aggregated into a single database that respects the existing standards (with a proposed extension). Our approach aims to show a new perspective and to update the recent progress in engaging the enthusiasm of citizen scientists and in including machine learning processes into image classification in camera trap research. This approach (combining machine learning and the input from citizen scientists) may significantly assist in streamlining the processing of camera trap data while simultaneously raising public environmental awareness. We have thus developed a conceptual framework and analytical concept for a web-based camera trap database, incorporating the above-mentioned aspects that respect a combination of the roles of experts' and citizens' evaluations, the way of training a neural network and adding a taxon complexity index. This initiative could well serve scientists and the general public, as well as assisting public authorities to efficiently set spatially and temporarily well-targeted conservation policies.
C1 [Adam, Matyas; Tomasek, Pavel; Lehejcek, Jiri] Tomas Bata Univ Zlin, Fac Logist & Crisis Management, Uherske Hradiste 68601, Czech Republic.
   [Trojan, Jakub] Czech Acad Sci, Inst Geon, Dept Environm Geog, Brno 60200, Czech Republic.
   [Junek, Tomas] Czech Univ Life Sci Prague, Fac Environm Sci, Prague 16500, Czech Republic.
RP Adam, M (corresponding author), Tomas Bata Univ Zlin, Fac Logist & Crisis Management, Uherske Hradiste 68601, Czech Republic.
EM madam@utb.cz; tomasek@utb.cz; lehejcek@utb.cz; jakub.trojan@ugn.cas.cz;
   tjunek@fzp.czu.cz
RI ; Trojan, Jakub/D-6643-2015
OI , Pavel/0000-0001-8404-3486; Adam, Matyas/0000-0003-3602-4955; Trojan,
   Jakub/0000-0002-6658-8586
FU TA C. R [TG03010052]; INTER-COST project Geographical Aspects of Citizen
   Science: mapping trends, scientific potential and societal impacts in
   the Czech Republic [LTC18067, CA15212A]
FX Development of the analytical model and the prototype of the Czech
   national CT database was supported by TA C. R grant TG03010052. The
   paper was also supported by the INTER-COST project Geographical Aspects
   of Citizen Science: mapping trends, scientific potential and societal
   impacts in the Czech Republic (No. LTC18067), conducted under the COST
   EU action CA15212A Framework in Science and Technology to promote
   creativity, scientific literacy, and innovation throughout Europe.
CR [Anonymous], 2007, ACCESS BIOLOGICAL CO
   Apps PJ, 2018, AFR J ECOL, V56, P702, DOI 10.1111/aje.12563
   Bauerfeind R., 2020, ZOONOSES INFECT DIS
   Berger-Wolf T.Y., 2017, ARXIV PREPRINT ARXIV
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Cadman M., 2014, PUBLISHING CAMERA TR
   Catlin-Groves C.L., 2012, INT J ZOOL, V2012, P1, DOI [10.1155/2012/349630, DOI 10.1155/2012/349630]
   Ceccaroni L., 2019, CITIZ SCI THEORY PRA, V4, P29, DOI [10.5334/cstp.241, DOI 10.5334/CSTP.241]
   Colbert-Lewis D, 2016, REF REV, V30, P29
   Deb D, 2018, INT CONF BIOMETR THE
   Dickman AJ, 2010, ANIM CONSERV, V13, P458, DOI 10.1111/j.1469-1795.2010.00368.x
   Distefano E., 2005, HUMAN WILDLIFE CONFL
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Franzen M., 2021, SCI CITIZEN SCI, P183, DOI [DOI 10.1007/978-3-030-58278-4_10, 10.1007/978-3-030-58278-4_10]
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Goldsmith F.B., 2012, MONITORING CONSERVAT
   Green SE, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010132
   Hampton SE, 2013, FRONT ECOL ENVIRON, V11, P156, DOI 10.1890/120103
   Heilbrun RD, 2006, WILDLIFE SOC B, V34, P69, DOI 10.2193/0091-7648(2006)34[69:EBAUAT]2.0.CO;2
   Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
   Korschens M., 2018, AUTOMATIC IDENTIFICA
   Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
   Lyons JE, 2008, J WILDLIFE MANAGE, V72, P1683, DOI 10.2193/2008-141
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek PD, 2019, REMOTE SENS ECOL CON, V5, P160, DOI 10.1002/rse2.96
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Nipko RB, 2020, WILDLIFE SOC B, V44, P424, DOI 10.1002/wsb.1086
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Parsons AW, 2018, PEERJ, V6, DOI 10.7717/peerj.4536
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Schipper J, 2008, SCIENCE, V322, P225, DOI 10.1126/science.1165115
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Sutherland WJ, 2015, BIOL J LINN SOC, V115, P779, DOI 10.1111/bij.12576
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Terry Andrew M.R., 2005, Frontiers in Zoology, V2, P1
   Vincent C, 2001, MAMMALIA, V65, P363, DOI 10.1515/mamm.2001.65.3.363
   Waits LP, 2005, J WILDLIFE MANAGE, V69, P1419, DOI 10.2193/0022-541X(2005)69[1419:NGSTFW]2.0.CO;2
   Wang SW, 2009, BIOL CONSERV, V142, P606, DOI 10.1016/j.biocon.2008.11.023
   Welbourne DJ, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0226913
   Welbourne DJ, 2016, REMOTE SENS ECOL CON, V2, P77, DOI 10.1002/rse2.20
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yang DQ, 2021, ECOL EVOL, V11, P7591, DOI 10.1002/ece3.7591
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
NR 50
TC 1
Z9 1
U1 6
U2 6
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2071-1050
J9 SUSTAINABILITY-BASEL
JI Sustainability
PD SEP
PY 2021
VL 13
IS 18
AR 10287
DI 10.3390/su131810287
PG 14
WC Green & Sustainable Science & Technology; Environmental Sciences;
   Environmental Studies
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Science & Technology - Other Topics; Environmental Sciences & Ecology
GA UZ2NO
UT WOS:000702047200001
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Schneider, S
   Taylor, GW
   Linquist, S
   Kremer, SC
AF Schneider, Stefan
   Taylor, Graham W.
   Linquist, Stefan
   Kremer, Stefan C.
TI Past, present and future approaches using computer vision for animal
   re-identification from camera trap data
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE animal reidentification; camera traps; computer vision; convolutional
   networks; deep learning; density estimation; monitoring; object
   detection
ID PATTERN-MATCHING ALGORITHM; IDENTIFICATION
AB The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Dept Comp Sci, Guelph, ON, Canada.
   [Taylor, Graham W.] Univ Guelph, Dept Engn, Guelph, ON, Canada.
   [Linquist, Stefan] Univ Guelph, Dept Philosophy, Guelph, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Dept Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca
OI Kremer, Stefan C./0000-0002-3667-4379
CR Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Arzoumanian Z, 2005, J APPL ECOL, V42, P999, DOI 10.1111/j.1365-2664.2005.01117.x
   Barz B., 2018, ARXIV181204418
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Burghardt T., 2007, 6 INT PENG C BIRDS T
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Carter SJB, 2014, J EXP MAR BIOL ECOL, V452, P105, DOI 10.1016/j.jembe.2013.12.010
   Deb D., 2018, ARXIV180408790
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Fukushima K., 1979, ELECTR COMMUN JPN, V62, P11
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   GROTH EJ, 1986, ASTRON J, V91, P1244, DOI 10.1086/114099
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Hiby L., 1990, Reports of the International Whaling Commission Special Issue, P57
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Hillman G.R., 2003, Aquatic Mammals, V29, P117, DOI 10.1578/016754203101023960
   Hinton GE, 2017, NEURIPS, DOI DOI 10.1371/JOURNAL.PONE.0035195
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   Huele R, 1998, MAR MAMMAL SCI, V14, P143, DOI 10.1111/j.1748-7692.1998.tb00697.x
   Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
   Kaggle, 2018, HUMPB WHAL ID CHALL
   Kelly MJ, 2001, J MAMMAL, V82, P440, DOI 10.1644/1545-1542(2001)082<0440:CAPMIS>2.0.CO;2
   Keutzer, 2014, ARXIV14041869
   Krebs C.J., 1989, ECOLOGICAL METHODOLO
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Martinel N, 2015, IEEE T PATTERN ANAL, V37, P1656, DOI 10.1109/TPAMI.2014.2377748
   Meek P. D., 2013, WILDLIFE BIOL PRACTI, V9, P461
   Mizroch S., 1990, COMPUTER ASSISTED PH, V12, P63
   Norouzzadeh M. S., 2017, P NATL ACAD SCI US
   Nowozin S, 2014, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2014.77
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pitts W., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   Ravela S., 2004, P AS C COMP VIS KI S, P742
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Rodner E., 2016, ARXIV161006756
   Rodner E., 2015, ARXIV150700913
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Scheel D, 2017, MAR FRESHW BEHAV PHY, V50, P285, DOI 10.1080/10236244.2017.1369851
   Schneider S., 2018, C COMP ROB VIS
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sherley Richard B., 2010, Endangered Species Research, V11, P101, DOI 10.3354/esr00267
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Souri Y., 2015, P 3 WORKSH FIN GRAIN
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C., 2015, P IEEE C COMP VIS PA, P461
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Town C, 2013, ECOL EVOL, V3, P1902, DOI 10.1002/ece3.587
   Whitehead H., 1990, Reports of the International Whaling Commission Special Issue, P71
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
NR 61
TC 38
Z9 38
U1 5
U2 55
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD APR
PY 2019
VL 10
IS 4
BP 461
EP 470
DI 10.1111/2041-210X.13133
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HR3KR
UT WOS:000463036400002
OA Bronze, Green Submitted
DA 2022-02-10
ER

PT J
AU Yousif, H
   Yuan, JH
   Kays, R
   He, ZH
AF Yousif, Hayder
   Yuan, Jianhe
   Kays, Roland
   He, Zhihai
TI Animal Scanner: Software for classifying humans, animals, and empty
   frames in camera trap images
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE background subtraction; camera trap images; deep convolutional neural
   networks; human-animal detection; wildlife monitoring
ID OCCUPANCY
AB Camera traps are a popular tool to sample animal populations because they are noninvasive, detect a variety of species, and can record many thousands of animal detections per deployment. Cameras are typically set to take bursts of multiple photographs for each detection and are deployed in arrays of dozens or hundreds of sites, often resulting in millions of photographs per study. The task of converting photographs to animal detection records from such large image collections is daunting, and made worse by situations that generate copious empty pictures from false triggers (e.g., camera malfunction or moving vegetation) or pictures of humans. We developed computer vision algorithms to detect and classify moving objects to aid the first step of camera trap image filteringseparating the animal detections from the empty frames and pictures of humans. Our new work couples foreground object segmentation through background subtraction with deep learning classification to provide a fast and accurate scheme for human-animal detection. We provide these programs as both Matlab GUI and command prompt developed with C++. The software reads folders of camera trap images and outputs images annotated with bounding boxes around moving objects and a text file summary of results. This software maintains high accuracy while reducing the execution time by 14 times. It takes about 6 seconds to process a sequence of ten frames (on a 2.6 GHZ CPU computer). For those cameras with excessive empty frames due to camera malfunction or blowing vegetation automatically removes 54% of the false-triggers sequences without influencing the human/animal sequences. We achieve 99.58% on image-level empty versus object classification of Serengeti dataset. We offer the first computer vision tool for processing camera trap images providing substantial time savings for processing large image datasets, thus improving our ability to monitor wildlife across large scales with camera traps.
C1 [Yousif, Hayder; Yuan, Jianhe; He, Zhihai] Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Kays, Roland] North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27695 USA.
   [Kays, Roland] North Carolina Museum Nat Sci, Raleigh, NC USA.
RP Yousif, H (corresponding author), Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM hyypp5@mail.missouri.edu
RI Yousif, Hayder/AAG-2259-2020
OI Yousif, Hayder/0000-0002-7638-9505; Kays, Roland/0000-0002-2947-6665
FU National Science FoundationNational Science Foundation (NSF)
   [CyberSEES-1539389]
FX National Science Foundation, Grant/Award Number: CyberSEES-1539389
CR BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P293, DOI 10.1109/36.377929
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bowler MT, 2017, REMOTE SENS ECOL CON, V3, P146, DOI 10.1002/rse2.35
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong P, 2016, IEEE T IMAGE PROCESS, V25, P5035, DOI 10.1109/TIP.2016.2598680
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Huang HC, 2015, SENSORS-BASEL, V15, P27116, DOI 10.3390/s151027116
   Kays R., 2016, CANDID CREATURES CAM
   Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee C.-Y., 2014, ARXIV14095185
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Ren S., 2015, ARXIV150601497
   Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495
   Steenweg R, 2016, BIOL CONSERV, V201, P192, DOI 10.1016/j.biocon.2016.06.020
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Yousif H., 2017, IEEE INT C IM PROC
   Yousif H., 2017, CIRC SYST 2017 ISCAS, P1894
NR 30
TC 16
Z9 16
U1 1
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD FEB
PY 2019
VL 9
IS 4
BP 1578
EP 1589
DI 10.1002/ece3.4747
PG 12
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA HO7HZ
UT WOS:000461114900005
PM 30847057
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Whytock, RC
   Swiezewski, J
   Zwerts, JA
   Pambo, AFK
   Rogala, M
   Bahaa-el-din, L
   Boekee, K
   Brittain, S
   Cardoso, AW
   Henschel, P
   Lehmann, D
   Momboua, B
   Opepa, CK
   Orbell, C
   Pitman, RT
   Robinson, HS
   Abernethy, KA
AF Whytock, Robin C.
   Swiezewski, Jedrzej
   Zwerts, Joeri A.
   Pambo, Aurelie Flore Koumba
   Rogala, Marek
   Bahaa-el-din, Laila
   Boekee, Kelly
   Brittain, Stephanie
   Cardoso, Anabelle W.
   Henschel, Philipp
   Lehmann, David
   Momboua, Brice
   Opepa, Cisquet Kiebou
   Orbell, Christopher
   Pitman, Ross T.
   Robinson, Hugh S.
   Abernethy, Katharine A.
TI Robust ecological analysis of camera trap data labelled by a machine
   learning model
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE artificial intelligence; biodiversity; birds; Central Africa; mammals
ID ESTIMATING SITE OCCUPANCY
AB Ecological data are collected over vast geographic areas using digital sensors such as camera traps and bioacoustic recorders. Camera traps have become the standard method for surveying many terrestrial mammals and birds, but camera trap arrays often generate millions of images that are time-consuming to label. This causes significant latency between data collection and subsequent inference, which impedes conservation at a time of ecological crisis. Machine learning algorithms have been developed to improve the speed of labelling camera trap data, but it is uncertain how the outputs of these models can be used in ecological analyses without secondary validation by a human.
   Here, we present our approach to developing, testing and applying a machine learning model to camera trap data for the purpose of achieving fully automated ecological analyses. As a case-study, we built a model to classify 26 Central African forest mammal and bird species (or groups). The model generalizes to new spatially and temporally independent data (n = 227 camera stations, n = 23,868 images), and outperforms humans in several respects (e.g. detecting 'invisible' animals). We demonstrate how ecologists can evaluate a machine learning model's precision and accuracy in an ecological context by comparing species richness, activity patterns (n = 4 species tested) and occupancy (n = 4 species tested) derived from machine learning labels with the same estimates derived from expert labels.
   Results show that fully automated species labels can be equivalent to expert labels when calculating species richness, activity patterns (n = 4 species tested) and estimating occupancy (n = 3 of 4 species tested) in a large, completely out-of-sample test dataset. Simple thresholding using the Softmax values (i.e. excluding 'uncertain' labels) improved the model's performance when calculating activity patterns and estimating occupancy but did not improve estimates of species richness.
   We conclude that, with adequate testing and evaluation in an ecological context, a machine learning model can generate labels for direct use in ecological analyses without the need for manual validation. We provide the user-community with a multi-platform, multi-language graphical user interface that can be used to run our model offline.
C1 [Whytock, Robin C.; Lehmann, David; Orbell, Christopher; Abernethy, Katharine A.] Univ Stirling, Fac Nat Sci, Stirling, Scotland.
   [Whytock, Robin C.; Pambo, Aurelie Flore Koumba; Lehmann, David; Momboua, Brice] Agence Natl Pares Nationaux, Libreville, Gabon.
   [Swiezewski, Jedrzej; Rogala, Marek] Appsilon AI Good, Warsaw, Poland.
   [Zwerts, Joeri A.] Univ Utrecht, Utrecht, Netherlands.
   [Bahaa-el-din, Laila] Univ KwaZulu Natal, Sch Life Sci, Pietermaritzburg, South Africa.
   [Boekee, Kelly] Program Sustainable Management Nat Resources, Buea, Cameroon.
   [Boekee, Kelly] Smithsonian Trop Res Inst, Ctr Trop Forest Sci, Balboa, Ancon, Panama.
   [Brittain, Stephanie] Univ Oxford, Interdisciplinary Ctr Conservat Sci, Dept Zool, Oxford, England.
   [Brittain, Stephanie] Zool Soc London, Inst Zool, London, England.
   [Cardoso, Anabelle W.] Yale Univ, Dept Ecol & Evolutionary Biol, New Haven, CT USA.
   [Henschel, Philipp; Orbell, Christopher; Pitman, Ross T.; Robinson, Hugh S.] Panthera, New York, NY USA.
   [Henschel, Philipp; Abernethy, Katharine A.] CENAREST, Inst Rech Ecol Trop, Libreville, Gabon.
   [Opepa, Cisquet Kiebou] Wildlife Conservat Soc, Kinshasa, DEM REP CONGO.
   [Robinson, Hugh S.] Univ Montana, Wildlife Biol Program, WA Franke Coll Forestry & Conservat, Missoula, MT 59812 USA.
RP Whytock, RC (corresponding author), Univ Stirling, Fac Nat Sci, Stirling, Scotland.; Whytock, RC (corresponding author), Agence Natl Pares Nationaux, Libreville, Gabon.
EM robbie.whytock1@stir.ac.uk
OI Rogala, Marek/0000-0002-9949-4551; Zwerts, Joeri/0000-0003-3841-6389;
   Swiezewski, Jedrzej/0000-0001-7005-8003; Cardoso, Anabelle
   Williamson/0000-0002-4327-7259; Brittain, Stephanie/0000-0002-7865-0391;
   Whytock, Robin/0000-0002-0127-6071
FU Google Cloud for EducationGoogle Incorporated; EU 11th FED ECOFAC6;
   University of Oxford Hertford College Mortimer May Fund
FX Google Cloud for Education; EU 11th FED ECOFAC6; University of Oxford
   Hertford College Mortimer May Fund
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
   Arje J., 2019, ARXIV170806899CSQBIO
   Baha-El-Din Laila, 2013, Small Carnivore Conservation, V48, P19
   Bahaa-el-din L, 2018, AFR J ECOL, V56, P690, DOI 10.1111/aje.12581
   Beery S., 2019, BIODIVERSITY INFORM, V3, pe37222, DOI [10.3897/biss.3.37222, DOI 10.3897/BISS.3.37222]
   Beery S., 2018, P EUR C COMP VIS ECC, P456
   Bessone M, 2020, J APPL ECOL, V57, P963, DOI 10.1111/1365-2664.13602
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Cardoso AW, 2020, ECOSYSTEMS, V23, P602, DOI 10.1007/s10021-019-00424-3
   Dietze MC, 2018, P NATL ACAD SCI USA, V115, P1424, DOI 10.1073/pnas.1710231115
   Farley SS, 2018, BIOSCIENCE, V68, P563, DOI 10.1093/biosci/biy068
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Howard J, 2020, INFORMATION, V11, DOI 10.3390/info11020108
   Kurakin A, 2017, ARXIV160702533CSSTAT ARXIV160702533CSSTAT
   Lin H, 2020, RESNEST SPLIT ATTENT RESNEST SPLIT ATTENT
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Norouzzadeh M. S., 2019, ARXIV191009716CSEESS
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2020, REMOTE SENS ECOL CON, V6, P168, DOI 10.1002/rse2.132
   Orbell C., 2021, DATASTORRE STIRLING
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
   Schneider S., 2018, ARXIV180310842CS
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Smith L. N., 2018, RXIV180309820CSSTAT
   Sun Y., 2017, ARXIV170604599CS
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tan M., 2020, ARXIV190511946CSSTAT
   Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
NR 38
TC 5
Z9 5
U1 6
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JUN
PY 2021
VL 12
IS 6
BP 1080
EP 1092
DI 10.1111/2041-210X.13576
EA MAR 2021
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SL1YX
UT WOS:000627162500001
OA Green Published, hybrid
DA 2022-02-10
ER

PT C
AU Yang, XY
   Mirmehdi, M
   Burghardt, T
AF Yang, Xinyu
   Mirmehdi, Majid
   Burghardt, Tilo
GP IEEE
TI Great Ape Detection in Challenging Jungle Camera Trap Footage via
   Attention-Based Spatial and Temporal Feature Blending
SO 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS
   (ICCVW)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF International Conference on Computer Vision (ICCV)
CY OCT 27-NOV 02, 2019
CL Seoul, SOUTH KOREA
SP IEEE, IEEE Comp Soc, CVF
AB We propose the first multi-frame video object detection framework trained to detect great apes. It is applicable to challenging camera trap footage in complex jungle environments and extends a traditional feature pyramid architecture by adding self-attention driven feature blending in both the spatial as well as the temporal domain. We demonstrate that this extension can detect distinctive species appearance and motion signatures despite significant partial occlusion. We evaluate the framework using 500 camera trap videos of great apes from the Pan African Programme containing 180K frames, which we manually annotated with accurate per-frame animal bounding boxes. These clips contain significant partial occlusions, challenging lighting, dynamic backgrounds, and natural camouflage effects. We show that our approach performs highly robustly and significantly outperforms frame-based detectors. We also perform detailed ablation studies and a validation on the full ILSVRC 2015 VID data corpus to demonstrate wider applicability at adequate performance levels. We conclude that the framework is ready to assist human camera trap inspection efforts. We publish key parts of the code as well as network weights and ground truth annotations with this paper.
C1 [Yang, Xinyu; Mirmehdi, Majid; Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
RP Yang, XY (corresponding author), Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
EM xinyu.yang@bristol.ac.uk; m.mirmehdi@bristol.ac.uk; tilo@cs.bris.ac.uk
OI Mirmehdi, Majid/0000-0002-6478-1403
FU Max Planck SocietyMax Planck SocietyFoundation CELLEX; Max Planck
   Society Innovation Fund
FX We would like to thank the entire team of the Pan African Programme:
   `The Cultured Chimpanzee' and its collaborators for allowing the use of
   their data for this paper. Please contact the copyright holder Pan
   African Programme at http://panafrican.eva.mpg.de to obtain the dataset.
   Particularly, we thank: H Kuehl, C Boesch, M Arandjelovic, and P
   Dieguez. We would also like to thank: K Zuberbuehler, K Corogenes, E
   Normand, V Vergnes, A Meier, J Lapuente, D Dowd, S Jones, V Leinert,
   EWessling, H Eshuis, K Langergraber, S Angedakin, S Marrocoli, K Dierks,
   T C Hicks, J Hart, K Lee, and M Murai. Thanks also to the team at
   https://www.chimpandsee.org.The work that allowed for the collection of
   the dataset was funded by the Max Planck Society, Max Planck Society
   Innovation Fund, and Heinz L. Krekeler. In this respect we would also
   like to thank: Foundation Ministre de la Recherche Scientifique, and
   Ministre des Eaux et Forlts in Cote d'Ivoire; Institut Congolais pour la
   Conservation de la Nature and Ministre de la Recherche Scientifique in
   DR Congo; Forestry Development Authority in Liberia; Direction des Eaux,
   Forlts Chasses et de la Conservation des Sols, Senegal; and Uganda
   National Council for Science and Technology, UgandaWildlife Authority,
   National Forestry Authority in Uganda.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao Yue, 2019, ARXIV190411492CS
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen K, 2018, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR.2018.00815
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   DrivenData, COMP PRIM FACT
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Han Wei, 2016, TECHNICAL REPORT
   ImageNet, IMAGENET CALL 2015 O
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Li B., 2018, ARXIV181211703
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, P568
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 31
TC 2
Z9 2
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2473-9936
BN 978-1-7281-5023-9
J9 IEEE INT CONF COMP V
PY 2019
BP 255
EP 262
DI 10.1109/ICCVW.2019.00034
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BP4UH
UT WOS:000554591600028
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Tabak, MA
   Norouzzadeh, MS
   Wolfson, DW
   Newton, EJ
   Boughton, RK
   Ivan, JS
   Odell, EA
   Newkirk, ES
   Conrey, RY
   Stenglein, J
   Iannarilli, F
   Erb, J
   Brook, RK
   Davis, AJ
   Lewis, J
   Walsh, DP
   Beasley, JC
   VerCauteren, KC
   Clune, J
   Miller, RS
AF Tabak, Michael A.
   Norouzzadeh, Mohammad S.
   Wolfson, David W.
   Newton, Erica J.
   Boughton, Raoul K.
   Ivan, Jacob S.
   Odell, Eric A.
   Newkirk, Eric S.
   Conrey, Reesa Y.
   Stenglein, Jennifer
   Iannarilli, Fabiola
   Erb, John
   Brook, Ryan K.
   Davis, Amy J.
   Lewis, Jesse
   Walsh, Daniel P.
   Beasley, James C.
   VerCauteren, Kurt C.
   Clune, Jeff
   Miller, Ryan S.
TI Improving the accessibility and transferability of machine learning
   algorithms for identification of animals in camera trap images: MLWIC2
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE computer vision; deep convolutional neural networks; image
   classification; machine learning; motion-activated camera; R package;
   remote sensing; species identification
AB Motion-activated wildlife cameras (or "camera traps") are frequently used to remotely and noninvasively observe animals. The vast number of images collected from camera trap projects has prompted some biologists to employ machine learning algorithms to automatically recognize species in these images, or at least filter-out images that do not contain animals. These approaches are often limited by model transferability, as a model trained to recognize species from one location might not work as well for the same species in different locations. Furthermore, these methods often require advanced computational skills, making them inaccessible to many biologists. We used 3 million camera trap images from 18 studies in 10 states across the United States of America to train two deep neural networks, one that recognizes 58 species, the "species model," and one that determines if an image is empty or if it contains an animal, the "empty-animal model." Our species model and empty-animal model had accuracies of 96.8% and 97.3%, respectively. Furthermore, the models performed well on some out-of-sample datasets, as the species model had 91% accuracy on species from Canada (accuracy range 36%-91% across all out-of-sample datasets) and the empty-animal model achieved an accuracy of 91%-94% on out-of-sample datasets from different continents. Our software addresses some of the limitations of using machine learning to classify images from camera traps. By including many species from several locations, our species model is potentially applicable to many camera trap studies in North America. We also found that our empty-animal model can facilitate removal of images without animals globally. We provide the trained models in an R package (MLWIC2: Machine Learning for Wildlife Image Classification in R), which contains Shiny Applications that allow scientists with minimal programming experience to use trained models and train new models in six neural network architectures with varying depths.
C1 [Tabak, Michael A.] Quantitat Sci Consulting LLC, Laramie, WY 82072 USA.
   [Tabak, Michael A.] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
   [Norouzzadeh, Mohammad S.] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Wolfson, David W.] Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, Minnesota Cooperat Fish & Wildlife Res Unit, St Paul, MN 55108 USA.
   [Newton, Erica J.] Ontario Minist Nat Resources & Forestry, Wildlife Res & Monitoring Sect, Peterborough, ON, Canada.
   [Boughton, Raoul K.] Univ Florida, Range Cattle Res & Educ Ctr, Wildlife Ecol & Conservat, Ona, FL USA.
   [Ivan, Jacob S.; Odell, Eric A.; Newkirk, Eric S.; Conrey, Reesa Y.] Colorado Pk & Wildlife, Ft Collins, CO USA.
   [Stenglein, Jennifer] Wisconsin Dept Nat Resources, Madison, WI USA.
   [Iannarilli, Fabiola] Univ Minnesota, Conservat Sci Grad Program, St Paul, MN 55108 USA.
   [Erb, John] Minnesota Dept Nat Resources, Forest Wildlife Populat & Res Grp, Grand Rapids, MN USA.
   [Brook, Ryan K.] Univ Saskatchewan, Dept Anim & Poultry Sci, Saskatoon, SK, Canada.
   [Davis, Amy J.] USDA, Natl Wildlife Res Ctr, Ft Collins, CO USA.
   [Lewis, Jesse] Arizona State Univ, Coll Integrat Sci & Arts, Mesa, AZ USA.
   [Walsh, Daniel P.] US Geol Survey, Natl Wildlife Hlth Ctr, Madison, WI USA.
   [Beasley, James C.] Univ Georgia, Savannah River Ecol Lab, Warnell Sch Forestry & Nat Resources, Aiken, SC USA.
   [VerCauteren, Kurt C.] US Anim & Plant Hlth Inspect Serv, Natl Wildlife Res Ctr, USDA, Ft Collins, CO USA.
   [Clune, Jeff] OpenAI, San Francisco, CA USA.
   [Miller, Ryan S.] USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO USA.
RP Tabak, MA (corresponding author), Quantitat Sci Consulting LLC, Laramie, WY 82072 USA.
EM tabakma@gmail.com
RI Iannarilli, Fabiola/AAG-7774-2021; Davis, Amy/ABE-2065-2021
OI Iannarilli, Fabiola/0000-0002-7018-3557; Tabak,
   Michael/0000-0002-2986-7885; Stenglein, Jennifer/0000-0003-4578-5908
FU DOEUnited States Department of Energy (DOE) [DE-EM0004391]; USFWS
   Pittman-Robertson Wildlife Restoration Program; Wisconsin Department of
   Natural Resources
FX Contributions of JCB were partially supported by the DOE under Award
   Number DE-EM0004391 to the University of Georgia Research Foundation.
   Support for this research was provided by the USFWS Pittman-Robertson
   Wildlife Restoration Program and Wisconsin Department of Natural
   Resources. For supplying camera trap images, we thank USDA Forest
   Service: Rocky Mountain Research station; Montana Fish, Wildlife and
   Parks; Wyoming Game and Fish Department; Washington Department of Fish
   and Wildlife; Idaho Department of Fish and Game; and Woodland Park Zoo.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Advanced Research Computing Center, 2018, TET COMP ENV, DOI [10.15786/M2FY47, DOI 10.15786/M2FY47, 10. 15786/M2FY47]
   Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
   Beery S., 2019, EFFICIENT PIPELINE C
   Beery S., 2018, P EUR C COMP VIS ECC, P456
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Guillera-Arroita G, 2017, METHODS ECOL EVOL, V8, P1081, DOI 10.1111/2041-210X.12743
   Harvey P., 2016, EXIFTOOL
   Huang J., 2020, P IEEE CVF C COMP VI, P13075
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   McIntyre T, 2020, WILDLIFE RES, V47, P177, DOI 10.1071/WR19040
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Norouzzadeh M. S., 2019, ARXIV191009716CSEESS
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak M. A., 2020, DRYAD, DOI [10.5061/dryad.x95x69pfx, DOI 10.5061/DRYAD.X95X69PFX]
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yousif H., 2019, IEEE T CIRCUITS SYST
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 26
TC 5
Z9 5
U1 6
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD OCT
PY 2020
VL 10
IS 19
BP 10374
EP 10383
DI 10.1002/ece3.6692
EA SEP 2020
PG 10
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA NY7YW
UT WOS:000569520700001
PM 33072266
OA Green Submitted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Yang, DQ
   Tan, K
   Huang, ZP
   Li, XW
   Chen, BH
   Ren, GP
   Xiao, W
AF Yang, Deng-Qi
   Tan, Kun
   Huang, Zhi-Pang
   Li, Xiao-Wei
   Chen, Ben-Hui
   Ren, Guo-Peng
   Xiao, Wen
TI An automatic method for removing empty camera trap images using ensemble
   learning
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE artificial intelligence; camera trap images; convolutional neural
   networks; deep learning; ensemble learning
ID ASSOCIATIONS; CLASSIFIERS
AB Camera traps often produce massive images, and empty images that do not contain animals are usually overwhelming. Deep learning is a machine-learning algorithm and widely used to identify empty camera trap images automatically. Existing methods with high accuracy are based on millions of training samples (images) and require a lot of time and personnel costs to label the training samples manually. Reducing the number of training samples can save the cost of manually labeling images. However, the deep learning models based on a small dataset produce a large omission error of animal images that many animal images tend to be identified as empty images, which may lead to loss of the opportunities of discovering and observing species. Therefore, it is still a challenge to build the DCNN model with small errors on a small dataset. Using deep convolutional neural networks and a small-size dataset, we proposed an ensemble learning approach based on conservative strategies to identify and remove empty images automatically. Furthermore, we proposed three automatic identifying schemes of empty images for users who accept different omission errors of animal images. Our experimental results showed that these three schemes automatically identified and removed 50.78%, 58.48%, and 77.51% of the empty images in the dataset when the omission errors were 0.70%, 1.13%, and 2.54%, respectively. The analysis showed that using our scheme to automatically identify empty images did not omit species information. It only slightly changed the frequency of species occurrence. When only a small dataset was available, our approach provided an alternative to users to automatically identify and remove empty images, which can significantly reduce the time and personnel costs required to manually remove empty images. The cost savings were comparable to the percentage of empty images removed by models.
C1 [Yang, Deng-Qi; Li, Xiao-Wei; Chen, Ben-Hui] Dali Univ, Dept Math & Comp Sci, Dali, Peoples R China.
   [Yang, Deng-Qi; Tan, Kun; Huang, Zhi-Pang; Ren, Guo-Peng; Xiao, Wen] Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali, Peoples R China.
   [Yang, Deng-Qi; Tan, Kun; Huang, Zhi-Pang; Ren, Guo-Peng; Xiao, Wen] Collaborat Innovat Ctr Biodivers Three Parallel R, Dali, Peoples R China.
   [Yang, Deng-Qi; Li, Xiao-Wei; Chen, Ben-Hui] Dali Univ, Data Secur & Applicat Innovat Team, Dali, Peoples R China.
RP Ren, GP (corresponding author), Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali, Peoples R China.
EM rengp@eastern-himalaya.cn
RI Ren, Guo-Peng/ABA-2138-2021
OI Ren, Guo-Peng/0000-0003-3381-3166; , dengqiyang/0000-0003-1437-3097
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [31960119, 31860164, 31860168]; Yunnan
   Provincial Science and Technology Department University Joint Project
   [2017FH001-027]
FX This study was partially supported by the National Natural Science
   Foundation of China (31960119, 31860164, 31860168) and Yunnan Provincial
   Science and Technology Department University Joint Project
   (2017FH001-027).
CR Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45
   Chen YS, 2019, IEEE J-STARS, V12, P1882, DOI 10.1109/JSTARS.2019.2915259
   Clark A, 2019, PIL PILLOW 5 2 0 FOR
   Dertien JS, 2017, J WILDLIFE MANAGE, V81, P1457, DOI 10.1002/jwmg.21308
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Faria FA, 2014, PATTERN RECOGN LETT, V39, P52, DOI 10.1016/j.patrec.2013.07.014
   Forsyth DM, 2019, J WILDLIFE MANAGE, V83, P1090, DOI 10.1002/jwmg.21675
   Galar M, 2012, IEEE T SYST MAN CY C, V42, P463, DOI 10.1109/TSMCC.2011.2161285
   Giraldo-Zuluaga J.-H., 2017, VISUAL COMPUT, P1
   GOMEZ A, 2017, ECOL INFORM, V41, P24, DOI DOI 10.1016/j.ecoinf.2017.07.004
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
   Hurt JA, 2019, INT GEOSCI REMOTE SE, P1326, DOI 10.1109/IGARSS.2019.8898596
   Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin M., 2013, ARXIV13124444
   Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Pathak Sudipta, 2018, 2018 IEEE 8th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS), DOI 10.1109/ICCABS.2018.8541985
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tabak M., 2019, APPL ECOLOGY, DOI [10.1101/346809, DOI 10.1101/346809]
   Thorne ED, 2017, J WILDLIFE MANAGE, V81, P1042, DOI 10.1002/jwmg.21282
   Huynh T, 2016, IEEE T MED IMAGING, V35, P174, DOI 10.1109/TMI.2015.2461533
   Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
   Vinyals O., 2014, P 31 INT C INT C MAC, V32
   Webb SM, 2016, J WILDLIFE MANAGE, V80, P1461, DOI 10.1002/jwmg.21137
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xia J, 2018, IEEE T GEOSCI REMOTE, V56, P202, DOI 10.1109/TGRS.2017.2744662
   Yang D.Q., 2019, CAMERA TRAP IMAGES M
   Yosinski J, 2014, ADV NEUR IN, V27
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 41
TC 1
Z9 1
U1 5
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD JUN
PY 2021
VL 11
IS 12
BP 7591
EP 7601
DI 10.1002/ece3.7591
EA MAY 2021
PG 11
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA SV8AX
UT WOS:000645977200001
PM 34188837
OA Green Published
DA 2022-02-10
ER

PT J
AU Yang, DQ
   Ren, GP
   Tan, K
   Huang, ZP
   Li, DP
   Li, XW
   Wang, JM
   Chen, BH
   Xiao, W
AF Yang, Deng-Qi
   Ren, Guo-Peng
   Tan, Kun
   Huang, Zhi-Pang
   Li, De-Pin
   Li, Xiao-Wei
   Wang, Jian-Ming
   Chen, Ben-Hui
   Xiao, Wen
TI An Adaptive Automatic Approach to Filtering Empty Images from Camera
   Traps Using a Deep Learning Model
SO WILDLIFE SOCIETY BULLETIN
LA English
DT Article
DE Artificial intelligence; camera traps; deep learning; empty images;
   image recognition; wildlife monitoring
ID ASSOCIATIONS
AB Camera traps are widely used in wildlife surveys because they are non-invasive, low-cost, and highly efficient. Camera traps deployed in the wild often produce large datasets, making it increasingly difficult to manually classify images. Deep learning is a machine learning method that provides a tool to automatically identify images, but it requires labeled training samples and high-performance servers with multiple Graphics Processing Units (GPUs). However, manually preparing large-scale training images for training deep learning models is labor intensive, and the high-performance servers with multiple GPUs are often not available for wildlife management agencies and field researchers. Our study explores an adaptive deep learning method to use small-scale training sets and a commonly-available, desktop personal computer (PC) to achieve automatic filtering of empty camera images. Our results showed that by using 29,192 training samples, the overall error, commission error, and omission error of the proposed method on a PC were 2.69%, 6.82%, and 6.45%, respectively. Moreover, the accuracy of our method can be adaptively improved on PCs in actual ecological monitoring projects, which would benefit researchers in field settings when only a PC is available. (c) 2021 The Wildlife Society.
C1 [Yang, Deng-Qi; Wang, Jian-Ming; Chen, Ben-Hui] Dali Univ, Dept Math & Comp Sci, Dali 671003, Yunnan, Peoples R China.
   [Ren, Guo-Peng; Tan, Kun; Huang, Zhi-Pang; Li, De-Pin; Xiao, Wen] Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali 671003, Yunnan, Peoples R China.
   [Li, Xiao-Wei] Dali Univ, Data Secur & Applicat Innovat Team, Dali 671003, Yunnan, Peoples R China.
RP Ren, GP (corresponding author), Dali Univ, Inst Eastern Himalaya Biodivers Res, Dali 671003, Yunnan, Peoples R China.
EM rengp@eastern-himalaya.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [31960119, 31860164, 31860168, 61902049];
   Yunnan Provincial Science and Technology Department University Joint
   Project [2017FH001-027, 2018FH001-063, 2018FH001-106]; Innovative
   Project of Dali University [ZKLX2020308]
FX We appreciate the support of the National Natural Science Foundation of
   China (31960119, 31860164, 31860168, 61902049), the Yunnan Provincial
   Science and Technology Department University Joint Project
   (2017FH001-027, 2018FH001-063,2018FH001-106) and the Innovative Project
   of Dali University (ZKLX2020308). We thank J. McRoberts (Associate
   Editor), A. Knipps (Editorial Assistant), and 2 reviewers for their
   comments, which improved the manuscript.
CR Dertien JS, 2017, J WILDLIFE MANAGE, V81, P1457, DOI 10.1002/jwmg.21308
   Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Forsyth DM, 2019, J WILDLIFE MANAGE, V83, P1090, DOI 10.1002/jwmg.21675
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   Myers N, 1988, Environmentalist, V8, P187, DOI 10.1007/BF02240252
   Myers N, 1990, Environmentalist, V10, P243, DOI 10.1007/BF02239720
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thorne ED, 2017, J WILDLIFE MANAGE, V81, P1042, DOI 10.1002/jwmg.21282
   Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
   Webb SM, 2016, J WILDLIFE MANAGE, V80, P1461, DOI 10.1002/jwmg.21137
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yang D., 2019, CAMERA TRAP IMAGES C
   Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 25
TC 0
Z9 0
U1 1
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2328-5540
J9 WILDLIFE SOC B
JI Wildl. Soc. Bull.
PD JUN
PY 2021
VL 45
IS 2
BP 230
EP 236
DI 10.1002/wsb.1176
EA MAY 2021
PG 7
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA TY3RZ
UT WOS:000650299700001
DA 2022-02-10
ER

PT J
AU Chauvenet, ALM
   Gill, RMA
   Smith, GC
   Ward, AI
   Massei, G
AF Chauvenet, Alienor L. M.
   Gill, Robin M. A.
   Smith, Graham C.
   Ward, Alastair I.
   Massei, Giovanna
TI Quantifying the bias in density estimated from distance sampling and
   camera trapping of unmarked individuals
SO ECOLOGICAL MODELLING
LA English
DT Article
DE Individual based model; Sus scrofa; Transects; Population monitoring;
   Population size; Random encounter model
ID BOAR SUS-SCROFA; WILD BOAR; SOUTHERN ENGLAND; ANIMAL DENSITY;
   POPULATIONS; MAMMALS; DIVERSITY; ABUNDANCE; WOODLAND; DESIGN
AB Population size estimates are an integral part of any species conservation or management project. They are often used to evaluate the impact of management intervention and can be critical for making decisions for future management. Distance sampling and camera trapping of unmarked populations are commonly used for such a task as they can yield rapid and relatively inexpensive estimates of density. Yet, while accuracy is key for decision-making, the potential bias associated with densities estimated with each method have seldom been investigated and compared. We built a spatially-explicit individual based model to investigate the accuracy and precision of both monitoring techniques in estimating known densities. We used the wild boar population of the Forest of Dean, UK, as a case study because both methods have been employed in situ and offer the chance of using real life parameters in the model. Moreover, this is an introduced species in the UK that has the potential to impact natural and agricultural ecosystems. Therefore, improving the accuracy of density estimates is a priority for the species' management. We found that both distance sampling and camera trapping produce biased density estimates for unmarked populations. Despite large uncertainties, distance sampling estimates were on average closer to known densities than those from camera trapping, and robust to group size. Camera trapping estimates were highly sensitive to group size but could be improved with better survey design. This is the first time that the amount of bias associated with each method is quantified. Our model could be used to correct estimated field-based densities from distance sampling and camera trapping of wild boar and other species with similar life-history traits. Our work serves to increase confidence in the results produced by these two commonly-used methods, ensuring they can in turn be relied upon by wildlife managers and conservationists. Crown Copyright (C) 2017 Published by Elsevier B.V. All rights reserved.
C1 [Chauvenet, Alienor L. M.; Smith, Graham C.; Ward, Alastair I.; Massei, Giovanna] Anim & Plant Hlth Agcy, Natl Wildlife Management Ctr, York Y041 1LZ, N Yorkshire, England.
   [Chauvenet, Alienor L. M.] Univ Queensland, Ctr Biodivers & Conservat Sci, Goddard 8,Level 5, St Lucia, Qld 4072, Australia.
   [Chauvenet, Alienor L. M.] Univ Queensland, ARC Ctr Excellence Environm Decis, Goddard 8,Level 5, St Lucia, Qld 4072, Australia.
   [Gill, Robin M. A.] Ctr Human & Ecol Sci, Forest Res, Farnham GU10 4LH, Surrey, England.
RP Chauvenet, ALM (corresponding author), Univ Queensland, Ctr Biodivers & Conservat Sci, Goddard 8,Level 5, St Lucia, Qld 4072, Australia.
EM a.chauvenet@uq.edu.au
RI Chauvenet, Alienor/L-9135-2015; Smith, Graham C/J-2593-2013
OI Chauvenet, Alienor/0000-0002-3743-7375; Smith, Graham
   C/0000-0002-9897-6794; Ward, Alastair/0000-0002-3305-3323
FU Department for the Environment, Food and Rural AffairsDepartment for
   Environment, Food & Rural Affairs (DEFRA)
FX We would like to thank Julia Coats, the Forest of Dean, George Watola
   and Sue Fox for their contribution towards estimating realistic rates
   used in the model. This project was funded by the Department for the
   Environment, Food and Rural Affairs.
CR Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Anderson K. A., 2002, MODEL SELECTION MULT
   Baker SJ, 2010, REV SCI TECH OIE, V29, P311, DOI 10.20506/rst.29.2.1981
   Bartolommei P., 2013, HYSTRIX ITALIAN J MA, V23, P91
   Buckland ST, 2000, BIOMETRICS, V56, P1, DOI 10.1111/j.0006-341X.2000.00001.x
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Engeman RM, 2013, ENVIRON SCI POLLUT R, V20, P8077, DOI 10.1007/s11356-013-2002-5
   Ferretti F, 2014, SPRINGERBRIEF LAW, P1, DOI 10.1007/978-3-319-08906-5
   Focardi S, 2000, J ZOOL, V250, P329, DOI 10.1111/j.1469-7998.2000.tb00777.x
   Focardi S., 2001, WILDI SOC B, P133
   Franzetti B, 2012, EUR J WILDLIFE RES, V58, P385, DOI 10.1007/s10344-011-0587-x
   Goulding MJ, 2003, WILDLIFE BIOL, V9, P15, DOI 10.2981/wlb.2003.059
   Hutchinson JMC, 2007, BIOL REV, V82, P335, DOI 10.1111/j.1469-185X.2007.00014.x
   Karanth KU, 2004, P NATL ACAD SCI USA, V101, P4854, DOI 10.1073/pnas.0306210101
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Li S, 2010, BIODIVERS CONSERV, V19, P3195, DOI 10.1007/s10531-010-9886-x
   Lindberg MS, 2012, J ORNITHOL, V152, pS355, DOI 10.1007/s10336-010-0533-9
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Massei Giovanna, 2004, Galemys, V16, P135
   Massei G, 2015, PEST MANAG SCI, V71, P492, DOI 10.1002/ps.3965
   Miller DL, 2014, DISTANCE SIMPLE WAY
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Parrott D, 2012, EUR J WILDLIFE RES, V58, P23, DOI 10.1007/s10344-011-0536-8
   Peres Carlos A., 1999, Neotropical Primates, V7, P11
   Plumptre AJ, 2000, J APPL ECOL, V37, P356, DOI 10.1046/j.1365-2664.2000.00499.x
   Pollock KH, 2002, ENVIRONMETRICS, V13, P105, DOI 10.1002/env.514
   Roberts Nathan James, 2011, Bioscience Horizons, V4, P40, DOI 10.1093/biohorizons/hzr006
   Rosenstock SS, 2002, AUK, V119, P46, DOI 10.1642/0004-8038(2002)119[0046:LCTCPA]2.0.CO;2
   Rossi S, 2005, VET RES, V36, P27, DOI 10.1051/vetres:2004050
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2013, J WILDLIFE MANAGE, V77, P876, DOI 10.1002/jwmg.533
   Ruette S, 2003, J APPL ECOL, V40, P32, DOI 10.1046/j.1365-2664.2003.00776.x
   Ruiz-Fons F, 2008, VET J, V176, P158, DOI 10.1016/j.tvjl.2007.02.017
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Smart JCR, 2004, MAMMAL REV, V34, P99, DOI 10.1046/j.0305-1838.2003.00026.x
   Team R.C., 2013, R LANG ENV STAT COMP
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Wilson C.J., 2014, Wildlife Biology in Practice, V10, P1
   Wilson CJ, 2004, MAMMAL REV, V34, P331, DOI 10.1111/j.1365-2907.2004.00050.x
   Wilson CJ, 2003, MAMMAL REV, V33, P302, DOI 10.1046/j.1365-2907.2003.00016.x
   Yoccoz NG, 2001, TRENDS ECOL EVOL, V16, P446, DOI 10.1016/S0169-5347(01)02205-4
NR 43
TC 20
Z9 20
U1 3
U2 97
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0304-3800
EI 1872-7026
J9 ECOL MODEL
JI Ecol. Model.
PD APR 24
PY 2017
VL 350
BP 79
EP 86
DI 10.1016/j.ecolmodel.2017.02.007
PG 8
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA ER5XE
UT WOS:000398876800007
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Islam, SB
   Valles, D
AF Islam, Sazida B.
   Valles, Damian
BE Charkrabarti, S
   Paul, R
TI Identification of Wild Species in Texas from Camera-trap Images using
   Deep Neural Network for Conservation Monitoring
SO 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE
   (CCWC)
LA English
DT Proceedings Paper
CT 10th Annual Computing and Communication Workshop and Conference (CCWC)
CY JAN 06-08, 2020
CL Univ Nevada, Las Vegas, CA
SP IEEE Reg 1, IEEE Reg 6, IEEE USA, Inst Engn & Management, Univ Engn & Management, UNLV
HO Univ Nevada
DE DCNN; image classification; species recognition; camera traps; wildlife
   monitoring
AB Protection of endangered species requires continuous monitoring and updated information about the existence, location, and behavioral alterations in their habitat. Remotely activated camera or "camera traps" is a reliable and effective method of photo documentation of local population size, locomotion, and predator-prey relationships of wild species. However, manual data processing from a large volume of images and captured videos is extremely laborious, time-consuming, and expensive. The recent advancement of deep learning methods has shown great outcomes for object and species identification in images. This paper proposes an automated wildlife monitoring system by image classification using computer vision algorithms and machine learning techniques. The goal is to train and validate a Convolutional Neural Network (CNN) that will be able to detect Snakes, Lizards and Toads/Frogs from camera trap images. The initial experiment implies building a flexible CNN architecture with labeled images accumulated from standard benchmark datasets of different citizen science projects. After accessing satisfactory accuracy, new camera-trap imagery data (collected from Bastrop County, Texas) will be implemented to the model to detect species. The performance will be evaluated based on the accuracy of prediction within their classification. The suggested hardware and software framework will offer an efficient monitoring system, speed up wildlife investigation analysis, and formulate resource management decisions.
C1 [Islam, Sazida B.; Valles, Damian] Texas Sate Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.
RP Islam, SB (corresponding author), Texas Sate Univ, Ingram Sch Engn, San Marcos, TX 78666 USA.
EM s_b608@txstate.edu; dvalles@txstate.edu
CR Al Bashit A, 2018, 2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON), P438, DOI 10.1109/IEMCON.2018.8615076
   [Anonymous], ZOONIVERSE DATABASE
   [Anonymous], IMAGENET LARGE SCALE
   Bashit A. A, 2019, INT S MEAS CONTR ROB
   Brownlee J, INTRO IMAGENET CHALL
   Brownlee J, PREPARE DATA MACHINE
   Che Yong Yeo, 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing & its Applications (CSPA 2011), P198, DOI 10.1109/CSPA.2011.5759872
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Gomez A., 2016, ARXIV160306169
   He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
   Kays R., 2010, ARXIV10095718V1CSNI
   Mech LD., 2002, CRITIQUE WILDLIFE RA
   Moreaux M, TOAD IMAGE DATA
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh M. S., 2017, ARXIV170305830V5
   Rosebrock A., 2017, DEEP LEARNING COMPUT
   Sahu R, 2019, VISUAL OBJECT TRACKI, DOI [10.5772/intechopen.88437, DOI 10.5772/INTECHOPEN.88437]
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yim J., 2017, P INT C DIG IM COMP, DOI 10.1109/DICTA.2017.8227427
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
NR 22
TC 0
Z9 0
U1 1
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-3783-4
PY 2020
BP 537
EP 542
AR 1570613456
PG 6
WC Computer Science, Theory & Methods; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BR7NI
UT WOS:000668567200085
DA 2022-02-10
ER

PT C
AU Zhu, CB
   Li, TH
   Li, G
AF Zhu, Chunbiao
   Li, Thomas H.
   Li, Ge
GP IEEE
TI Towards Automatic Wild Animal Detection in Low Quality Camera-trap
   Images Using Two-channeled Perceiving Residual Pyramid Networks
SO 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT 16th IEEE International Conference on Computer Vision (ICCV)
CY OCT 22-29, 2017
CL Venice, ITALY
SP IEEE, IEEE Comp Soc
AB Monitoring animals in the wild without disturbing them is possible using camera trapping framework, which is a technique to study wildlife using automatically triggered cameras and produces great volumes of data. However, camera trapping collects images often result in low image quality and includes a lot of false positives (images without animals), which must be detection before the post-processing step. This paper presents a two-channeled perceiving residual pyramid networks (TPRPN) for cameratrap images objection. Our TPRPN model attends to generating high-resolution and high-quality results. In order to provide enough local information, we extract depth cue from the original images and use two-channeled perceiving model as input to training our networks. Finally, the proposed three-layer residual blocks learn to merge all the information and generate full size detection results. Besides, we construct a new high-quality dataset with the help of Wildlife Thailand's Community and eMammal Organization. Experimental results on our dataset demonstrate that our method is superior to the existing object detection methods.
C1 [Zhu, Chunbiao; Li, Ge] Peking Univ, Shenzhen Grad Sch, SECE, Shenzhen, Peoples R China.
   [Li, Thomas H.] Gpower Semicond Inc, Suzhou, Peoples R China.
RP Zhu, CB (corresponding author), Peking Univ, Shenzhen Grad Sch, SECE, Shenzhen, Peoples R China.
EM zhuchunbiao@pku.edu.cn; thomas.li@gpower-semi.com; geli@ece.pku.edu.cn
RI Zhu, Chunbiao/AAK-6667-2020
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [U1611461]; Science and Technology Planning
   Project of Guangdong Province, China [2014B090910001]; Shenzhen Peacock
   Plan [20130408-183003656]
FX We would like to thank anonymous reviewers for their helpful comments on
   the paper. This work was supported by the grant of National Natural
   Science Foundation of China (No. U1611461), the grant of Science and
   Technology Planning Project of Guangdong Province, China (No.
   2014B090910001) and the grant of Shenzhen Peacock Plan (No.
   20130408-183003656).
CR Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Giraldozuluaga J. H., 2017, CAMERA TRAP IMAGES S
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Lucchi A, 2012, IEEE T MED IMAGING, V31, P474, DOI 10.1109/TMI.2011.2171705
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Ravela S., 2008, VISUAL RECAPTURE MOV
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Shukla A, 2016, IEEE IMAGE PROC, P3982, DOI 10.1109/ICIP.2016.7533107
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhu C., 2017, 2017 IEEE INT C COMP
   Zhu C., 2017, ACM MULTIMEDIA WORKS
   Zhu C., 2017, MULTILAYER BACKPROPA, P14
   Zhu CB, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P33, DOI 10.1109/BigMM.2017.22
NR 18
TC 37
Z9 38
U1 2
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2473-9936
BN 978-1-5386-1034-3
J9 IEEE INT CONF COMP V
PY 2017
BP 2860
EP 2864
DI 10.1109/ICCVW.2017.337
PG 5
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4OB
UT WOS:000425239602109
DA 2022-02-10
ER

PT J
AU Scotson, L
   Johnston, LR
   Iannarilli, F
   Wearn, OR
   Mohd-Azlan, J
   Wong, WM
   Gray, TNE
   Dinata, Y
   Suzuki, A
   Willard, CE
   Frechette, J
   Loken, B
   Steinmetz, R
   Mossbrucker, AM
   Clements, GR
   Fieberg, J
AF Scotson, Lorraine
   Johnston, Lisa R.
   Iannarilli, Fabiola
   Wearn, Oliver R.
   Mohd-Azlan, Jayasilan
   Wong, Wai Ming
   Gray, Thomas N. E.
   Dinata, Yoan
   Suzuki, Ai
   Willard, Clarie E.
   Frechette, Jackson
   Loken, Brent
   Steinmetz, Robert
   Mossbrucker, Alexander M.
   Clements, Gopalasamy Reuben
   Fieberg, John
TI Best practices and software for the management and sharing of camera
   trap data for small and large scales studies
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Review
DE Bycatch data; data management; macrosystem ecology; metadata; population
   trends; species identification
ID WILDLIFE PICTURE INDEX; HABITAT PREFERENCES; FOREST MAMMALS; ECOLOGY;
   CONSERVATION; FUTURE; ISSUES; CARNIVORES; DIVERSITY; COMMUNITY
AB Camera traps typically generate large amounts of bycatch data of non-target species that are secondary to the study's objectives. Bycatch data pooled from multiple studies can answer secondary research questions; however, variation in field and data management techniques creates problems when pooling data from multiple sources. Multi-collaborator projects that use standardized methods to answer broad-scale research questions are rare and limited in geographical scope. Many small, fixed-term independent camera trap studies operate in poorly represented regions, often using field and data management methods tailored to their own objectives. Inconsistent data management practices lead to loss of bycatch data, or an inability to share it easily. As a case study to illustrate common problems that limit use of bycatch data, we discuss our experiences processing bycatch data obtained by multiple research groups during a range-wide assessment of sun bears Helarctos malayanus in Southeast Asia. We found that the most significant barrier to using bycatch data for secondary research was the time required, by the owners of the data and by the secondary researchers (us), to retrieve, interpret and process data into a form suitable for secondary analyses. Furthermore, large quantities of data were lost due to incompleteness and ambiguities in data entry. From our experiences, and from a review of the published literature and online resources, we generated nine recommendations on data management best practices for field site metadata, camera trap deployment metadata, image classification data and derived data products. We cover simple techniques that can be employed without training, special software and Internet access, as well as options for more advanced users, including a review of data management software and platforms. From the range of solutions provided here, researchers can employ those that best suit their needs and capacity. Doing so will enhance the usefulness of their camera trap bycatch data by improving the ease of data sharing, enabling collaborations and expanding the scope of research.
C1 [Scotson, Lorraine; Iannarilli, Fabiola; Fieberg, John] Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, 2003 Upper Buford Circle, St Paul, MN 55108 USA.
   [Johnston, Lisa R.] Univ Minnesota Twin Cities Lib, Minneapolis, MN 55455 USA.
   [Wearn, Oliver R.] Zool Soc London, Inst Zool, Regents Pk, London NW1 4RY, England.
   [Mohd-Azlan, Jayasilan] Univ Malaysia Sarawak, Fac Resource Sci & Technol, Dept Zool, Kota Samarahan 94300, Sarawak, Malaysia.
   [Wong, Wai Ming] Panthera, 8 West 40th St,Floor 18, New York, NY 10018 USA.
   [Gray, Thomas N. E.] Wildlife Alliance, 86,St 123,Toultompong 1, Chamcamon, Phnom Penh, Cambodia.
   [Dinata, Yoan] ZSL, Indonesia Programme, Jalan Papandayan 18, Bogor, West Java, Indonesia.
   [Suzuki, Ai] Kyoto Univ, Grad Sch Asian & African Area Studies, Div Southeast Asian Studies, Ecol & Environm, Kyoto, Japan.
   [Willard, Clarie E.] WCS Cambodia Programme, 21 St 21, Khan Chamkarmorn 12000, Phnom Penh, Cambodia.
   [Frechette, Jackson] Fauna & Flora Int, 19 St 360, Phnom Penh, Cambodia.
   [Loken, Brent] EAT Initiat, POB 1232 Vika, N-0110 Oslo, Norway.
   [Loken, Brent] Stockholm Univ, Stockholm Resilience Ctr, Kraftriket 2B, SE-10691 Stockholm, Sweden.
   [Steinmetz, Robert] WWF Thailand, 92-2 Soi Phaholyothin 5,Phaholyothin Rd, Bangkok 10400, Thailand.
   [Mossbrucker, Alexander M.] FZS, Jl A Chatib 60, Jambi 36124, Indonesia.
   [Clements, Gopalasamy Reuben] Sunway Univ, Dept Biol Sci, 5 Jalan Univ, Bandar Sunway 47500, Selangor, Malaysia.
RP Scotson, L (corresponding author), Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, 2003 Upper Buford Circle, St Paul, MN 55108 USA.
EM scotsonuk@gmail.com
RI Gray, Thomas NE/AAK-6800-2020; Mohd-Azlan, Jayasilan/AAJ-7304-2020;
   Iannarilli, Fabiola/AAG-7774-2021; Fieberg, John/Y-3988-2019
OI Gray, Thomas NE/0000-0002-3642-4724; Mohd-Azlan,
   Jayasilan/0000-0001-5513-6237; Iannarilli, Fabiola/0000-0002-7018-3557;
   Fieberg, John/0000-0002-3180-7021; Dinata, Yoan/0000-0001-5507-3379;
   Wearn, Oliver/0000-0001-8258-3534; Clements, Gopalasamy
   Reuben/0000-0002-9715-4385
FU Ministry of Higher Education (MOHE), Niche Research Grant Scheme
   [NRGS/1087/2013(01)]; Minnesota Department of Natural Resources;
   University of Minnesota Doctoral Dissertation FellowshipUniversity of
   Minnesota System; University of Minnesota Conservation Biology Summer
   Grant
FX MAJ was supported by a Ministry of Higher Education (MOHE), Niche
   Research Grant Scheme: NRGS/1087/2013(01), Iannarilli was funded by the
   Minnesota Department of Natural Resources, and Scotson was funded by a
   University of Minnesota Doctoral Dissertation Fellowship and University
   of Minnesota Conservation Biology Summer Grant while this manuscript was
   prepared.
CR Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Balmford A, 2003, TRENDS ECOL EVOL, V18, P326, DOI 10.1016/S0169-5347(03)00067-3
   Balmford A., 2005, HIMALAYAN J SCI, V3, P43
   Beaudrot L, 2016, PLOS BIOL, V14, DOI 10.1371/journal.pbio.1002357
   Bengsen AJ, 2011, J WILDLIFE MANAGE, V75, P1222, DOI 10.1002/jwmg.132
   Borer E.T., 2009, B ECOL SOC AM, V90, P205, DOI DOI 10.1890/0012-9623-90.2.205
   Briney K., 2015, DATA MANAGEMENT RES
   Bubnicki JW, 2016, METHODS ECOL EVOL, V7, P1209, DOI 10.1111/2041-210X.12571
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chutipong W, 2014, RAFFLES B ZOOL, V62, P521
   Clements G. R., 2013, THESIS
   Collen B, 2008, TROP CONSERV SCI, V1, P75, DOI 10.1177/194008290800100202
   Council of Science Editors, 2014, SCI STYL FORM CSE MA, V2, P44
   Cutler TL, 1999, WILDLIFE SOC B, V27, P571
   Dinata Y., 2008, THESIS
   Dobson A, 2010, ANIM CONSERV, V13, P347, DOI 10.1111/j.1469-1795.2010.00381.x
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Goring SJ, 2014, FRONT ECOL ENVIRON, V12, P39, DOI 10.1890/120370
   Gray TNE, 2012, BIOTROPICA, V44, P531, DOI 10.1111/j.1744-7429.2011.00846.x
   Gray TNE, 2011, RAFFLES B ZOOL, V59, P311
   Hampton SE, 2013, FRONT ECOL ENVIRON, V11, P156, DOI 10.1890/120103
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Heffernan JB, 2014, FRONT ECOL ENVIRON, V12, P5, DOI 10.1890/130017
   Herold P., 2015, J LIBRARIANSHIP SCHO, V3
   Ivan JS, 2016, METHODS ECOL EVOL, V7, P499, DOI 10.1111/2041-210X.12503
   Jansen PA, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P263
   Kratz JE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0117619
   Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
   Linkie M, 2013, BIOL CONSERV, V162, P107, DOI 10.1016/j.biocon.2013.03.028
   Loken B, 2013, AM J PRIMATOL, V75, P1129, DOI 10.1002/ajp.22174
   Lynam AJ, 2012, INTEGR ZOOL, V7, P389, DOI 10.1111/1749-4877.12002
   Mathai J, 2016, RAFFLES B ZOOL, P186
   Maxwell S, 2016, NATURE, V536, P143, DOI 10.1038/536143a
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McGill B., 2016, DYNAMIC ECOLOGY WEB
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meek P., 2014, CAMERA TRAPPING WILD
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Michener WK, 2012, TRENDS ECOL EVOL, V27, P85, DOI 10.1016/j.tree.2011.11.016
   Mohd-Azlan J, 2013, RAFFLES B ZOOL, V61, P397
   Nichols JD, 2010, ANIM CONSERV, V13, P344, DOI 10.1111/j.1469-1795.2010.00382.x
   Niedballa J., 2016, METHODS ECOL EVOL EA
   O'Brien TG, 2010, ANIM CONSERV, V13, P350, DOI 10.1111/j.1469-1795.2010.00384.x
   Olsen AR, 1999, ENVIRON MONIT ASSESS, V54, P1, DOI 10.1023/A:1005823911258
   R Core Team, 2017, R LANG ENV STAT COMP
   Reichman OJ, 2011, SCIENCE, V331, P703, DOI 10.1126/science.1197962
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Sanderson JG, 2005, AM SCI, V93, P148, DOI 10.1511/2005.52.958
   Sandve GK, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003285
   Schipper J, 2008, SCIENCE, V322, P225, DOI 10.1126/science.1165115
   Spehar SN, 2015, BIOL CONSERV, V191, P185, DOI 10.1016/j.biocon.2015.06.013
   Strasser CA, 2012, ECOSPHERE, V3, DOI 10.1890/ES12-00139.1
   Sunarto, 2013, RAFFLES B ZOOL, P21
   Sundaresan S.R., 2011, B ECOL SOC AM, V92, P188, DOI DOI 10.1890/0012-9623-92.2.188
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Thorn M, 2009, S AFR J WILDL RES, V39, P1, DOI 10.3957/056.039.0101
   Tobler M., 2007, CAMERA BASE
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Whitlock MC, 2011, TRENDS ECOL EVOL, V26, P61, DOI 10.1016/j.tree.2010.11.006
   Wong WM, 2013, ANIM CONSERV, V16, P216, DOI 10.1111/j.1469-1795.2012.00587.x
   Wong WM, 2013, DIVERS DISTRIB, V19, P700, DOI 10.1111/ddi.12020
NR 65
TC 17
Z9 20
U1 3
U2 37
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
PD SEP
PY 2017
VL 3
IS 3
SI SI
BP 158
EP 172
DI 10.1002/rse2.54
PG 15
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA FG8YO
UT WOS:000410724800006
OA Green Accepted, gold
DA 2022-02-10
ER

PT J
AU Cappelle, N
   Despres-Einspenner, ML
   Howe, EJ
   Boesch, C
   Kuhl, HS
AF Cappelle, Noemie
   Despres-Einspenner, Marie-Lyne
   Howe, Eric J.
   Boesch, Christophe
   Kuehl, Hjalmar S.
TI Validating camera trap distance sampling for chimpanzees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE animal survey; comparative evaluation; monitoring; Pan troglodytes
   verus; spatial-explicit capture-recapture
ID TAI-NATIONAL-PARK; POPULATION; DENSITY; ABUNDANCE; FOREST
AB The extension of distance sampling methods to accommodate observations from camera traps has recently enhanced the potential to remotely monitor multiple species without the need of additional data collection (sign production and decay rates) or individual identification. However, the method requires that the proportion of time is quantifiable when animals can be detected by the cameras. This can be problematic, for instance, when animals spend time above the ground, which is the case for most primates. In this study, we aimed to validate camera trap distance sampling (CTDS) for the semiarboreal western chimpanzee (Pan troglodytes verus) in Tai National Park, Cote d'Ivoire by estimating abundance of a population of known size and comparing estimates to those from other commonly applied methods. We estimated chimpanzee abundance using CTDS and accounted for limited availability for detection (semiarboreal). We evaluated bias and precision of estimates, as well as costs and efforts required to obtain them, and compared them to those from spatially explicit capture-recapture (SECR) and line transect nest surveys. Abundance estimates obtained by CTDS and SECR produced a similar negligible bias, but CTDS yielded a larger coefficient of variation (CV = 39.70% for CTDS vs. 1%/19% for SECR). Line transects generated the most biased abundance estimates but yielded a better coefficient of variation (27.40-27.85%) than CTDS. Camera trap surveys were twice more costly than line transects because of the initial cost of cameras, while line transects surveys required more than twice as much time in the field. This study demonstrates the potential to obtain unbiased estimates of the abundance of semiarboreal species like chimpanzees by CTDS.
C1 [Cappelle, Noemie; Despres-Einspenner, Marie-Lyne; Boesch, Christophe; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Howe, Eric J.] Observ Univ St Andrews, Ctr Res Ecol & Environm Modeling, St Andrews, Fife, Scotland.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Sustainabil & Complex Ape Habitat, Leipzig, Germany.
RP Cappelle, N (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM noemie_cappelle@eva.mpg.de
OI Cappelle, Noemie/0000-0002-9176-4609
FU Centre for Forest Research Fonds de Recherche Quebec Nature et
   Technologies International internship program; Max-Planck-Institut fur
   Evolutionare Anthropologie
FX Centre for Forest Research Fonds de Recherche Quebec Nature et
   Technologies International internship program; Max-Planck-Institut fur
   Evolutionare Anthropologie
CR Anderson DP, 2005, BIOTROPICA, V37, P631, DOI 10.1111/j.1744-7429.2005.00080.x
   Boesch C, 2006, AM J PHYS ANTHROPOL, V130, P103, DOI 10.1002/ajpa.20341
   Boesch C, 2008, AM J PRIMATOL, V70, P519, DOI 10.1002/ajp.20524
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Buckland S.T., 2001, pi
   Buckland S. T., 2010, INT J PRIMATOL, P1
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Doran D., 1989, CHIMPANZEE PYGMY CHI
   Freytag A, 2014, LECT NOTES COMPUT SC, V8753, P144, DOI 10.1007/978-3-319-11752-2_12
   Grooten M., 2018, LIV PLAN REP 2018 AI
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Herbinger I, 2001, INT J PRIMATOL, V22, P143, DOI 10.1023/A:1005663212997
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Kalan A.K., CURRENT BIOL
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl H., 2008, LIGNES DIRECTRICES M
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Mrovlje J., 2008, 9 INT PHD WORKSH SYS, P1
   Plumptre AJ, 2006, PRIMATES, V47, P65, DOI 10.1007/s10329-005-0146-8
   Plumptre AJ, 1996, INT J PRIMATOL, V17, P85, DOI 10.1007/BF02696160
   Rovero F, 2004, TROP ZOOL, V17, P267, DOI 10.1080/03946975.2004.10531208
   Rovero F., 2016, CAMERA TRAPPING WILD
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Soisalo MK, 2006, BIOL CONSERV, V129, P487, DOI 10.1016/j.biocon.2005.11.023
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Tjandranegara E., 2005, DISTANCE ESTIMATION
   Trolle M, 2005, MAMMALIA, V69, P409, DOI 10.1515/mamm.2005.032
   Wagenmakers EJ, 2004, PSYCHON B REV, V11, P192, DOI 10.3758/BF03206482
   Walsh PD, 2005, ECOL APPL, V15, P1342, DOI 10.1890/03-5283
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 38
TC 17
Z9 17
U1 3
U2 22
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD MAR
PY 2019
VL 81
IS 3
AR e22962
DI 10.1002/ajp.22962
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA HP7RX
UT WOS:000461887600004
PM 30811079
DA 2022-02-10
ER

PT J
AU Ahumada, JA
   Fegraus, E
   Birch, T
   Flores, N
   Kays, R
   O'Brien, TG
   Palmer, J
   Schuttler, S
   Zhao, JY
   Jetz, W
   Kinnaird, M
   Kulkarni, S
   Lyet, A
   Thau, D
   Duong, M
   Oliver, R
   Dancer, A
AF Ahumada, Jorge A.
   Fegraus, Eric
   Birch, Tanya
   Flores, Nicole
   Kays, Roland
   O'Brien, Timothy G.
   Palmer, Jonathan
   Schuttler, Stephanie
   Zhao, Jennifer Y.
   Jetz, Walter
   Kinnaird, Margaret
   Kulkarni, Sayali
   Lyet, Arnaud
   Thau, David
   Duong, Michelle
   Oliver, Ruth
   Dancer, Anthony
TI Wildlife Insights: A Platform to Maximize the Potential of Camera Trap
   and Other Passive Sensor Wildlife Data for the Planet
SO ENVIRONMENTAL CONSERVATION
LA English
DT Article
DE artificial intelligence; camera traps; data analysis; data sharing;
   protected areas; technology platform; wildlife
ID BIODIVERSITY; DEFAUNATION; MANAGEMENT; SOFTWARE; ECOLOGY; SCIENCE; TREE
AB Wildlife is an essential component of all ecosystems. Most places in the globe do not have local, timely information on which species are present or how their populations are changing. With the arrival of new technologies, camera traps have become a popular way to collect wildlife data. However, data collection has increased at a much faster rate than the development of tools to manage, process and analyse these data. Without these tools, wildlife managers and other stakeholders have little information to effectively manage, understand and monitor wildlife populations. We identify four barriers that are hindering the widespread use of camera trap data for conservation. We propose specific solutions to remove these barriers integrated in a modern technology platform called Wildlife Insights. We present an architecture for this platform and describe its main components. We recognize and discuss the potential risks of publishing shared biodiversity data and a framework to mitigate those risks. Finally, we discuss a strategy to ensure platforms like Wildlife Insights are sustainable and have an enduring impact on the conservation of wildlife.
C1 [Ahumada, Jorge A.; Flores, Nicole] Moore Ctr Sci Conservat Int, 2011 Crystal Dr Suite 600, Arlington, VA 22202 USA.
   [Ahumada, Jorge A.] Arizona State Univ, Ctr Biodivers Outcomes, Julia Ann Wrigley Global Inst Sustainabil, Tempe, AZ 85281 USA.
   [Fegraus, Eric] Conservat Int, Ctr Environm Leadership & Business, 2011 Crystal Dr Suite 600, Arlington, VA 22202 USA.
   [Birch, Tanya; Kulkarni, Sayali] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Kays, Roland; Schuttler, Stephanie] North Carolina Museum Nat Sci, 11 West Jones St, Raleigh, NC 27601 USA.
   [O'Brien, Timothy G.; Palmer, Jonathan] Wildlife Conservat Soc, 2300 Southern Blvd, Bronx, NY 10460 USA.
   [Zhao, Jennifer Y.] Smithsonian Conservat Biol Inst, 1500 Remount Rd, Front Royal, VA 22630 USA.
   [Jetz, Walter; Duong, Michelle; Oliver, Ruth] Yale Univ, Dept Ecol & Evolutionary Biol, 165 Prospect St, New Haven, CT 06520 USA.
   [Kinnaird, Margaret] WWF Int, Mvuli Rd, Westlands, Kenya.
   [Lyet, Arnaud; Thau, David] WWF, 1250 24th St NW, Washington, DC 20037 USA.
   [Dancer, Anthony] Zool Soc London, Conservat & Policy, Regents Pk, London NW1 4RY, England.
RP Ahumada, JA (corresponding author), Moore Ctr Sci Conservat Int, 2011 Crystal Dr Suite 600, Arlington, VA 22202 USA.; Ahumada, JA (corresponding author), Arizona State Univ, Ctr Biodivers Outcomes, Julia Ann Wrigley Global Inst Sustainabil, Tempe, AZ 85281 USA.
EM jahumada@conservation.org
RI Kulkarni, Sayali/AAD-6144-2022; Jetz, Walter/ABF-1517-2020
OI Kays, Roland/0000-0002-2947-6665; Ahumada, Jorge/0000-0003-0953-9101
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation; Lyda
   Hill; GoogleGoogle Incorporated
FX We thank The Gordon and Betty Moore Foundation, Lyda Hill and Google for
   partial support of this work.
CR Ahumada JA, 2016, CAMERA TRAPPING WILD, P196
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Farley SS, 2018, BIOSCIENCE, V68, P563, DOI 10.1093/biosci/biy068
   Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Griscom BW, 2017, P NATL ACAD SCI USA, V114, P11645, DOI 10.1073/pnas.1710465114
   Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
   Harrison RD, 2013, ECOL LETT, V16, P687, DOI 10.1111/ele.12102
   IUCN, 2017, IUCN GREEN LIST PROT
   Jetz W, 2019, NAT ECOL EVOL, V3, P539, DOI 10.1038/s41559-019-0826-1
   Jetz W, 2012, TRENDS ECOL EVOL, V27, P151, DOI 10.1016/j.tree.2011.09.007
   Kissling WD, 2018, BIOL REV, V93, P600, DOI 10.1111/brv.12359
   Kurten EL, 2013, BIOL CONSERV, V163, P22, DOI 10.1016/j.biocon.2013.04.025
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meyer C, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9221
   Michener WK, 2012, TRENDS ECOL EVOL, V27, P85, DOI 10.1016/j.tree.2011.11.016
   O'Brien TG, 2010, ANIM CONSERV, V13, P335, DOI 10.1111/j.1469-1795.2010.00357.x
   O'Brien Timothy G., 2013, P45
   Osuri AM, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11351
   Pereira HM, 2013, SCIENCE, V339, P277, DOI 10.1126/science.1229931
   Peres CA, 2016, P NATL ACAD SCI USA, V113, P892, DOI 10.1073/pnas.1516525113
   Ripple WJ, 2016, BIOSCIENCE, V66, P807, DOI 10.1093/biosci/biw092
   Schuttler SG, 2019, BIOSCIENCE, V69, P69, DOI 10.1093/biosci/biy141
   Scotson L, 2017, REMOTE SENS ECOL CON, V3, P158, DOI 10.1002/rse2.54
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stephenson PJ, 2017, FRONT ECOL ENVIRON, V15, P124
   Szegedy C, 2016, RETHINKING INCEPTION
   TEAM NETWORK, 2011, TERR VERT MON PROT V
   Tulloch AIT, 2018, NAT ECOL EVOL, V2, P1209, DOI 10.1038/s41559-018-0608-1
   Wearn O.R., 2017, CAMERA TRAPPING CONS
NR 30
TC 21
Z9 24
U1 5
U2 16
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0376-8929
EI 1469-4387
J9 ENVIRON CONSERV
JI Environ. Conserv.
PD MAR
PY 2020
VL 47
IS 1
SI SI
BP 1
EP 6
AR PII S0376892919000298
DI 10.1017/S0376892919000298
PG 6
WC Biodiversity Conservation; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KK2WI
UT WOS:000512608100001
OA hybrid
DA 2022-02-10
ER

PT J
AU Palencia, P
   Fernandez-Lopez, J
   Vicente, J
   Acevedo, P
AF Palencia, Pablo
   Fernandez-Lopez, Javier
   Vicente, Joaquin
   Acevedo, Pelayo
TI Innovations in movement and behavioural ecology from camera traps: Day
   range as model parameter
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE activity; machine learning; mammals; REM; simulation; speed
ID HETEROGENEOUS LANDSCAPES; ACTIVITY PATTERNS; ANIMAL MOVEMENTS; WOLVES
AB Camera-trapping methods have been used to monitor movement and behavioural ecology parameters of wildlife. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise DR results will be biased. For instance, some wildlife populations present movement patterns characteristic of each behaviour (e.g. foraging or displacement between habitat patches), and further research is needed to integrate the behaviours in the estimation of movement parameters. In this respect, the day range (average daily distance travelled by an individual, DR) is a model parameter that relies on movement and behaviour. This study aims to provide a step forward concerning the use of camera-trapping in movement and behavioural ecology.
   We describe a machine learning procedure to differentiate movement behaviours from camera-trap data, and revisit the approach to consider different behaviours in the estimation of DR. Second, working within a simulated framework we tested the performance of three approaches to estimate DR: DROB (i.e. estimating DR without behavioural identification), DRTB (i.e. estimating DR by identifying behaviours manually and weighting each behaviour on the basis of the encounter rate obtained) and DRRB (i.e. estimating DR based on the classification of movement behaviours by a machine learning procedure and the ratio between speeds). Finally, we evaluated these approaches for 24 wild mammal species with different behavioural and ecological traits.
   The machine learning procedure to differentiate behaviours showed high accuracy (mean = 0.97). The DROB approach generated accurate results in scenarios with a speed-ratio (fast relative to slow behaviours) lower than 10, and for scenarios in which the animals spend most of the activity period on the slow behaviour. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise the DR results will be biased. The new approach, DRRB, generated accurate results in all the scenarios. The results obtained from real populations were consistent with the simulations.
   In conclusion, the integration of behaviours and speed-ratio in camera-trap studies makes it possible to obtain unbiased DR. Speed-ratio should be considered so that fast behaviour is not overrepresented. The procedures described in this work extend the applicability of camera-trap-based approaches in both movement and behavioural ecology.
C1 [Palencia, Pablo; Fernandez-Lopez, Javier; Vicente, Joaquin; Acevedo, Pelayo] Inst Invest Recursos Cineget IREC CSIC UCLM JCCM, Ciudad Real, Spain.
RP Palencia, P (corresponding author), Inst Invest Recursos Cineget IREC CSIC UCLM JCCM, Ciudad Real, Spain.
EM palencia.pablo.m@gmail.com
RI Fernández-López, Javier/AAU-7282-2020; Acevedo, Pelayo/L-6737-2013;
   Vicente, Joaquin/K-7822-2013
OI Fernández-López, Javier/0000-0003-4352-0252; Palencia,
   Pablo/0000-0002-2928-4241; Acevedo, Pelayo/0000-0002-3509-7696; Vicente,
   Joaquin/0000-0001-8416-3672
FU MINECO-FEDERSpanish Government [PID2019-111699RB-I00]; MINECO-UCLM
   [FPU16/00039]
FX MINECO-UCLM, Grant/Award Number: FPU16/00039; MINECO-FEDER, Grant/ Award
   Number: PID2019-111699RB-I00
CR AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Barraquand F, 2008, ECOLOGY, V89, P3336, DOI 10.1890/08-0162.1
   Buderman FE, 2018, ECOGRAPHY, V41, P126, DOI 10.1111/ecog.03030
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2020, CONSERV SCI PRACT, V2, DOI 10.1111/csp2.239
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Charrad M, 2014, J STAT SOFTW, V61, P1
   Dalloz MF, 2012, MAMM BIOL, V77, P307, DOI 10.1016/j.mambio.2012.03.001
   Duffy KJ, 2011, S AFR J WILDL RES, V41, P21, DOI 10.3957/056.041.0107
   ENETwild Consortium, 2018, EFSA SUPPORTING PUBL, DOI [DOI 10.2903/SP.EFSA.2018.EN-1449, 10.2903/sp.efsa.2018.EN-1449]
   Erdtmann D, 2020, PEERJ, V8, DOI 10.7717/peerj.10409
   Fortin D, 2005, ECOLOGY, V86, P1320, DOI 10.1890/04-0953
   Fowler J., 2013, PRACTICAL STAT FIELD
   Fryxell JM, 2008, P NATL ACAD SCI USA, V105, P19114, DOI 10.1073/pnas.0801737105
   GOODMAN LA, 1960, J AM STAT ASSOC, V55, P708, DOI 10.2307/2281592
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Holyoak M, 2008, P NATL ACAD SCI USA, V105, P19060, DOI 10.1073/pnas.0800483105
   Iijima H, 2020, MAMM STUDY, V45, P177, DOI 10.3106/ms2019-0082
   Joo R, 2020, J ANIM ECOL, V89, P248, DOI 10.1111/1365-2656.13116
   Kery M, 2011, CONSERV BIOL, V25, P356, DOI 10.1111/j.1523-1739.2010.01616.x
   Kindberg J, 2009, BIOL CONSERV, V142, P159, DOI 10.1016/j.biocon.2008.10.009
   Kovacs V, 2017, APPL ANIM BEHAV SCI, V195, P112, DOI 10.1016/j.applanim.2017.05.019
   Larrucea ES, 2007, J WILDLIFE MANAGE, V71, P1682, DOI 10.2193/2006-407
   Leuchtenberger C, 2014, ETHOL ECOL EVOL, V26, P19, DOI 10.1080/03949370.2013.821673
   Likas A, 2003, PATTERN RECOGN, V36, P451
   Lopez-Bao JV, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-20675-9
   Macdonald Peter, 2018, CRAN
   Martin J., 2009, THESIS CLAUDE BERNAR, P89
   MARTIN R, 1995, OECOLOGIA, V101, P45, DOI 10.1007/BF00328898
   Michelot T, 2016, METHODS ECOL EVOL, V7, P1308, DOI 10.1111/2041-210X.12578
   Morales JM, 2002, ECOLOGY, V83, P2240
   Neilson EW, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2092
   Neumann W, 2015, MOV ECOL, V3, DOI 10.1186/s40462-015-0036-7
   Niedballa J, 2019, REMOTE SENS ECOL CON, V5, P272, DOI 10.1002/rse2.107
   Ogurtsov SS, 2018, NAT CONSERV RES, V3, P68, DOI 10.24189/ncr.2018.031
   Owen-Smith N, 2020, MAMMAL REV, V50, P252, DOI 10.1111/mam.12193
   Palencia P, 2019, J ZOOL, V309, P182, DOI 10.1111/jzo.12710
   Palencia P., 2020, TRAPPINGMOTION INTEG
   Palencia P., 2021, ZENODO, DOI [10.5281/zenodo.4623758, DOI 10.5281/ZENODO.4623758]
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2012, METHODS ECOL EVOL, V3, P653, DOI 10.1111/j.2041-210X.2012.00197.x
   Rowcliffe JM, 2019, Activity: animal activity statistics. R package version 1.3
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Schaus J, 2020, REMOTE SENS ECOL CON, V6, P514, DOI 10.1002/rse2.153
   Scherer C, 2020, OIKOS, V129, P651, DOI 10.1111/oik.07002
   Sennhenn-Reulen Holger, 2017, Primate Biol, V4, P143, DOI 10.5194/pb-4-143-2017
   Smith JA, 2020, J ANIM ECOL, V89, P1997, DOI 10.1111/1365-2656.13264
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Tobler MW, 2009, J TROP ECOL, V25, P261, DOI 10.1017/S0266467409005896
   Vazquez C, 2019, METHODS ECOL EVOL, V10, P2057, DOI 10.1111/2041-210X.13290
   Wearn O.R., 2017, WWF CONSERVATION TEC, V1, P70
   Zuur AF, 2010, METHODS ECOL EVOL, V1, P3, DOI 10.1111/j.2041-210X.2009.00001.x
NR 56
TC 4
Z9 4
U1 4
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JUL
PY 2021
VL 12
IS 7
BP 1201
EP 1212
DI 10.1111/2041-210X.13609
EA MAY 2021
PG 12
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA TC0ZC
UT WOS:000647824700001
DA 2022-02-10
ER

PT J
AU Carl, C
   Schonfeld, F
   Profft, I
   Klamm, A
   Landgraf, D
AF Carl, Christin
   Schoenfeld, Fiona
   Profft, Ingolf
   Klamm, Alisa
   Landgraf, Dirk
TI Automated detection of European wild mammal species in camera trap
   images with an existing and pre-trained computer vision model
SO EUROPEAN JOURNAL OF WILDLIFE RESEARCH
LA English
DT Article
DE Computer vision; Image analysis; Camera trap; Pre-trained model; Wild
   mammal species
AB The use of camera traps is a nonintrusive monitoring method to obtain valuable information about the appearance and behavior of wild animals. However, each study generates thousands of pictures and extracting information remains mostly an expensive, time-consuming manual task. Nevertheless, image recognition and analyzing technologies combined with machine learning algorithms, particularly deep learning models, improve and speed up the analysis process. Therefore, we tested the usability of a pre-trained deep learning model available on the TensorFlow hub-FasterRCNN+InceptionResNet V2 network applied to images of ten different European wild mammal species such as wild boar (Sus scrofa), roe deer (Capreolus capreolus), or red fox (Vulpes vulpes) in color as well as black and white infrared images. We found that the detection rate of the correct region of interest (region of the animal) was 94%. The classification accuracy was 71% for the correct species' name as mammals and 93% for the correct species or higher taxonomic ranks such as "carnivore" as order. In 7% of cases, the classification was incorrect as the wrong species' name was classified. In this technical note, we have shown the potential of an existing and pre-trained image classification model for wildlife animal detection, classification, and analysis. A specific training of the model on European wild mammal species could further increase the detection and classification accuracy of the models. Analysis of camera trap images could thus become considerably faster, less expensive, and more efficient.
C1 [Carl, Christin; Schoenfeld, Fiona; Landgraf, Dirk] Univ Appl Sci Erfurt, Forestry & Ecosyst Management, Leipziger Str 77, D-99085 Erfurt, Germany.
   [Profft, Ingolf] ThuringenForst AoR, Forstliches Forsch & Kompetenzzentrum, Jagerstr 1, D-99867 Gotha, Germany.
   [Klamm, Alisa] Natl Pk Verwaltung Hainich, Bei Marktkirche 9, D-99947 Bad Langensalza, Germany.
RP Carl, C (corresponding author), Univ Appl Sci Erfurt, Forestry & Ecosyst Management, Leipziger Str 77, D-99085 Erfurt, Germany.
EM christin.carl@fh-erfurt.de; fiona.schoenfeld@fh-erfurt.de;
   Ingolf.Profft@forst.thueringen.de; Alisa.Klamm@nnl.thueringen.de;
   dirk.landgraf@fh-erfurt.de
RI Landgraf, Dirk/AAD-8062-2020
OI Landgraf, Dirk/0000-0002-1891-3751
FU University of Applied Sciences Erfurt
FX This research was supported by the University of Applied Sciences Erfurt
   (FHE).
CR Abadi, 2015, TENSORFLOW LARGE SCA
   Beery S., 2019, ARXIV190405986
   Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
   Casaer J, 2019, BIODIVERSITY INFORM
   Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
   Google LLC, 2019, OP IM DAT V4 CC 4 0
   Google LLC Colaboratory, 2019, WELC COL
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Hui Jonathan, 2018, OBJECT DETECTION SPE
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Inik o, 2018, J NEW RESULTS SCI, V7, P9
   Jayakumar R., 2020, J CRIT REV, V7, P434
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kluyver T, 2016, POSITIONING AND POWER IN ACADEMIC PUBLISHING: PLAYERS, AGENTS AND AGENDAS, P87, DOI 10.3233/978-1-61499-649-1-87
   LILA BC, 2019, LAB INF LIB AL BIOL
   Lundh F, 2016, PILLOW PYTHON IMAGIN
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Nationalparkverwaltung Hainich FFK Gotha, 2019, SCHW HAIN
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Oliphant T. E, 2006, A GUIDE TO NUMPY, P85
   Python Software Foundation, 2019, PYTH STAND LIBR TEMP
   Python Software Foundation, 2019, PYTH STAND LIB IO CO
   Python Software Foundation, 2019, PYTH STAND LIB URLL
   Python Software Foundation, 2019, PYTH STAND LIB TIM T
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Weingarth K, 2011, GRENZUBERSCHREITENDE
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 29
TC 3
Z9 3
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1612-4642
EI 1439-0574
J9 EUR J WILDLIFE RES
JI Eur. J. Wildl. Res.
PD JUL 14
PY 2020
VL 66
IS 4
AR 62
DI 10.1007/s10344-020-01404-y
PG 7
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA MJ1WH
UT WOS:000547883800001
DA 2022-02-10
ER

PT J
AU Green, AM
   Chynoweth, MW
   Sekercioglu, CH
AF Green, Austin M.
   Chynoweth, Mark W.
   Sekercioglu, Cagan Hakki
TI Spatially Explicit Capture-Recapture Through Camera Trapping: A Review
   of Benchmark Analyses for Wildlife Density Estimation
SO FRONTIERS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE citizen science; conservation biology; biodiversity monitoring; mammals;
   Carnivora; wildlife ecology; density estimation
ID TIGER PANTHERA-TIGRIS; CITIZEN SCIENCE; POPULATION; TRAPS; FOREST;
   BIODIVERSITY; RECOMMENDATIONS; PARAMETERS; PRINCIPLES; OCCUPANCY
AB Camera traps have become an important research tool for both conservation biologists and wildlife managers. Recent advances in spatially explicit capture-recapture (SECR) methods have increasingly put camera traps at the forefront of population monitoring programs. These methods allow for benchmark analysis of species density without the need for invasive fieldwork techniques. We conducted a review of SECR studies using camera traps to summarize the current focus of these investigations, as well as provide recommendations for future studies and identify areas in need of future investigation. Our analysis shows a strong bias in species preference, with a large proportion of studies focusing on large felids, many of which provide the only baseline estimates of population density for these species. Furthermore, we found that a majority of studies produced density estimates that may not be precise enough for long-term population monitoring. We recommend simulation and power analysis be conducted before initiating any particular study design and provide examples using readily available software. Furthermore, we show that precision can be increased by including a larger study area that will subsequently increase the number of individuals photo-captured. As many current studies lack the resources or manpower to accomplish such an increase in effort, we recommend that researchers incorporate new technologies such as machine-learning, web-based data entry, and online deployment management into their study design. We also cautiously recommend the potential of citizen science to help address these study design concerns. In addition, modifications in SECR model development to include species that have only a subset of individuals available for individual identification (often called mark-resight models), can extend the process of explicit density estimation through camera trapping to species not individually identifiable.
C1 [Green, Austin M.; Sekercioglu, Cagan Hakki] Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
   [Chynoweth, Mark W.] Utah State Univ Uintah Basin, Dept Wildland Resources, Vernal, UT USA.
   [Sekercioglu, Cagan Hakki] Koc Univ, Coll Sci, Istanbul, Turkey.
RP Green, AM (corresponding author), Univ Utah, Sch Biol Sci, Salt Lake City, UT 84112 USA.
EM austin.m.green@utah.edu
OI Sekercioglu, Cagan H./0000-0003-3193-0377
FU Global Change and Sustainability Center at the University of Utah
FX AG would like to thank the Global Change and Sustainability Center at
   the University of Utah for supporting this work. CS thanks Hamit Batubay
   ozkan and Barbara J. Watkins for their generous support. The authors
   would like to thank Roland Kays, Adam Duarte, a reviewer and the
   handling editor for their helpful comments. Their revisions greatly
   improved the quality of the manuscript.
CR Abolafya M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068037
   Aceves-Bueno E., 2017, B ECOL SOC AM, V98, P278, DOI [10.1002/bes2.1336, DOI 10.1002/BES2.1336]
   Adler FR, 2020, ANN NY ACAD SCI, V1469, P52, DOI 10.1111/nyas.14340
   Alexandrino E.R., 2019, CITIZ SCI THEORY PRA, V4, P1, DOI DOI 10.5334/CSTP.198
   Beschta RL, 2009, BIOL CONSERV, V142, P2401, DOI 10.1016/j.biocon.2009.06.015
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Bowser A., 2018, CITIZEN SCI INNOVATI, DOI [10.2307/j.ctv550cf2, DOI 10.2307/J.CTV550CF2]
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cardinale BJ, 2006, NATURE, V443, P989, DOI 10.1038/nature05202
   Chandler M, 2017, BIOL CONSERV, V213, P280, DOI 10.1016/j.biocon.2016.09.004
   Conrad CC, 2011, ENVIRON MONIT ASSESS, V176, P273, DOI 10.1007/s10661-010-1582-5
   Dalerum F, 2009, BIOL LETTERS, V5, P35, DOI 10.1098/rsbl.2008.0520
   Dalerum F, 2008, BIODIVERS CONSERV, V17, P2939, DOI 10.1007/s10531-008-9406-4
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Devictor V, 2010, DIVERS DISTRIB, V16, P354, DOI 10.1111/j.1472-4642.2009.00615.x
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Dillon A, 2008, J ZOOL, V275, P391, DOI 10.1111/j.1469-7998.2008.00452.x
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford M., 2010, SECR 4 1 SPATIALLY E
   Efford M. G., 2019, SECRDESIGNAPP 1 3 IN
   Efford MG, 2019, METHODS ECOL EVOL, V10, P1529, DOI 10.1111/2041-210X.13239
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Efford MG, 2011, ECOLOGY, V92, P2202, DOI 10.1890/11-0332.1
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Ergon T, 2014, METHODS ECOL EVOL, V5, P1327, DOI 10.1111/2041-210X.12133
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Gallo T, 2011, BIOSCIENCE, V61, P459, DOI 10.1525/bio.2011.61.6.8
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Gardner B, 2010, J WILDLIFE MANAGE, V74, P318, DOI 10.2193/2009-101
   GERRODETTE T, 1993, WILDLIFE SOC B, V21, P515
   GERRODETTE T, 1987, ECOLOGY, V68, P1364, DOI 10.2307/1939220
   Gilbert NA, 2021, CONSERV BIOL, V35, P88, DOI 10.1111/cobi.13517
   Glen AS, 2003, WILDLIFE RES, V30, P29, DOI 10.1071/WR01059
   Gopal R, 2010, ORYX, V44, P383, DOI 10.1017/S0030605310000529
   Gopalaswamy AM, 2012, METHODS ECOL EVOL, V3, P1067, DOI 10.1111/j.2041-210X.2012.00241.x
   GRIFFITHS M, 1993, CONSERV BIOL, V7, P623, DOI 10.1046/j.1523-1739.1993.07030623.x
   Harihar A, 2009, EUR J WILDLIFE RES, V55, P97, DOI 10.1007/s10344-008-0219-2
   Hawthorne TL, 2015, APPL GEOGR, V56, P187, DOI 10.1016/j.apgeog.2014.10.005
   Hirakawa Hirofumi, 2005, Mammal Study, V30, P69, DOI 10.3106/1348-6160(2005)30[69:LBTTCA]2.0.CO;2
   Hooper DU, 2012, NATURE, V486, P105, DOI 10.1038/nature11118
   Horns JJ, 2018, BIOL CONSERV, V221, P151, DOI 10.1016/j.biocon.2018.02.027
   Jarvis RM, 2015, MAR POLICY, V57, P21, DOI 10.1016/j.marpol.2015.03.011
   Jimenez J, 2017, SCI REP-UK, V7, DOI 10.1038/srep41036
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
   Laundre J.W., 2010, OPEN ECOL J, V3, P1
   Linden DW, 2017, J APPL ECOL, V54, P2043, DOI 10.1111/1365-2664.12883
   Linkie M, 2006, J APPL ECOL, V43, P576, DOI 10.1111/j.1365-2664.2006.01153.x
   Loock DJE, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-34936-0
   Luskin MS, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01656-4
   Lyra-Jorge MC, 2008, EUR J WILDLIFE RES, V54, P739, DOI 10.1007/s10344-008-0205-8
   McClintock BT, 2012, ECOL MONOGR, V82, P335, DOI 10.1890/11-0326.1
   McClintock BT, 2009, BIOMETRICS, V65, P237, DOI 10.1111/j.1541-0420.2008.01047.x
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Neate-Clegg MHC, 2020, BIOL CONSERV, V248, DOI 10.1016/j.biocon.2020.108653
   Nichols JD, 2014, SPATIAL CAPTURE-RECAPTURE, P125, DOI 10.1016/B978-0-12-405939-9.00005-0
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   O'Connell AF, 2006, J WILDLIFE MANAGE, V70, P1625, DOI 10.2193/0022-541X(2006)70[1625:ESOADP]2.0.CO;2
   Obbard ME, 2010, J APPL ECOL, V47, P76, DOI 10.1111/j.1365-2664.2009.01758.x
   OTIS DL, 1978, WILDLIFE MONOGR, P1
   Paviolo A, 2008, ORYX, V42, P554, DOI 10.1017/S0030605308000641
   Pesenti E, 2013, J MAMMAL, V94, P73, DOI 10.1644/11-MAMM-A-322.1
   Petersen Wyatt Joseph, 2019, Journal of Threatened Taxa, V11, P13448, DOI 10.11609/jott.4553.11.4.13448-13458
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Roberts Nathan James, 2011, Bioscience Horizons, V4, P40, DOI 10.1093/biohorizons/hzr006
   Rotman D., 2012, P ACM 2012 C COMP SU, P217, DOI DOI 10.1145/2145204.2145238
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Royle J.A., 2008, HIERARCHICAL MODELIN
   Royle JA, 2008, ECOLOGY, V89, P2281, DOI 10.1890/07-0601.1
   Royle JA, 2018, ECOGRAPHY, V41, P444, DOI 10.1111/ecog.03170
   Royle JA, 2016, POPUL ECOL, V58, P53, DOI 10.1007/s10144-015-0524-z
   Royle JA, 2011, J WILDLIFE MANAGE, V75, P604, DOI 10.1002/jwmg.79
   Royle JA, 2009, ECOLOGY, V90, P3233, DOI 10.1890/08-1481.1
   Sauermann H, 2015, P NATL ACAD SCI USA, V112, P679, DOI 10.1073/pnas.1408907112
   Schaub M, 2014, METHODS ECOL EVOL, V5, P1316, DOI 10.1111/2041-210X.12134
   SEIDENSTICKER J, 1993, SYM ZOOL S, P105
   Seymour V., 2017, CITIZ, V2, P5, DOI [10.5334/cstp.66, DOI 10.5334/CSTP.66]
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Sullivan BL, 2017, BIOL CONSERV, V208, P5, DOI 10.1016/j.biocon.2016.04.031
   Tobler MW, 2013, BIOL CONSERV, V159, P109, DOI 10.1016/j.biocon.2012.12.009
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Turner A., 1997, BIG CATS THEIR FOSSI
   Vann-Sander S, 2016, MAR POLICY, V72, P82, DOI 10.1016/j.marpol.2016.06.026
   Venter O, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12558
   Wald DM, 2016, CONSERV BIOL, V30, P562, DOI 10.1111/cobi.12627
   Welbourne DJ, 2016, REMOTE SENS ECOL CON, V2, P77, DOI 10.1002/rse2.20
   White G., 1982, CAPTURE RECAPTURE RE
   Whittington J, 2018, J APPL ECOL, V55, P157, DOI 10.1111/1365-2664.12954
NR 92
TC 10
Z9 10
U1 8
U2 20
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 2296-701X
J9 FRONT ECOL EVOL
JI Front. Ecol. Evol.
PD DEC 18
PY 2020
VL 8
AR 563477
DI 10.3389/fevo.2020.563477
PG 11
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA PM8KV
UT WOS:000604041600001
OA gold, Green Published
DA 2022-02-10
ER

PT J
AU Kalan, AK
   Hohmann, G
   Arandjelovic, M
   Boesch, C
   McCarthy, MS
   Agbor, A
   Angedakin, S
   Bailey, E
   Balongelwa, CW
   Bessone, M
   Bocksberger, G
   Coxe, SJ
   Deschner, T
   Despres-Einspenner, ML
   Dieguez, P
   Fruth, B
   Herbinger, I
   Granjon, AC
   Head, J
   Kablan, YA
   Langergraber, KE
   Lokasola, AL
   Maretti, G
   Marrocoli, S
   Mbende, M
   Moustgaard, J
   N'Goran, PK
   Robbins, MM
   van Schijndel, J
   Sommer, V
   Surbeck, M
   Tagg, N
   Willie, J
   Wittig, RM
   Kuhl, HS
AF Kalan, Ammie K.
   Hohmann, Gottfried
   Arandjelovic, Mimi
   Boesch, Christophe
   McCarthy, Maureen S.
   Agbor, Anthony
   Angedakin, Samuel
   Bailey, Emma
   Balongelwa, Cosma Wilungula
   Bessone, Mattia
   Bocksberger, Gaelle
   Coxe, Sally Jewel
   Deschner, Tobias
   Despres-Einspenner, Marie-Lyne
   Dieguez, Paula
   Fruth, Barbara
   Herbinger, Ilka
   Granjon, Anne-Celine
   Head, Josephine
   Kablan, Yves Aka
   Langergraber, Kevin E.
   Lokasola, Albert Lotana
   Maretti, Giovanna
   Marrocoli, Sergio
   Mbende, Menard
   Moustgaard, Jennifer
   N'Goran, Paul Kouame
   Robbins, Martha M.
   van Schijndel, Joost
   Sommer, Volker
   Surbeck, Martin
   Tagg, Nikki
   Willie, Jacob
   Wittig, Roman M.
   Kuehl, Hjalmar S.
TI Novelty Response of Wild African Apes to Camera Traps
SO CURRENT BIOLOGY
LA English
DT Article
ID PERSONALITY-DEVELOPMENT; CHIMPANZEES; EVOLUTION; RISK; TEMPERAMENT;
   PREFERENCES; EXPLORATION; NEOPHOBIA; CURIOSITY; BOLDNESS
AB Temperament and personality research in humans and nonhuman animals measures behavioral variation in individual, population, or species-specific traits with implications for survival and fitness, such as social status, foraging, and mating success [1-5]. Curiosity and risk-taking tendencies have been studied extensively across taxa by measuring boldness and exploration responses to experimental novelty exposure [3, 4, 6-15]. Here, we conduct a natural field experiment using wildlife monitoring technology to test variation in the reaction of wild great apes (43 groups of naive chimpanzees, bonobos, and western gorillas across 14 field sites in Africa) to a novel object, the camera trap. Bonobo and gorilla groups demonstrated a stronger looking impulse toward the camera trap device compared to chimpanzees, suggesting higher visual attention and curiosity. Bonobos were also more likely to show alarm and other fearful behaviors, although such neophobic (and conversely, neophilic) responses were generally rare. Among all three species, individuals looked at cameras longer when they were young, were associating with fewer individuals, and did not live near a long-term research site. Overall, these findings partially validate results from great ape novelty paradigms in captivity [7, 8]. We further suggest that species-typical leadership styles [16] and social and environmental effects, including familiarity with humans, best explain novelty responses of wild great apes. In sum, this study illustrates the feasibility of large-scale field experiments and the importance of both intrinsic and extrinsic factors in shaping animal curiosity.
C1 [Kalan, Ammie K.; Hohmann, Gottfried; Arandjelovic, Mimi; Boesch, Christophe; McCarthy, Maureen S.; Agbor, Anthony; Angedakin, Samuel; Bailey, Emma; Bocksberger, Gaelle; Deschner, Tobias; Despres-Einspenner, Marie-Lyne; Dieguez, Paula; Granjon, Anne-Celine; Head, Josephine; Kablan, Yves Aka; Maretti, Giovanna; Marrocoli, Sergio; Robbins, Martha M.; van Schijndel, Joost; Surbeck, Martin; Wittig, Roman M.; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Boesch, Christophe; Kablan, Yves Aka] WCF, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Balongelwa, Cosma Wilungula] ICCN, 13 Ave Clin, Kinshasa, DEM REP CONGO.
   [Bessone, Mattia] Ludwig Maximilians Univ Munchen, Fac Biol, Dept Neurobiol, Grossaderner Str 2, D-82152 Planegg Martinsried, Germany.
   [Coxe, Sally Jewel; Lokasola, Albert Lotana; Moustgaard, Jennifer; Surbeck, Martin] Bonobo Conservat Initiat, 2701 Connecticut Ave,NW 702, Washington, DC 20008 USA.
   [Fruth, Barbara] Liverpool John Moores Univ, Sch Nat Sci & Psychol, Liverpool L3 3AF, Merseyside, England.
   [Fruth, Barbara; Tagg, Nikki; Willie, Jacob] Royal Zool Soc Antwerp, Ctr Res & Conservat, B-2018 Antwerp, Belgium.
   [Herbinger, Ilka] WWF Germany, Dept Africa & South Amer, Reinhardtstr 18, D-10117 Berlin, Germany.
   [Langergraber, Kevin E.] Arizona State Univ, Sch Human Evolut & Social Change, 900 Cady Mall, Tempe, AZ 85287 USA.
   [Langergraber, Kevin E.] Arizona State Univ, Inst Human Origins, 900 Cady Mall, Tempe, AZ 85287 USA.
   [Mbende, Menard] WWF Democrat Republ Congo DRC, 14 Ave Sergent Moke, Kinshasa, DEM REP CONGO.
   [N'Goran, Paul Kouame] WWF Reg Off Africa Yaounde Hub, POB 6776, Yaounde, Cameroon.
   [Sommer, Volker] UCL, Dept Anthropol, London WC1H 0BW, England.
   [Wittig, Roman M.] Ctr Suisse Rech Scient, Tai Chimpanzee Project, BP 1301, Abidjan 01, Cote Ivoire.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv Halle Leip, D-04103 Leipzig, Germany.
RP Kalan, AK (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM ammie_kalan@eva.mpg.de
RI Bessone, Mattia/AAL-6716-2021; Kalan, Ammie/M-8027-2019
OI Bessone, Mattia/0000-0002-8066-6413; Kalan, Ammie/0000-0003-1542-7077;
   Bocksberger, Gaelle/0000-0002-3399-0405
FU Max Planck SocietyMax Planck SocietyFoundation CELLEX; Max Planck
   Society Innovation Fund; Heinz L. Krekeler Foundation; Kreditanstalt fur
   Wiederaufbau (KfW Group); WWF Germany
FX This work was funded by the Max Planck Society, Max Planck Society
   Innovation Fund, and Heinz L. Krekeler Foundation, and data related to
   the Salonga National Park in the Democratic Republic of the Congo was
   funded by the Kreditanstalt fur Wiederaufbau (KfW Group) on behalf of
   the German Government and WWF Germany. We sincerely thank Karsten
   Dierks, Henk Eshuis, Veerle Hermans, Sonja Nicholl, Luc Tedonzong, and
   Rodolphe Violleau for assistance in the field; Andrew Dunn, John Hart,
   Martin Ter Heegde, Thurston Cleveland Hicks, and Inaoyom Imong for
   facilitating field work; Chimp&See citizen scientists for help with
   video coding for multiple PanAf sites
   (https://www.chimpandsee.org/#/about/authors); Kristin Havercamp,
   Stefano Lucchesi, Anna Preis, Liran Samuni, and Claudia Wilke for help
   with the identification of individuals; and Vittoria Estienne for her
   participation in the interobserver reliability test. We also thank the
   following wildlife and government authorities for permissions to conduct
   and host research sites in their countries: Ministere de la Recherche
   Scientifique et de l'Innovation, Ministere des Forets et de la Faune in
   Cameroon, Ministere de la Recherche Scientifique, and Ministere des Eaux
   et Forets in Cote d'Ivoire; Institut Congolais pour la Conservation de
   la Nature and Ministere de la Recherche Scientifique in DR Congo; Agence
   Nationale des Parcs Nationaux, Centre National de la Recherche
   Scientifique et Technologique, and Societe Equatoriale d'Exploitation
   Forestiere in Gabon; Ministere de l'Agriculture de l'Elevage et des Eaux
   et Forets in Guinea; Instituto da Biodiversidade e das Areas Protegidas
   in Guinea-Bissau; Forestry Development Authority in Liberia; National
   Park Service and Conservation Society of Mbe Mountains in Nigeria;
   Direction des Eaux, Forets Chasses et de la Conservation des Sols, and
   Reserve Naturelle Communautaire de Dindefelo in Senegal; and Uganda
   National Council for Science and Technology, Uganda Wildlife Authority
   in Uganda. We also thank three anonymous reviewers for their feedback,
   which significantly improved this manuscript.
CR Almeling L, 2016, CURR BIOL, V26, P1744, DOI 10.1016/j.cub.2016.04.066
   Anderson JR, 2017, PRIMATES, V58, P51, DOI 10.1007/s10329-016-0574-7
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   BERLYNE DE, 1966, SCIENCE, V153, P25, DOI 10.1126/science.153.3731.25
   Boesch C, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22613
   Boesch L, 2017, ECOL SOC, V22, DOI 10.5751/ES-09516-220436
   Boinski S., 2000, MOVE WHY ANIMALS TRA
   Bowerman B.L., 2000, LINEAR STAT MODELS A
   Brown GE, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2712
   Byrnes JP, 1999, PSYCHOL BULL, V125, P367, DOI 10.1037/0033-2909.125.3.367
   Damerius LA, 2017, ANIM BEHAV, V134, P57, DOI 10.1016/j.anbehav.2017.10.005
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Dobson A.J., 2008, INTRO GEN LINEAR MOD, Vthird
   Duda P, 2013, J HUM EVOL, V65, P424, DOI 10.1016/j.jhevol.2013.07.009
   Forss SIF, 2015, AM J PRIMATOL, V77, P1109, DOI 10.1002/ajp.22445
   Forstmeier W, 2011, BEHAV ECOL SOCIOBIOL, V65, P47, DOI 10.1007/s00265-010-1038-5
   Fox J, 2001, R COMPANION APPL REG
   Friard O, 2016, METHODS ECOL EVOL, V7, P1325, DOI 10.1111/2041-210X.12584
   Greenberg JR, 2017, ANIM BEHAV, V132, P303, DOI 10.1016/j.anbehav.2017.08.023
   Greenberg R, 2001, CURR ORNITHOL, V16, P119
   Greenberg R., 1990, STUD AVIAN BIOL, V13, P431
   Hare B, 2012, ANIM BEHAV, V83, P573, DOI 10.1016/j.anbehav.2011.12.007
   HAUDE RH, 1976, ANIM LEARN BEHAV, V4, P163, DOI 10.3758/BF03214028
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Heilbronner SR, 2008, BIOL LETTERS, V4, P246, DOI 10.1098/rsbl.2008.0081
   Herrmann E, 2011, DEVELOPMENTAL SCI, V14, P1393, DOI 10.1111/j.1467-7687.2011.01082.x
   Hohmann G., 2006, FEEDING ECOLOGY APES
   Kablan YA, 2019, ORYX, V53, P469, DOI 10.1017/S0030605317001272
   King AJ, 2009, CURR BIOL, V19, pR911, DOI 10.1016/j.cub.2009.07.027
   Kuhl HS, 2016, SCI REP-UK, V6, DOI 10.1038/srep22219
   Kurvers RHJM, 2010, P ROY SOC B-BIOL SCI, V277, P601, DOI 10.1098/rspb.2009.1474
   Kurvers RHJM, 2009, ANIM BEHAV, V78, P447, DOI 10.1016/j.anbehav.2009.06.002
   Laird, 2004, ANAL LONGITUDINAL CL
   Lefebvre L, 2004, BRAIN BEHAV EVOLUT, V63, P233, DOI 10.1159/000076784
   Levin LE, 1996, BEHAV PROCESS, V37, P1, DOI 10.1016/0376-6357(95)00067-4
   Lucas RE, 2011, J PERS SOC PSYCHOL, V101, P847, DOI 10.1037/a0024298
   Biondi LM, 2010, ANIM COGN, V13, P701, DOI 10.1007/s10071-010-0319-8
   Massen JJM, 2013, AM J PRIMATOL, V75, P947, DOI 10.1002/ajp.22159
   McCarthy MS, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22904
   Moretti L, 2015, ANIM BEHAV, V107, P159, DOI 10.1016/j.anbehav.2015.06.008
   PULLIAM HR, 1973, J THEOR BIOL, V38, P419, DOI 10.1016/0022-5193(73)90184-7
   R Development Core Team, 2017, R LANG ENV STAT COMP
   Reale D, 2007, BIOL REV, V82, P291, DOI 10.1111/j.1469-185X.2007.00010.x
   Rosati AG, 2012, ANIM BEHAV, V84, P869, DOI 10.1016/j.anbehav.2012.07.010
   Schielzeth H, 2010, METHODS ECOL EVOL, V1, P103, DOI 10.1111/j.2041-210X.2010.00012.x
   Schielzeth H, 2009, BEHAV ECOL, V20, P416, DOI 10.1093/beheco/arn145
   Sih A, 2004, TRENDS ECOL EVOL, V19, P372, DOI 10.1016/j.tree.2004.04.009
   Surbeck M, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.161081
   Tagg N, 2018, AM J PHYS ANTHROPOL, V166, P510, DOI 10.1002/ajpa.23478
   Tokuyama N, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2277-5
   Visalberghi E, 2003, INT J PRIMATOL, V24, P653, DOI 10.1023/A:1023700800113
   WILSON DS, 1994, TRENDS ECOL EVOL, V9, P442, DOI 10.1016/0169-5347(94)90134-1
   Wright TF, 2010, ETHOL ECOL EVOL, V22, P393, DOI 10.1080/03949370.2010.505580
NR 54
TC 16
Z9 16
U1 1
U2 37
PU CELL PRESS
PI CAMBRIDGE
PA 50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA
SN 0960-9822
EI 1879-0445
J9 CURR BIOL
JI Curr. Biol.
PD APR 1
PY 2019
VL 29
IS 7
BP 1211
EP +
DI 10.1016/j.cub.2019.02.024
PG 10
WC Biochemistry & Molecular Biology; Biology; Cell Biology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Biochemistry & Molecular Biology; Life Sciences & Biomedicine - Other
   Topics; Cell Biology
GA HR1YD
UT WOS:000462931400030
PM 30880013
OA Green Accepted, Green Submitted, Bronze
DA 2022-02-10
ER

PT J
AU Burns, PA
   Parrott, ML
   Rowe, KC
   Phillips, BL
AF Burns, Phoebe A.
   Parrott, Marissa L.
   Rowe, Kevin C.
   Phillips, Benjamin L.
TI Identification of threatened rodent species using infrared and
   white-flash camera traps
SO AUSTRALIAN MAMMALOGY
LA English
DT Article
DE Muridae; Pseudomys fumeus; P. novaehollandiae; sensitivity; small
   mammals; specificity
ID ANURAN CALL SURVEYS; MANAGEMENT; AUSTRALIA; IMPACTS; MAMMALS; ERROR;
   RATES; BIAS
AB Camera trapping has evolved into an efficient technique for gathering presence/absence data for many species; however, smaller mammals such as rodents are often difficult to identify in images. Identification is inhibited by co-occurrence with similar-sized small mammal species and by camera set-ups that do not provide adequate image quality. Here we describe survey procedures for identification of two small, threatened rodent species - smoky mouse (Pseudomys fumeus) and New Holland mouse (P. novaehollandiae) - using white-flash and infrared camera traps. We tested whether observers could accurately identify each species and whether experience with small mammals influenced accuracy. Pseudomys fumeus was 20 times less likely to be misidentified on white-flash images than infrared, and observer experience affected accuracy only for infrared images, where it accounted for all observer variance. Misidentifications of P. novaehollandiae were more common across both flash types: false positives (>0.21) were more common than false negatives (<0.09), and experience accounted for only 31% of variance in observer accuracy. For this species, accurate identification appears to be, in part, an innate skill. Nonetheless, using an appropriate setup, camera trapping clearly has potential to provide broad-scale occurrence data for these and other small mammal species.
C1 [Burns, Phoebe A.; Phillips, Benjamin L.] Univ Melbourne, Sch BioSci, Parkville, Vic 3010, Australia.
   [Burns, Phoebe A.; Rowe, Kevin C.] Museum Victoria, Sci Dept, GPO Box 666, Melbourne, Vic 3001, Australia.
   [Parrott, Marissa L.] Zoos Victoria, Wildlife & Conservat Sci, Elliott Ave, Parkville, Vic 3010, Australia.
RP Burns, PA (corresponding author), Univ Melbourne, Sch BioSci, Parkville, Vic 3010, Australia.; Burns, PA (corresponding author), Museum Victoria, Sci Dept, GPO Box 666, Melbourne, Vic 3001, Australia.
EM pburns@museum.vic.gov.au
RI Burns, Phoebe A./U-6746-2019
OI Burns, Phoebe A./0000-0003-1015-3775
FU Australian Postgraduate AwardAustralian Government; Zoos Victoria; Parks
   Victoria; Holsworth Wildlife Research Endowment; Royal Zoological
   Society of NSW Ethel Mary Read Research Fund; Linnaean Society of NSW
   Joyce W. Vickery Scientific Research Fund; Field Naturalists Victorian
   Environment Fund
FX Surveys were conducted under DELWP Research Permits nos 10007493,
   10007606, 10007934 and approval from the Zoos Victoria and Museum
   Victoria Animal Ethics Committees. This research was conducted with
   support awarded to PAB from an Australian Postgraduate Award, Zoos
   Victoria, Parks Victoria, the Holsworth Wildlife Research Endowment, the
   Royal Zoological Society of NSW Ethel Mary Read Research Fund, the
   Linnaean Society of NSW Joyce W. Vickery Scientific Research Fund, and
   the Field Naturalists Victorian Environment Fund. Thank you to Zoos
   Victoria, Wildlife Unlimited Pty Ltd, and the Department of Environment,
   Land, Water and Planning (Victoria) for lending cameras and bait
   stations. Thank you to the observers who participated in the trials and
   to two anonymous reviewers for helpful comments and suggestions on the
   manuscript.
CR Ballard G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P189
   Burns PA, 2015, WILDLIFE RES, V42, P668, DOI 10.1071/WR15096
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chades I, 2008, P NATL ACAD SCI USA, V105, P13936, DOI 10.1073/pnas.0805265105
   Clemann Nick, 2014, Memoirs of Museum Victoria, V72, P141
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Diefenbach DR, 2003, AUK, V120, P1168, DOI 10.1642/0004-8038(2003)120[1168:VIGBCR]2.0.CO;2
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Fancourt BA, 2018, AUST MAMMAL, V40, P118, DOI 10.1071/AM17004
   Farmer RG, 2012, AUK, V129, P76, DOI 10.1525/auk.2012.11129
   Field SA, 2004, ECOL LETT, V7, P669, DOI 10.1111/j.1461-0248.2004.00625.x
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Gu WD, 2004, BIOL CONSERV, V116, P195, DOI 10.1016/S0006-3207(03)00190-3
   Harley DKP, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P233
   Hollis G. J., 1999, PRE POSTFIRE TRAPPIN
   Holmes B, 2012, DETECTION DISTRIBUTI
   Lock M, 2017, AUST J ZOOL, V65, P60, DOI 10.1071/ZO16084
   Lotz A, 2007, J WILDLIFE MANAGE, V71, P675, DOI 10.2193/2005-759
   Manzo E, 2012, ACTA THERIOL, V57, P165, DOI 10.1007/s13364-011-0055-8
   McCall C., 2015, ASSESSMENT STATUS NE
   Mcclintock BT, 2010, J WILDLIFE MANAGE, V74, P1882, DOI 10.2193/2009-321
   McDonald PJ, 2015, BIOL CONSERV, V191, P93, DOI 10.1016/j.biocon.2015.06.027
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Meek PD, 2016, AUST MAMMAL, V38, P44, DOI 10.1071/AM15016
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Meek PD, 2015, AUST MAMMAL, V37, P1, DOI 10.1071/AM14021
   Miller DAW, 2012, ECOL APPL, V22, P1665
   Nelson J., 2009, STATUS SMOKY MOUSE P
   Nelson J. L., 2010, STATUS SMOKY MOUSE P
   Oliveira-Santos LGR, 2010, MAMM BIOL, V75, P375, DOI 10.1016/j.mambio.2009.08.005
   Paull DJ, 2012, WILDLIFE RES, V39, P546, DOI 10.1071/WR12034
   Quin Bruce R., 1996, Victorian Naturalist (Blackburn), V113, P236
   Quin Bruce R., 1996, Victorian Naturalist (Blackburn), V113, P281
   R Core Team, 2017, R LANG ENV STAT COMP
   Roberts Nathan James, 2011, Bioscience Horizons, V4, P40, DOI 10.1093/biohorizons/hzr006
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Russell R, 2009, PSYCHON B REV, V16, P252, DOI 10.3758/PBR.16.2.252
   Swan M, 2014, BIODIVERS CONSERV, V23, P343, DOI 10.1007/s10531-013-0604-3
   TAYLOR BL, 1993, CONSERV BIOL, V7, P489, DOI 10.1046/j.1523-1739.1993.07030489.x
   Taylor BD, 2014, AUST MAMMAL, V36, P60, DOI 10.1071/AM13012
   Tyre AJ, 2003, ECOL APPL, V13, P1790, DOI 10.1890/02-5078
   Villette P, 2016, J MAMMAL, V97, P32, DOI 10.1093/jmammal/gyv150
NR 43
TC 11
Z9 12
U1 1
U2 22
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 0310-0049
EI 1836-7402
J9 AUST MAMMAL
JI Aust. Mammal.
PY 2018
VL 40
IS 2
BP 188
EP 197
DI 10.1071/AM17016
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA GO1FC
UT WOS:000439693400008
DA 2022-02-10
ER

PT J
AU Rowcliffe, JM
   Jansen, PA
   Kays, R
   Kranstauber, B
   Carbone, C
AF Rowcliffe, J. Marcus
   Jansen, Patrick A.
   Kays, Roland
   Kranstauber, Bart
   Carbone, Chris
TI Wildlife speed cameras: measuring animal travel speed and day range
   using camera traps
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article
DE Animal tracking; image analysis; length-biased distributions; movement
   ecology; travel distance; video capture
ID ENERGY-COST; HOME-RANGE; BODY-SIZE; MOVEMENT; DENSITY; DIET; MASS
AB Travel speed (average speed of travel while active) and day range (average speed over the daily activity cycle) are behavioural metrics that influence processes including energy use, foraging success, disease transmission and human-wildlife interactions, and which can therefore be applied to a range of questions in ecology and conservation. These metrics are usually derived from telemetry or direct observations. Here, we describe and validate an entirely new alternative approach, using camera traps recording passing animals to measure movement paths at very fine scale. Dividing the length of a passage by its duration gives a speed observation, and average travel speed is estimated by fitting size-biased probability distributions to a sample of speed observations. Day range is then estimated as the product of travel speed and activity level (proportion of time spent active), which can also be estimated from camera-trap data. We field tested the procedure with data from a survey of terrestrial mammals on Barro Colorado Island, Panama. Travel speeds and day ranges estimated for 12 species scaled positively with body mass, and were higher in faunivores than in herbivores, patterns that are consistent with those obtained using independent estimates derived from tracked individuals. Comparisons of our day range estimates with independent telemetry-based estimates for three species also showed very similar values in absolute terms. We conclude that these methods are accurate and ready to use for estimating travel speed and day range in wildlife. Key advantages of the methods are that they are non-invasive, and that measurements are made at very high resolution in time and space, yielding estimates that are comparable across species and studies. Combined with emerging techniques in computer vision, we anticipate that these methods will help to expand the range of species for which we can estimate movement rate in the wild.
C1 [Rowcliffe, J. Marcus; Carbone, Chris] ZSL Inst Zool, Regents Pk, London NW1 4RY, England.
   [Jansen, Patrick A.; Kays, Roland] Smithsonian Trop Res Inst, Ctr Trop Forest Sci, Panama City, Panama.
   [Jansen, Patrick A.] Wageningen Univ, Dept Environm Sci, Wageningen, Netherlands.
   [Kays, Roland] North Carolina State Univ, Raleigh, NC 27695 USA.
   [Kays, Roland] Museum Nat Sci, Raleigh, NC USA.
   [Kranstauber, Bart] Max Planck Inst Ornithol, Dept Migrat & Immunoecol, Radolfzell am Bodensee, Germany.
   [Kranstauber, Bart] Univ Konstanz, Dept Biol, Constance, Germany.
RP Rowcliffe, JM (corresponding author), ZSL Inst Zool, Regents Pk, London NW1 4RY, England.
EM marcus.rowcliffe@ioz.ac.uk
RI Rowcliffe, Marcus/G-3713-2018; Jansen, Patrick/G-2545-2015
OI Rowcliffe, Marcus/0000-0002-4286-6887; Jansen,
   Patrick/0000-0002-4660-0314; Kays, Roland/0000-0002-2947-6665
FU National Science FoundationNational Science Foundation (NSF) [NSF-DEB
   0717071]; British Ecological Society; Netherlands Organisation for
   Scientific ResearchNetherlands Organization for Scientific Research
   (NWO) [NWO-ALW863-07-008]; Direct For Biological SciencesNational
   Science Foundation (NSF)NSF - Directorate for Biological Sciences (BIO)
   [1232442] Funding Source: National Science Foundation; Division of
   Computing and Communication FoundationsNational Science Foundation
   (NSF)NSF - Directorate for Computer & Information Science & Engineering
   (CISE) [1539622] Funding Source: National Science Foundation
FX The work was funded by the National Science Foundation (NSF-DEB
   0717071), the British Ecological Society, and the Netherlands
   Organisation for Scientific Research (NWO-ALW863-07-008).
CR Aliaga-Rossel E, 2008, J TROP ECOL, V24, P367, DOI 10.1017/S0266467408005129
   Anderson K. A., 2002, MODEL SELECTION MULT
   Bolker, 2008, ECOLOGICAL MODELS DA
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Carbone C, 2005, AM NAT, V165, P290, DOI 10.1086/426790
   Carbone C, 2014, ECOL LETT, V17, P1553, DOI 10.1111/ele.12375
   Core Team R, 2014, R LANG ENV STAT COMP
   Cross PC, 2005, ECOL LETT, V8, P587, DOI 10.1111/j.1461-0248.2005.00760.x
   Davis P. J., 1972, HDB MATH FUNCTIONS, P253
   Emmons L.H., 1990, NEOTROPICAL RAINFORE
   EMMONS LH, 1988, REV ECOL-TERRE VIE, V43, P133
   GALDIKAS BMF, 1988, INT J PRIMATOL, V9, P1, DOI 10.1007/BF02740195
   GARLAND T, 1983, J ZOOL, V199, P157, DOI 10.1111/j.1469-7998.1983.tb02087.x
   GARLAND T, 1983, AM NAT, V121, P571, DOI 10.1086/284084
   GOODMAN LA, 1960, J AM STAT ASSOC, V55, P708, DOI 10.2307/2281592
   Graham MD, 2009, ANIM CONSERV, V12, P445, DOI 10.1111/j.1469-1795.2009.00272.x
   HEGLUND NC, 1988, J EXP BIOL, V138, P301
   Hutchinson JMC, 2007, BIOL REV, V82, P335, DOI 10.1111/j.1469-185X.2007.00014.x
   Isaac NJB, 2013, GLOBAL ECOL BIOGEOGR, V22, P1, DOI 10.1111/j.1466-8238.2012.00782.x
   Jansen PA, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P263
   Jetz W, 2004, SCIENCE, V306, P266, DOI 10.1126/science.1102138
   Kays Roland, 2011, International Journal of Research and Reviews in Wireless Sensor Networks, V1, P19
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   Leigh E.G., 1999, TROPICAL FOREST ECOL
   MCMAHON TA, 1975, J APPL PHYSIOL, V39, P619, DOI 10.1152/jappl.1975.39.4.619
   Meek PD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110832
   Miller CS, 2014, BIOL CONSERV, V170, P120, DOI 10.1016/j.biocon.2013.12.012
   Pedersen MW, 2013, METHODS ECOL EVOL, V4, P920, DOI 10.1111/2041-210X.12086
   Piegorsch, 2002, ENCY ENV
   PYKE GH, 1981, AM NAT, V118, P475, DOI 10.1086/283842
   Reid, 1997, FIELD GUIDE MAMMALS
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2013, J WILDLIFE MANAGE, V77, P876, DOI 10.1002/jwmg.533
   Rowcliffe JM, 2012, METHODS ECOL EVOL, V3, P653, DOI 10.1111/j.2041-210X.2012.00197.x
   SCHMIDTNIELSEN K, 1972, SCIENCE, V177, P222, DOI 10.1126/science.177.4045.222
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   SIGG H, 1981, FOLIA PRIMATOL, V36, P40, DOI 10.1159/000156008
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Turchin Peter, 1998
   Vaughan CS, 1999, REV BIOL TROP, V47, P263
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   WERNER EE, 1993, AM NAT, V142, P242, DOI 10.1086/285537
   Woodroffe R, 1998, SCIENCE, V280, P2126, DOI 10.1126/science.280.5372.2126
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 47
TC 35
Z9 38
U1 4
U2 11
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
PD JUN
PY 2016
VL 2
IS 2
BP 84
EP 94
DI 10.1002/rse2.17
PG 11
WC Ecology; Remote Sensing
WE Emerging Sources Citation Index (ESCI)
SC Environmental Sciences & Ecology; Remote Sensing
GA VG7IN
UT WOS:000448241100002
OA gold, Green Published, Green Accepted
DA 2022-02-10
ER

PT J
AU Avrin, AC
   Pekins, CE
   Sperry, JH
   Allen, ML
AF Avrin, Alexandra C.
   Pekins, Charles E.
   Sperry, Jinelle H.
   Allen, Maximilian L.
TI Evaluating the efficacy and decay of lures for improving carnivore
   detections with camera traps
SO ECOSPHERE
LA English
DT Article
DE attractant; bait; detection; fatty acid tablets; mesocarnivore;
   predator; sardines
ID CENTRAL NEW-MEXICO; OCCUPANCY; CAPTURE; RATES; BAIT; ATTRACTANTS;
   MOUNTAINS; DENSITY; MODELS; SCENT
AB Abundance and occupancy estimates are essential to wildlife research, but are often hampered by limited detections, especially for cryptic species like carnivores. While scientists can account for limited detections during statistical analyses, increasing detections in the field is the best way to reduce uncertainty. Camera traps are an effective, noninvasive method of monitoring wildlife, and using attractants with camera traps can increase the likelihood of detecting carnivores. We tested two scent lures (sardines and fatty acid tablets) against a control of no lure to determine whether either lure increased detections of six carnivore species, bobcat (Lynx rufus), coyote (Canis latrans), gray fox (Urocyon cinereoargenteus), raccoon (Procyon lotor), striped skunk (Memphitis memphitis), and ringtail (Bassariscus astutus). We also examined how detection of carnivores was affected as the lure decayed over time. We used occupancy modeling for each species to determine whether either lure increased detection probability. We then modeled how lure decay affected carnivore detections and determined the optimal length of deployment using generalized linear mixed models. Sardines increased detections across all carnivores, but also had a high rate of decay and were no different than the control at day 18. Fatty acid tablets decayed more slowly, but were not significantly different from the control at any point. Among species, detections of gray foxes and raccoons increased with both sardines and fatty acid tablets, while detections of ringtails increased only with sardines, and other species did not respond significantly to either lure. Our analysis shows that lures can increase detections of carnivores, but species-specific responses and study objectives must be considered when choosing a lure. These results will allow future researchers to improve the accuracy of abundance and occupancy estimates through increased detections of difficult to study species which ultimately leads to better conservation and management of those species.
C1 [Avrin, Alexandra C.; Sperry, Jinelle H.; Allen, Maximilian L.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin, Urbana, IL 61801 USA.
   [Pekins, Charles E.] US Army Garrison, Fort Hood Nat Resources Management Branch, Bldg 1939 Rod & Gun Club Loop, Ft Hood, TX 76544 USA.
   [Sperry, Jinelle H.] US Army Corps Engineers, Engn Res & Dev Ctr, 2902 Newmark Dr, Champaign, IL 61822 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
RP Avrin, AC (corresponding author), Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin, Urbana, IL 61801 USA.
EM alex.avrin@gmail.com
OI Avrin, Alexandra/0000-0003-4037-1685
CR Allen ML, 2020, BIODIVERS CONSERV, V29, P3591, DOI 10.1007/s10531-020-02039-w
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Bahaa-el-din L, 2015, MAMMAL REV, V45, P63, DOI 10.1111/mam.12033
   Bender LC, 2017, MAMMAL RES, V62, P323, DOI 10.1007/s13364-017-0318-0
   Brooks ME, 2017, R J, V9, P378, DOI 10.32614/RJ-2017-066
   Colyn RB, 2019, BIRD CONSERV INT, V29, P463, DOI 10.1017/S0959270918000400
   Cove M, 2014, HYSTRIX, V25, P113, DOI 10.4404/hystrix-25.2-9945
   Cove MV, 2012, AM MIDL NAT, V168, P456, DOI 10.1674/0003-0031-168.2.456
   Eckrich GH, 1999, STUD AVIAN BIOL, P267
   Edwards Cody W., 1998, Occasional Papers Museum of Texas Tech University, V185, P1
   Evans BE, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0217543
   Ferreira-Rodriguez N, 2019, MAMMAL RES, V64, P155, DOI 10.1007/s13364-018-00414-1
   Findlay MA, 2020, MAMMAL RES, V65, P167, DOI 10.1007/s13364-020-00478-y
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Gerber BD, 2012, POPUL ECOL, V54, P43, DOI 10.1007/s10144-011-0276-3
   Gese EM, 2001, CONSERV BIOL SER, V5, P372
   Gilbert A, 2018, J WILDLIFE DIS, V54, P122, DOI 10.7589/2017-04-073
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Hackett HM, 2007, AM MIDL NAT, V158, P123, DOI 10.1674/0003-0031(2007)158[123:DROESS]2.0.CO;2
   Haidir IA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202876
   Harrison RL, 2013, WEST N AM NATURALIST, V73, P365, DOI 10.3398/064.073.0313
   Hayden TJ, 2000, ECOLOGY AND MANAGEMENT OF COWBIRDS AND THEIR HOSTS, P357
   Hearn AJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151046
   Heinlein BW, 2020, WILDLIFE RES, V47, P338, DOI 10.1071/WR19117
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Kilshaw K, 2015, ORYX, V49, P207, DOI 10.1017/S0030605313001154
   Larson RN, 2015, WEST N AM NATURALIST, V75, P339, DOI 10.3398/064.075.0311
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Melo GL, 2012, IHERINGIA SER ZOOL, V102, P88, DOI 10.1590/S0073-47212012000100012
   Mills D, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0216447
   O'Connor KM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175684
   Osorio-Olvera L, 2019, ECOGRAPHY, V42, P1415, DOI 10.1111/ecog.04442
   R Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
   Rocha DG, 2016, J ZOOL, V300, P205, DOI 10.1111/jzo.12372
   ROUGHTON RD, 1982, J WILDLIFE MANAGE, V46, P217, DOI 10.2307/3808424
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x
   Russel L., 2020, EMMEANS ESTIMATED MA
   Schlexer Fredrick V., 2008, P263
   Sebastian-Gonzalez E., 2019, GLOBAL CHANGE BIOL, V25, P3005, DOI [10.1111/gcb.14708, DOI 10.1111/gcb.14708]
   Sebastian-Gonzalez E, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-01439-1
   Stewart FEC, 2019, J WILDLIFE MANAGE, V83, P985, DOI 10.1002/jwmg.21657
   Suarez-Tangil BD, 2017, EUR J WILDLIFE RES, V63, DOI 10.1007/s10344-017-1150-1
   Tanner D, 2012, WILDLIFE SOC B, V36, P594, DOI 10.1002/wsb.160
   Thorn M, 2009, S AFR J WILDL RES, V39, P1, DOI 10.3957/056.039.0101
   Thornton DH, 2015, WILDLIFE RES, V42, P394, DOI 10.1071/WR15092
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   White GC, 2005, WILDLIFE RES, V32, P211, DOI 10.1071/WR03123
   Zielinski WJ, 2006, WILDLIFE SOC B, V34, P1152, DOI 10.2193/0091-7648(2006)34[1152:TEOWAG]2.0.CO;2
NR 51
TC 0
Z9 0
U1 7
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2150-8925
J9 ECOSPHERE
JI Ecosphere
PD AUG
PY 2021
VL 12
IS 8
AR e03710
DI 10.1002/ecs2.3710
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA UG7VB
UT WOS:000689453800017
OA gold
DA 2022-02-10
ER

PT C
AU Hines, G
   Swanson, A
   Kosmala, M
   Lintott, C
AF Hines, Greg
   Swanson, Alexandra
   Kosmala, Margaret
   Lintott, Chris
GP AAAI
TI Aggregating User Input in Ecology Citizen Science Projects
SO PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL
   INTELLIGENCE
LA English
DT Proceedings Paper
CT 29th Association-for-the-Advancement-of-Artificial-Intelligence (AAAI)
   Conference on Artificial Intelligence
CY JAN 25-30, 2015
CL Austin, TX
SP Assoc Advancement Artificial Intelligence
AB Camera traps (remote, automatic cameras) are revolutionizing large-scale studies in ecology. The Serengeti Lion Project has used camera traps to produce over 1.5 million pictures of animals in the Serengeti. To analyze these pictures, the Project created Snapshot Serengeti, a citizen science website where volunteers can help classify animals. To increase accuracy, each photo is shown to multiple users and a critical step is aggregating individual classifications. In this paper, we present a new aggregation algorithm which achieves an accuracy of 98.6%, better than many human experts. Our algorithm also requires fewer users per photo than existing methods. The algorithm is intuitive and designed so that non experts can understand the end results.
C1 [Hines, Greg; Swanson, Alexandra; Lintott, Chris] Univ Oxford, Dept Phys, Citizen Sci Grp, Oxford, England.
   [Kosmala, Margaret] Harvard Univ, Dept Organism & Evolutionary Biol, Cambridge, MA 02138 USA.
RP Hines, G (corresponding author), Univ Oxford, Dept Phys, Citizen Sci Grp, Oxford, England.
EM greg@zooniverse.org; ali@zooniverse.org; kosmala@fas.harvard.edu;
   cjl@astro.ox.ac.uk
FU MICO
FX These authors are part of the Zooniverse project, funded in part by
   MICO.
CR Dalvi N., 2013, P 22 INT C WORLD WID
   Dawid A. P, 1977, J ROYAL STAT SOC C
   Karger D. R., 2011, ADV NEURAL INFORM PR
   Kim H.-C., 2012, P 15 INT C ART INT S
   Kosmala M., 2013, SUMMARY EXPERTS
   Littlestone N, 1994, INFORM COMPUTATION
   Liu Q., 2012, NIPS, V25, P692
   Simpson E., 2013, DECISION MAKING IMPE, P1, DOI [DOI 10.1007/978-3-642-36406-8_1, DOI 10.1007/978-3-642-36406-8]
   Swanson A, 2014, ECOLOGY EVOLUTION
   Venanzi M., 2014, P 23 INT C WORLD WID
NR 10
TC 8
Z9 8
U1 0
U2 2
PU ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI PALO ALTO
PA 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
PY 2015
BP 3975
EP 3980
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BN6JS
UT WOS:000485625504003
DA 2022-02-10
ER

PT J
AU Bohm, T
   Hofer, H
AF Bohm, Torsten
   Hofer, Heribert
TI Population numbers, density and activity patterns of servals in savannah
   patches of Odzala-Kokoua National Park, Republic of Congo
SO AFRICAN JOURNAL OF ECOLOGY
LA English
DT Article
DE camera-trapping; Central Africa; Crocuta crocuta; Leptailurus serval;
   spatial capture-recapture
ID CAMERA-TRAP; FOREST; CONSEQUENCES; CARNIVORES; LANDSCAPE; ABUNDANCE;
   SELECTION; WILDLIFE; ECOLOGY; HABITAT
AB Despite its wide distribution in continental Africa, the serval (Leptailurus serval Schreber) has received relatively little scientific attention so far. We did camera-trapping in the forest-savannah mosaic of the Odzala-Kokoua National Park, Republic of Congo. The park's savannahs represent the northernmost extension of the savannahs of the Bateke Plateaux, a large ecoregion of open habitat in Central Africa. During 8 months of camera-trapping, we recorded 51 individuals. Almost two-thirds of individuals recorded belonged to the servaline morph, with a pattern mutation of small "freckled" spots. Using maximum likelihood (ML) and Bayesian spatially explicit capture-recapture methods serval density was 7.7-9.8 individuals/100 km(2). ML analyses favoured a model with trap placement and gender as covariates. Serval males were largely nocturnal whereas females were mainly diurnal. Differences in activity patterns were likely related to the occurrence of spotted hyaenas (Crocuta crocuta Erxleben). Spotted hyaenas were highly nocturnal and, consequently, had a higher overlap in activity patterns with male servals. Our study provided the first robust density estimates for this medium-sized carnivore in Central Africa. To achieve sufficient precision in density estimates, we recommend that future studies also include individual and trap placement covariates in analyses.
C1 [Bohm, Torsten; Hofer, Heribert] Leibniz Inst Zoo & Wildlife Res, Dept Evolutionary Ecol, Berlin, Germany.
   [Bohm, Torsten] African Pk, Brazzaville, Rep Congo.
RP Bohm, T (corresponding author), Leibniz Inst Zoo & Wildlife Res, Dept Evolutionary Ecol, Berlin, Germany.; Bohm, T (corresponding author), African Pk, Brazzaville, Rep Congo.
EM torstenb@african-parks.org
RI Hofer, Heribert/AAF-7854-2021
OI Hofer, Heribert/0000-0002-2813-7442; Bohm, Torsten/0000-0001-5446-2321
FU SAVE - Wildlife Conservation Fund
FX We thank the Direction Generale de l'Economie Forestiere and Direction
   de la Faune et des Aires Protegees for the permit to conduct this study
   in the OKNP, L. Lamprecht and D. Zeller, directors of the OKNP, for
   their support during fieldwork. We especially thank our Congolese
   research assistants for their assistance in the field and the staff of
   African Parks, in particular, G. LeFlohic and G.-A. Malanda for their
   support in Congo. We are in particular grateful to the SAVE - Wildlife
   Conservation Fund and F. Weiss for providing financial support. The
   authors further thank Dr. T. Breuer and Dr. R. Sollmann for their
   reviews of an earlier version of this paper.
CR Abernethy KA, 2013, PHILOS T R SOC B, V368, DOI 10.1098/rstb.2012.0303
   Allen J. A., 1924, Bulletin of the American Museum of Natural History, V47, P73
   Anderson K. A., 2002, MODEL SELECTION MULT
   Blake S, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003546
   Bohm T., 2014, SPOTTED HYENA UNPUB
   Bongui A., 2015, AIRES PROTEGEES AFRI, P89
   Bout N., 2006, PARC NATL PLATEAUX B
   Bout N, 2011, AFR J ECOL, V49, P127, DOI 10.1111/j.1365-2028.2010.01240.x
   Breuer T., 2015, STUDYING FOREST ELEP, P14
   Breuer T, 2016, CONSERV BIOL, V30, P1019, DOI 10.1111/cobi.12679
   Di Silvestre I, 2000, AFR J ECOL, V38, P102
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   Dowsett-Lemaire F., 1996, Bulletin du Jardin Botanique National de Belgique, V65, P253, DOI 10.2307/3668453
   Gray TNE, 2012, J WILDLIFE MANAGE, V76, P163, DOI 10.1002/jwmg.230
   Efford M.G., 2018, secr: spatially explicit capture-recapture models. R package version 3.1.6
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   GEERTSEMA AA, 1985, NETH J ZOOL, V35, P527
   Gopalaswamy AM, 2012, METHODS ECOL EVOL, V3, P1067, DOI 10.1111/j.2041-210X.2012.00241.x
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Hecketsweiler P., 1991, PARC NATL ODZALA CON
   Henschel P, 2011, J ZOOL, V285, P11, DOI 10.1111/j.1469-7998.2011.00826.x
   Henschel P, 2014, J MAMMAL, V95, P882, DOI 10.1644/13-MAMM-A-306
   Henschel Philipp, 2009, P206, DOI 10.1002/9781444312034.ch10
   Honer OP, 2005, OIKOS, V108, P544, DOI 10.1111/j.0030-1299.2005.13533.x
   Karanth KU, 1998, ECOLOGY, V79, P2852
   Kingdon J., 2004, KINGDON POCKET GUIDE
   Laurance WF, 2015, CURR BIOL, V25, P3202, DOI 10.1016/j.cub.2015.10.046
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Linkie M, 2008, BIOL CONSERV, V141, P2410, DOI 10.1016/j.biocon.2008.07.002
   Maisels F, 1996, SYNTHESIS INFORM PAR
   Maisels F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059469
   Malbrant R., 1949, FAUNE EQUATEUR AFRIC, VII
   Mathot L., 2006, RAPPORT ANN MONITORI
   Mills MGL, 1990, KALAHARI HYAENAS COM, DOI [10.1007/978-94-015-1101-8, DOI 10.1007/978-94-015-1101-8]
   Nasi R, 2011, INT FOREST REV, V13, P355, DOI 10.1505/146554811798293872
   Nowell K., 1996, WILD CATS STATUS SUR
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Olson DM, 2001, BIOSCIENCE, V51, P933, DOI 10.1641/0006-3568(2001)051[0933:TEOTWA]2.0.CO;2
   Pettorelli N, 2010, ANIM CONSERV, V13, P131, DOI 10.1111/j.1469-1795.2009.00309.x
   Pocock RI, 1907, P ZOOL SOC LOND, V1907, P656
   R Core Team, 2015, R LANG ENV STAT COMP
   Ramesh T, 2015, ECOL INDIC, V52, P8, DOI 10.1016/j.ecolind.2014.11.021
   Ramesh T, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2271-y
   Ramesh T, 2016, J MAMMAL, V97, P554, DOI 10.1093/jmammal/gyv201
   Ramesh T, 2016, FOREST ECOL MANAG, V360, P20, DOI 10.1016/j.foreco.2015.10.005
   Ramesh T, 2015, MAMMALIA, V79, P399, DOI 10.1515/mammalia-2014-0053
   Ramesh T, 2013, J MAMMAL, V94, P1460, DOI 10.1644/13-MAMM-A-063.1
   Ray JC, 1997, AFR J ECOL, V35, P237, DOI 10.1111/j.1365-2028.1997.086-89086.x
   Ray JC, 2001, OECOLOGIA, V127, P395, DOI 10.1007/s004420000604
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Simcharoen S, 2008, BIOL CONSERV, V141, P2242, DOI 10.1016/j.biocon.2008.06.015
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Stokes EJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010294
   Thiel C., 2011, THESIS
   Thiel C., 2015, IUCN RED LIST THREAT, DOI [10. 2305/iucn. uk. 2015-2. rlts. t11638a50654625. en, DOI 10.2305/IUCN.UK.2015-2.RLTS.T11638A50654625.EN]
   Treves A, 2010, BIOL CONSERV, V143, P521, DOI 10.1016/j.biocon.2009.11.025
   White G., 1982, CAPTURE RECAPTURE RE
NR 59
TC 7
Z9 7
U1 2
U2 13
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0141-6707
EI 1365-2028
J9 AFR J ECOL
JI Afr. J. Ecol.
PD DEC
PY 2018
VL 56
IS 4
SI SI
BP 841
EP 849
DI 10.1111/aje.12520
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HC1PV
UT WOS:000451574200017
DA 2022-02-10
ER

PT J
AU Jackson, K
   Wilmers, CC
   Wittmer, HU
   Allen, ML
AF Jackson, Kathrina
   Wilmers, Christopher C.
   Wittmer, Heiko U.
   Allen, Maximilian L.
TI First documentation of scent-marking behaviors in striped skunks
   (Mephitis mephitis)
SO MAMMAL RESEARCH
LA English
DT Article
DE Camera trap; Chemical communication; Communication; Mephitis mephitis;
   Novel behaviors; Scent marking; Striped skunk
AB Communication behaviors play a critical role in both an individual's fitness as well as the viability of populations. Solitary animals use chemical communication (i.e., scent marking) to locate mates and defend their territory to increase their own fitness. Previous research has suggested that striped skunks (Mephitis mephitis) do not perform scent-marking behaviors, despite being best known for using odor as chemical defense. We used video camera traps to document behaviors exhibited by striped skunks at a remote site in coastal California between January 2012 and April 2015. Our camera traps captured a total of 71 visits by striped skunks, the majority of which (73%) included a striped skunk exhibiting scent-marking behaviors. Overall, we documented 8 different scent-marking behaviors. The most frequent behaviors we documented were cheek rubbing (45.1%), investigating (40.8%), and claw marking (35.2%). The behaviors exhibited for the longest durations on average were grooming (x = 34. 4 s) and investigating (x = 21.2 s). Although previous research suggested that striped skunks do not scent mark, we documented that at least some populations do and our findings suggest that certain sites are used for communication via scent marking. Our study further highlights how camera traps allow researchers to discover previously undocumented animal behaviors.
C1 [Jackson, Kathrina] Univ Illinois, Dept Anim Sci, 1816 S Oak St, Champaign, IL 61820 USA.
   [Wilmers, Christopher C.] Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Environm Studies Dept, Santa Cruz, CA 95064 USA.
   [Wittmer, Heiko U.] Victoria Univ Wellington, Sch Biol Sci, Box 600,PO 6140, Wellington, New Zealand.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
RP Jackson, K (corresponding author), Univ Illinois, Dept Anim Sci, 1816 S Oak St, Champaign, IL 61820 USA.
EM kj11@illinois.edu
RI Wittmer, Heiko U/D-4172-2015
OI Wittmer, Heiko U/0000-0002-8861-188X
FU NSFNational Science Foundation (NSF) [0963022, 1255913]; Gordon and
   Betty Moore FoundationGordon and Betty Moore Foundation; University of
   California at Santa Cruz; Illinois Natural History Survey; University of
   Illinois Champaign-Urbana
FX Funding was provided by NSF Grants 0963022 and 1255913, the Gordon and
   Betty Moore Foundation, the University of California at Santa Cruz, the
   Illinois Natural History Survey, and the University of Illinois
   Champaign-Urbana.
CR Allen ML, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2366-5
   Allen ML, 2017, J ETHOL, V35, P13, DOI 10.1007/s10164-016-0492-6
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep27257
   Allen ML, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139087
   Allen ML, 2015, J ETHOL, V33, P9, DOI 10.1007/s10164-014-0418-0
   Allen ML, 2014, BEHAVIOUR, V151, P819, DOI 10.1163/1568539X-00003173
   BAILEY TN, 1974, J WILDLIFE MANAGE, V38, P435, DOI 10.2307/3800874
   Baldwin RA, 2015, UC ANR PUBLICATION, V74118
   Fisher KA, 2018, ANIM BEHAV, V143, P25, DOI 10.1016/j.anbehav.2018.06.023
   Gosling LM, 2001, ADV STUD BEHAV, V30, P169, DOI 10.1016/S0065-3454(01)80007-3
   Krofel M, 2017, MAMM BIOL, V87, P36, DOI 10.1016/j.mambio.2017.05.003
   Lariviere S, 1998, J APPL ECOL, V35, P207, DOI 10.1046/j.1365-2664.1998.00301.x
   Lariviere S, 1998, J WILDLIFE MANAGE, V62, P199, DOI 10.2307/3802279
   Lariviere S, 1996, ETHOLOGY, V102, P986
   MELLEN JD, 1993, AM ZOOL, V33, P151
   NAMS VO, 1991, BEHAVIOUR, V119, P267, DOI 10.1163/156853991X00472
   R Core Team, 2017, R LANG ENV STAT COMP
   RALLS K, 1971, SCIENCE, V171, P443, DOI 10.1126/science.171.3970.443
   Russell AF, 2003, BEHAV ECOL, V14, P486, DOI 10.1093/beheco/arg022
   SMITH JLD, 1989, ANIM BEHAV, V37, P1
   Sokal R.R., 2012, BIOMETRY PRINCIPLES, V4
   Steiger S, 2011, P ROY SOC B-BIOL SCI, V278, P970, DOI 10.1098/rspb.2010.2285
   Taylor AP, 2015, BEHAVIOUR, V152, P1097, DOI 10.1163/1568539X-00003270
   Verts B. J., 1967, BIOL STRIPED SKUNK
   Vogt K, 2014, BEHAV PROCESS, V106, P98, DOI 10.1016/j.beproc.2014.04.017
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wooldridge RL, 2019, J MAMMAL, V100, P445, DOI 10.1093/jmammal/gyz055
NR 28
TC 0
Z9 0
U1 4
U2 7
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2199-2401
EI 2199-241X
J9 MAMMAL RES
JI Mammal Res.
PD APR
PY 2021
VL 66
IS 2
BP 399
EP 404
DI 10.1007/s13364-021-00565-8
EA APR 2021
PG 6
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA RQ2DF
UT WOS:000639073000001
DA 2022-02-10
ER

PT C
AU Chen, GB
   Han, TX
   He, ZH
   Kays, R
   Forrester, T
AF Chen, Guobin
   Han, Tony X.
   He, Zhihai
   Kays, Roland
   Forrester, Tavis
GP IEEE
TI DEEP CONVOLUTIONAL NEURAL NETWORK BASED SPECIES RECOGNITION FOR WILD
   ANIMAL MONITORING
SO 2014 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
SE IEEE International Conference on Image Processing ICIP
LA English
DT Proceedings Paper
CT IEEE International Conference on Image Processing (ICIP)
CY OCT 27-30, 2014
CL Paris, FRANCE
SP IEEE
DE Species recognition; wild animal monitor; image classification; deep
   convolutional neural networks; large scale learning
ID POPULATIONS
AB We proposed a novel deep convolutional neural network based species recognition algorithm for wild animal classification on very challenging camera-trap imagery data. The imagery data were captured with motion triggered camera trap and were segmented automatically using the state of the art graph-cut algorithm. The moving foreground is selected as the region of interests and is fed to the proposed species recognition algorithm. For the comparison purpose, we use the traditional bag of visual words model as the baseline species recognition algorithm. It is clear that the proposed deep convolutional neural network based species recognition achieves superior performance. To our best knowledge, this is the first attempt to the fully automatic computer vision based species recognition on the real camera-trap images. We also collected and annotated a standard camera-trap dataset of 20 species common in North America, which contains 14, 346 training images and 9, 530 testing images, and is available to public for evaluation and benchmark purpose.
C1 [Chen, Guobin; Han, Tony X.; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65203 USA.
   [Kays, Roland; Forrester, Tavis] N Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27607 USA.
RP Chen, GB (corresponding author), Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65203 USA.
EM gcn38@missouri.edu; hantx@missouri.edu; hezhi@missouri.edu;
   rokays@gmail.com; ForresterT@si.edu
RI Chen, Guobin/AAN-1575-2021; He, Zhihai/A-5885-2019
OI Kays, Roland/0000-0002-2947-6665
CR Akyildiz IF, 2002, COMPUT NETW, V38, P393, DOI 10.1016/S1389-1286(01)00302-4
   Bengio, 2012, AISTATS, P127
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Freeman SN, 2007, BIRD STUDY, V54, P61, DOI 10.1080/00063650709461457
   GHARAVI H, 2002, WORLD WIR C 3G WIR 2
   Hinton GE, 1989, NEURAL COMPUT, V1, P143, DOI 10.1162/neco.1989.1.1.143
   Hulbert IAR, 2001, J APPL ECOL, V38, P869, DOI 10.1046/j.1365-2664.2001.00624.x
   Juang P., 2002, 10 INT C ARCH SUPP P
   Kavukcuoglu Koray, 2010, P ADV NEUR INF PROC, V23, P1090
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Link WA, 2008, J WILDLIFE MANAGE, V72, P44, DOI 10.2193/2007-299
   Markovchick-Nicholls L, 2008, CONSERV BIOL, V22, P99, DOI 10.1111/j.1523-1739.2007.00846.x
   Mech LD, 1983, HDB ANIMAL RADIO TRA
   Moil RJ, 2007, TRENDS ECOL EVOL, V22, P660, DOI 10.1016/j.tree.2007.09.007
   Ng, 2011, ADV NEURAL INFORM PR, P801
   Pidgeon AM, 2007, ECOL APPL, V17, P1989, DOI 10.1890/06-1489.1
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Riley SJ, 2003, ECOSCIENCE, V10, P455, DOI 10.1080/11956860.2003.11682793
   Sauer JR, 2008, N AM BREEDING BIRD S
   Szewczyk R., 2004, P 2 ACM C EMB NETW S
   Veech JA, 2006, CONSERV BIOL, V20, P1422, DOI 10.1111/j.1523-1739.2006.00487.x
   Wikle CK, 2003, ECOLOGY, V84, P1382, DOI 10.1890/0012-9658(2003)084[1382:HBMFPT]2.0.CO;2
   Williams M., 1998, AGROS NEWSLETTER, V53
   Young J., 2007, ANIMAL TRACKING BASI
   Yu D, 2013, IEEE T AUDIO SPEECH, V21, P388, DOI 10.1109/TASL.2012.2227738
NR 28
TC 59
Z9 59
U1 1
U2 24
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1522-4880
BN 978-1-4799-5751-4
J9 IEEE IMAGE PROC
PY 2014
BP 858
EP 862
PG 5
WC Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA BE2TU
UT WOS:000370063601007
DA 2022-02-10
ER

PT J
AU Verma, GK
   Gupta, P
AF Verma, Gyanendra K.
   Gupta, Pragya
TI Wild Animal Detection from Highly Cluttered Images Using Deep
   Convolutional Neural Network
SO INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS
LA English
DT Article
DE Wild animal detection; convolutional neural network; VGGNet; ResNet;
   SVM; ensemble tree; KNN; natural scenes
AB Monitoring wild animals became easy due to camera trap network, a technique to explore wildlife using automatically triggered camera on the presence of wild animal and yields a large volume of multimedia data. Wild animal detection is a dynamic research field since the last several decades. In this paper, we propose a wild animal detection system to monitor wildlife and detect wild animals from highly cluttered natural images. The data acquired from the camera-trap network comprises of scenes that are highly cluttered that poses a challenge for detection of wild animals bringing about low recognition rates and high false discovery rates. To deal with the issue, we have utilized a camera trap database that provides candidate regions utilizing multilevel graph cut in the spatiotemporal area. The regions are utilized to make a validation stage that recognizes whether animals are present or not in a scene. These features from cluttered images are extracted using Deep Convolutional Neural Network (CNN). We have implemented the system using two prominent CNN models namely VGGNet and ResNet, on standard camera trap database. Finally, the CNN features fed to some of the best in class machine learning techniques for classification. Our outcomes demonstrate that our proposed system is superior compared to existing systems reported in the literature.
C1 [Verma, Gyanendra K.; Gupta, Pragya] Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
RP Verma, GK (corresponding author), Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
EM gyanendra@nitkkr.ac.in; pragyagupta10@gmail.com
CR Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Chatfield K., 2014, ARXIV201414053531
   Chauhan A. K., 2013, INT J ADV RES COMPUT, V3
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P104, DOI 10.1109/CCAA.2017.8229781
   Hongfei Yu W. L., 2014, IET COMPUT VIS, V9, P13
   Joshi KA., 2012, INT J SOFT COMPUT EN, V2, P44
   Kays R., 2014, P N AM CONSERVATION, P80
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lee WT, 2009, PROC CVPR IEEE, P1590, DOI 10.1109/CVPRW.2009.5206521
   Mahadevan V., 2008, P 2008 IEEE C COMP V, P1
   Mammeri A, 2014, IEEE ICC, P1854, DOI 10.1109/ICC.2014.6883593
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Rakibe Rupali S., 2013, INT J SCI RES PUBLIC, V3, P2250
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sermanet P., 2013, P INT C LEARN REPR
   Szegedy C., 2014, ARXIV14121441
   Szegedy C., 2013, ADV NEURAL INF PROCE, V26, P2553, DOI DOI 10.5555/2999792.2999897
   Tilak S., 2011, INT J RES REV WIRELE, V1, P19
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 27
TC 0
Z9 0
U1 2
U2 7
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1469-0268
EI 1757-5885
J9 INT J COMPUT INTELL
JI Int. J. Comput. Intell. Appl.
PD DEC
PY 2018
VL 17
IS 4
AR 1850021
DI 10.1142/S1469026818500219
PG 17
WC Computer Science, Artificial Intelligence
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA HI2FS
UT WOS:000456261200004
DA 2022-02-10
ER

PT J
AU Allen, ML
   Peterson, B
   Krofel, M
AF Allen, Maximilian L.
   Peterson, Brittany
   Krofel, Miha
TI No respect for apex carnivores: Distribution and activity patterns of
   honey badgers in the Serengeti
SO MAMMALIAN BIOLOGY
LA English
DT Article
DE Abundance; Activity patterns; Distribution; Honey badger; Interspecific
   interactions; Mellivora capensis
ID SCENT-MARKING; COMMUNICATION BEHAVIORS; MELLIVORA-CAPENSIS; CAMERA TRAP;
   ABUNDANCE; DENSITY; MOUNTAINS; MODELS; LYNX; SIZE
AB Honey badgers are cryptic carnivores that occur at low densities and range across large areas. The processes behind site-level honey badger abundance and detection rates are poorly understood, and there are conflicting results about their avoidance of larger carnivores from different regions. We used data from 224 camera traps set up in the Serengeti National Park, Tanzania to evaluate patterns in detection rates, spatial distribution, and activity patterns of honey badgers. Our top models showed that the relative abundance of larger carnivores (e.g., African lions, Panthera leo, and spotted hyenas, Crocuta crocuta) was important, but surprisingly was positively related to honey badger distribution. These results suggest that honey badgers were not avoiding larger carnivores, but were instead potentially seeking out similar habitats and niches. We also found no temporal avoidance of larger carnivores. Honey badgers exhibited seasonal variation in activity patterns, being active at all times during the wet season with peaks during crepuscular hours, but having a strong nocturnal peak during the dry season. Our detection rates of honey badgers at individual camera traps were low ( 3402 trap nights/detection), but our study shows that with adequate effort camera traps can be used successfully as a research tool for this elusive mustelid. (c) 2018 Deutsche Gesellschaft fur Saugetierkunde. Published by Elsevier GmbH. All rights reserved.
C1 [Allen, Maximilian L.; Peterson, Brittany] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Wildlife Ecol Res Grp, Vena Pot 83, SI-1000 Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM maximilian.allen@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X; Krofel, Miha/0000-0002-2010-5219
CR Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2015, J ETHOL, V33, P9, DOI 10.1007/s10164-014-0418-0
   Baha-El-Din Laila, 2013, Small Carnivore Conservation, V48, P19
   Balme GA, 2017, BEHAV ECOL, V28, P1348, DOI 10.1093/beheco/arx098
   Begg C. M., 2001, THESIS
   Begg CM, 2005, J ZOOL, V265, P23, DOI 10.1017/S0952836904005989
   Begg CM, 2003, J ZOOL, V260, P301, DOI 10.1017/S0952836903003789
   Bird Tania L. F., 2013, Small Carnivore Conservation, V48, P47
   BROWN JH, 1984, AM NAT, V124, P255, DOI 10.1086/284267
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Do Linh San, 2016, BADGERS SYSTEMATICS, P161
   Do Linh San E, 2016, IUCN RED LIST THREAT
   Durant SM, 2010, J ANIM ECOL, V79, P1012, DOI 10.1111/j.1365-2656.2010.01717.x
   Durant SM, 1998, J ANIM ECOL, V67, P370, DOI 10.1046/j.1365-2656.1998.00202.x
   Estes R.D., 1992, BEHAV GUIDE AFRICAN
   Greengrass Elizabeth J., 2013, Small Carnivore Conservation, V48, P30
   Hayward MW, 2009, S AFR J WILDL RES, V39, P109, DOI 10.3957/056.039.0207
   Heurich M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114143
   Hopcraft JGC, 2005, J ANIM ECOL, V74, P559, DOI 10.1111/j.1365-2656.2005.00955.x
   Johnson DDP, 2000, MAMMAL REV, V30, P171, DOI 10.1046/j.1365-2907.2000.00066.x
   Lynam AJ, 2013, RAFFLES B ZOOL, V61, P407
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Newsome TM, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15469
   NORTON-GRIFFITHS M, 1975, East African Wildlife Journal, V13, P347, DOI 10.1111/j.1365-2028.1975.tb00144.x
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Proulx G., 2016, BADGERS SYSTEMATICS, P31
   R Core Team, 2017, R LANG ENV STAT COMP
   Rabinowitz D. S., 1986, BIOL ASPECTS RARE PL, P182
   Ramesh T, 2017, BEHAV ECOL SOCIOBIOL, V71, DOI 10.1007/s00265-017-2271-y
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Sagarin RD, 2006, TRENDS ECOL EVOL, V21, P524, DOI 10.1016/j.tree.2006.06.008
   Sinclair A. R. E., 1979, SERENGETI DYNAMICS E, P31
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Vogt K, 2014, BEHAV PROCESS, V106, P98, DOI 10.1016/j.beproc.2014.04.017
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wegge P, 2004, ANIM CONSERV, V7, P251, DOI 10.1017/S1367943004001441
NR 39
TC 15
Z9 16
U1 8
U2 59
PU ELSEVIER GMBH, URBAN & FISCHER VERLAG
PI JENA
PA OFFICE JENA, P O BOX 100537, 07705 JENA, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
PD MAR
PY 2018
VL 89
BP 90
EP 94
DI 10.1016/j.mambio.2018.01.001
PG 5
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA FY0ZA
UT WOS:000426540000012
OA Green Published
DA 2022-02-10
ER

PT C
AU Cunha, F
   dos Santos, EM
   Barreto, R
   Colonna, JG
AF Cunha, Fagner
   dos Santos, Eulanda M.
   Barreto, Raimundo
   Colonna, Juan G.
GP IEEE Comp Soc
TI Filtering Empty Camera Trap Images in Embedded Systems
SO 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGITION
   WORKSHOPS (CVPRW 2021)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 19-25, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, CVF
AB Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency.1
C1 [Cunha, Fagner; dos Santos, Eulanda M.; Barreto, Raimundo; Colonna, Juan G.] Univ Fed Amazonas, Manaus, Amazonas, Brazil.
RP Cunha, F (corresponding author), Univ Fed Amazonas, Manaus, Amazonas, Brazil.
EM fagner.cunha@icomp.ufam.edu.br; emsantos@icomp.ufam.edu.br;
   rbarreto@icomp.ufam.edu.br; juancolonna@icomp.ufam.edu.br
FU Samsung Electronics of Amazonia LtdaSamsung [003/2019, 6.008/2006,
   8.387/1991, 48]; ICOMP/UFAM; Foundation for Research Support of the
   State of Amazonas (FAPEAM) - POSGRAD Project; Coordination for the
   Improvement of Higher Education Personnel Brazil (CAPES)Coordenacao de
   Aperfeicoamento de Pessoal de Nivel Superior (CAPES) [001]
FX This research, according to Article 48 of Decree no 6.008/2006, was
   partially funded by Samsung Electronics of Amazonia Ltda, under the
   terms of Federal Law no 8.387/1991, through agreement no 003/2019,
   signed with ICOMP/UFAM. This study was supported by the Foundation for
   Research Support of the State of Amazonas (FAPEAM) - POSGRAD Project,
   and the Coordination for the Improvement of Higher Education Personnel
   Brazil (CAPES) - Finance Code 001. The funders had no role in study
   design, data collection and analysis, decision to publish, or
   preparation of the manuscript.
CR Beery S., 2019, BIODIVERSITY INFORM, V3, pe37222, DOI [10.3897/biss.3.37222, DOI 10.3897/BISS.3.37222]
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Cubuk Ekin D., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Kolesnikov A., 2020, COMPUTER VISION ECCV, DOI DOI 10.1007/978-3-030-58558-7_29
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Olsson Sara, 2020, THESIS
   Ren SQ, 2015, ADV NEUR IN, V28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schnaubelt S, 2020, EUR HEART J-CASE REP, V4, DOI 10.1093/ehjcr/ytaa166
   Swanson AB, 2015, DATA SNAPSHOT SERENG
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tan MX, 2019, PR MACH LEARN RES, V97
   TAN MX, 2019, P IEEE CVF C COMP VI, V97
   TEAM Network, 2011, TERR VER PROT IMPL M
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Xiong Y., 2020, ARXIV PREPRINT ARXIV
   Zualkernan IA, 2020, 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT), P111, DOI 10.1109/GCAIOT51063.2020.9345858
NR 25
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2160-7508
BN 978-1-6654-4899-4
J9 IEEE COMPUT SOC CONF
PY 2021
BP 2438
EP 2446
DI 10.1109/CVPRW53098.2021.00276
PG 9
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS2PO
UT WOS:000705890202061
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Shepley, A
   Falzon, G
   Lawson, C
   Meek, P
   Kwan, P
AF Shepley, Andrew
   Falzon, Greg
   Lawson, Christopher
   Meek, Paul
   Kwan, Paul
TI U-Infuse: Democratization of Customizable Deep Learning for Object
   Detection
SO SENSORS
LA English
DT Article
DE animal identification; artificial intelligence; camera-trap images;
   camera trapping; deep convolutional neural networks; deep learning;
   environmental software; wildlife ecology; wildlife monitoring;
   ecological object detection
ID CAMERA TRAP IMAGES
AB Image data is one of the primary sources of ecological data used in biodiversity conservation and management worldwide. However, classifying and interpreting large numbers of images is time and resource expensive, particularly in the context of camera trapping. Deep learning models have been used to achieve this task but are often not suited to specific applications due to their inability to generalise to new environments and inconsistent performance. Models need to be developed for specific species cohorts and environments, but the technical skills required to achieve this are a key barrier to the accessibility of this technology to ecologists. Thus, there is a strong need to democratize access to deep learning technologies by providing an easy-to-use software application allowing non-technical users to train custom object detectors. U-Infuse addresses this issue by providing ecologists with the ability to train customised models using publicly available images and/or their own images without specific technical expertise. Auto-annotation and annotation editing functionalities minimize the constraints of manually annotating and pre-processing large numbers of images. U-Infuse is a free and open-source software solution that supports both multiclass and single class training and object detection, allowing ecologists to access deep learning technologies usually only available to computer scientists, on their own device, customised for their application, without sharing intellectual property or sensitive data. It provides ecological practitioners with the ability to (i) easily achieve object detection within a user-friendly GUI, generating a species distribution report, and other useful statistics, (ii) custom train deep learning models using publicly available and custom training data, (iii) achieve supervised auto-annotation of images for further training, with the benefit of editing annotations to ensure quality datasets. Broad adoption of U-Infuse by ecological practitioners will improve ecological image analysis and processing by allowing significantly more image data to be processed with minimal expenditure of time and resources, particularly for camera trap images. Ease of training and use of transfer learning means domain-specific models can be trained rapidly, and frequently updated without the need for computer science expertise, or data sharing, protecting intellectual property and privacy.
C1 [Shepley, Andrew; Falzon, Greg; Lawson, Christopher] Univ New England, Sch Sci & Technol, Armidale, NSW 2350, Australia.
   [Falzon, Greg] Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA 5001, Australia.
   [Meek, Paul] NSW Dept Primary Ind, Vertebrate Pest Res Unit, POB 530, Coffs Harbour, NSW 2450, Australia.
   [Meek, Paul] Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2350, Australia.
   [Kwan, Paul] Melbourne Inst Technol, Sch IT & Engn, Melbourne, Vic 3000, Australia.
RP Shepley, A (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW 2350, Australia.
EM asheple2@une.edu.au; greg.falzon@flinders.edu.au;
   christopher.lawson@uon.edu.au; paul.meek@dpi.nsw.gov.au;
   pkwan@mit.edu.au
OI Shepley, Andrew/0000-0001-7511-4967; Falzon,
   Gregory/0000-0002-1989-9357; Kwan, Paul Wing Hing/0000-0002-4959-5274
FU NSW Environmental Trust "Developing Strategies for Effective Feral Cat
   Management" project; Australian Government Research Training Program
   (RTP) ScholarshipAustralian GovernmentDepartment of Industry, Innovation
   and Science; University of New England
FX This research was funded by the NSW Environmental Trust "Developing
   Strategies for Effective Feral Cat Management" project. Andrew Shepley
   acknowledges the support provided through the Australian Government
   Research Training Program (RTP) Scholarship. The APC was funded by the
   University of New England.
CR Abadi M., 2016, 12 USENIX S OPERATIN, P265
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   [Anonymous], 2017, DRIVEN DATA PROJECT
   [Anonymous], 2015, TZUTALIN LABELIMG GI
   Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
   Beery S, 2018, RECOGNITION TERRA IN
   Bengsen A, 2014, ECOL MANAG RESTOR, V15, P97, DOI 10.1111/emr.12086
   Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
   Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
   Fegraus E.H., 2016, CAMERA TRAPPING WILD, P33
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Greenberg S, 2019, ECOL EVOL, V9, P13706, DOI 10.1002/ece3.5767
   Hendry H, 2018, ORYX, V52, P15, DOI 10.1017/S0030605317001818
   Lashley MA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22638-6
   Legge S, 2020, WILDLIFE RES, V47, P731, DOI 10.1071/WR20089
   Legge S, 2020, WILDLIFE RES, V47, P523, DOI [10.1071/WR19174, 10.1071/WRv47n8_ED]
   Li XY, 2018, DIVERS DISTRIB, V24, P1560, DOI 10.1111/ddi.12792
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Meek P., 2014, CAMERA TRAPPING WILD
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   R Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
   Rahman DA, 2017, ORYX, V51, P665, DOI 10.1017/S0030605316000429
   Redmon J, 2016, ARXIV 161208242
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Shepley A, 2021, ECOL EVOL, V11, P4494, DOI [10.1002/ece3.7344, 10.5281/ZENODO.4544073, 10.5281/ZENODO.4544074]
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tiwary U.S, 2018, INT HUM COMP INT 10
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zaumyslova O.Y., 2015, ACHIEV LIFE SCI, V9, P15, DOI [10.1016/j.als.2015.05.003, DOI 10.1016/J.ALS.2015.05.003]
   Zhang E., 2009, ENCY DATABASE SYSTEM, DOI [DOI 10.1007/978-0-387-39940-9_482, 10.1007/978-0-387-39940-9_482]
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
NR 42
TC 0
Z9 0
U1 1
U2 2
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD APR
PY 2021
VL 21
IS 8
AR 2611
DI 10.3390/s21082611
PG 17
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA RU0CQ
UT WOS:000644820300001
PM 33917792
OA gold, Green Published
DA 2022-02-10
ER

PT C
AU Timm, M
   Maji, S
   Fuller, T
AF Timm, Mikayla
   Maji, Subhransu
   Fuller, Todd
GP IEEE
TI Large-Scale Ecological Analyses of Animals in the Wild using Computer
   Vision
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
SP IEEE Comp Soc
AB Camera traps are increasingly being deployed by ecologists and citizen-scientists as a cost-effective way of obtaining large amounts of animal images in the wild. In order to analyze this data, the images are labeled manually by ecologists, where they identify species of animals and more fine-grained details, such as animal sex or age, or even individual animal identities. However, with the number of camera trap images quickly outgrowing the capacity of the labelers, ecologists are unable to keep up with the wealth of data they are obtaining. Using computer vision, we can automatically generate labels for new camera trap images at the rate that they are being obtained, allowing ecologists to uncover ecological and biological information at a scale previously not possible. In this paper, we explore computer vision approaches for species identification in camera trap images and for individual jaguar identification, both of which show promising results. We make this novel dataset publicly available for future research directions and further exploration.
C1 [Timm, Mikayla; Maji, Subhransu; Fuller, Todd] Univ Massachusetts, Amherst, MA 01003 USA.
RP Timm, M (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.
EM mtimm@cs.umass.edu; smaji@cs.umass.edu; tkfuller@eco.umass.edu
CR Deng J., 2009, IEEE CVPR
   Girshick R., 2014, IEEE CVPR
   Krizhevsky A., 2012, IMAGENET CLASSIFICAT, V25
   Long J., 2015, IEEE CVPR
   RoyChowdhury A., 2015, ICCV
   Szegedy C., 2016, IEEE CVPR
   Van Horn G., 2018, IEEE CVPR
NR 7
TC 1
Z9 1
U1 1
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 1977
EP 1979
DI 10.1109/CVPRW.2018.00252
PG 3
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800245
DA 2022-02-10
ER

PT J
AU Aximoff, I
   Neto, EP
   de Paula, W
   Hofmann, GS
   Keuroghlian, A
   Jorge, ML
   Lima, E
   Barquero, G
AF Aximoff, Izar
   Neto, Ennio Painkow
   de Paula, Wlainer
   Hofmann, Gabriel Selbach
   Keuroghlian, Alexine
   Jorge, Maria Luisa
   Lima, Edson
   Barquero, Gonzalo
TI Sticking out in a herd? Records of anomalous pigmentation in a social
   herd- forming ungulate (Tayassu pecari)
SO NORTH-WESTERN JOURNAL OF ZOOLOGY
LA English
DT Article
DE mammals; anomalous pigmentation; white-lipped peccary; camera traps;
   Brazilian biomes
ID HOME-RANGE; 1ST RECORDS; PECCARY; ALBINISM; MAMMALS; COLOR; BEHAVIOR;
   TAJACU; MONKEY; AREA
AB Piebaldism, leucism or albinism are different types of pigmentation anomalies related to the excess or deficit of pigmentation in some regions or in the whole body of an animal. It is extremely rare in peccaries and has never been reported for Tayassu, a genus endemic to Neotropical region. Tayassu pecari, known as white-lipped peccaries (WLP's), have behaviors and ecology well documented in literature, but there is no published scientific information about pigmentation anomalies. Here, we report the first records of anomalous individuals (leucism and piebaldism) for WLP's in locations of Central Brazil. During fieldwork for mammal survey (2010-2020) search for individuals with pigmentation anomalies were carried out in ten sites spread across four Brazilian biomes: Atlantic Forest, Cerrado, Amazon Forest, and Pantanal. Thirteen individuals were registered throughout eight locations in the last three biomes. All these individuals were part of a herd in which most individuals occurred with normal pigmentation. The Cerrado was the most representative biome in which individuals with abnormal pigmentation were recorded (46.2%) followed by the Amazon (30.8 %) and the Pantanal (23.0%). Despite the long-term survey by camera traps (70,000 nights of camera traps), only 0.1% of the records were of WLP with anomalous coloration. Our work presents, for the first time, WLP with leucism and piebaldism, and a range of chromatic variations. It was not possible to determine how leucism affects the health and survival of the individuals that have it, therefore, our work encourages future field studies that can monitor the behavior and fitness of those individuals for extended periods.
C1 [Aximoff, Izar] Univ Estado Rio De Janeiro, Radioecol & Global Changes Lab, 524 PHLC Subsolo, Rio De Janeiro, Brazil.
   [Neto, Ennio Painkow; Barquero, Gonzalo] Trop Sustainabil Inst, Estr Tambau 9, Sao Paulo, Brazil.
   [de Paula, Wlainer] Neofauna Ambiental Quadra 406 Norte, Qc 02,Alameda 07,Lote 03, Palmas, Tocantins, Brazil.
   [Hofmann, Gabriel Selbach] Rio Grande Sul Fed Univ, Inst Geosci, Ave Bento Goncalves 9500,Bldg 43113, Porto Alegre, RS, Brazil.
   [Keuroghlian, Alexine] IUCN SSC Peccary Specialist Grp, Queixada Project, Peccary Project, R Spipe Calarge 2355, Campo Grande, MS, Brazil.
   [Jorge, Maria Luisa] Vanderbilt Univ, Earth & Environm Sci, PMB 351805,2301 Vanderbilt Pl, Nashville, TN 37235 USA.
   [Lima, Edson] Cachorro Vinagre Program, Pernambuco St 179, Nova Xavantina, Mato Grosso, Brazil.
RP Aximoff, I (corresponding author), Univ Estado Rio De Janeiro, Radioecol & Global Changes Lab, 524 PHLC Subsolo, Rio De Janeiro, Brazil.
EM izar.aximoff@gmail.com
FU Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior -Brasil
   (CAPES)Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior
   (CAPES) [001]; Innovation Department -InovUERJ
FX We thank Luiz Flamarion B. de Oliveira (MN/UFRJ), Igor P. Coelho,
   Guilherme Ribeiro and SESC Pantanal team for the partnership in our
   projects. Julia Oshima for assisting and registering the capture and
   contributing with the photos (2C and 2D). Dorival Miguel Stank for
   contributing with the photo (2H). We want to thank the Peccary Project
   staff for support in the field and during the capture. We also thank
   UNESP-Rio Claro (LEEC) for their field support in Corguinho, MS. To
   Sindicato Rural de Chapadao do Ceu, management of Emas National Park,
   ICMBio, Centro Nacional de Pesquisa e Conservacao de Mamiferos
   Carnivoros (CENAP), to team of Tropical Sustainability Institute and
   Neofauna Ambiental for supporting the use of records. GSH was supported
   by Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior -Brasil
   (CAPES) -Finance Code 001. We also thank the Innovation Department
   -InovUERJ, for the granting (Qualitec Superior) to first author, and we
   also thank the editors and anonymous reviewers for the important
   contributions.
CR Abreu MSL, 2013, BRAZ J BIOL, V73, P185, DOI 10.1590/S1519-69842013000100020
   Acevedo J, 2008, REV BIOL MAR OCEANOG, V43, P413
   Aximoff I, 2020, FOLIA PRIMATOL, V91, P149, DOI 10.1159/000501186
   Aximoff Izar, 2016, Oecologia Australis, V20, P122
   Aximoff Izar A., 2016, Oecologia Australis, V20, P526
   Beck H, 2018, ECOLOGY, CONSERVATION AND MANAGEMENT OF WILD PIGS AND PECCARIES, P265
   Borteiro C, 2021, SALAMANDRA, V57, P124
   Brito Jorge, 2016, Therya, V7, P483, DOI 10.12933/therya-16-408
   Cavalcanti SMC, 2010, J MAMMAL, V91, P722, DOI 10.1644/09-MAMM-A-171.1
   da Silva VL, 2019, TROP ECOL, V60, P303, DOI 10.1007/s42965-019-00036-x
   Jacomo ATD, 2013, J MAMMAL, V94, P137, DOI 10.1644/11-MAMM-A-411.1
   de Mello Luciano Moura, 2020, Oecologia Australis, V24, P191, DOI 10.4257/oeco.2020.2401.17
   Desbiez Arnaud Leonard Jean, 2009, International Journal of Biodiversity and Conservation, V1, P11
   DONKIN RA, 1985, T AM PHILOS SOC, V75, P1, DOI 10.2307/1006340
   Eaton DP, 2017, BIOL CONSERV, V208, P29, DOI 10.1016/j.biocon.2016.09.010
   Espinal Mario, 2016, Mastozool. neotrop., V23, P63
   Fertl D, 2009, ENCYCLOPEDIA OF MARINE MAMMALS, 2ND EDITION, P24
   Fowler ME., 2015, FOWLERS ZOO WILD ANI, P568, DOI [DOI 10.1016/B978-1-4557-7397-8.00058-X, 10.1016/B978-1-4557-7397-8.00058-X]
   Garcia-Casimiro Erika, 2020, Neotropical Biology and Conservation, V15, P195, DOI 10.3897/neotropical.15.e50951
   Hendges CD, 2019, J MAMMAL, V100, P475, DOI 10.1093/jmammal/gyz061
   Hofreiter M, 2010, CELL MOL LIFE SCI, V67, P2591, DOI 10.1007/s00018-010-0333-7
   Hubel M., 2020, OECOL AUST
   Jorge M.L.S., 2019, MOVEMENT ECOLOGY NEO, P39, DOI [10.1007/978-3-030-03463-4_4, DOI 10.1007/978-3-030-03463-4_4]
   Keuroghlian A, 2004, BIOL CONSERV, V120, P411, DOI 10.1016/j.biocon.2004.03.016
   Keuroghlian A, 2008, BIOTROPICA, V40, P62, DOI 10.1111/j.1744-7429.2007.00351.x
   Keuroghlian A, 2018, ECOLOGY, CONSERVATION AND MANAGEMENT OF WILD PIGS AND PECCARIES, P277
   Keuroghlian A, 2015, MAMMALIA, V79, P491, DOI 10.1515/mammalia-2014-0094
   Keuroghlian Alexine, 2012, Biodiversidade Brasileira, V1, P84
   Keuroghlian A, 2009, BIODIVERS CONSERV, V18, P1733, DOI 10.1007/s10531-008-9554-6
   Landis MB, 2020, MAMMALIA, V84, P601, DOI 10.1515/mammalia-2019-0084
   Leite DA, 2018, MAMM BIOL, V92, P111, DOI 10.1016/j.mambio.2018.05.005
   Lima M, 2019, PERSPECT ECOL CONSER, V17, P36, DOI 10.1016/j.pecon.2018.12.001
   Mazza Isabela, 2018, Oecologia Australis, V22, P74, DOI 10.4257/oeco.2018.2201.07
   Mello L.M., 2016, REV CIENCIAS AMBIENT, V10, P157, DOI [10.18316/1981-8858.16.37, DOI 10.18316/1981-8858.16.37]
   Paviolo A, 2016, SCI REP-UK, V6, DOI 10.1038/srep37147
   Reyna-Hurtado R, 2009, J MAMMAL, V90, P1199, DOI 10.1644/08-MAMM-A-246.1
   Ribeiro Raquel, 2020, Acta Scientiarum Biological Sciences, V42, pe46734, DOI 10.4025/actascibiolsci.v42i2.46734
   Sazima I., 1992, Memorias do Instituto Butantan (Sao Paulo), V53, P167
   Sobroza TV, 2016, STUD NEOTROP FAUNA E, V51, P231, DOI 10.1080/01650521.2016.1227137
   Sowls L.K., 1997, JAVELINAS OTHER PECC
   Sowls L. K., 1984, PECCARIES
   Summers CG, 2009, OPTOMETRY VISION SCI, V86, P659, DOI 10.1097/OPX.0b013e3181a5254c
   Taber A., 2016, TROPICAL CONSERVATIO, P255
   Veiga L. A., 1994, Revista Brasileira de Zoologia, V11, P341
NR 44
TC 0
Z9 0
U1 0
U2 0
PU UNIV ORADEA PUBL HOUSE
PI ORADEA
PA UNIVERSITATII NR 1, ORADEA, 410087, ROMANIA
SN 1584-9074
EI 1842-6441
J9 NORTH-WEST J ZOOL
JI North-West. J. Zool.
PD DEC
PY 2021
VL 17
IS 2
BP 288
EP 293
AR e211702
PG 6
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA XQ0YR
UT WOS:000731281200003
DA 2022-02-10
ER

PT J
AU Cappele, N
   Howe, EJ
   Boesch, C
   Kuhl, HS
AF Cappele, Noemie
   Howe, Eric J.
   Boesch, Christophe
   Kuehl, Hjalmar S.
TI Estimating animal abundance and effort-precision relationship with
   camera trap distance sampling
SO ECOSPHERE
LA English
DT Article
DE chimpanzee; design; elephant; leopard; Maxwell's duiker; monitoring;
   precision; sampling effort
AB Effective monitoring methods are needed for assessing the state of biodiversity and detecting population trends. The popularity of camera trapping in wildlife surveys continues to increase as they are able to detect species in remote and difficult-to-access areas. As a result, several statistical estimators of the abundance of unmarked animal populations have been developed, but none have been widely tested. Even where the potential for accurate estimation has been demonstrated, whether these methods estimators can yield estimates of sufficient precision to detect trends and inform conservation action remains questionable. Here, we assess the effort-precision relationship of camera trap distance sampling (CTDS) in order to help researchers design efficient surveys. A total of 200 cameras were deployed for 10 months across 200 km(2) in the Tai National Park, Cote d'Ivoire. We estimated abundance of Maxwell's duikers, western chimpanzees, leopards, and forest elephants that are challenging to enumerate due to rarity or semi-arboreality. To test the effects of spatial and temporal survey effort on the precision of CTDS estimates, we calculated coefficient of variation (CV) of the encounter rate from subsets of our complete data sets. Estimated abundance of leopard and Maxwell's duiker density (20% < CV < 30% and CV = 11%, respectively) were similar to prior estimates from the same area. Abundances of chimpanzees (20% < CV < 30%) were underestimated, but the quality of inference was similar to that reported after labor-intensive line transect surveys to nests. Estimates for the rare forest elephants were potentially unreliable since they were too imprecise (60% < CV < 200%). Generalized linear models coefficients indicated that for relatively common, ground-dwelling species, CVs between 10% and 20% are achievable from a variety of survey designs, including long-term (6+ months) surveys at few locations (50), or short term (2-week to 2-month) surveys at 100-150 locations. We conclude that CTDS can efficiently provide estimates of abundance of multiple species of sufficient quality and precision to inform conservation decisions. However, estimates for the rarest species will be imprecise even from ambitious surveys and may be biased for species that exhibit strong reactions to cameras.
C1 [Cappele, Noemie; Boesch, Christophe; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Howe, Eric J.] Univ St Andrews, Ctr Res Ecol & Environm Modeling, St Andrews, Fife, Scotland.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP Cappele, N (corresponding author), Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
EM noemie_cappelle@eva.mpg.de
FU Max Planck SocietyMax Planck SocietyFoundation CELLEX; ARCUS foundation
FX The authors thank the Max Planck Society in Germany, The Robert Bosch
   Foundation, the Wild Chimpanzee Foundation, the Ministere des Eaux et
   Forets, Ministere de l'Enseignement superieur et de la Recherche
   scientifique, and the Office Ivoirien des Parcs et Reserves for
   permitting this research, as well as the Tai Chimpanzee Project. This
   study was conducted with the financial support of the Max Planck Society
   and ARCUS foundation. We thank Roger Mundry for the statistical support.
   We particularly want to thank Serge N'Goran for his invaluable help in
   supervising field and data processing assistants. We thank Alphonse,
   Emile, Eric, Lambert, Martin, and Nicaise for their support in
   fieldwork, Alejandro Estrella, Adiko Noel, Ange, Benjamin Debetencourt,
   Christina, Coulibaly, Daniel, Diomande, Hanna, Heather Cohen, Iko Noel,
   Ines, Kouadio, Lisa Orth, Amoakon et Sita Scherer for their
CR Anderson DP, 2005, BIOTROPICA, V37, P631, DOI 10.1111/j.1744-7429.2005.00080.x
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155
   Anile S, 2014, J ZOOL, V293, P252, DOI 10.1111/jzo.12141
   Balestrieri A., 2016, THESIS U MILAN MILAN
   Bessone M, 2020, J APPL ECOL, V57, P963, DOI 10.1111/1365-2664.13602
   Boesch C, 2006, AM J PHYS ANTHROPOL, V130, P103, DOI 10.1002/ajpa.20341
   Boesch C, 2008, AM J PRIMATOL, V70, P519, DOI 10.1002/ajp.20524
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Buckland S.T., 2001, pi
   Buckland ST., 2004, ADV DISTANCE SAMPLIN
   Buckland ST, 2006, AUK, V123, P345, DOI 10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campos-Candela A., 2018, J ANIM ECOL, V38, P42
   Cappelle N, 2019, AM J PRIMATOL, V81, DOI 10.1002/ajp.22962
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Doran D., 1989, THESIS U NEW YORK NE
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Gilbert NA, 2021, CONSERV BIOL, V35, P88, DOI 10.1111/cobi.13517
   Hoppe-Dominik B, 2011, AFR J ECOL, V49, P450, DOI 10.1111/j.1365-2028.2011.01277.x
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Hutchinson JMC, 2007, BIOL REV, V82, P335, DOI 10.1111/j.1469-185X.2007.00014.x
   Jenny D, 1996, J ZOOL, V240, P427, DOI 10.1111/j.1469-7998.1996.tb05296.x
   Jourdain NOAS, 2020, J AGR BIOL ENVIR ST, V25, P148, DOI 10.1007/s13253-020-00385-4
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
   Luo G, 2020, WILDLIFE SOC B, V44, P173, DOI 10.1002/wsb.1060
   Marini F, 2009, EUR J WILDLIFE RES, V55, P107, DOI 10.1007/s10344-008-0222-7
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Marshall AR, 2008, AM J PRIMATOL, V70, P452, DOI 10.1002/ajp.20516
   McCullagh P., 1989, GEN LINEAR MODELS, V2
   Moeller AK, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2331
   Nakashima Y, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108381
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero Francesco, 2010, Abc Taxa, V8, P100
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Rowcliffe JM, 2013, J WILDLIFE MANAGE, V77, P876, DOI 10.1002/jwmg.533
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Si XF, 2014, PEERJ, V2, DOI 10.7717/peerj.374
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Tiedoue M.R., 2016, ETAT CONSERVATION PA
   WCF, 2016, RAPP ANN 2016
   Zero VH, 2013, ORYX, V47, P410, DOI 10.1017/S0030605312000324
NR 52
TC 4
Z9 4
U1 5
U2 13
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2150-8925
J9 ECOSPHERE
JI Ecosphere
PD JAN
PY 2021
VL 12
IS 1
AR e03299
DI 10.1002/ecs2.3299
PG 16
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QB4KR
UT WOS:000614109800003
OA gold, Green Published
DA 2022-02-10
ER

PT C
AU Cheema, GS
   Anand, S
AF Cheema, Gullal Singh
   Anand, Saket
BE Altun, Y
   Das, K
   Mielikainen, T
   Malerba, D
   Stefanowski, J
   Read, J
   Zitnik, M
   Ceci, M
   Dzeroski, S
TI Automatic Detection and Recognition of Individuals in Patterned Species
SO MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, ECML PKDD 2017,
   PT III
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECML PKDD)
CY SEP 18-22, 2017
CL Skopje, MACEDONIA
SP Deutsche Post DHL Grp, Google, AGT, ASML, Deloitte, NEC Europe Ltd, Siemens, Cambridge Univ Press, IEEE CAA Journal Automatica Sinica, Springer, IBM Res, Data Min & Knowledge Discovery, Machine Learning, EurAi, GrabIT
DE Animal biometrics; Wildlife monitoring; Detection; Recognition;
   Convolutional neural network; Computer vision
AB Visual animal biometrics is rapidly gaining popularity as it enables a non-invasive and cost-effective approach for wildlife monitoring applications. Widespread usage of camera traps has led to large volumes of collected images, making manual processing of visual content hard to manage. In this work, we develop a framework for automatic detection and recognition of individuals in different patterned species like tigers, zebras and jaguars. Most existing systems primarily rely on manual input for localizing the animal, which does not scale well to large datasets. In order to automate the detection process while retaining robustness to blur, partial occlusion, illumination and pose variations, we use the recently proposed Faster-RCNN object detection framework to efficiently detect animals in images. We further extract features from AlexNet of the animal's flank and train a logistic regression (or Linear SVM) classifier to recognize the individuals. We primarily test and evaluate our framework on a camera trap tiger image dataset that contains images that vary in overall image quality, animal pose, scale and lighting. We also evaluate our recognition system on zebra and jaguar images to show generalization to other patterned species. Our framework gives perfect detection results in camera trapped tiger images and a similar or better individual recognition performance when compared with state-of-the-art recognition techniques.
C1 [Cheema, Gullal Singh; Anand, Saket] IIIT Delhi, New Delhi, India.
RP Cheema, GS (corresponding author), IIIT Delhi, New Delhi, India.
EM gullal1408@iiitd.ac.in; anands@iiitd.ac.in
OI Cheema, Gullal Singh/0000-0003-4354-9629
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Burghardt T., 2004, EWIMT
   Burghardt T, 2006, NEUREL 2006: EIGHT SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL ENGINEERING, PROCEEDINGS, P27
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Jain AK, 2000, IEEE T IMAGE PROCESS, V9, P846, DOI 10.1109/83.841531
   Jiang XD, 2000, INT C PATT RECOG, P1038, DOI 10.1109/ICPR.2000.906252
   KLINGEL H, 1974, Zeitschrift fuer Tierpsychologie, V36, P37
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Lahiri M., 2011, P 1 ACM INT C MULT R, P6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mizroch S. A., 2003, MAR FISH REV, V65, P25
   Norouzzadeh MS, 2017, ARXIV170305830
   Prodger Phillip, 2009, DARWINS CAMERA ART P
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Scott D. K., 1978, RECOGNITION MARKING, P160
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tisse C.-L., 2002, P VISION INTERFACE, P294
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 31
TC 11
Z9 13
U1 1
U2 10
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-71273-4; 978-3-319-71272-7
J9 LECT NOTES ARTIF INT
PY 2017
VL 10536
BP 27
EP 38
DI 10.1007/978-3-319-71273-4_3
PN III
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BK8MC
UT WOS:000443111100003
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Sibarani, MC
   Krofel, M
AF Allen, Maximilian L.
   Sibarani, Marsya C.
   Krofel, Miha
TI Predicting preferred prey of Sumatran tigers Panthera tigris sumatrae
   via spatio-temporal overlap
SO ORYX
LA English
DT Article
DE Activity patterns; composite score; Panthera tigris; prey preference;
   spatial overlap; Sumatra; temporal overlap; tiger
AB Encounter rates of carnivores with prey are dependent on spatial and temporal overlap, and are often highest with their preferred prey. The Critically Endangered Sumatran tiger Panthera tigris sumatrae is dependent on prey populations, but little is known about its prey preferences. We collected camera-trap data for 7 years (2010-2016) in Bukit Barisan Selatan National Park, Sumatra, to investigate spatial and temporal overlap of tigers with potential prey species. We also developed a novel method to predict predator-prey encounter rates and potential prey preferences from camera-trap data. We documented at least 10 individual tigers, with an overall detection rate of 0.24 detections/100 trap nights. Tigers exhibited a diurnal activity pattern and had highest temporal overlap with wild boar Sus scrofa and pig-tailed macaques Macaca nemestrina, but highest spatial overlap with wild boar and sambar deer Rusa unicolor. We created a spatial and temporal composite score and three additional composite scores with adjustments for the spatial overlap and preferred prey mass. Wild boars ranked highest for all composite scores, followed by sambar deer, and both are known as preferred tiger prey in other areas. Spatial and temporal overlaps are often considered as separate indices, but a composite score may facilitate better predictions of encounter rates and potential prey preferences. Our findings suggest that prey management efforts in this area should focus on wild boar and sambar deer, to ensure a robust prey base for this Critically Endangered tiger population.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816S Oak St, Champaign, IL 61820 USA.
   [Sibarani, Marsya C.] Wildlife Conservat Soc Indonesia Program, Bogor, West Java, Indonesia.
   [Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
OI Krofel, Miha/0000-0002-2010-5219
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation;
   Illinois Natural History Survey; Slovenian Research AgencySlovenian
   Research Agency - Slovenia [P4-0059]
FX All data used in this study were collected by the Tropical Ecology
   Assessment and Monitoring Network, a collaboration between Conservation
   International, the Missouri Botanical Garden, the Smithsonian
   Institution and the Wildlife Conservation Society. The work was
   partially funded by these institutions, the Gordon and Betty Moore
   Foundation, the Illinois Natural History Survey, the Slovenian Research
   Agency (P4-0059), and other donors. Monitoring activities were managed
   by the Wildlife Conservation Society in collaboration with the Bukit
   Barisan Selatan National Park and the Ministry of Environment and
   Forestry, Republic of Indonesia. We thank all field staff and forest
   rangers involved in camera-trap deployment, and W. Marthy for help in
   the field and coordination.
CR Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Allen ML, 2017, J ETHOL, V35, P13, DOI 10.1007/s10164-016-0492-6
   Allen ML, 2016, ECOLOGY, V97, P1905, DOI 10.1002/ecy.1462
   Barber-Meyer SM, 2013, J ZOOL, V289, P10, DOI 10.1111/j.1469-7998.2012.00956.x
   Basak Krishnendu, 2018, Proceedings of the Zoological Society (Calcutta), V71, P92, DOI 10.1007/s12595-016-0196-5
   Begon M., 2006, ECOLOGY INDIVIDUALS
   Clinchy M, 2016, BEHAV ECOL, V27, P1826, DOI 10.1093/beheco/arw117
   du Preez B, 2017, J ZOOL, V302, P149, DOI 10.1111/jzo.12443
   Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088
   Fortin D, 2015, P ROY SOC B-BIOL SCI, V282, P99, DOI 10.1098/rspb.2015.0973
   Hayward MW, 2012, J ZOOL, V286, P221, DOI 10.1111/j.1469-7998.2011.00871.x
   HOLLING C. S., 1959, CANADIAN ENT, V91, P293
   Karanth KU, 2004, P NATL ACAD SCI USA, V101, P4854, DOI 10.1073/pnas.0306210101
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Linkie M, 2003, ORYX, V37, P41, DOI 10.1017/S0030605303000103
   Linkie M., 2008, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2008.RLTS.T15966A5334836.en, DOI 10.2305/IUCN.UK.2008.RLTS.T15966A5334836.EN]
   Meredith M., 2017, OVERVIEW OVERLAP PAC
   Miquelle Dale G., 1999, P71
   Ngoprasert D, 2012, BIOTROPICA, V44, P810, DOI 10.1111/j.1744-7429.2012.00878.x
   Nowak R.M., 1999, WALKERS MAMMALS WORL
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien Timothy G., 1996, Oryx, V30, P207
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Pattanavibool A, 2015, IUCN RED LIST THREAT
   Pusparini W, 2018, ORYX, V52, P25, DOI 10.1017/S0030605317001144
   Ramakrishnan U, 1999, BIOL CONSERV, V89, P113, DOI 10.1016/S0006-3207(98)00159-1
   Rich LN, 2019, BIOL CONSERV, V233, P12, DOI 10.1016/j.biocon.2019.02.018
   Rich LN, 2016, J APPL ECOL, V53, P1225, DOI 10.1111/1365-2664.12650
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rovero F., 2016, CAMERA TRAPPING WILD
   Saggiomo L., 2017, FOOD WEBS, V12, P35, DOI DOI 10.1016/J.FOOWEB.2017.01.001
   Sanderson EW, 2010, TIGERS OF THE WORLD: THE SCIENCE, POLITICS, AND CONSERVATION OF PANTHERA TIGRIS, 2ND EDITION, P143, DOI 10.1016/B978-0-8155-1570-8.00009-8
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   SEIDENSTICKER J, 1993, SYM ZOOL S, P105
   Seidensticker J, 2010, INTEGR ZOOL, V5, P285, DOI 10.1111/j.1749-4877.2010.00214.x
   Sibarani MC, 2019, J APPL ECOL, V56, P1220, DOI 10.1111/1365-2664.13360
   Song MK, 2013, NURS RES, V62, P45, DOI 10.1097/NNR.0b013e3182741948
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   TEAM Network, 2011, TERRESTRIAL VERTEBRA
   Team RC, 2014, R LANG ENV STAT COMP
   Walston J, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000485
NR 41
TC 0
Z9 0
U1 9
U2 11
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0030-6053
EI 1365-3008
J9 ORYX
JI Oryx
PD MAR
PY 2021
VL 55
IS 2
BP 197
EP 203
DI 10.1017/S0030605319000577
PG 7
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA RF0WN
UT WOS:000634568800011
OA Green Published, hybrid
DA 2022-02-10
ER

PT J
AU Crunchant, AS
   Egerer, M
   Loos, A
   Burghardt, T
   Zuberbuhler, K
   Corogenes, K
   Leinert, V
   Kulik, L
   Kuhl, HS
AF Crunchant, Anne-Sophie
   Egerer, Monika
   Loos, Alexander
   Burghardt, Tilo
   Zuberbuhler, Klaus
   Corogenes, Katherine
   Leinert, Vera
   Kulik, Lars
   Kuehl, Hjalmar S.
TI Automated face detection for occurrence and occupancy estimation in
   chimpanzees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE animal biometrics; apes; automated image recognition; camera placement;
   site use
ID ESTIMATING SITE OCCUPANCY; DENSITY-ESTIMATION; BUDONGO FOREST; GREAT
   APES; DECLINE; CONSERVATION; MODELS; DUNG
AB Surveying endangered species is necessary to evaluate conservation effectiveness. Camera trapping and biometric computer vision are recent technological advances. They have impacted on the methods applicable to field surveys and these methods have gained significant momentum over the last decade. Yet, most researchers inspect footage manually and few studies have used automated semantic processing of video trap data from the field. The particular aim of this study is to evaluate methods that incorporate automated face detection technology as an aid to estimate site use of two chimpanzee communities based on camera trapping. As a comparative baseline we employ traditional manual inspection of footage. Our analysis focuses specifically on the basic parameter of occurrence where we assess the performance and practical value of chimpanzee face detection software. We found that the semi-automated data processing required only 2-4% of the time compared to the purely manual analysis. This is a non-negligible increase in efficiency that is critical when assessing the feasibility of camera trap occupancy surveys. Our evaluations suggest that our methodology estimates the proportion of sites used relatively reliably. Chimpanzees are mostly detected when they are present and when videos are filmed in high-resolution: the highest recall rate was 77%, for a false alarm rate of 2.8% for videos containing only chimpanzee frontal face views. Certainly, our study is only a first step for transferring face detection software from the lab into field application. Our results are promising and indicate that the current limitation of detecting chimpanzees in camera trap footage due to lack of suitable face views can be easily overcome on the level of field data collection, that is, by the combined placement of multiple high-resolution cameras facing reverse directions. This will enable to routinely conduct chimpanzee occupancy surveys based on camera trapping and semi-automated processing of footage.
C1 [Crunchant, Anne-Sophie; Egerer, Monika; Corogenes, Katherine; Leinert, Vera; Kulik, Lars; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Loos, Alexander] Fraunhofer Inst Digital Media Technol IDMT, Ilmenau, Germany.
   [Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
   [Zuberbuhler, Klaus] Univ Neuchatel, Dept Comparat Cognit, Neuchatel, Switzerland.
   [Zuberbuhler, Klaus] Univ St Andrews, Sch Psychol & Neurosci, St Andrews, Scotland.
   [Zuberbuhler, Klaus] Budongo Conservat Field Stn, Masindi, Uganda.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res IDiv Halle Leip, Leipzig, Germany.
RP Crunchant, AS (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM as.crunchant@gmail.com
RI Zuberbühler, Klaus/A-9053-2011; Egerer, Monika/AAV-6902-2021
OI Zuberbühler, Klaus/0000-0001-8378-088X; Egerer,
   Monika/0000-0002-3304-0725
FU Max Planck Society Innovation Fund; Krekeler Foundation; Robert Bosch
   Foundation; Pact for Research and Innovation
FX Max Planck Society Innovation Fund; Krekeler Foundation; Robert Bosch
   Foundation; Pact for Research and Innovation
CR Andresen L, 2014, J ZOOL, V292, P212, DOI 10.1111/jzo.12098
   Araabi BN, 2000, ANN BIOMED ENG, V28, P1269, DOI 10.1114/1.1317532
   Arandjelovic M, 2010, BIOL CONSERV, V143, P1780, DOI 10.1016/j.biocon.2010.04.030
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Bengsen AJ, 2011, J WILDLIFE MANAGE, V75, P1222, DOI 10.1002/jwmg.132
   Bermejo M, 2006, SCIENCE, V314, P1564, DOI 10.1126/science.1133105
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers D. L., 2002, ESTIMATING ANIMAL AB, P314
   Buckland S.T., 2001, pi
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Campbell G, 2008, CURR BIOL, V18, pR903, DOI 10.1016/j.cub.2008.08.015
   Carlsen F., 2012, W CHIMPANZEE POPULAT, P124
   Dunn A., 2014, REVISED REGIONAL ACT, P49
   EGGELING WJ, 1947, J ECOL, V34, P20, DOI 10.2307/2256760
   Ernst Andreas, 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P279, DOI 10.1109/AVSS.2011.6027337
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Funwi-Gabga N., 2014, STATE APES 2013 EXTR, P252
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
   Greengrass Elizabeth J., 2009, Primate Conservation, V24, P77
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Hughes B., 2015, 26 BRIT MACH VIS C B
   IUCN & ICCN, 2012, BON PAN PAN CONS STR, P65
   Junker J, 2012, DIVERS DISTRIB, V18, P1077, DOI 10.1111/ddi.12005
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Kondgen S, 2008, CURR BIOL, V18, P260, DOI 10.1016/j.cub.2008.01.012
   Kormos R., 2003, REGIONAL ACTION PLAN, P24
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kublbeck C, 2006, IMAGE VISION COMPUT, V24, P564, DOI 10.1016/j.imavis.2005.08.005
   Kuehl H. S., 2008, OCCASIONAL PAPERS IU, P32
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kuehl HS, 2009, BIOL CONSERV, V142, P1500, DOI 10.1016/j.biocon.2009.02.032
   Lahiri M., 2011, P 1 ACM INT C MULT R
   Leendertz FH, 2006, BIOL CONSERV, V131, P325, DOI 10.1016/j.biocon.2006.05.002
   Leendertz FH, 2004, NATURE, V430, P451, DOI 10.1038/nature02722
   Loos A., 2016, THESIS, P279
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Loos A, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P116, DOI 10.1109/ISM.2012.30
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   MacKenzie DI., 2006, OCCUPANCY ESTIMATION, P324
   Macmillan N. A., 2004, DETECTION THEORY USE, P512
   Maldonado O., 2012, GRAUERS GORILLAS AND, P66
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Morgan B, 2011, REGIONAL ACTION PLAN
   Murai M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075024
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   O'Connell A. F., 2012, ANIMAL CONSERVATION, V15, P527
   O'Connell A. F., 2010, CAMERA TRAPS ANIMAL, P271
   Oates J., 2007, Regional action plan for the conservation of the Cross River Gorilla (Gorilla gorilla diehli)
   Oates JF, 1996, AUST J ECOL, V21, P1, DOI 10.1111/j.1442-9993.1996.tb00580.x
   Plumptre A. J., 2010, E CHIMPANZEE PAN TRO, P52
   Plumptre AJ, 2006, PRIMATES, V47, P65, DOI 10.1007/s10329-005-0146-8
   Plumptre AJ, 1996, FOREST ECOL MANAG, V89, P101, DOI 10.1016/S0378-1127(96)03854-6
   Plumptre AJ, 1996, INT J PRIMATOL, V17, P85, DOI 10.1007/BF02696160
   Robinson P.T., 1981, Zoonooz, V54, P7
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Sherley Richard B., 2010, Endangered Species Research, V11, P101, DOI 10.3354/esr00267
   Todd AF, 2008, INT J PRIMATOL, V29, P549, DOI 10.1007/s10764-008-9247-8
   Tuyttens FAM, 2014, ANIM BEHAV, V90, P273, DOI 10.1016/j.anbehav.2014.02.007
   Tweh CG, 2015, ORYX, V49, P710, DOI 10.1017/S0030605313001191
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Walsh PD, 2003, NATURE, V422, P611, DOI 10.1038/nature01566
   Welch G., 2006, TECH REP
   Wich SA, 2008, ORYX, V42, P329, DOI 10.1017/S003060530800197X
   Wich SA, 2014, CURR BIOL, V24, P1659, DOI 10.1016/j.cub.2014.05.077
   Williamson E., 2011, REGIONAL ACTION PLAN, P49
   Williamson E. A., 2014, REVISED REGIONAL ACT, P49
   Woodford MH, 2002, ORYX, V36, P153, DOI 10.1017/S0030605302000224
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151
NR 74
TC 11
Z9 12
U1 1
U2 25
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD MAR
PY 2017
VL 79
IS 3
AR e22627
DI 10.1002/ajp.22627
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA EL5HG
UT WOS:000394651600013
PM 28095593
OA Green Submitted, Green Accepted
DA 2022-02-10
ER

PT C
AU Zotin, AG
   Proskurin, AV
AF Zotin, A. G.
   Proskurin, A., V
BE Zheltov, S
   Knyaz, V
TI ANIMAL DETECTION USING A SERIES OF IMAGES UNDER COMPLEX SHOOTING
   CONDITIONS
SO INTERNATIONAL WORKSHOP ON PHOTOGRAMMETRIC AND COMPUTER VISION TECHNIQUES
   FOR VIDEO SURVEILLANCE, BIOMETRICS AND BIOMEDICINE
SE International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences
LA English
DT Proceedings Paper
CT International Workshop on Photogrammetric and Computer Vision Techniques
   for Video Surveillance, Biometrics and Biomedicine
CY MAY 13-15, 2019
CL Moscow, RUSSIA
DE Animal Detection; Background Modeling; Camera Traps; MSR Algorithm
ID BACKGROUND SUBTRACTION; TRACKING
AB Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection.
C1 [Zotin, A. G.; Proskurin, A., V] Reshetnev Siberian State Univ Sci & Technol, Inst Comp Sci & Telecommun, Krasnoyarsk, Russia.
RP Proskurin, AV (corresponding author), Reshetnev Siberian State Univ Sci & Technol, Inst Comp Sci & Telecommun, Krasnoyarsk, Russia.
EM zotin@sibsau.ru; proskurin.av.wof@gmail.com
RI Zotin, Aleksandr G/Q-3028-2018
OI Zotin, Aleksandr G/0000-0001-9954-9826
FU Russian Foundation for Basic Research, Government of Krasnoyarsk
   Territory, Krasnoyarsk Regional Fund of Science [18-47-240001]
FX The reported study was funded by Russian Foundation for Basic Research,
   Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of
   Science, to the research project 18-47-240001.
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bouwmans T., 2019, ABS190103577 CORR
   Bouwmans T., 2010, HDB PATTERN RECOGNIT, P181
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Chen ML, 2014, LECT NOTES COMPUT SC, V8695, P521, DOI 10.1007/978-3-319-10584-0_34
   Favorskaya M, 2016, SMART INNOV SYST TEC, V55, P121, DOI 10.1007/978-3-319-39345-2_11
   Gonzalez R. C, 2008, DIGITAL IMAGE PROCES, V3rd
   Heikkila M., 2004, BRIT MACH VIS C, P187, DOI DOI 10.5244/C.18.21
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Liao SJ, 2017, PROC SPIE, V10605, DOI 10.1117/12.2295105
   Liu HJ, 2016, CHIN CONT DECIS CONF, P3712, DOI 10.1109/CCDC.2016.7531629
   Manzanera A, 2007, LECT NOTES COMPUT SC, V4756, P42
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Castelblanco LP, 2017, PROC SPIE, V10341, DOI 10.1117/12.2268732
   Raju A., 2013, International Journal of Signal Processing, Image Processing and Pattern Recognition, V6, P353, DOI [10.14257/ijsip.2013.6.5.31, DOI 10.14257/IJSIP.2013.6.5.31]
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Van Droogenbroeck M., 2014, VIBE DISRUPTIVE METH
   Wang B, 2014, IEEE COMPUT SOC CONF, P401, DOI 10.1109/CVPRW.2014.64
   Wang HZ, 2007, PATTERN RECOGN, V40, P1091, DOI 10.1016/j.patcog.2006.05.024
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zotin Alexander, 2018, Procedia Computer Science, V131, P6, DOI 10.1016/j.procs.2018.04.179
NR 26
TC 1
Z9 1
U1 2
U2 2
PU INTL SOC PHOTOGRAMMETRY & REMOTE SENSING-ISPRS
PI HANNOVER
PA LEIBNIZ UNIV HANNOVER, INST PHOTOGRAMMETRY  & GEOINFORMATION, NIENBURGER
   STR 1, HANNOVER, HANNOVER, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PY 2019
VL 42-2
IS W12
BP 249
EP 257
DI 10.5194/isprs-archives-XLII-2-W12-249-2019
PG 9
WC Computer Science, Artificial Intelligence; Mathematical & Computational
   Biology; Remote Sensing; Imaging Science & Photographic Technology;
   Radiology, Nuclear Medicine & Medical Imaging
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematical & Computational Biology; Remote Sensing;
   Imaging Science & Photographic Technology; Radiology, Nuclear Medicine &
   Medical Imaging
GA BQ2ZX
UT WOS:000582728500039
OA Green Submitted, gold
DA 2022-02-10
ER

PT J
AU Allen, ML
   Harris, RE
   Olson, LO
   Olson, ER
   Van Stappen, J
   Van Deelen, TR
AF Allen, Maximilian L.
   Harris, Rachel E.
   Olson, Lucas O.
   Olson, Erik R.
   Van Stappen, Julie
   Van Deelen, Timothy R.
TI Resource limitations and competitive interactions affect carnivore
   community composition at different ecological scales in a temperate
   island system
SO MAMMALIA
LA English
DT Article
DE American marten; camera trap; carnivores; community ecology; island
   biogeography; Martes americana; scale
ID MARTENS MARTES-AMERICANA; POPULATION; FISHERS; GUILD
AB Selective pressures (i.e. resource limitation and competitive interaction) that drive the composition of ecological communities vary, and often operate on different ecological scales (ecological variables across varying spatial scales) than observed patterns. We studied the drivers of distribution and abundance of the American marten (Martes americana) and the carnivore community at three ecological scales on a Great Lakes island archipelago using camera traps. We found different drivers appeared important at each ecological scale and studying any of the three scales alone would give a biased understanding of the process driving the system. Island biogeography (resource limitation) was most important for carnivore richness, with higher richness on larger islands and lower richness as distance from the mainland increased. Marten presence on individual islands appeared to be driven by island size (resource limitation) and human avoidance (competitive interaction). Marten abundance at camera trap sites was driven by the cascading effect of coyotes (Canis latrans) on fishers (Pekania pennanti) (competitive interaction). Incorporating three ecological scales gave novel insights into the varying effects of resource limitation and competitive interaction processes. Our data suggests that ecological communities are structured through multiple competing ecological forces, and effective management and conservation relies on our ability to understand ecological forces operating at multiple ecological scales.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Harris, Rachel E.; Olson, Lucas O.; Van Deelen, Timothy R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, Madison, WI USA.
   [Olson, Erik R.] Northland Coll, Nat Resources, 1411 Ellis Ave S, Ashland, WI 54806 USA.
   [Van Stappen, Julie] Apostle Isl Natl Lakeshore, Planning & Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Apostle Islands National Lakeshore (GLNF CESU) [P14AC01180]; Northland
   College (Department of Natural Resources); Northland College (Sigurd
   Olson Professorship in the Natural Sciences); Northland College (Morris
   O. Ristvedt Professorship in the Natural Sciences); University of
   Wisconsin (Schorger fund, Department of Forest and Wildlife Ecology);
   University of Wisconsin (Beers-Bascom Professorship in Conservation)
FX This project was supported by the Apostle Islands National Lakeshore
   (GLNF CESU Agreement P14AC01180), Northland College (Department of
   Natural Resources; Sigurd Olson Professorship in the Natural Sciences;
   Morris O. Ristvedt Professorship in the Natural Sciences), and the
   University of Wisconsin (Schorger fund, Department of Forest and
   Wildlife Ecology; Beers-Bascom Professorship in Conservation). We thank
   students and technicians from the National Park Service, University of
   Wisconsin-Madison, and Northland College for their assistance with
   fieldwork.
CR Allen ML, 2018, COMMUNITY ECOL, V19, P272, DOI 10.1556/168.2018.19.3.8
   Allen ML, 2018, AM MIDL NAT, V179, P294
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Anderson K. A., 2002, MODEL SELECTION MULT
   [Anonymous], 2016, WISCLAND 2 0 USER GU
   Brown JS, 1999, J MAMMAL, V80, P385, DOI 10.2307/1383287
   Carlson JE, 2014, J WILDLIFE MANAGE, V78, P1499, DOI 10.1002/jwmg.785
   Emerson BC, 2008, TRENDS ECOL EVOL, V23, P619, DOI 10.1016/j.tree.2008.07.005
   ESRI, 2014, ARCGIS DESKT
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Fisher JT, 2013, ECOGRAPHY, V36, P240, DOI 10.1111/j.1600-0587.2012.07556.x
   Gilbert Jonathan H., 1997, P135
   HAIRSTON NG, 1960, AM NAT, V94, P421, DOI 10.1086/282146
   Howk F, 2009, J GREAT LAKES RES, V35, P159, DOI 10.1016/j.jglr.2008.11.002
   Jackman, 2015, PSCL CLASSES METHODS
   Judziewicz E.J., 1993, MICHIGAN BOT, V32, P43
   Leibold MA, 2004, ECOL LETT, V7, P601, DOI 10.1111/j.1461-0248.2004.00608.x
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   LEVIN SA, 1992, ECOLOGY, V73, P1943, DOI 10.2307/1941447
   MAC ARTHUR ROBERT H., 1967
   Naing H, 2015, RAFFLES B ZOOL, V63, P376
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Oksanen L, 2000, AM NAT, V155, P703, DOI 10.1086/303354
   POWER ME, 1992, ECOLOGY, V73, P733, DOI 10.2307/1940153
   R Core Team, 2017, R LANG ENV STAT COMP
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   RUGGIERO LF, 1994, CONSERV BIOL, V8, P364, DOI 10.1046/j.1523-1739.1994.08020364.x
   Shnberloff D.S., 1974, Annual Rev Ecol Syst, V5, P161
   Thompson ID., 2012, BIOL CONSERVATION MA, P209
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Whittaker R.J., 2007, ISLAND BIOGEOGRAPHY, V2, P416
   Woodford J.E., 2011, CONSERVATION MANAGEM, P43
   Zeileis A, 2008, J STAT SOFTW, V27, P1, DOI 10.18637/jss.v027.i08
   Zielinski WJ, 2008, J WILDLIFE MANAGE, V72, P1558, DOI 10.2193/2007-397
   Zielinski WJ, 2004, J MAMMAL, V85, P470, DOI 10.1644/1545-1542(2004)085<0470:DOSPOA>2.0.CO;2
NR 36
TC 8
Z9 8
U1 0
U2 9
PU WALTER DE GRUYTER GMBH
PI BERLIN
PA GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY
SN 0025-1461
EI 1864-1547
J9 MAMMALIA
JI Mammalia
PD NOV
PY 2019
VL 83
IS 6
BP 552
EP 561
DI 10.1515/mammalia-2017-0162
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA JL9SZ
UT WOS:000495866900004
DA 2022-02-10
ER

PT J
AU Askerov, E
   Trepet, SA
   Eskina, TG
   Bibina, KV
   Narkevich, AI
   Pkhitikov, AB
   Zazanashvili, N
   Akhmadova, K
AF Askerov, E.
   Trepet, S. A.
   Eskina, T. G.
   Bibina, K., V
   Narkevich, A., I
   Pkhitikov, A. B.
   Zazanashvili, N.
   Akhmadova, K.
TI ESTIMATION OF THE POPULATION DENSITIES OF SPECIES PREY OR COMPETITOR TO
   THE LEOPARD (PANTHERA PARDUS) IN THE HYRCAN NATIONAL PARK, AZERBAIJAN
SO ZOOLOGICHESKY ZHURNAL
LA Russian
DT Article
DE Talysh Mountains; camera trap; abundance index; random encounter model
ID CAMERA TRAPS; PHOTOGRAPHIC RATES; PATTERNS; RANGE; PORCUPINES;
   ABUNDANCE; CAUCASUS; MOVEMENT; RESERVE; TIGERS
AB Based on the random encounter model, the population densities of species potentially prey or competitor to the leopard (Panthera pardus) were estimated in the Hyrcan National Park, Azerbaijan. Data obtained from 18 camera traps were processed, 11 of which were installed in the southern part of the park, and 7 in the northern part. The total operating time of the cameras amounted to 3950 traps per day for the period from April 2018 to December 2019. A very high Wild boar population density was revealed in the southern part of the park (32.5 ind./1000 ha). On the contrary, no Roe deer was recorded in the southern part, while it had a rather low density in the northern part (4.4 ind./1000 ha). The population densities of the Golden jackal (0.6 ind./1000 ha), the Grey wolf (0.1 ind./1000 ha), the Jungle cat (6.7 ind./1000 ha), and the Raccoon (46.9 ind./1000 ha) ranged within the normal limits characteristic of similar populations in other parts of their global distribution. The population densities of the Indian porcupine (9.7 ind./1000 ha) and the Brown bear (0.4 ind./1000 ha) seem to have also corresponded to the mountain forest conditions of the study area. From a view point of food supply, the territory of the Hyrcan National Park is quite suitable for permanently supporting 3-4 leopard individuals.
C1 [Askerov, E.; Akhmadova, K.] WWF Azerbaijan Off, Baku 1001, Azerbaijan.
   [Askerov, E.] Azerbaijan Natl Acad Sci, Inst Zool, Baku 1073, Azerbaijan.
   [Askerov, E.; Zazanashvili, N.] Ilia State Univ, GE-0162 Tbilisi, Georgia.
   [Trepet, S. A.; Pkhitikov, A. B.] Russian Acad Sci, Tembotov Inst Ecol Mt Terr, Nalchik 360051, Russia.
   [Trepet, S. A.; Eskina, T. G.; Bibina, K., V; Narkevich, A., I] Shaposhnikov Caucasian State Biosphere Nat Reserv, Soci 354340, Russia.
   [Zazanashvili, N.] WWF Caucasus Programme Off, GE-0193 Tbilisi, Georgia.
RP Trepet, SA (corresponding author), Russian Acad Sci, Tembotov Inst Ecol Mt Terr, Nalchik 360051, Russia.
EM trepetsergey@gmail.com
CR Acosta-Pankov I., 2019, LYNX PRAHA, V50, P113
   Anile S., 2012, Wildlife Biology in Practice, V8, P1
   [Anonymous], 2017, STRATEGY CONSERVATIO
   Askerov E, 2019, ZOOL MIDDLE EAST, V65, P88, DOI 10.1080/09397140.2018.1552349
   Askerov E, 2015, ZOOL MIDDLE EAST, V61, P95, DOI 10.1080/09397140.2015.1035003
   Banea OC, 2012, ACTA ZOOL BULGAR, V64, P353
   Belousova A.V., 1993, Lutreola (Moscow), V2, P16
   Bragg CJ, 2005, J ARID ENVIRON, V61, P261, DOI 10.1016/j.jaridenv.2004.09.007
   Breitenmoser U, 2014, RECOVERY LEOPARD AZE
   Butterfield Robert T., 1944, TRANS NORTH AMER WILDLIFE CONF, V9, P337
   Carbone C, 2001, ANIM CONSERV, V4, P75, DOI 10.1017/S1367943001001081
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Gray TNE, 2018, J ZOOL, V305, P173, DOI 10.1111/jzo.12547
   Hazaryan, 2004, BEITRAGE JAGD WILDFO, V29, P303
   Jennelle CS, 2002, ANIM CONSERV, V5, P119, DOI 10.1017/S1367943002002160
   Jourdain NOAS, 2020, J AGR BIOL ENVIR ST, V25, P148, DOI 10.1007/s13253-020-00385-4
   Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
   LAWTON JH, 1993, TRENDS ECOL EVOL, V8, P409, DOI 10.1016/0169-5347(93)90043-O
   LOMOLINO MV, 1995, J MAMMAL, V76, P335, DOI 10.2307/1382345
   Lukarevsky Victor, 2007, Cat News, P15
   Maharramova Elmira, 2018, Cat News, V67, P8
   Majumder Aniruddha, 2011, Journal of Threatened Taxa, V3, P2221
   Nickerson B.S., 2019, ESTIMATING POPULATIO, DOI 10.13140/RG.2.2.28655.18083
   Pfeffer SE, 2018, REMOTE SENS ECOL CON, V4, P173, DOI 10.1002/rse2.67
   Romani T, 2018, MAMMAL RES, V63, P477, DOI 10.1007/s13364-018-0386-9
   Rotem G., 2008, EFFECT LANDSCAPE HET
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Sanei Arezoo, 2016, Cat News, P43
   SEVER Z, 1991, MAMMALIA, V55, P187, DOI 10.1515/mamm.1991.55.2.187
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Spassov N., 2020, LYNX N S PRAHA, V50, P113
   Strampelli P, 2020, ORYX, V54, P405, DOI 10.1017/S0030605318000121
   Zazanashvili N., 2007, CAT NEWS, V2, P4
NR 35
TC 0
Z9 0
U1 1
U2 1
PU MAIK NAUKA-INTERPERIODICA PUBL
PI MOSCOW
PA GSP-1, MARONOVSKII PER 26, MOSCOW, 119049, RUSSIA
SN 0044-5134
J9 ZOOL ZH
JI Zool. Zhurnal
PD AUG
PY 2021
VL 100
IS 8
BP 947
EP 955
DI 10.31857/S0044513421080031
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA UX0RA
UT WOS:000700555000009
DA 2022-02-10
ER

PT J
AU Farmer, MJ
   Allen, ML
   Olson, ER
   Van Stappen, J
   Van Deelen, TR
AF Farmer, M. J.
   Allen, M. L.
   Olson, E. R.
   Van Stappen, J.
   Van Deelen, T. R.
TI Agonistic interactions and island biogeography as drivers of carnivore
   spatial and temporal activity at multiple scales
SO CANADIAN JOURNAL OF ZOOLOGY
LA English
DT Article
DE Apostle Islands; camera traps; carnivores; island biogeography; spatial
   segregation; temporal segregation
ID INTRAGUILD PREDATION; RED FOXES; COMPETITION; COYOTES; EXAMPLE; PREY;
   COLONIZATION; COEXISTENCE; OCCUPANCY; FISHERS
AB Carnivore communities can be diverse and complex, and lack of knowledge regarding intraguild interactions and alternative drivers of carnivore distributions can preclude effective conservation of co-occurring species. As such, our objectives were to evaluate the relative importance of intraguild interactions and island biogeography to carnivore community spatiotemporal activity at multiple spatial scales. We monitored the carnivore community of the Apostle Islands National Lakeshore (Wisconsin, USA) using a grid of camera traps from 2014 to 2018. We used generalized linear mixed-effects models and information-theoretic model selection to evaluate whether subordinate carnivore presence was related to dominant carnivore relative abundance (interactions) or to island biogeography at the island level and camera site level, and we calculated temporal overlap between each pair of species to determine whether subordinate carnivores were using temporal segregation. At the island level, the relative importance of interactions and island biogeography was species dependent. At the site level, relative abundance of dominant carnivores was not a significant predictor of subordinate carnivore presence, and all pairs exhibited high or neutral temporal overlap. At the island level, island biogeography and interactions may both impact species distributions; however, at finer spatial scales, the carnivore community may be using alternative segregation strategies, or the island system may preclude segregation.
C1 [Farmer, M. J.; Van Deelen, T. R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Allen, M. L.] Univ Illinois, Illinois Nat Hist Survey, 1816 South Oak St, Champaign, IL 61820 USA.
   [Olson, E. R.] Northland Coll, Nat Resources, 1411 Ellis Ave South, Ashland, WI 54806 USA.
   [Van Stappen, J.] Apostle Isl Natl Lakeshore, Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
RP Farmer, MJ (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM mjmorales@wisc.edu
FU Apostle Islands National Lakeshore, Bayfield, Wisconsin (GLNF CESU)
   [P14AC01180]; Northland College, Ashland, Wisconsin (Department of
   Natural Resources); NASA Earth and Space Science Fellowship
   [NNX16AO61H]; University of Wisconsin-Madison, Madison (Schorger fund,
   Department of Forest and Wildlife Ecology, Beers-Bascom Professorship in
   Conservation); Northland College, Ashland, Wisconsin (Morris O. Ristvedt
   Professorship in the Natural Sciences); Northland College, Ashland,
   Wisconsin (Sigurd Olson Professorship in the Natural Sciences)
FX We thank the students, employees, and volunteers from each group who
   contributed to this project by deploying and checking camera traps,
   tagging the camera photographs, and providing boat transport. We also
   acknowledge and thank K.M. Gaynor for providing R code to create the
   temporal overlap figures. This work was supported by the Apostle Islands
   National Lakeshore, Bayfield, Wisconsin (GLNF CESU Agreement
   P14AC01180); Northland College, Ashland, Wisconsin (Department of
   Natural Resources, Sigurd Olson Professorship in the Natural Sciences,
   Morris O. Ristvedt Professorship in the Natural Sciences); NASA Earth
   and Space Science Fellowship (grant number NNX16AO61H); and the
   University of Wisconsin-Madison, Madison (Schorger fund, Department of
   Forest and Wildlife Ecology, Beers-Bascom Professorship in
   Conservation).
CR Adams JR, 2011, P ROY SOC B-BIOL SCI, V278, P3336, DOI 10.1098/rspb.2011.0261
   Allen ML, 2020, ANIM BIODIV CONSERV, V43, P97, DOI 10.32800/abc.2020.43.0097
   Allen ML, 2018, COMMUNITY ECOL, V19, P272, DOI 10.1556/168.2018.19.3.8
   Allen ML, 2019, MAMMALIA, V83, P552, DOI 10.1515/mammalia-2017-0162
   Allen ML, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102257
   Anderson D.R, 2002, TECHNOMETRICS
   Arim M, 2004, ECOL LETT, V7, P557, DOI 10.1111/j.1461-0248.2004.00613.x
   Bates D, 2013, J STAT SOFTW, V52, P1, DOI 10.18637/jss.v052.i05
   Berger J, 2001, ECOL APPL, V11, P947, DOI 10.1890/1051-0761(2001)011[0947:AMPPIG]2.0.CO;2
   COLE BJ, 1981, AM NAT, V117, P629, DOI 10.1086/283749
   CRAVEN S R, 1987, Colonial Waterbirds, V10, P64, DOI 10.2307/1521232
   DIAMOND JM, 1974, SCIENCE, V184, P803, DOI 10.1126/science.184.4138.803
   Durant SM, 1998, J ANIM ECOL, V67, P370, DOI 10.1046/j.1365-2656.1998.00202.x
   Esri Inc., 2017, ARCMAP VERS 10 5 1
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Farmer, 2019, URBAN NAT, V29, P1
   Farmer M.J, 2020, THESIS U WISCONSIN M
   Fedriani JM, 2000, OECOLOGIA, V125, P258, DOI 10.1007/s004420000448
   Fisher JT, 2013, ECOGRAPHY, V36, P240, DOI 10.1111/j.1600-0587.2012.07556.x
   Gosselink TE, 2003, J WILDLIFE MANAGE, V67, P90, DOI 10.2307/3803065
   HARRISON DJ, 1989, J WILDLIFE MANAGE, V53, P181, DOI 10.2307/3801327
   Henke SE, 1999, J WILDLIFE MANAGE, V63, P1066, DOI 10.2307/3802826
   Jenks KE, 2011, TROP CONSERV SCI, V4, P113, DOI 10.1177/194008291100400203
   Judziewicz E.J., 1993, MICHIGAN BOT, V32, P43
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Lanszki J, 2011, ACTA ZOOL ACAD SCI H, V57, P291
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   LEVIN SA, 1992, ECOLOGY, V73, P1943, DOI 10.2307/1941447
   Linnell John D. C., 2000, Diversity and Distributions, V6, P169, DOI 10.1046/j.1472-4642.2000.00069.x
   Lomolino M.V., 1988, P185
   Lomolino MV, 2000, GLOBAL ECOL BIOGEOGR, V9, P1, DOI 10.1046/j.1365-2699.2000.00185.x
   MacArthur R.H., 1967, THEORY ISLAND BIOGEO
   Manlick PJ, 2017, J MAMMAL, V98, P690, DOI 10.1093/jmammal/gyx030
   Mcdonald RA, 2002, J ANIM ECOL, V71, P185, DOI 10.1046/j.1365-2656.2002.00588.x
   Meredith M., 2017, OVERVIEW OVERLAP PAC
   Millien-Parra V, 1999, J BIOGEOGR, V26, P959, DOI 10.1046/j.1365-2699.1999.00346.x
   Monterroso P, 2016, J MAMMAL, V97, P928, DOI 10.1093/jmammal/gyw016
   Mueller MA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190971
   NCEI, 2019, CLIM DAT ONL
   Neale JCC, 2001, CAN J ZOOL, V79, P1794, DOI 10.1139/cjz-79-10-1794
   Nouvellet P, 2012, J ZOOL, V286, P179, DOI 10.1111/j.1469-7998.2011.00864.x
   Olson ER, 2019, CAN J ZOOL, V97, P1030, DOI 10.1139/cjz-2018-0254
   Palomares F, 1999, AM NAT, V153, P492, DOI 10.1086/303189
   Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
   Preisser EL, 2005, ECOLOGY, V86, P501, DOI 10.1890/04-0719
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Radloff FGT, 2004, J ANIM ECOL, V73, P410, DOI 10.1111/j.0021-8790.2004.00817.x
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Robin X, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-77
   Sax D.F., 2011, THEORY ECOLOGY, P219
   Schmitz OJ, 2010, ECOL LETT, V13, P1199, DOI 10.1111/j.1461-0248.2010.01511.x
   SIMBERLOFF D, 1978, AM NAT, V112, P713, DOI 10.1086/283313
   Sollmann R, 2018, AFR J ECOL, V56, P740, DOI 10.1111/aje.12557
   Swanson A, 2014, J ANIM ECOL, V83, P1418, DOI 10.1111/1365-2656.12231
   Vanak AT, 2013, ECOLOGY, V94, P2619, DOI 10.1890/13-0217.1
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
NR 57
TC 0
Z9 0
U1 3
U2 5
PU CANADIAN SCIENCE PUBLISHING
PI OTTAWA
PA 65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA
SN 0008-4301
EI 1480-3283
J9 CAN J ZOOL
JI Can. J. Zool.
PD APR
PY 2021
VL 99
IS 4
BP 309
EP 317
DI 10.1139/cjz-2020-0195
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA RN6HJ
UT WOS:000640452000008
DA 2022-02-10
ER

PT J
AU Allen, ML
   Ward, MP
   Juznic, D
   Krofel, M
AF Allen, Maximilian L.
   Ward, Michael P.
   Juznic, Damjan
   Krofel, Miha
TI SCAVENGING BY OWLS: A GLOBAL REVIEW AND NEW OBSERVATIONS FROM EUROPE AND
   NORTH AMERICA
SO JOURNAL OF RAPTOR RESEARCH
LA English
DT Review
DE Great Horned Owl; Bubo virginianus; Barred Owl; Strix varia; Ural Owl;
   Strix uralensis; camera trap; diet; owl; pellet analysis; scavenging
ID CARRION CONSUMPTION; 1ST RECORD; PREY; ACQUISITION; MORTALITY;
   PREDATION; FOREST; BUBO
AB Scavenging is an important ecological function that increases individual fitness and transfers energy between trophic levels. Scavenging by owls has been documented opportunistically through direct observations, camera trapping, and pellet analyses, but it is unknown how frequent or widespread the behavior is. We documented three new scavenging events front North America and Europe, and also performed a systematic literature review of the reports documenting scavenging by owls. The number of such reports was similar in each decade from the 1970s to the 2000s, but the decade of the 2010s had more reports than all previous decades combined. Owls scavenged primarily on mammals (81%), followed by birds (16%) and reptiles (3%); almost half (47%) of carrion scavenged were Artiodactyla (hoofed mammals) and most of the species scavenged were larger than the feeding owl. Most reports documenting scavenging by owls were from either Europe ( n= 14) or North America (n=11), with few reports from Asia (n=2), South America ( n =2), or Australia (n=1), and none from Africa. The most frequent type of report was direct observations (n= 14), followed by camera trapping (n=9) and pellet analyses (n=6). Our review indicates that scavenging is a widespread behavior among owl species, but most observations were opportunistic, suggesting that additional incidents of scavenging by owls are likely unobserved. Further research is needed to establish the frequency of scavenging by owls and the effects that scavenging may have on owl populations and scavenging communities.
C1 [Allen, Maximilian L.; Ward, Michael P.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Ward, Michael P.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 S Goodwin, Urbana, IL 61801 USA.
   [Juznic, Damjan; Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Vecna Pot 83, SI-1000 Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Illinois Natural History Survey; Slovenian Research AgencySlovenian
   Research Agency - Slovenia [P4-0059]
FX Funding for the study was generously provided by The Illinois Natural
   History Survey and the Slovenian Research Agency (P4-0059).
CR Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Allen ML, 2013, WILSON J ORNITHOL, V125, P417, DOI 10.1676/12-176.1
   Anza J, 2015, REV BRAS ORNITOL, V23, P377
   Bishop CA, 2013, AVIAN CONSERV ECOL, V8, DOI 10.5751/ACE-00604-080202
   Borda-de-Agua L, 2014, ECOL MODEL, V276, P29, DOI 10.1016/j.ecolmodel.2013.12.022
   Boves TJ, 2012, J WILDLIFE MANAGE, V76, P1381, DOI 10.1002/jwmg.378
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   del Hoyo J, 1999, HDB BIRDS WORLD BARN
   Diaz-Ruiz F, 2010, J RAPTOR RES, V44, P78, DOI 10.3356/JRR-09-21.1
   DunningJr J. B., 2007, CRC HDB AVIAN BODY M, VSecond
   Dunsire C., 1978, Scottish Birds, V10, P56
   Graves GR, 2006, J RAPTOR RES, V40, P175, DOI 10.3356/0892-1016(2006)40[175:PPOGGO]2.0.CO;2
   Hebblewhite Mark, 2010, P69
   Hiraldo F., 1975, Doiiana Acta vert, V2, P161
   Jedrzejewska B., 1998, PREDATION VERTEBRATE
   Kapfer JM, 2011, WILSON J ORNITHOL, V123, P646, DOI 10.1676/11-015.1
   Konig C., 1999, OWLS GUIDE OWLS WORL
   KORPIMAKI E, 1987, IBIS, V129, P499, DOI 10.1111/j.1474-919X.1987.tb08237.x
   Krofel M., 2005, ACROCEPHALUS, V26, P49
   Krofel Miha, 2011, Acrocephalus, V32, P45, DOI 10.2478/v10100-011-0003-3
   Leditznig Christoph, 2001, Egretta, V44, P45
   Macdonald D, 1984, ENCY MAMMALS
   MARTI CD, 1979, AUK, V96, P319
   Milchev Boyan, 2017, Ornis Hungarica, V25, P58
   Mori E, 2014, ITAL J ZOOL, V81, P471, DOI 10.1080/11250003.2014.920928
   Mori E, 2015, BIRD STUDY, V62, P257, DOI 10.1080/00063657.2015.1013522
   Mueller MA, 2019, LANDSCAPE URBAN PLAN, V189, P362, DOI 10.1016/j.landurbplan.2019.04.023
   Mueller MA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190971
   Novoa Fernando, 2016, Revista Chilena de Ornitologia, V22, P200
   Patterson J. Micheal, 2007, Northwestern Naturalist, V88, P12, DOI 10.1898/1051-1733(2007)88[12:AAOSOB]2.0.CO;2
   Peers Michael J.L., 2018, Northwestern Naturalist, V99, P232
   Peers MJL, 2017, WILSON J ORNITHOL, V129, P875, DOI 10.1676/16-170.1
   Pleshak T.V., 1998, Russkii Ornitologicheskii Zhurnal Ekspress Vypusk, V44, P12
   Proudfoot GA, 1997, WILSON BULL, V109, P741
   SEEBECK J H, 1976, Emu, V76, P167
   Selva N, 2007, P ROY SOC B-BIOL SCI, V274, P1101, DOI 10.1098/rspb.2006.0232
   Serrano David, 2000, Ardeola, V47, P101
   Smallwood KS, 2010, J WILDLIFE MANAGE, V74, P1089, DOI 10.2193/2009-266
   Smith D.G., 1974, Canadian Field Naturalist, V88, P96
   van de Velde E., 1980, Veldornitologisch Tijdschrift, V3, P137
   Varland DE, 2018, J RAPTOR RES, V52, P291, DOI 10.3356/JRR-17-38.1
   Walker LE, 2018, ECOL EVOL, V8, P11158, DOI 10.1002/ece3.4583
   Wassink Gejo, 2003, Limosa, V76, P1
   Welch Stephen, 2012, Scottish Birds, V32, P300
   Wilmers CC, 2003, J ANIM ECOL, V72, P909, DOI 10.1046/j.1365-2656.2003.00766.x
   Wilson EE, 2011, TRENDS ECOL EVOL, V26, P129, DOI 10.1016/j.tree.2010.12.011
NR 46
TC 3
Z9 3
U1 3
U2 34
PU RAPTOR RESEARCH FOUNDATION INC
PI HASTINGS
PA 14377 117TH STREET SOUTH, HASTINGS, MN 55033 USA
SN 0892-1016
EI 2162-4569
J9 J RAPTOR RES
JI J. Raptor Res.
PD DEC
PY 2019
VL 53
IS 4
BP 410
EP 418
DI 10.3356/0892-1016-53.4.410
PG 9
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA JN2KV
UT WOS:000496730800006
DA 2022-02-10
ER

PT J
AU de Oliveira, ML
   Peres, PHD
   Gatti, A
   Morales-Donoso, JA
   Mangini, PR
   Duarte, JMB
AF de Oliveira, Marcio Leite
   de Faria Peres, Pedro Henrique
   Gatti, Andressa
   Morales-Donoso, Jorge Alfonso
   Mangini, Paulo Rogerio
   Duarte, Jose Mauricio Barbanti
TI Faecal DNA and camera traps detect an evolutionarily significant unit of
   the Amazonian brocket deer in the Brazilian Atlantic Forest
SO EUROPEAN JOURNAL OF WILDLIFE RESEARCH
LA English
DT Article
DE Conservation; Cytochrome b; Detection dogs; Mazama nemorivaga
ID HAIR-TRAP; CYTOGENETIC DESCRIPTION; MITOCHONDRIAL; ARTIODACTYLA;
   CONSERVATION; PHYLOGENY; MAMMALIA; DENSITY
AB The Amazonian grey brocket deer (Mazama nemorivaga) is a large mammal species that until now has been assumed to be limited to the Amazon region and has not been categorized to be threatened. In this study, we provide evidences, obtained by camera traps and faecal DNA, of the existence of two populations of this species in the Brazilian Atlantic Forest, more than a thousand kilometres away from its assumed distribution limit. Furthermore, we employed genetic analysis to identify the collected faecal samples using detection dogs in six protected areas that were within 500 km of the first photographic records. Phylogenetic analysis, performed on hair samples, indicated that these populations were genetically related to the M. nemorivaga population of the western Amazon. The discovery of these populations emphasizes the importance of noninvasive techniques for species detection of elusive or rare populations. It is necessary to re-evaluate the conservation status of this species, with special attention to the detected populations (Linhares-Sooretama forest complex and the Una Biological Reserve). The conservation of these two new populations of evolutionarily significant units is urgent, and we recommend the adoption of measures against highly impacting deer threats, such as hunting and predation by domestic dogs. Finally, before any drastic population management is taken, it is necessary to determine whether there is historical or recent genetic isolation among the M. nemorivaga populations of the Atlantic Forest.
C1 [de Oliveira, Marcio Leite; de Faria Peres, Pedro Henrique; Morales-Donoso, Jorge Alfonso; Duarte, Jose Mauricio Barbanti] Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
   [Gatti, Andressa] Inst Pesquisas Mata Atlantica IPEMA, Vitoria, ES, Brazil.
   [Mangini, Paulo Rogerio] Brazilian Inst Conservat Med TRIADE, Curitiba, Parana, Brazil.
RP de Oliveira, ML (corresponding author), Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
EM oliveiraml1@yahoo.com.br
RI de Oliveira, Márcio L/F-3792-2012; Duarte, José Maurício
   Barbanti/AAG-5149-2019
OI de Oliveira, Márcio L/0000-0002-7705-0626; Duarte, José Maurício
   Barbanti/0000-0002-7805-0265; Peres, Pedro Henrique/0000-0002-3158-0963;
   Morales Donoso, Jorge Alfonso/0000-0002-1684-1512; Mangini, Paulo
   Rogerio/0000-0002-2912-2242
FU FAPESPFundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)
   [15/25742-5, 17/02200-8, 17/07014-8]; CNPqConselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPQ) [302368/2018-3]
FX This study was funded by FAPESP (15/25742-5, 17/02200-8, 17/07014-8) and
   CNPq (302368/2018-3).
CR Abril VV, 2010, CYTOGENET GENOME RES, V128, P177, DOI 10.1159/000298819
   Duarte JMB, 2017, ORYX, V51, P656, DOI 10.1017/S0030605316000405
   Batalha H, 2013, J ORNITHOL, V154, P41, DOI 10.1007/s10336-012-0866-7
   Beja-Pereira A, 2009, MOL ECOL RESOUR, V9, P1279, DOI 10.1111/j.1755-0998.2009.02699.x
   Bickford D, 2007, TRENDS ECOL EVOL, V22, P148, DOI 10.1016/j.tree.2006.11.004
   Biondo Cibele, 2010, Suiform Soundings, V10, P24
   Bollback T, 2004, DNA EXTRACTION PROTO
   Bovendorp Ricardo S, 2017, Ecology, V98, P2226, DOI 10.1002/ecy.1893
   Buso AA, 2013, RADIOCARBON, V55, P1747, DOI 10.1017/S0033822200048669
   Carstens BC, 2013, MOL ECOL, V22, P4369, DOI 10.1111/mec.12413
   Castro-Arellano I, 2008, J WILDLIFE MANAGE, V72, P1405, DOI 10.2193/2007-476
   Cullingham CI, 2010, J WILDLIFE MANAGE, V74, P849, DOI 10.2193/2008-292
   Cursino MS, 2014, BMC EVOL BIOL, V14, DOI 10.1186/1471-2148-14-40
   Darriba D, 2012, NAT METHODS, V9, P772, DOI 10.1038/nmeth.2109
   de Oliveira ML, 2012, ZOOLOGIA-CURITIBA, V29, P183, DOI 10.1590/S1984-46702012000200012
   de Oliveira ML, 2016, MAMM BIOL, V81, P281, DOI 10.1016/j.mambio.2016.01.004
   De Queiroz K, 2007, SYST BIOL, V56, P879, DOI 10.1080/10635150701701083
   de Souza JN, 2013, CONSERV GENET RESOUR, V5, P639, DOI 10.1007/s12686-013-9870-3
   Ledo RMD, 2017, J BIOGEOGR, V44, P2551, DOI 10.1111/jbi.13049
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2003, MAMMALIA, V67, P403, DOI 10.1515/mamm.2003.67.3.403
   Duarte JMB, 2015, THE IUCN RED LIST OF, DOI [10.2305/IUCN.UK.2015-4.RLTS.T29621A22154379.en, DOI 10.2305/IUCN.UK.2015-4.RLTS.T29621A22154379.EN]
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Escobedo-Morales LA, 2016, MAMM BIOL, V81, P303, DOI 10.1016/j.mambio.2016.02.003
   Fiorillo BF, 2013, COMP CYTOGENET, V7, P25, DOI 10.3897/CompCytogen.v7i1.4314
   Gilbert C, 2006, MOL PHYLOGENET EVOL, V40, P101, DOI 10.1016/j.ympev.2006.02.017
   Gonzalez S, 2009, MOL ECOL RESOUR, V9, P754, DOI 10.1111/j.1755-0998.2008.02390.x
   Grotta-Neto F, 2019, J MAMMAL, V100, P454, DOI 10.1093/jmammal/gyz056
   Gutierrez EE, 2017, ZOOKEYS, P87, DOI 10.3897/zookeys.697.15124
   Gutierrez EE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129113
   Hall T.A., 1999, NUCL ACIDS S SERIES, V41, P95, DOI DOI 10.1021/BK-1999-0734.CH008
   Hebert PDN, 2003, P ROY SOC B-BIOL SCI, V270, pS96, DOI 10.1098/rsbl.2003.0025
   Heckeberg NS, 2016, PEERJ, V4, DOI 10.7717/peerj.2307
   Instituto Brasileiro de Geografia e Estatistica (IBGE), 1992, MAP VEG BRAS
   Kamgaing TOW, 2018, AFR J ECOL, V56, P908, DOI 10.1111/aje.12518
   Karanth KU, 2004, ANIM CONSERV, V7, P285, DOI 10.1017/S1367943004001477
   Katoh K., 2017, BRIEF BIOINFORM
   Kumar S, 2018, MOL BIOL EVOL, V35, P1547, DOI 10.1093/molbev/msy096
   Lima Fernando, 2017, Ecology, V98, P2979, DOI 10.1002/ecy.1998
   Luduverio DJ, 2018, THESIS UNESP BRASIL
   Martins GS, 2015, THESIS
   Miranda-Ribeiro A, 1919, REV MUS PAULISTA, V11, P209, DOI [10.5962/bhl.part.9305, DOI 10.5962/BHL.PART.9305]
   Morales-Donoso JA, 2017, THESIS
   Moritz C, 1995, AM FISH S S, V17, P249
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   NEFF DJ, 1968, J WILDLIFE MANAGE, V32, P597, DOI 10.2307/3798941
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Oliveira ML, 2019, EUR J WILDLIFE RES, V65, P21, DOI [10.1007/s10344-019-1258-6, DOI 10.1007/S10344-019-1258-6]
   Pauli JN, 2008, J WILDLIFE MANAGE, V72, P1650, DOI 10.2193/2007-588
   Pfeiffer WT, 2010, GAT COMP ENV WORKSH, DOI [10.1109/GCE.2010.5676129, DOI 10.1109/GCE.2010.5676129]
   Pitra C, 2004, MOL PHYLOGENET EVOL, V33, P880, DOI 10.1016/j.ympev.2004.07.013
   Por FD, 1992, SOORETAMA ATLANTIC R
   Ramos-Robles M, 2013, TROP CONSERV SCI, V6, P70, DOI 10.1177/194008291300600109
   Ratnasingham S, 2007, MOL ECOL NOTES, V7, P355, DOI 10.1111/j.1471-8286.2007.01678.x
   Reiners TE, 2011, EUR J WILDLIFE RES, V57, P991, DOI 10.1007/s10344-011-0543-9
   Ribeiro MC, 2009, BIOL CONSERV, V142, P1141, DOI 10.1016/j.biocon.2009.02.021
   Rodriguez PC, 2010, MEDICC REV, V12, P17, DOI 10.37757/MR2010.V12.N1.4
   Rossi Rogerio Vieira, 2010, P202
   Salviano MB, 2017, BIOL REPROD, V96, P1279, DOI 10.1093/biolre/iox041
   Santos PM, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2663
   Smith DA, 2001, SCIENCE, V291, P435, DOI 10.1126/science.291.5503.435B
   Valeri MP, 2018, CYTOGENET GENOME RES, V154, P147, DOI 10.1159/000488377
   Vogliotti A, 2016, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2016-1.RLTS.T41023A22155086.en, DOI 10.2305/IUCN.UK.2016-1.RLTS.T41023A22155086.EN]
NR 63
TC 6
Z9 6
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1612-4642
EI 1439-0574
J9 EUR J WILDLIFE RES
JI Eur. J. Wildl. Res.
PD FEB 21
PY 2020
VL 66
IS 2
AR 28
DI 10.1007/s10344-020-1367-2
PG 10
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA KO1HS
UT WOS:000515300100001
DA 2022-02-10
ER

PT J
AU Miao, ZQ
   Liu, ZW
   Gaynor, KM
   Palmer, MS
   Yu, SX
   Getz, WM
AF Miao, Zhongqi
   Liu, Ziwei
   Gaynor, Kaitlyn M.
   Palmer, Meredith S.
   Yu, Stella X.
   Getz, Wayne M.
TI Iterative human and automated identification of wildlife images
SO NATURE MACHINE INTELLIGENCE
LA English
DT Article
ID BIODIVERSITY
AB Camera trapping is increasingly being used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has substantially advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static datasets, whereas wildlife data are intrinsically dynamic and involve long-tailed distributions. These drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning, which facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve an similar to 90% accuracy employing only similar to 20% of the human annotations of existing approaches. Our synergistic collaboration of humans and machines transforms deep learning from a relatively inefficient post-annotation tool to a collaborative ongoing annotation tool that vastly reduces the burden of human annotation and enables efficient and constant model updates.
   Camera trapping is a widely adopted method for monitoring terrestrial mammals. However, a drawback is the amount of human annotation needed to keep pace with continuous data collection. The authors developed a hybrid system of machine learning and humans in the loop, which minimizes annotation load and improves efficiency.
C1 [Miao, Zhongqi; Yu, Stella X.; Getz, Wayne M.] Univ Calif Berkeley, Dept Environm Sci Policy & Management, Berkeley, CA 94720 USA.
   [Miao, Zhongqi; Yu, Stella X.] Univ Calif Berkeley, Int Comp Sci Inst, Berkeley, CA 94720 USA.
   [Liu, Ziwei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Gaynor, Kaitlyn M.] Univ Calif Santa Barbara, Natl Ctr Ecol Anal & Synth, Santa Barbara, CA 93106 USA.
   [Palmer, Meredith S.] Princeton Univ, Dept Ecol & Evolutionary Biol, Princeton, NJ 08544 USA.
   [Getz, Wayne M.] Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
RP Miao, ZQ; Getz, WM (corresponding author), Univ Calif Berkeley, Dept Environm Sci Policy & Management, Berkeley, CA 94720 USA.; Miao, ZQ (corresponding author), Univ Calif Berkeley, Int Comp Sci Inst, Berkeley, CA 94720 USA.; Getz, WM (corresponding author), Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
EM zhongqi.miao@berkeley.edu; wgetz@berkeley.edu
RI ; Getz, Wayne/I-4521-2013
OI Gaynor, Kaitlyn/0000-0002-5747-0543; Miao, Zhongqi/0000-0002-0439-8592;
   Getz, Wayne/0000-0001-8784-9354
FU HHMI BioInteractive; Rufford Foundation; Idea Wild; Explorers Club; UC
   Berkeley Center for African Studies; NTU NAP; Rhodes Trust; National
   Center for Ecological Analysis and Synthesis Director's Postdoctoral
   Fellowship; National Science FoundationNational Science Foundation (NSF)
   [1810586]; Schmidt Science Fellows
FX We thank T. Gu, A. Ke, H. Rosen, A. Wu, C. Jurgensen, E. Lai, M. Levy
   and E. Silverberg for annotating the images used in this study, as well
   as everyone else involved in this project. Data collection was supported
   by J. Brashares and through grants to K.M.G. from HHMI BioInteractive,
   the Rufford Foundation, Idea Wild, the Explorers Club and the UC
   Berkeley Center for African Studies. We are grateful for the support of
   Gorongosa National Park, especially M. Stalmans, in permitting and
   facilitating this research. Z.L. is supported by NTU NAP. K.M.G. is
   supported by Schmidt Science Fellows in partnership with the Rhodes
   Trust, and the National Center for Ecological Analysis and Synthesis
   Director's Postdoctoral Fellowship. M.S.P. is funded by National Science
   Foundation grant no. PRFB #1810586.
CR Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Anderson TM, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0314
   Arjovsky Martin, 2019, ARXIV190702893
   Barlow J, 2016, NATURE, V535, P144, DOI 10.1038/nature18326
   Barnosky AD, 2011, NATURE, V471, P51, DOI 10.1038/nature09678
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   CHEN T, 2020, PREPRINT
   Clavero M, 2005, TRENDS ECOL EVOL, V20, P110, DOI 10.1016/j.tree.2005.01.003
   Darrell T., 2020, ARXIV200805659
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Gaynor KM, 2021, ANIM CONSERV, V24, P510, DOI 10.1111/acv.12661
   Girshick, 2020, P IEEE CVF C COMP VI, P9729, DOI DOI 10.1109/CVPR42600.2020.00975
   Hautier Y, 2015, SCIENCE, V348, P336, DOI 10.1126/science.aaa1788
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, STAT-US, V1050, P9
   Kays R, 2020, DIVERS DISTRIB, V26, P644, DOI 10.1111/ddi.12993
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Lee Dong-Hyun, 2013, PSEUDO LABEL SIMPLE
   Liu Weitang, 2020, ADV NEURAL INFORM PR
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Liu Ziwei, 2020, P IEEE CVF C COMP VI
   Mech L. David, 2019, Canadian Field-Naturalist, V133, P60, DOI 10.22621/cfn.v133i1.2078
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Palmer MS, 2017, ECOL LETT, V20, P1364, DOI 10.1111/ele.12832
   Pardo LE, 2021, S AFR J SCI, V117, DOI 10.17159/sajs.2021/8134
   Pimm SL, 2014, SCIENCE, V344, P987, DOI 10.1126/science.1246752
   Prach K, 2011, TRENDS ECOL EVOL, V26, P119, DOI 10.1016/j.tree.2010.12.007
   Rich LN, 2017, GLOBAL ECOL BIOGEOGR, V26, P918, DOI 10.1111/geb.12600
   Ripple WJ, 2017, BIOSCIENCE, V67, P197
   Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2020, ECOL EVOL, V10, P10374, DOI 10.1002/ece3.6692
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Taylor G, 2017, TRENDS ECOL EVOL, V32, P873, DOI 10.1016/j.tree.2017.08.002
   Wallach H., 2019, P ADV NEUR INF PROC, P8024, DOI DOI 10.1038/S41591-021-01287-9
   Whytock RC, 2021, METHODS ECOL EVOL, V12, P1080, DOI 10.1111/2041-210X.13576
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yosinski J, 2014, ADV NEUR IN, V27
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NATURE PORTFOLIO
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
EI 2522-5839
J9 NAT MACH INTELL
JI Nat. Mach. Intell.
PD OCT
PY 2021
VL 3
IS 10
BP 885
EP +
DI 10.1038/s42256-021-00393-0
PG 14
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WI6LY
UT WOS:000708470700004
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Eakin, CJ
   Hunter, ML
   Calhoun, AJK
AF Eakin, Carly J.
   Hunter, Malcolm L.
   Calhoun, Aram J. K.
TI Bird and mammal use of vernal pools along an urban development gradient
SO URBAN ECOSYSTEMS
LA English
DT Article
DE Vernal pool; Camera trap; Urban gradient; Urban wildlife; Subsidized
   wildlife; Urban wetlands
ID SPECIES RICHNESS; LAND-USE; CAMERA TRAPS; URBANIZATION; BIODIVERSITY;
   FORESTS; FRAGMENTATION; ACCUMULATION; COMMUNITIES; MANAGEMENT
AB Vernal pools in the northeastern US are of conservation concern primarily because of their role as habitat for specialized pool-breeding amphibians, but their use by birds and mammals may also be of interest, especially from the perspective of the impact of urbanization. We describe camera-trapped wildlife (CTW) at 38 vernal pools along an urban development gradient in greater Bangor, Maine, USA. We detected 20 mammal and 39 bird taxa (29 contacted pool water; 39 detected at >1 site). Land cover type within 1000m (%), within-pool vegetation (%), and amphibian egg mass numbers explained a substantial portion of the variance (40.8%) in CTW assemblage composition. Submerged vegetation within pools and cover by water and impervious surfaces within 1000m of pools were key site characteristics defining assemblages. We scored the urban-affiliation of taxa and modeled the relationship between weighted assemblage scores for each site and impervious cover. Impervious cover within 1000m of pools was positively (p<0.01) related to site urban-affiliation scores. Use probability for red fox increased and snowshoe hare decreased with impervious cover at 1000m. These results indicate that within-pool vegetation and land cover types at 1000m influenced bird and mammal assemblages that used study pools and greater impervious cover at 100 and 1000m was correlated with a shift in assemblages from being dominated by urban-avoider to urban-adapted species. We encourage land use planners and managers to consider the influence of land use practices within 1000m of vernal pools on birds and mammals, especially near amphibian breeding pools.
C1 [Eakin, Carly J.; Hunter, Malcolm L.; Calhoun, Aram J. K.] Univ Maine, Dept Wildlife Fisheries & Conservat Biol, 5755 Nutting Hall, Orono, ME 04469 USA.
RP Eakin, CJ (corresponding author), Univ Maine, Dept Wildlife Fisheries & Conservat Biol, 5755 Nutting Hall, Orono, ME 04469 USA.
EM carly.eakin@gmail.com; mhunter@maine.edu; calhoun@maine.edu
FU McIntire-Stennis; Hatch ActUnited States Department of Agriculture
   (USDA); National Science FoundationNational Science Foundation (NSF)
   [313627]; USDA National Institute of Food and Agriculture, Hatch project
   through the Maine Agricultural & Forest Experiment Station [ME021705];
   Division Of Behavioral and Cognitive SciNational Science Foundation
   (NSF)NSF - Directorate for Social, Behavioral & Economic Sciences (SBE)
   [1313627] Funding Source: National Science Foundation
FX We are grateful for support for this study provided by McIntire-Stennis,
   the Hatch Act, and the National Science Foundation under grant no.
   313627. We thank A. Mortelliti for assistance with occupancy modeling,
   H. Greig, R. Holberton, and M. Kinnison for help with study and analysis
   design, and D. Dunham for hundreds of hours of visually scanning trail
   camera photos for animals. This is a Maine Agricultural and Forest
   Experiment Station Publication Number 3611. This project was supported
   by the USDA National Institute of Food and Agriculture, Hatch project
   number #ME021705 through the Maine Agricultural & Forest Experiment
   Station.
CR Anderson K. A., 2002, MODEL SELECTION MULT
   Aronson MFJ, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2013.3330
   BEISSINGER SR, 1982, CONDOR, V84, P75, DOI 10.2307/1367825
   BERVEN KA, 1990, ECOLOGY, V71, P1599, DOI 10.2307/1938295
   Blair RB, 1996, ECOL APPL, V6, P506, DOI 10.2307/2269387
   Blair RB, 2001, BIOTIC HOMOGENIZATION, P33
   Borcard D, 2011, USE R, P1, DOI 10.1007/978-1-4419-7976-6
   Boren JC, 1999, J RANGE MANAGE, V52, P420, DOI 10.2307/4003767
   Calhoun AJK, 2017, BIOL CONSERV, V211, P3, DOI 10.1016/j.biocon.2016.11.024
   Chace JF, 2006, LANDSCAPE URBAN PLAN, V74, P46, DOI 10.1016/j.landurbplan.2004.08.007
   Chamberlain MJ, 2002, AM MIDL NAT, V147, P102, DOI 10.1674/0003-0031(2002)147[0102:SHSBRP]2.0.CO;2
   CHILDS HE, 1953, EVOLUTION, V7, P228, DOI 10.1111/j.1558-5646.1953.tb00084.x
   Chupp AD, 2013, NORTHEAST NAT, V20, P631, DOI 10.1656/045.020.0415
   Clergeau P, 1998, CONDOR, V100, P413, DOI 10.2307/1369707
   Colburn EA., 2004, VERNAL POOLS NATURAL
   Cox RR, 1998, J WILDLIFE MANAGE, V62, P124, DOI 10.2307/3802270
   CROONQUIST MJ, 1991, ENVIRON MANAGE, V15, P701, DOI 10.1007/BF02589628
   Crouch WB, 2000, WILDLIFE SOC B, V28, P895
   Dorazio RM, 2006, ECOLOGY, V87, P842, DOI 10.1890/0012-9658(2006)87[842:ESRAAB]2.0.CO;2
   Faulkner Stephen, 2004, Urban Ecosystems, V7, P89, DOI 10.1023/B:UECO.0000036269.56249.66
   Fischer JD, 2012, BIOSCIENCE, V62, P809, DOI 10.1525/bio.2012.62.9.6
   Fischer J, 2007, GLOBAL ECOL BIOGEOGR, V16, P265, DOI 10.1111/j.1466-8238.2007.00287.x
   Fiske I, 2017, PACKAGE BUNMARKED, P116
   Friesen LE, 1995, CONSERV BIOL, V9, P1408, DOI 10.1046/j.1523-1739.1995.09061408.x
   Fuller TK, 2003, FOREST ECOL MANAG, V185, P75, DOI 10.1016/S0378-1127(03)00247-0
   Gotelli NJ, 2001, ECOL LETT, V4, P379, DOI 10.1046/j.1461-0248.2001.00230.x
   Gray MJ, 2009, DIS AQUAT ORGAN, V87, P243, DOI 10.3354/dao02138
   Hanowski J, 2006, FOREST ECOL MANAG, V229, P63, DOI 10.1016/j.foreco.2006.03.011
   Hansen AJ, 2005, ECOL APPL, V15, P1893, DOI 10.1890/05-5221
   Homan RN, 2004, ECOL APPL, V14, P1547, DOI 10.1890/03-5125
   Hunter M, 2016, ECOL INDIC, V63, P121, DOI 10.1016/j.ecolind.2015.11.049
   JOHNSON DH, 1980, ECOLOGY, V61, P65, DOI 10.2307/1937156
   Lawler JJ, 2014, P NATL ACAD SCI USA, V111, P7492, DOI 10.1073/pnas.1405557111
   Legendre P, 2001, OECOLOGIA, V129, P271, DOI 10.1007/s004420100716
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   MacKenzie DI, 2006, J WILDLIFE MANAGE, V70, P367, DOI 10.2193/0022-541X(2006)70[367:MTPORU]2.0.CO;2
   MacKenzie DI, 2004, J AGR BIOL ENVIR ST, V9, P300, DOI 10.1198/108571104X3361
   McKinney M. L., 2008, Urban Ecosystems, V11, P161, DOI 10.1007/s11252-007-0045-4
   McKinney ML, 2006, BIOL CONSERV, V127, P247, DOI 10.1016/j.biocon.2005.09.005
   McKinney ML, 2002, BIOSCIENCE, V52, P883, DOI 10.1641/0006-3568(2002)052[0883:UBAC]2.0.CO;2
   McLoughlin PD, 2000, ECOSCIENCE, V7, P123, DOI 10.1080/11956860.2000.11682580
   Minor Emily, 2010, Urban Ecosystems, V13, P51, DOI 10.1007/s11252-009-0103-1
   Mitchell JC, 2008, SCI CONSERVATION VER, P169
   Murray MP, 2005, CAN FIELD NAT, V119, P291, DOI 10.22621/cfn.v119i2.116
   NAGELKERKE NJD, 1991, BIOMETRIKA, V78, P691, DOI 10.1093/biomet/78.3.691
   Nilon CH, 1986, P NAT S URB WILDL 19, P53
   Oksanen J, 2017, vegan: Community Ecology Package. Vegan. 2.5-7
   R Core Team, 2017, R LANG ENV STAT COMP
   RACEY GD, 1982, CAN J ZOOL, V60, P865, DOI 10.1139/z82-119
   Rittenhouse TAG, 2007, J HERPETOL, V41, P645, DOI 10.1670/07-015.1
   Rodewald AD, 2003, WILDLIFE SOC B, V31, P586
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Royle J.A., 2008, HIERARCHICAL MODELIN
   Shurin JB, 2006, P ROY SOC B-BIOL SCI, V273, P1, DOI 10.1098/rspb.2005.3377
   Silveira Joseph G., 1998, P92
   Thompson GG, 2003, AUSTRAL ECOL, V28, P355, DOI 10.1046/j.1442-9993.2003.01294.x
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Trzcinski MK, 1999, ECOL APPL, V9, P586, DOI 10.1890/1051-0761(1999)009[0586:IEOFCA]2.0.CO;2
   Wake DB, 2008, P NATL ACAD SCI USA, V105, P11466, DOI 10.1073/pnas.0801921105
   Welsh AH, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0052015
   Zedler P. H., 1987, ECOLOGY SO CALIFORNI
   ZEDLER PH, 1992, AM MIDL NAT, V128, P1, DOI 10.2307/2426407
   [No title captured]
NR 63
TC 4
Z9 4
U1 7
U2 54
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1083-8155
EI 1573-1642
J9 URBAN ECOSYST
JI Urban Ecosyst.
PD DEC
PY 2018
VL 21
IS 6
BP 1029
EP 1041
DI 10.1007/s11252-018-0782-6
PG 13
WC Biodiversity Conservation; Ecology; Environmental Sciences; Urban
   Studies
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology; Urban
   Studies
GA HD2ZX
UT WOS:000452382200002
DA 2022-02-10
ER

PT J
AU de Oliveira, ML
   Grotta-Netto, F
   Peres, PHD
   Vogliotti, A
   Brocardo, CR
   Cherem, JJ
   Landis, M
   Paolino, RM
   Fusco-Costa, R
   Gatti, A
   Moreira, DO
   Ferreira, PM
   Mendes, SL
   Huguenin, J
   Zanin, M
   Nodari, JZ
   Leite, YLR
   Lyrio, GS
   Ferraz, KMPMD
   Passos, FC
   Duarte, JMB
AF de Oliveira, Marcio Leite
   Grotta-Netto, Francisco
   de Faria Peres, Pedro Henrique
   Vogliotti, Alexandre
   Brocardo, Carlos Rodrigo
   Cherem, Jorge Jose
   Landis, Mariana
   Paolino, Roberta Montanheiro
   Fusco-Costa, Roberto
   Gatti, Andressa
   Moreira, Danielle Oliveira
   Ferreira, Paula Modenesi
   Mendes, Sergio Lucena
   Huguenin, Jade
   Zanin, Marina
   Nodari, Joana Zorzal
   Reis Leite, Yuri Luiz
   Lyrio, Georgea Silva
   Micchi de Barros Ferraz, Katia Maria Paschoaletto
   Passos, Fernando C.
   Barbanti Duarte, Jose Mauricio
TI Elusive deer occurrences at the Atlantic Forest: 20 years of surveys
SO MAMMAL RESEARCH
LA English
DT Article
DE Atlantic Forest; Elusive species; Mammals survey; Mazama; Tropical
   forest
ID EVOLUTIONARY HISTORY; GENUS MAZAMA; JAGUARS
AB The Atlantic Forest, a hotspot for biodiversity conservation, harbours five forest deer species (Mazama spp.). Due to their elusiveness, there is a severe scarcity of occurrence data to support ecological studies and conservation planning. Thus, we assembled an occurrence dataset of Atlantic Forest deer with reliable taxonomic information aggregating data from scat and camera traps surveys, and opportunistic data collection over the last 20 years. From 2002 to 2019, we surveyed 77 protected areas using scats detection dogs and genetically identifying the faecal samples. We successfully identified 1,147 out of 1,450 collected samples. From 2000 to 2020, we sampled six protected areas in 92 sampling points with 13,328 camera trap days of sampling effort. In addition, we established an active search for potential contributors within the scientific community and environmental consultants since 2010, offering a taxonomic identification service for camera traps images, and biological field-collected samples. With our efforts, we assembled a dataset with 1,456 records of forest deer occurrence at the Atlantic Forest. Of these records, 494 are from M. americana, 350 from M. bororo, 309 from M. gouazoubira, 268 from M. nana and 35 from M. nemorivaga. The faecal sampling was the most predominant method in these records (n = 1043) followed by photographs from camera traps (n = 388); both methods represent 98.2% of our dataset records. Most of the records (79.5%) in the dataset are inside protected areas (n = 1,130). Our dataset is the most comprehensive source of information on Neotropical forest deer occurrence to date.
C1 [de Oliveira, Marcio Leite; Grotta-Netto, Francisco; de Faria Peres, Pedro Henrique; Barbanti Duarte, Jose Mauricio] Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
   [Vogliotti, Alexandre] Fed Univ Latin Amer Integrat, Foz Do Iguacu, PR, Brazil.
   [Brocardo, Carlos Rodrigo] Neotrop Inst Res & Conservat, Curitiba, Parana, Brazil.
   [Brocardo, Carlos Rodrigo] Univ Fed Oeste Para Santarem, Programa Posgrad Biodiversidade, Santarem, PA, Brazil.
   [Cherem, Jorge Jose] Caipora Cooperat, Florianopolis, SC, Brazil.
   [Landis, Mariana; Paolino, Roberta Montanheiro; Micchi de Barros Ferraz, Katia Maria Paschoaletto] Univ Sao Paulo, Luiz de Queiroz Coll Agr, Forest Sci Dept, Wildlife Ecol Management & Conservat Lab LEMaC, Piracicaba, SP, Brazil.
   [Landis, Mariana] Manaca Inst, Sao Miguel Arcanjo, SP, Brazil.
   [Fusco-Costa, Roberto] Univ Fed Parana, Programa Posgrad Ecol & Conservacao, Curitiba, Parana, Brazil.
   [Fusco-Costa, Roberto] Inst Pesquisas Cananeia, Cananeia, SP, Brazil.
   [Gatti, Andressa; Moreira, Danielle Oliveira; Ferreira, Paula Modenesi; Huguenin, Jade; Nodari, Joana Zorzal; Lyrio, Georgea Silva] Protapir Inst Biodivers, Vila Velha, ES, Brazil.
   [Moreira, Danielle Oliveira; Mendes, Sergio Lucena] Inst Nacl Mata Atlantica, Santa Teresa, ES, Brazil.
   [Mendes, Sergio Lucena; Nodari, Joana Zorzal; Reis Leite, Yuri Luiz] Univ Fed Espirito Santo, Vitoria, ES, Brazil.
   [Zanin, Marina] Univ Fed Maranhao, Sao Luis, Maranhao, Brazil.
   [Passos, Fernando C.] Univ Fed Parana UFPR, Dept Zool, Lab Biodiversidade Conservacao & Ecol Anim Silves, Programa Posgrad & Ecol & Conservacao, Curitiba, Parana, Brazil.
RP de Oliveira, ML (corresponding author), Sao Paulo State Univ, Deer Res & Conservat Ctr, Jaboticabal, SP, Brazil.
EM oliveiraml1@yahoo.com.br
OI Grotta Neto, Francisco/0000-0002-2390-936X; Leite de Oliveira,
   Marcio/0000-0002-7705-0626; Montanheiro Paolino,
   Roberta/0000-0002-6948-3333; Cherem, Jorge/0000-0001-9718-9628
FU Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)Fundacao de
   Amparo a Pesquisa do Estado de Sao Paulo (FAPESP) [2014/09300-0,
   2015/25742-5, 2017/00331-8, 2017/02200-8]; National Council for
   Scientific and Technological Development (CNPq)Conselho Nacional de
   Desenvolvimento Cientifico e Tecnologico (CNPQ) [302368/2018-3,
   307303/2017-9, 308503/2014-7]; Araucaria Foundation to Support
   Scientific Development of ParanaFundacao Araucaria de Apoio ao
   Desenvolvimento Cientifico e Tecnologico do Estado do Parana FA
   [008/2017]; Fundacao Grupo Boticario [1001_ 20141, 1097_ 20171]; Rufford
   Foundation; Legado das Aguas-Reserva Votorantim; Fundacao de Amparo a
   Pesquisa do Estado do Espirito Santo (FAPES) [511187434/2010, 148/2019];
   Vale S.A. [529/2016]; Coordination for the Improvement of Higher
   Education Personnel (CAPES)Coordenacao de Aperfeicoamento de Pessoal de
   Nivel Superior (CAPES) [88887.478136/2020-00]
FX This study was funded by the Fundacao de Amparo a Pesquisa do Estado de
   Sao Paulo (FAPESP) (#2014/09300-0, #2015/25742-5, #2017/00331-8,
   #2017/02200-8), the National Council for Scientific and Technological
   Development (CNPq) (#302368/2018-3, #307303/2017-9, #308503/2014-7), the
   Araucaria Foundation to Support Scientific Development of Parana
   (008/2017), Fundacao Grupo Boticario (#1001_ 20141 and #1097_ 20171),
   Rufford Foundation, Legado das Aguas-Reserva Votorantim, Fundacao de
   Amparo a Pesquisa do Estado do Espirito Santo (FAPES) (#511187434/2010,
   #148/2019), Vale S.A. (#529/2016) and Coordination for the Improvement
   of Higher Education Personnel (CAPES) (#88887.478136/2020-00).
CR Angeli T, 2014, STUD NEOTROP FAUNA E, V49, P199, DOI 10.1080/01650521.2014.958898
   Asfora Paulo Henrique, 2009, Biota Neotrop., V9
   Aubry KB, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179152
   Azevedo, 2009, ANALISE COMP PERIODO
   Duarte JMB, 2017, ORYX, V51, P656, DOI 10.1017/S0030605316000405
   Beck H, 2006, J MAMMAL, V87, P519, DOI 10.1644/05-MAMM-A-174R1.1
   Beck H, 2013, BIOL CONSERV, V163, P115, DOI 10.1016/j.biocon.2013.03.012
   Beier P, 2017, CONSERV LETT, V10, P288, DOI 10.1111/conl.12300
   Bollback T, 2004, DNA EXTRACTION PROTO
   Mantellatto AMB, 2020, GENET MOL BIOL, V43, DOI [10.1590/1678-4685-GMB-2019-0008, 10.1590/1678-4685-gmb-2019-0008]
   Bonney R, 2014, SCIENCE, V343, P1436, DOI 10.1126/science.1251554
   Bovendorp Ricardo S, 2017, Ecology, V98, P2226, DOI 10.1002/ecy.1893
   Cavalcanti SMC, 2010, J MAMMAL, V91, P722, DOI 10.1644/09-MAMM-A-171.1
   Cifuentes-Rincon A, 2020, ZOOKEYS, P143, DOI 10.3897/zookeys.958.50300
   Costa LP, 2000, BIOTROPICA, V32, P872
   da Silva BFS, 2020, IHERINGIA SER ZOOL, V110, DOI 10.1590/1678-4766e2020029
   de Oliveira ML, 2012, ZOOLOGIA-CURITIBA, V29, P183, DOI 10.1590/S1984-46702012000200012
   de Oliveira ML, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-1367-2
   de Oliveira ML, 2019, EUR J WILDLIFE RES, V65, DOI 10.1007/s10344-019-1258-6
   de Oliveira ML, 2016, MAMM BIOL, V81, P281, DOI 10.1016/j.mambio.2016.01.004
   de Souza JN, 2013, CONSERV GENET RESOUR, V5, P639, DOI 10.1007/s12686-013-9870-3
   DUARTE J.M.B, 2016, IUCN RED LIST THREAT, V2016, DOI [10.2305/IUCN.UK.2016-1.RLTS.T29619A22154827.en, DOI 10.2305/IUCN.UK.2016-1.RLTS.T29619A22154827.EN]
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Ferraz KMPMD, 2021, CONSERV SCI PRACT, V3, DOI 10.1111/csp2.330
   Ferreguetti AC, 2015, J MAMMAL, V96, P1245, DOI 10.1093/jmammal/gyv132
   Foster RJ, 2010, J ZOOL, V280, P309, DOI 10.1111/j.1469-7998.2009.00663.x
   Frey Jennifer K., 2013, Animals, V3, P327, DOI 10.3390/ani3020327
   Gascon, 2011, BIODIVERSITY HOTSPOT, P3, DOI DOI 10.1007/978-3-642-20992-5_1
   Gayot M, 2004, J TROP ECOL, V20, P31, DOI 10.1017/S0266467404006157
   Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
   Gonzalez Susana, 2020, Mastozoologia Neotropical, V27, P37, DOI 10.31687/saremMN_SI.20.27.1.05
   Gonzalez S, 2009, MOL ECOL RESOUR, V9, P754, DOI 10.1111/j.1755-0998.2008.02390.x
   Grotta-Neto F., 2020, ECOLOGIA CERVIDEOS F
   Grotta-Neto F, 2020, WILDLIFE SOC B, V44, P640, DOI 10.1002/wsb.1121
   Grotta-Neto F, 2019, J MAMMAL, V100, P454, DOI 10.1093/jmammal/gyz056
   Gutierrez EE, 2017, ZOOKEYS, P87, DOI 10.3897/zookeys.697.15124
   Hurt A., 2009, CANINE ERGONOMICS, P175
   Instituto Brasileiro de Geografia e Estatistica (IBGE), 1992, MAP VEG BRAS
   IUCN, 2001, IUCN RED LIST CATEGO
   IUCN, 2012, IUCN RED LIST CAT CR
   IUCN Standards and Petitions Committee, 2019, GUIDELINES USING IUC
   Lima Fernando, 2017, Ecology, V98, P2979, DOI 10.1002/ecy.1998
   McShea WJ., 2012, DEER ANIMAL ANSWER G
   Morales-Donoso JA, 2017, CARACTERIZACAO MORFO
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   Nagy-Reis M, 2020, ECOLOGY, V101, DOI 10.1002/ecy.3128
   Pearson RG, 2007, SYNTH AM MUSEUM NAT, V50, P54
   Peres, 2020, FILOGENIA DELIMITACA
   Peres, 2015, USO ESPACO PELO VEAD
   Rezende CL, 2018, PERSPECT ECOL CONSER, V16, P208, DOI 10.1016/j.pecon.2018.10.002
   Ribeiro MC, 2009, BIOL CONSERV, V142, P1141, DOI 10.1016/j.biocon.2009.02.021
   Rivero K, 2005, MAMMALIA, V69, P169, DOI 10.1515/mamm.2005.015
   Rodrigues TF, 2017, J MAMMAL, V98, P1301, DOI 10.1093/jmammal/gyx099
   Rosa CAD, 2020, ECOLOGY, V101, DOI 10.1002/ecy.3115
   Sales LP, 2020, GLOBAL CHANGE BIOL, V26, P7036, DOI 10.1111/gcb.15374
   Sandoval EDP, 2019, CARACTERIZACAO MORFO
   Santos PM, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2663
   Silva TR, 2018, PRIM PLANO REDUCAO I
   Smith DA, 2001, SCIENCE, V291, P435, DOI 10.1126/science.291.5503.435B
   Souza Y, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2785
   Costa EBV, 2017, STUD NEOTROP FAUNA E, V52, P37, DOI 10.1080/01650521.2016.1263418
   Vogliotti, 2003, HIST NATURAL MAZAMA
   Vogliotti, 2009, PARTICAO HABITATS EN
   Vogliotti PA, 2016, IUCN RED LIST THREAT, P8235
   Williams PH, 2002, J BIOSCIENCES, V27, P327, DOI 10.1007/BF02704963
NR 66
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2199-2401
EI 2199-241X
J9 MAMMAL RES
JI Mammal Res.
PD JAN
PY 2022
VL 67
IS 1
BP 51
EP 59
DI 10.1007/s13364-021-00604-4
EA OCT 2021
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA XZ9MD
UT WOS:000704522300001
OA Bronze
DA 2022-02-10
ER

PT J
AU Schindler, F
   Steinhage, V
AF Schindler, Frank
   Steinhage, Volker
TI Saving costs for video data annotation in wildlife monitoring
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Automatic annotation; Neural networks; Wildlife monitoring; Tracking;
   Instance segmentation; Artificial intelligence
AB In wildlife monitoring, large amounts of video data are generated by recordings from camera traps. Training of deep learning methods demands for annotated video data, i.e. video data where each frame is annotated with the correct number and species designation of the observed animals. But manual annotation of video clips is extremely time-consuming and laborious. In this proof of concept we compare three different state-of-the-art approaches to the annotation of video data: Manual annotation using the VGG Image Annotator, interactive annotation using the MiVOS video annotator and automated annotation utilizing an adapted and customized Tracktor approach that propagates annotations from frame to frame through complete video clips. An experimental proof of concept on wildlife video clips captured by camera traps show extreme time savings from hours down to minutes (i.e. in order of a magnitude) thereby not only maintaining the detection scores of animals in each frame but also improving detection scores from 54.7% to 58.5% compared to the employment of perfect but costly manual annotations in training.
C1 [Schindler, Frank; Steinhage, Volker] Univ Bonn, Dept Comp Sci 4, Friedrich Hirzebruch Allee 8, D-53115 Bonn, Germany.
RP Schindler, F (corresponding author), Univ Bonn, Dept Comp Sci 4, Friedrich Hirzebruch Allee 8, D-53115 Bonn, Germany.
EM schindl@cs.uni-bonn.de
FU German Federal Ministry of Education and Research (Bundesministerium fur
   Bildung und Forschung (BMBF) , Bonn, Germany)Federal Ministry of
   Education & Research (BMBF) [FKZ 01LC1903B]
FX We want to thank Morris Klasen and Timm Haucke for fruitful discussions
   on aspects of this study. This work was partially done within the
   project "Automated Multisensor station for Monitoring Of species
   Diversity" (AMMOD) which is funded by the German Federal Ministry of
   Education and Research (Bundesministerium fur Bildung und Forschung
   (BMBF) , Bonn, Germany (FKZ 01LC1903B) .
CR Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096
   Athar Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P158, DOI 10.1007/978-3-030-58621-8_10
   Berg A, 2019, IEEE INT CONF COMP V, P2242, DOI 10.1109/ICCVW.2019.00277
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertasius G., 2020, CVPR
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Cheng H.K., ARXIV210307941
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Drozdov D., 2017, SUPERVISELY
   Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Hu YT, 2017, ADV NEUR IN, V30
   Jiale Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P1, DOI 10.1007/978-3-030-58568-6_1
   Lin Chung-Ching, 2020, P IEEE CVF C COMP VI, P13147
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Paszke A., ADV NEURAL INFORM PR, V32, P8024
   Porzi L, 2020, PROC CVPR IEEE, P6845, DOI 10.1109/CVPR42600.2020.00688
   Qi J., ARXIV210201558
   Reid I., 2016, ARXIV160300831
   Reza MA, 2019, IEEE INT C INT ROBOT, P4970, DOI 10.1109/IROS40897.2019.8968230
   Schindler F, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101215
   Wang, 2019, ARXIV190912605
   Wang Y., 2020, ARXIV201114503
   Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 30
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD NOV
PY 2021
VL 65
AR 101418
DI 10.1016/j.ecoinf.2021.101418
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA WB6TN
UT WOS:000703703200002
DA 2022-02-10
ER

PT J
AU Sundaram, DM
   Loganathan, A
AF Sundaram, Divya Meena
   Loganathan, Agilandeeswari
TI A New Supervised Clustering Framework Using Multi Discriminative Parts
   and Expectation-Maximization Approach for a Fine-Grained Animal Breed
   Classification (SC-MPEM)
SO NEURAL PROCESSING LETTERS
LA English
DT Article
DE Expectation-maximization; Fine-grained classification; Multi-part CNN;
   Snapshot serengeti; Supervised clustering
AB Fine-grained image classification is active research in the field of computer vision. Specifically, animal breed classification is an arduous task due to the challenges in camera traps images like occlusion, camouflage, poor illumination, pose variation, etc. In this paper, we propose a fine-grained animal breed classification model using supervised clustering based on Multi Part-Convolutional Neural Network (MP-CNN) and Expectation-Maximization (EM) clustering. The proposed model follows a straightforward pipeline that combines the deep feature extraction using the CNN pre-trained on ImageNet and classifies unsupervised data using EM clustering. Further, we also propose a multi discriminative part selection and detection for the precise classification of animal breeds without using bounding box and annotations on both training and testing phases. The model is tested on several benchmark datasets for animals, including the largest camera trap Snapshot Serengeti dataset and has achieved a cumulative accuracy of 98.4%. The results from the proposed model strengthen the belief that supervised training of deep CNN on a large and versatile dataset, extracts better features than most of the traditional approaches, even for the unsupervised tasks.
C1 [Sundaram, Divya Meena; Loganathan, Agilandeeswari] Vellore Inst Technol, Sch Informat Technol & Engn, Vellore, Tamil Nadu, India.
RP Loganathan, A (corresponding author), Vellore Inst Technol, Sch Informat Technol & Engn, Vellore, Tamil Nadu, India.
EM agila.l@vit.ac.in
CR Antonio WHS, 2019, APPL ARTIF INTELL, V33, P1093, DOI 10.1080/08839514.2019.1673993
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Boots B., 2018, ARXIV180707760
   Branson S., 2014, ARXIV14062952
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divya Meena S., 2019, INT J ENG ADV TECHNO, V9, P495, DOI [10.35940/ijeat.A1089.1291S319, DOI 10.35940/IJEAT.A1089.1291S319, 10.35940/ijeat.a1089.1291s319]
   Dubey A., 2018, P EUR C COMP VIS ECC, P70
   Feng H, 2018, ARXIVABS180803935
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Guerin J, 2017, ARXIVABS170701700
   Guo, 2019, ARXIV190406252
   Gupta P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P104, DOI 10.1109/CCAA.2017.8229781
   Hu T., 2019, ABS190109891 CORR
   Hu T, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P8441
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jasko G, 2017, INT C INTELL COMP CO, P363, DOI 10.1109/ICCP.2017.8117031
   Khosla A., 2011, P CVPR WORKSH FIN GR, V2, P1
   Kolesnikov Alexander, 2019, ARXIV191211370
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Lee J., 2020, ARXIV200106268
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu DYH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4146
   Liu JE, 2020, SCI PROGRAMMING-NETH, V2020, DOI 10.1155/2020/7607612
   Liu X., 2016, ARXIV160306765
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Meena SD, 2020, ADV INTELL SYST, V1057, P513, DOI 10.1007/978-981-15-0184-5_44
   Meena SD, 2019, IEEE ACCESS, V7, P151783, DOI 10.1109/ACCESS.2019.2947717
   Mulligan K, 2019, 21 INT C ART INT ICA
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Qiu WT, 2012, 2012 INTERNATIONAL CONFERENCE ON CONTROL ENGINEERING AND COMMUNICATION TECHNOLOGY (ICCECT 2012), P177, DOI 10.1109/ICCECT.2012.78
   Sharma SU, 2017, IEEE ACCESS, V5, P347, DOI 10.1109/ACCESS.2016.2642981
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Sun G, 2019, ARXIV191206842
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Touvron H., 2019, ADV NEURAL INFORM PR, P8250
   Xie LX, 2013, IEEE I CONF COMP VIS, P1641, DOI 10.1109/ICCV.2013.206
   Xie LX, 2014, IEEE T IMAGE PROCESS, V23, P1994, DOI 10.1109/TIP.2014.2310117
   Xu Z., 2016, IEEE T PATTERN ANAL
   Xu Z, 2017, IEEE T IMAGE PROCESS, V26, P135, DOI 10.1109/TIP.2016.2621661
   Yan XF, 2019, ISPRS J PHOTOGRAMM, V150, P259, DOI 10.1016/j.isprsjprs.2019.02.010
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yue YL, 2018, INT WIREL COMMUN, P805, DOI 10.1109/IWCMC.2018.8450320
   Zhang L, 2019, ARXIV191106866
   Zhang LM, 2016, IEEE T IMAGE PROCESS, V25, P553, DOI 10.1109/TIP.2015.2502147
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, IEEE T IMAGE PROCESS, V25, P878, DOI 10.1109/TIP.2015.2509425
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhuang P., 2020, ARXIV200210191
NR 53
TC 5
Z9 5
U1 1
U2 10
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1370-4621
EI 1573-773X
J9 NEURAL PROCESS LETT
JI Neural Process. Lett.
PD AUG
PY 2020
VL 52
IS 1
SI SI
BP 727
EP 766
DI 10.1007/s11063-020-10246-3
EA JUN 2020
PG 40
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MZ8GU
UT WOS:000541206000001
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Stephens, C
   Dieguez, P
   Samuni, L
   Despres-Einspenner, ML
   Harder, B
   Landsmann, A
   Lynn, LK
   Maldonado, N
   Rockaiova, Z
   Widness, J
   Wittig, RM
   Boesch, C
   Kuhl, HS
   Arandjelovic, M
AF McCarthy, Maureen S.
   Stephens, Colleen
   Dieguez, Paula
   Samuni, Liran
   Despres-Einspenner, Marie-Lyne
   Harder, Briana
   Landsmann, Anja
   Lynn, Laura K.
   Maldonado, Nuria
   Rockaiova, Zuzana
   Widness, Jane
   Wittig, Roman M.
   Boesch, Christophe
   Kuehl, Hjalmar S.
   Arandjelovic, Mimi
TI Chimpanzee identification and social network construction through an
   online citizen science platform
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera trap; chimpanzee; citizen science; Pan troglodytes; social
   network analysis
ID CAMERA TRAPS; DATA QUALITY; CONSERVATION; VARIABILITY; POPULATION;
   VALIDATION; AGREEMENT
AB Citizen science has grown rapidly in popularity in recent years due to its potential to educate and engage the public while providing a means to address a myriad of scientific questions. However, the rise in popularity of citizen science has also been accompanied by concerns about the quality of data emerging from citizen science research projects. We assessed data quality in the online citizen scientist platform Chimp&See, which hosts camera trap videos of chimpanzees (Pan troglodytes) and other species across Equatorial Africa. In particular, we compared detection and identification of individual chimpanzees by citizen scientists with that of experts with years of experience studying those chimpanzees. We found that citizen scientists typically detected the same number of individual chimpanzees as experts, but assigned far fewer identifications (IDs) to those individuals. Those IDs assigned, however, were nearly always in agreement with the IDs provided by experts. We applied the data sets of citizen scientists and experts by constructing social networks from each. We found that both social networks were relatively robust and shared a similar structure, as well as having positively correlated individual network positions. Our findings demonstrate that, although citizen scientists produced a smaller data set based on fewer confirmed IDs, the data strongly reflect expert classifications and can be used for meaningful assessments of group structure and dynamics. This approach expands opportunities for social research and conservation monitoring in great apes and many other individually identifiable species.
C1 [McCarthy, Maureen S.; Stephens, Colleen; Dieguez, Paula; Maldonado, Nuria; Wittig, Roman M.; Boesch, Christophe; Kuehl, Hjalmar S.; Arandjelovic, Mimi] Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
   [Samuni, Liran] Harvard Univ, Dept Human Evolutionary Biol, Cambridge, MA 02138 USA.
   [Samuni, Liran; Wittig, Roman M.] Ctr Suisse Rech Sci, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Despres-Einspenner, Marie-Lyne] Ecocorridors Laurentiens, Saint Jerome, PQ, Canada.
   [Harder, Briana; Landsmann, Anja; Lynn, Laura K.; Rockaiova, Zuzana; Widness, Jane] Max Planck Inst Evolutionary Anthropol, Zooniverse Citizen Scientist, Leipzig, Germany.
   [Landsmann, Anja] Univ Leipzig, Fac Med, Inst Drug Discovery, Leipzig, Germany.
   [Maldonado, Nuria] iScapes, Valencia, Spain.
   [Widness, Jane] Yale Univ, Dept Anthropol, New Haven, CT 06520 USA.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Halle, Germany.
RP McCarthy, MS (corresponding author), Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
EM maureen_mc@eva.mpg.de
FU Centre for Forest Research-Fonds de Recherche Quebec Nature et
   Technologies International; Robert Bosch Foundation; Max Planck
   SocietyMax Planck SocietyFoundation CELLEX; Heinz L. Krekeler
   Foundation; Max Planck Society Innovation Fund; Centre Suisse de
   Recherches Scientifiques; Centre for Forest ResearchFonds de Recherche
   Quebec Nature et Technologies International internship program
FX Centre for Forest Research-Fonds de Recherche Quebec Nature et
   Technologies International; Robert Bosch Foundation; Max Planck Society;
   Heinz L. Krekeler Foundation; Max Planck Society Innovation Fund; Max
   Planck Society, the Robert Bosch Foundation; Centre Suisse de Recherches
   Scientifiques; Centre for Forest ResearchFonds de Recherche Quebec
   Nature et Technologies International internship program
CR Aplin LM, 2012, P ROY SOC B-BIOL SCI, V279, P4199, DOI 10.1098/rspb.2012.1591
   Bird TJ, 2014, BIOL CONSERV, V173, P144, DOI 10.1016/j.biocon.2013.07.037
   Boesch C, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22613
   Bonney R, 2014, SCIENCE, V343, P1436, DOI 10.1126/science.1251554
   Ontl KB, 2020, INT J PRIMATOL, V41, P916, DOI 10.1007/s10764-020-00165-4
   Brassine E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0142508
   Brossard D, 2005, INT J SCI EDUC, V27, P1099, DOI 10.1080/09500690500069483
   CAIRNS SJ, 1987, ANIM BEHAV, V35, P1454, DOI 10.1016/S0003-3472(87)80018-0
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Cox TE, 2012, ECOL APPL, V22, P1201, DOI 10.1890/11-1614.1
   Crall AW, 2011, CONSERV LETT, V4, P433, DOI 10.1111/j.1755-263X.2011.00196.x
   Croft DP, 2008, EXPLORING ANIMAL SOCIAL NETWORKS, P1
   Csardi G, 2006, INTERJOURNAL, P1695
   Danielsen F, 2014, BIOSCIENCE, V64, P236, DOI 10.1093/biosci/biu001
   Davis GH, 2018, ANIM BEHAV, V141, P29, DOI 10.1016/j.anbehav.2018.04.012
   Dekker D, 2007, PSYCHOMETRIKA, V72, P563, DOI 10.1007/s11336-007-9016-1
   Delaney DG, 2008, BIOL INVASIONS, V10, P117, DOI 10.1007/s10530-007-9114-0
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Edgar GJ, 2009, MAR ECOL PROG SER, V388, P51, DOI 10.3354/meps08149
   Farine DR, 2015, CURR BIOL, V25, P2184, DOI 10.1016/j.cub.2015.06.071
   Farine DR, 2013, METHODS ECOL EVOL, V4, P1187, DOI 10.1111/2041-210X.12121
   Foster-Smith J, 2003, BIOL CONSERV, V113, P199, DOI 10.1016/S0006-3207(02)00373-7
   Frigerio D, 2018, ETHOLOGY, V124, P365, DOI 10.1111/eth.12746
   Galloway AWE, 2006, WILDLIFE SOC B, V34, P1425, DOI 10.2193/0091-7648(2006)34[1425:TROCSA]2.0.CO;2
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Gardiner MM, 2012, FRONT ECOL ENVIRON, V10, P471, DOI 10.1890/110185
   Goldenberg SZ, 2016, CURR BIOL, V26, P75, DOI 10.1016/j.cub.2015.11.005
   Gollan J, 2012, ENVIRON MANAGE, V50, P969, DOI 10.1007/s00267-012-9924-4
   Goodall J., 1986, CHIMPANZEES GOMBE PA
   Green SE, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010132
   Gura Trisha, 2013, Nature, V496, P259
   He Y., 2015, COMMUNITY SERVICE DA, P1
   Hobaiter C, 2014, PLOS BIOL, V12, DOI 10.1371/journal.pbio.1001960
   Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
   Johansson O, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63367-z
   Johnson KVA, 2017, ANIM BEHAV, V128, P21, DOI 10.1016/j.anbehav.2017.04.001
   Johnson MF, 2014, GLOBAL ENVIRON CHANG, V29, P235, DOI 10.1016/j.gloenvcha.2014.10.006
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Kelling S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139600
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Kremen C, 2011, CONSERV BIOL, V25, P607, DOI 10.1111/j.1523-1739.2011.01657.x
   Kuhl HS, 2016, SCI REP-UK, V6, DOI 10.1038/srep22219
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kullenberg C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0147152
   McCarthy MS, 2019, ANIM BEHAV, V157, P227, DOI 10.1016/j.anbehav.2019.08.008
   McCarthy MS, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22904
   Moyer-Horner L, 2012, J WILDLIFE MANAGE, V76, P1472, DOI 10.1002/jwmg.373
   Nagy C, 2012, NORTHEAST NAT, V19, P143, DOI 10.1656/045.019.s611
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Patton F, 2008, PACHYDERM, P35
   Pebsworth PA, 2014, INT J PRIMATOL, V35, P825, DOI 10.1007/s10764-014-9802-4
   R Core Team, 2018, STATS PACK LANG ENV
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Siegel S., 1988, NONPARAMETRIC STAT B, V2nd ed
   Silk MJ, 2017, BIOSCIENCE, V67, P245, DOI 10.1093/biosci/biw175
   Silk MJ, 2015, ANIM BEHAV, V104, P1, DOI 10.1016/j.anbehav.2015.03.005
   Silvertown J, 2015, ZOOKEYS, P125, DOI 10.3897/zookeys.480.8803
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Stevick PT, 2001, CAN J FISH AQUAT SCI, V58, P1861, DOI 10.1139/cjfas-58-9-1861
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Toomey AH, 2013, HUM ECOL REV, V20, P50
   Van Horn RC, 2014, WILDLIFE BIOL, V20, P291, DOI 10.2981/wlb.00023
   VanderWaal KL, 2009, ANIM BEHAV, V77, P949, DOI 10.1016/j.anbehav.2008.12.028
   Viera AJ, 2005, FAM MED, V37, P360
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wittig R, 2019, CHIMPANZEES OF THE TAI FOREST: 40 YEARS OF RESEARCH, P44
   Wursig B., 1990, Reports of the International Whaling Commission Special Issue, P43
NR 70
TC 1
Z9 1
U1 1
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD FEB
PY 2021
VL 11
IS 4
BP 1598
EP 1608
DI 10.1002/ece3.7128
EA DEC 2020
PG 11
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA QG9IV
UT WOS:000599096200001
PM 33613992
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU de Oliveira, ML
   Peres, PHD
   Vogliotti, A
   Grotta-Neto, F
   de Azevedo, ADK
   Cerveira, JF
   do Nascimento, GB
   Peruzzi, NJ
   Carranza, J
   Duarte, JMB
AF de Oliveira, Marcio Leite
   de Faria Peres, Pedro Henrique
   Vogliotti, Alexandre
   Grotta-Neto, Francisco
   Koester de Azevedo, Allyson Diaz
   Cerveira, Josi Fernanda
   do Nascimento, Guilherme Batista
   Peruzzi, Nelson Jose
   Carranza, Juan
   Barbanti Duarte, Jose Mauricio
TI Phylogenetic signal in the circadian rhythm of morphologically
   convergent species of Neotropical deer
SO MAMMALIAN BIOLOGY
LA English
DT Article
DE Activity pattern; Mazama; Phylogenetic constrain; Camera trap; Brocket
   deer
ID DAILY ACTIVITY PATTERNS; NICHE CONSERVATISM; ACTIVITY PERIODS;
   PLASTICITY; SPECIATION; AMERICANA; ECOLOGY; TIME
AB Deer species included in the genus Mazama descend from two different clades that experienced a strong evolutionary convergence in morphology and behaviour when they adapted to Neotropical forests. We would expect that circadian activity rhythms also converged according to habitat features or responded to temporal niche segregation in sympatric species. We used camera trapping in four study areas, representing three main biomes in Brazil, together with data taken from the literature, to analyse activity patterns of five Mazama species in four biomes in South America. Our results show that Glade assignment was the main predictor of diurnal versus nocturnal activity, thus suggesting a phylogenetic constraint rather than any other ecological influence on circadian activity. We discuss how the evolutionary history of both lineages may have influenced their activity patterns. (C) 2016 Deutsche Gesellschaft fur Saugetierkunde. Published by Elsevier GmbH. All rights reserved.
C1 [de Oliveira, Marcio Leite; de Faria Peres, Pedro Henrique; Grotta-Neto, Francisco; Koester de Azevedo, Allyson Diaz; Cerveira, Josi Fernanda; Carranza, Juan; Barbanti Duarte, Jose Mauricio] Univ Estadual Paulista UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, Jaboticabal, SP, Brazil.
   [do Nascimento, Guilherme Batista] Univ Estadual Paulista UNESP, Dept Ciencias Exatas, Lab Estat Aplicada Genet & Melhoramento Anim, Jaboticabal, SP, Brazil.
   [Peruzzi, Nelson Jose] Univ Estadual Paulista UNESP, Dept Ciencias Exatas, Jaboticabal, SP, Brazil.
   [Carranza, Juan] Univ Cordoba, Ungulate Res Unit, Catedra Recursos Cineget & Piscicolas CRCP, E-14071 Cordoba, Spain.
   [Barbanti Duarte, Jose Mauricio] Univ Estadual Paulista UNESP, Dept Zootecnia, Jaboticabal, SP, Brazil.
   [Vogliotti, Alexandre] Univ Fed Integracao Latinoamer UNILA, Inst Latinoamer Ciencias Vida & Nat, Foz Do Iguacu, PR, Brazil.
   [Grotta-Neto, Francisco] Univ Fed Parana UFPR, Lab Biodiversidade Conservacao & Ecol Anim Silves, Curitiba, Parana, Brazil.
   [Carranza, Juan] Univ Agr Ecuador, Fac Med Vet & Zootecnia, Guayaquil, Ecuador.
RP de Oliveira, ML (corresponding author), Univ Estadual Paulista UNESP, Nucleo Pesquisa & Conservacao Cervideos NUPECCE, Jaboticabal, SP, Brazil.
EM oliveiraml1@yahoo.com.br
RI Nascimento, Guilherme/D-1926-2014; Carranza, Juan/E-4472-2010; de
   Oliveira, Márcio L/F-3792-2012; Duarte, José Maurício
   Barbanti/AAG-5149-2019
OI Nascimento, Guilherme/0000-0003-2370-322X; de Oliveira, Márcio
   L/0000-0002-7705-0626; Duarte, José Maurício
   Barbanti/0000-0002-7805-0265; Peres, Pedro Henrique/0000-0002-3158-0963;
   Grotta Neto, Francisco/0000-0002-2390-936X
FU Sao Paulo Research Foundation (FAPESP)Fundacao de Amparo a Pesquisa do
   Estado de Sao Paulo (FAPESP); National Council for Scientific and
   Technological Development (CNPq)Conselho Nacional de Desenvolvimento
   Cientifico e Tecnologico (CNPQ); Coordination for the Improvement of
   Higher Education Personnel (CAPES)Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior (CAPES); Project for the Conservation and
   Sustainable Use of Brazilian Biological Diversity (PROBIO)
FX This research was supported by the Sao Paulo Research Foundation
   (FAPESP), the National Council for Scientific and Technological
   Development (CNPq), the Coordination for the Improvement of Higher
   Education Personnel (CAPES) and the Project for the Conservation and
   Sustainable Use of Brazilian Biological Diversity (PROBIO). Collection
   licenses were provided by the Chico Mendes Institute for Biodiversity
   Conservation (ICMBio) and the Forestry Institute of Sao Paulo (IFSP). We
   thank Concha Mateos for advice on statistical procedures. Two anonymous
   reviewers made constructive comments that improved the manuscript.
CR Abril Vanessa Veltrini, 2010, P160
   Andersen R, 1998, EUROPEAN ROE DEER: THE BIOLOGY OF SUCCESS, P285
   Ayres M, 2007, BIOESTAT APLICACOES
   BEIER P, 1990, Wildlife Monographs, P1
   Black-Decima Patricia, 2000, Mastozoologia Neotropical, V7, P5
   Black-Decima Patricia, 2010, P190
   Boczko R., 1988, CONCEITOS ASTRONOMIA
   de Vivo M, 2004, J BIOGEOGR, V31, P943, DOI 10.1111/j.1365-2699.2004.01068.x
   Di Bitetti MS, 2008, BIOTROPICA, V40, P636, DOI 10.1111/j.1744-7429.2008.00413.x
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Foster SA, 2013, ANIM BEHAV, V85, P1003, DOI 10.1016/j.anbehav.2013.04.006
   Fox RJ, 2011, FUNCT ECOL, V25, P1096, DOI 10.1111/j.1365-2435.2011.01874.x
   Gallina Sonia, 2010, P101
   Geist, 1998, DEER WORLD THEIR EVO
   Gomez H, 2005, STUD NEOTROP FAUNA E, V40, P91, DOI 10.1080/01650520500129638
   Gonzalez Susana, 2010, P119
   HAFFER J, 1969, SCIENCE, V165, P131, DOI 10.1126/science.165.3889.131
   HARDIN G, 1960, SCIENCE, V131, P1292, DOI 10.1126/science.131.3409.1292
   JACOBS GH, 1993, BIOL REV, V68, P413, DOI 10.1111/j.1469-185X.1993.tb00738.x
   JARMAN PJ, 1974, BEHAVIOUR, V48, P215, DOI 10.1163/156853974X00345
   KAMMERMEYER KE, 1977, J WILDLIFE MANAGE, V41, P315, DOI 10.2307/3800612
   Kronfeld-Schor N, 2003, ANNU REV ECOL EVOL S, V34, P153, DOI 10.1146/annurev.ecolsys.34.011802.132435
   Kronfeld-Schor N, 2001, AM NAT, V158, P451, DOI 10.1086/321991
   Kronfeld-Schor N, 2008, BIOL RHYTHM RES, V39, P193, DOI 10.1080/09291010701683268
   Losos JB, 2008, ECOL LETT, V11, P995, DOI 10.1111/j.1461-0248.2008.01229.x
   Maffei L., 2002, REV B ECOL, V11, P55
   Merino Mariano L., 2010, P2
   Monterroso P, 2014, BEHAV ECOL SOCIOBIOL, V68, P1403, DOI 10.1007/s00265-014-1748-1
   Nowak R.M., 1999, WALKERS MAMMALS WORL
   Oliveira-Santos LGR, 2013, ANIM BEHAV, V85, P269, DOI 10.1016/j.anbehav.2012.09.033
   Owen-Smith N, 2014, J ZOOL, V293, P181, DOI 10.1111/jzo.12132
   Perez-Barberia FJ, 2002, EVOLUTION, V56, P1276
   Piersma T, 2003, TRENDS ECOL EVOL, V18, P228, DOI 10.1016/S0169-5347(03)00036-3
   Piovezan Ubiratan, 2010, P66
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rivero K, 2005, MAMMALIA, V69, P169, DOI 10.1515/mamm.2005.015
   Rivero K, 2004, EUR J WILDLIFE RES, V50, P161, DOI 10.1007/s10344-004-0064-x
   Roll U, 2006, EVOL ECOL, V20, P479, DOI 10.1007/s10682-006-0015-y
   Rossi Rogerio Vieira, 2010, P202
   Tobler MW, 2009, J TROP ECOL, V25, P261, DOI 10.1017/S0266467409005896
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Varela Diego Martin, 2010, P151
   Vila Alejandro R., 2010, P89
   Vogliotti A, 2010, NEOTROPICAL CERVIDOL, P218
   Webb Stephen L., 2010, International Journal of Ecology, V2010, P1
   Weber M, 2003, ECOSCIENCE, V10, P443, DOI 10.1080/11956860.2003.11682792
   Wiens JJ, 2004, EVOLUTION, V58, P193, DOI 10.1111/j.0014-3820.2004.tb01586.x
NR 48
TC 9
Z9 11
U1 1
U2 25
PU ELSEVIER GMBH
PI MUNICH
PA HACKERBRUCKE 6, 80335 MUNICH, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
PD MAY
PY 2016
VL 81
IS 3
BP 281
EP 289
DI 10.1016/j.mambio.2016.01.004
PG 9
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA DN1CV
UT WOS:000376804800007
OA Green Published
DA 2022-02-10
ER

PT J
AU Durbach, I
   Borchers, D
   Sutherland, C
   Sharma, K
AF Durbach, Ian
   Borchers, David
   Sutherland, Chris
   Sharma, Koustubh
TI Fast, flexible alternatives to regular grid designs for spatial
   capture-recapture
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera trap; population ecology; sampling; spatial capture&#8208;
   recapture; surveys
AB Spatial capture-recapture (SCR) methods use the location of detectors (camera traps, hair snares and live-capture traps) and the locations at which animals were detected (their spatial capture histories) to estimate animal density. Despite the often large expense and effort involved in placing detectors in a landscape, there has been relatively little work on how detectors should be located. A natural criterion is to place traps so as to maximize the precision of density estimators, but the lack of a closed-form expression for precision has made optimizing this criterion computationally demanding.
   Recent results by Efford and Boulanger (2019) show that precision can be well approximated by a function of the expected number of detected individuals and expected number of recapture events, both of which can be evaluated at low computational cost. We use these results to develop a method for obtaining survey designs that optimize this approximate precision for SCR studies using count or binary proximity detectors, or multi-catch traps.
   We show how the basic design protocol can be extended to incorporate spatially varying distributions of activity centres and animal detectability. We illustrate our approach by simulating from a camera trap study of snow leopards in Mongolia and comparing estimates from our designs to those generated by regular or optimized grid designs. Optimizing detector placement increased the number of detected individuals and recaptures, but this did not always lead to more precise density estimators due to less precise estimation of the effective sampling area. In most cases, the precision of density estimators was comparable to that obtained with grid designs, with improvement in some scenarios where approximate CV(D) < 20% and density varied spatially.
   Designs generated using our approach are transparent and statistically grounded. They can be produced for survey regions of any shape, adapt to known information about animal density and detectability, and are potentially easier and less costly to implement. We recommend their use as good, flexible candidate designs for SCR surveys when reasonable knowledge of model parameters exists. We provide software for researchers to construct their own designs, in the form of updates to design functions in the r package oSCR.
C1 [Durbach, Ian; Borchers, David; Sutherland, Chris] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
   [Durbach, Ian] Univ Cape Town, Ctr Stat Ecol Environm & Conservat, Cape Town, South Africa.
   [Sutherland, Chris] Univ Massachusetts, Dept Environm Conservat, Amherst, MA 01003 USA.
   [Sharma, Koustubh] Snow Leopard Trust, Seattle, WA USA.
RP Durbach, I (corresponding author), Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.; Durbach, I (corresponding author), Univ Cape Town, Ctr Stat Ecol Environm & Conservat, Cape Town, South Africa.
EM id52@st-andrews.ac.uk
RI Sutherland, Chris/H-6624-2019
OI Sutherland, Chris/0000-0003-2073-1751; Durbach, Ian/0000-0003-0769-2153;
   Sharma, Koustubh/0000-0001-7301-441X; Borchers,
   David/0000-0002-3944-0754
FU National Research Foundation of South AfricaNational Research Foundation
   - South Africa [105782, 90782]; Global Snow Leopard & Ecosystem
   Protection Program
FX National Research Foundation of South Africa, Grant/Award Number: 105782
   and 90782; Global Snow Leopard & Ecosystem Protection Program
CR Alexander JS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134815
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Clark JD, 2019, POPUL ECOL, V61, P93, DOI 10.1002/1438-390X.1011
   Dupont G., 2020, ECOLOGY, DOI DOI 10.1101/2020.04.16.045740V3
   Efford MG, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2580
   Efford MG, 2019, METHODS ECOL EVOL, V10, P1529, DOI 10.1111/2041-210X.13239
   Efford MG, 2013, OIKOS, V122, P918, DOI 10.1111/j.1600-0706.2012.20440.x
   Johansson O, 2016, BIODIVER WORL CONS, P355, DOI 10.1016/B978-0-12-802213-9.00026-2
   Kristensen TV, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2217
   McCarthy KP, 2008, J WILDLIFE MANAGE, V72, P1826, DOI 10.2193/2008-040
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Seber G.A.F., 1982, ESTIMATION ANIMAL AB, V8
   Sharma K, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0101319
   Sharma RK, 2015, BIOL CONSERV, V190, P8, DOI 10.1016/j.biocon.2015.04.026
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Sutherland C, 2015, METHODS ECOL EVOL, V6, P169, DOI 10.1111/2041-210X.12316
   Wolters, 2015, J STAT SOFTW CODE SN, V68, P1455, DOI [10.18637/jss.v068.c01, DOI 10.18637/JSS.V068.C01]
NR 18
TC 3
Z9 3
U1 0
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD FEB
PY 2021
VL 12
IS 2
BP 298
EP 310
DI 10.1111/2041-210X.13517
EA NOV 2020
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QB7SQ
UT WOS:000587573800001
OA Green Accepted
DA 2022-02-10
ER

PT C
AU Nguyen, H
   Maclagan, SJ
   Nguyen, TD
   Nguyen, T
   Flemons, P
   Andrews, K
   Ritchie, EG
   Phung, D
AF Nguyen, Hung
   Maclagan, Sarah J.
   Tu Dinh Nguyen
   Nguyen, Thin
   Flemons, Paul
   Andrews, Kylie
   Ritchie, Euan G.
   Dinh Phung
GP IEEE
TI Animal Recognition and Identification with Deep Convolutional Neural
   Networks for Automated Wildlife Monitoring
SO 2017 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED
   ANALYTICS (DSAA)
SE Proceedings of the International Conference on Data Science and Advanced
   Analytics
LA English
DT Proceedings Paper
CT 4th IEEE / ACM / ASA International Conference on Data Science and
   Advanced Analytics (DSAA)
CY OCT 19-21, 2017
CL Tokyo, JAPAN
SP IEEE, IEEE Computat Intelligence Soc, Kozo Keikaku Engn Inc, NEC Corp, Air Force Off Sci Res, Asian Off Aerosp Res & Dev, U S Army RDECOM, FEG, NS Solut Grp, KDDI Res, ACM, Amer Stat Assoc, Off Naval Res Global, Int Technol Ctr Pacific, Financial Engn Grp, Gunosy, FRONTEO, Automagi, Sansan, XCompass, Google, RECRUIT Holfdings, LIFULL, FINATEXT, Yahoo Japan Corp, Panasonic, Honda Res Inst Japan
DE deep learning; convolutional neural networks; large scale image
   classification; animal recognition; wildlife monitoring; citizen science
ID CITIZEN SCIENCE; TOOL
AB Efficient and reliable monitoring of wild animals in their natural habitats is essential to inform conservation and management decisions. Automatic covert cameras or "camera traps" are being an increasingly popular tool for wildlife monitoring due to their effectiveness and reliability in collecting data of wildlife unobtrusively, continuously and in large volume. However, processing such a large volume of images and videos captured from camera traps manually is extremely expensive, time-consuming and also monotonous. This presents a major obstacle to scientists and ecologists to monitor wildlife in an open environment. Leveraging on recent advances in deep learning techniques in computer vision, we propose in this paper a framework to build automated animal recognition in the wild, aiming at an automated wildlife monitoring system. In particular, we use a single-labeled dataset from Wildlife Spotter project, done by citizen scientists, and the state-of-the-art deep convolutional neural network architectures, to train a computational system capable of filtering animal images and identifying species automatically. Our experimental results achieved an accuracy at 96.6% for the task of detecting images containing animal, and 90.4% for identifying the three most common species among the set of images of wild animals taken in South-central Victoria, Australia, demonstrating the feasibility of building fully automated wildlife observation. This, in turn, can therefore speed up research findings, construct more efficient citizen science based monitoring systems and subsequent management decisions, having the potential to make significant impacts to the world of ecology and trap camera images analysis.
C1 [Nguyen, Hung; Tu Dinh Nguyen; Nguyen, Thin; Dinh Phung] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
   [Maclagan, Sarah J.; Ritchie, Euan G.] Deakin Univ, Ctr Integrat Ecol, Burwood, Australia.
   [Flemons, Paul] Australian Museum Res Inst, Sydney, NSW, Australia.
   [Andrews, Kylie] ABC Radio Natl, Ultimo, NSW, Australia.
RP Nguyen, H (corresponding author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
EM hung@deakin.edu.au; smaclaga@deakin.edu.au; tu.nguyen@deakin.edu.au;
   thin.nguyen@deakin.edu.au; paul.flemons@austmus.gov.au;
   andrews.kylie@abc.net.au; e.ritchie@deakin.edu.au;
   dinh.phung@deakin.edu.au
OI Phung, Dinh/0000-0002-9977-8247
FU Telstra-Deakin Centre of Excellence in Big Data and Machine Learning
FX This work is partially supported by the Telstra-Deakin Centre of
   Excellence in Big Data and Machine Learning.
CR Abadi Martin., 2016, ARXIV PREPRINT ARXIV
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bishop C. M., 2006, MACH LEARN, V128, P1, DOI DOI 10.1002/9780471740360.EBS0904
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Chollet F., 2020, KERAS DOCUMENTATION
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Collobert R, 2008, P 25 INT C MACH LEAR, P160, DOI 10.1145/1390156.1390177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Garrott R.A., 2012, ANAL WILDLIFE RADIO
   Gehring J, 2016, ARXIV PREPRINT ARXIV
   Gehring J., 2017, ARXIV E PRINTS
   Godley B. J., 2008, Endangered Species Research, V4, P3, DOI 10.3354/esr00060
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Gomez A., 2016, ARXIV160306169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hulbert IAR, 2001, J APPL ECOL, V38, P869, DOI 10.1046/j.1365-2664.2001.00624.x
   Irwin A., 1995, CITIZEN SCI STUDY PE
   Kays R., 2010, INT J RES REV WIREL, V1, P19
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   SZEWCZYK R., 2004, P 2 INT C EMB NETW S, DOI DOI 10.1145/1031495.1031521
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Vitousek PM, 1997, SCIENCE, V277, P494, DOI 10.1126/science.277.5325.494
   Wang J., 2010, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2010.5540018
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 37
TC 45
Z9 47
U1 5
U2 22
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2472-1573
BN 978-1-5090-5004-8
J9 PR INT CONF DATA SC
PY 2017
BP 40
EP 49
DI 10.1109/DSAA.2017.31
PG 10
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BL6PK
UT WOS:000454622300005
DA 2022-02-10
ER

PT J
AU Pereira, AD
   Bogoni, JA
   Bazilio, S
   Orsi, ML
AF Pereira, Alan Deivid
   Bogoni, Juliano A.
   Bazilio, Sergio
   Orsi, Mario Luis
TI Mammalian defaunation across the Devonian kniferidges and meridional
   plateaus of the Brazilian Atlantic Forest
SO BIODIVERSITY AND CONSERVATION
LA English
DT Article
DE Deforestation; Forest fragmentation; Land use; Mammal assemblages;
   Protected areas; Tropical forest
ID HABITAT STRUCTURE; CAMERA TRAPS; EXTRAPOLATION; BIODIVERSITY;
   RAREFACTION; DISPERSAL; DIVERSITY; POPULATIONS; DISTURBANCE; EXTINCTION
AB The vast majority of empirical studies on regional mammal defaunation across the Atlantic Forest biome of South America are concentrated in Southeastern and Northeastern regions. Thus, the lack of empirical information on the medium- to large-bodied mammal population declines across the subtropical region of this tropical biome and its major causes are paramount to conservation efforts. We investigate the influence of land use on mammal defaunation across the subtropical Atlantic Forest by sampling 91 points using 65 camera-traps installed for five consecutive years-totaling an effort of 30,189 camera-trap-days. We observed that the average defaunation across the Devonian Kniferidges Environmental Protection Area meta-region (spanning over subtropical Atlantic Forest plateaus) was 42% (D = 0.42; +/- 0.17) when compared to presumed assemblages of historical times going back to the Pre-Columbian era (ca. 500 years ago). The main landscape predictors of regional defaunation were silviculture and agriculture, once the highest defaunation indexes were concentered in sites intensely human-modified by these features. Protected areas had significantly lower defaunation indexes in comparison with the unprotected areas. Although our results of defaunation were 29.2% lesser than the average for the entire biome, the negative consequences of regional defaunation on ecosystem services are already occurring, once two in each five local species presumably are locally extinct or present low abundances (i.e. functionally extinct). Given that habitat conversion is the primary cause of global biodiversity decline, our results reinforce that biodiversity conservation is still strongly dependent on natural protected areas networks.
C1 [Pereira, Alan Deivid; Bogoni, Juliano A.] Univ Estadual Londrina, Ctr Ciencias Biol, Dept Biol Anim & Vegetal, Programa Posgrad Ciencias Biol, Rodovia Celso Garcia Cid,PR 445,Km 380, BR-86057970 Londrina, Parana, Brazil.
   [Bogoni, Juliano A.] Univ Sao Paulo, Escola Super Agr Luiz de Queiroz, Lab Ecol, Manejo & Conservacao Fauna Silvestre LEMaC, Sao Paulo, Brazil.
   [Bogoni, Juliano A.] Univ East Anglia, Sch Environm Sci, Norwich NR4 7TJ, Norfolk, England.
   [Bazilio, Sergio] Univ Estadual Parana UNESPAR, Campus Uniao da Vitoria,Caixa Postal 241, BR-84600970 Paranavai, Parana, Brazil.
   [Orsi, Mario Luis] Univ Estadual Londrina, Ctr Ciencias Biol, Dept Biol Anim & Vegetal, Lab Ecol Peixes & Invasoes Biol, Rodovia Celso Garcia Cid,PR 445,Km 380, BR-86057970 Londrina, Parana, Brazil.
RP Pereira, AD (corresponding author), Univ Estadual Londrina, Ctr Ciencias Biol, Dept Biol Anim & Vegetal, Programa Posgrad Ciencias Biol, Rodovia Celso Garcia Cid,PR 445,Km 380, BR-86057970 Londrina, Parana, Brazil.
EM alandeivid_bio@live.com
OI Pereira, Alan/0000-0002-3182-2344
FU Coordination for the Upgrading of Higher Education Personnel
   (CAPES)Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior
   (CAPES); Sao Paulo Research Foundation (FAPESP)Fundacao de Amparo a
   Pesquisa do Estado de Sao Paulo (FAPESP) [2018-05970-1, 2019-11901-5]
FX Coordination for the Upgrading of Higher Education Personnel (CAPES).
   JAB is supported by the Sao Paulo Research Foundation (FAPESP)
   postdoctoral fellowship grants 2018-05970-1 and 2019-11901-5.
CR Altrichter M, 2012, ORYX, V46, P87, DOI 10.1017/S0030605311000421
   Arroyo-Rodriguez V, 2020, ECOL LETT, V23, P1404, DOI 10.1111/ele.13535
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Beck H, 2006, J MAMMAL, V87, P519, DOI 10.1644/05-MAMM-A-174R1.1
   Beisiegel Beatriz de Mello, 2012, Cat News, P14
   Bello C, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1501105
   Bogoni JA, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-72010-w
   Bogoni JA, 2020, ECOSYST SERV, V45, DOI 10.1016/j.ecoser.2020.101173
   Bogoni JA, 2019, ECOGRAPHY, V42, P1803, DOI 10.1111/ecog.04670
   Bogoni JA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204515
   Bogoni JA, 2016, BIODIVERS CONSERV, V25, P1661, DOI 10.1007/s10531-016-1147-1
   Brown JH, 2004, ECOLOGY, V85, P1771, DOI 10.1890/03-9000
   Canale GR, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041671
   Cardillo M, 2005, SCIENCE, V309, P1239, DOI 10.1126/science.1116030
   Chao A, 2014, ECOL MONOGR, V84, P45, DOI 10.1890/13-0133.1
   Chao A, 2012, ECOLOGY, V93, P2533, DOI 10.1890/11-1952.1
   Chiarello A. G., 2000, Revista Brasileira de Biologia, V60, P237, DOI 10.1590/S0034-71082000000200007
   Chiarello AG, 2000, CONSERV BIOL, V14, P1649, DOI 10.1046/j.1523-1739.2000.99071.x
   Conte C.E., 2017, REVISOES ZOOLOGIA MA, P391
   Culot L, 2013, BIOL CONSERV, V163, P79, DOI 10.1016/j.biocon.2013.04.004
   Da Fonseca G.A.B., 2004, HOTSPOTS REVISITED
   Dean W, 1996, FERRO FOGO HIST DEV
   Devictor V, 2008, OIKOS, V117, P507, DOI 10.1111/j.2008.0030-1299.16215.x
   DIRZO R, 1990, CONSERV BIOL, V4, P444, DOI 10.1111/j.1523-1739.1990.tb00320.x
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Dobbins M, 2020, J APPL ECOL, V57, P2100, DOI 10.1111/1365-2664.13750
   Elschot K, 2015, MAR ECOL PROG SER, V537, P9, DOI 10.3354/meps11447
   Fahrig L, 2003, ANNU REV ECOL EVOL S, V34, P487, DOI 10.1146/annurev.ecolsys.34.011802.132419
   Farago PV., 2001, PUBLICATIO UEPG PONT, V7, P57, DOI [10.5212/publicatio, DOI 10.5212/PUBLICATIO]
   Ferraz, 2021, MAMMAL REV
   Galetti M, 2006, BOT J LINN SOC, V151, P141, DOI 10.1111/j.1095-8339.2006.00529.x
   Galetti M, 2017, ANIM CONSERV, V20, P270, DOI 10.1111/acv.12311
   Galetti M, 2015, GLOB ECOL CONSERV, V3, P824, DOI 10.1016/j.gecco.2015.04.008
   Galetti M, 2015, BIOL CONSERV, V190, P2, DOI 10.1016/j.biocon.2015.04.032
   Galetti M, 2013, BIOL CONSERV, V163, P1, DOI 10.1016/j.biocon.2013.04.020
   Galetti M, 2009, BIOL CONSERV, V142, P1229, DOI 10.1016/j.biocon.2009.01.023
   Gardner TA, 2009, ECOL LETT, V12, P561, DOI 10.1111/j.1461-0248.2009.01294.x
   Gayot M, 2004, J TROP ECOL, V20, P31, DOI 10.1017/S0266467404006157
   Giacomini HC, 2013, BIOL CONSERV, V163, P33, DOI 10.1016/j.biocon.2013.04.007
   Gonzalez-Maya JF, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0175931
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Herkt KMB, 2017, GLOBAL ECOL BIOGEOGR, V26, P930, DOI 10.1111/geb.12601
   Hines NJ, 2017, OCCUPANCY ESTIMATION
   Hoskins AJ, 2016, ECOL EVOL, V6, P3040, DOI 10.1002/ece3.2104
   Hsieh TC, 2016, METHODS ECOL EVOL, V7, P1451, DOI 10.1111/2041-210X.12613
   IBGE, 2019, I BRAS GEOGR EST CON
   IUCN, 2019, IUCN RED LIST THREAT
   Jorge MLSP, 2013, BIOL CONSERV, V163, P49, DOI 10.1016/j.biocon.2013.04.018
   Kurten EL, 2015, ECOLOGY, V96, P1923, DOI 10.1890/14-1735.1
   Kurten EL, 2013, BIOL CONSERV, V163, P22, DOI 10.1016/j.biocon.2013.04.025
   Lanz B, 2018, ECOL ECON, V144, P260, DOI 10.1016/j.ecolecon.2017.07.018
   Lyra-Jorge MC, 2008, EUR J WILDLIFE RES, V54, P739, DOI 10.1007/s10344-008-0205-8
   Maack R, 2017, GEOGRAFA FISICA ESTA
   Magioli M, 2016, EUR J WILDLIFE RES, V62, P431, DOI 10.1007/s10344-016-1017-x
   Markl JS, 2012, CONSERV BIOL, V26, P1072, DOI 10.1111/j.1523-1739.2012.01927.x
   Melo FPL, 2013, TRENDS ECOL EVOL, V28, P462, DOI 10.1016/j.tree.2013.01.001
   Moro RS, 2007, TERR PLURAL, V1, P115
   Newbold T, 2015, NATURE, V520, P45, DOI 10.1038/nature14324
   Nja, 2012, J MATH RES, V4, P148, DOI [10.5539/jmr.v4n2p148, DOI 10.5539/JMR.V4N2P148]
   Nolte S, 2013, ESTUAR COAST SHELF S, V135, P296, DOI 10.1016/j.ecss.2013.10.026
   Nunez-Regueiro MM, 2015, BIOL CONSERV, V187, P19, DOI 10.1016/j.biocon.2015.04.001
   O'Farrill G, 2013, INTEGR ZOOL, V8, P4, DOI 10.1111/j.1749-4877.2012.00316.x
   Paglia A. P., 2012, CONS BIOL, V6, P1
   PARANA, 2010, LIST ESP MAM PERT FA
   Paviolo A, 2016, SCI REP-UK, V6, DOI 10.1038/srep37147
   Peel MC, 2007, HYDROL EARTH SYST SC, V11, P1633, DOI 10.5194/hess-11-1633-2007
   Pereira AD, 2020, STUD NEOTROP FAUNA E, V55, P139, DOI 10.1080/01650521.2019.1707419
   PERES CA, 1990, BIOL CONSERV, V54, P47, DOI 10.1016/0006-3207(90)90041-M
   Peres CA, 2016, P NATL ACAD SCI USA, V113, P892, DOI 10.1073/pnas.1516525113
   Polishchuk LV, 2010, EVOL ECOL RES, V12, P1
   Puttker T, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108368
   R Core Team, 2020, LANGUAGE ENV STAT CO
   REDFORD KH, 1992, BIOSCIENCE, V42, P412, DOI 10.2307/1311860
   Ribeiro MC, 2009, BIOL CONSERV, V142, P1141, DOI 10.1016/j.biocon.2009.02.021
   Rondinini C, 2011, PHILOS T R SOC B, V366, P2633, DOI 10.1098/rstb.2011.0113
   Schmidt BR, 2005, AQUAT CONSERV, V15, P681, DOI 10.1002/aqc.740
   Schrama M, 2013, OECOLOGIA, V172, P231, DOI 10.1007/s00442-012-2484-8
   Srbek-Araujo AC, 2017, ORYX, V51, P246, DOI 10.1017/S0030605315001222
   Srbek-Araujo AC, 2013, BIOTA NEOTROP, V13, P51, DOI 10.1590/S1676-06032013000200005
   Stoner C, 2007, AFR J ECOL, V45, P202, DOI 10.1111/j.1365-2028.2006.00705.x
   Taylor RA, 2016, ECOLOGY, V97, P951, DOI 10.1890/15-0885.1
   TERBORGH J, 1988, CONSERV BIOL, V2, P402, DOI 10.1111/j.1523-1739.1988.tb00207.x
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Trigo TC, 2013, CURR BIOL, V23, P2528, DOI 10.1016/j.cub.2013.10.046
   Venables W.N., 2002, MODERN APPL STAT S P, VFourth, DOI [10.1007/978-0-387-21706-2, DOI 10.1007/978-0-387-21706-2]
   Weckel M, 2006, J ZOOL, V270, P25, DOI 10.1111/j.1469-7998.2006.00106.x
   WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269
   Wilkie DS, 2011, ANN NY ACAD SCI, V1223, P120, DOI 10.1111/j.1749-6632.2010.05908.x
   Wilson MC, 2016, LANDSCAPE ECOL, V31, P219, DOI 10.1007/s10980-015-0312-3
   Zimbres B, 2017, BIOL CONSERV, V206, P283, DOI 10.1016/j.biocon.2016.11.033
   Zuur Alain F., 2009, P1
NR 91
TC 0
Z9 0
U1 2
U2 2
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0960-3115
EI 1572-9710
J9 BIODIVERS CONSERV
JI Biodivers. Conserv.
PD NOV
PY 2021
VL 30
IS 13
BP 4005
EP 4022
DI 10.1007/s10531-021-02288-3
EA SEP 2021
PG 18
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA WH5KB
UT WOS:000697616800002
DA 2022-02-10
ER

PT J
AU Whitworth, A
   Whittaker, L
   Huarcaya, RP
   Flatt, E
   Morales, ML
   Connor, D
   Priego, MG
   Forsyth, A
   Beirne, C
AF Whitworth, Andrew
   Whittaker, Lawrence
   Pillco Huarcaya, Ruthmery
   Flatt, Eleanor
   Morales, Marvin Lopez
   Connor, Danielle
   Priego, Marina Garrido
   Forsyth, Adrian
   Beirne, Chris
TI Spider Monkeys Rule the Roost: Ateline Sleeping Sites Influence
   Rainforest Heterogeneity
SO ANIMALS
LA English
DT Article
DE rainforest; wildlife; camera traps; Ateline; primates; biodiversity;
   seed dispersal; seed predation; trophic interactions
ID ATELES-GEOFFROYI-YUCATANENSIS; SEED DISPERSAL; CAMERA TRAPS; ACTIVITY
   RHYTHM; HOWLER MONKEYS; DUNG; PRIMATES; PATTERNS; MAMMALS; IMPACT
AB Simple Summary Spider monkeys are important dispersers of many hardwood trees that contribute greatly to the carbon sequestration of tropical forests. One way in which Spider monkeys influence tropical ecosystem structure and function is through the creation of visible terrestrial latrines beneath their "sleeping sites"-trees in which they frequently return to sleep. Spider monkey latrines are thought to create high quality resource patches for rainforest plants and other wildlife to exploit. We investigate this using camera traps placed in both the canopy and on the rainforest floor to determine which rainforest wildlife are attracted to the latrines beneath the sleeping sites of spider monkeys. We also assess the tree species and dung beetles found within the latrines compared with other areas of the forest. Our evidence suggests that spider monkey roosting sites are a hub of activity for other rainforest wildlife, and act as germinating beds for many rainforest trees. If rainforests were to lose spider monkeys, from intensive hunting for example, many other rainforest wildlife species would be affected, and forests would therefore be made up of different tree communities than landscapes where spider monkeys exist.
   Abstract The sleeping site behavior of Ateline primates has been of interest since the 1980s, yet limited focus has been given to their influence upon other rainforest species. Here, we use a combination of arboreal and terrestrial camera traps, and dung beetle pitfall traps, to characterize spider monkey sleeping site use and quantify the impact of their associated latrines on terrestrial vertebrate and dung beetle activity. We also characterize the physical characteristics of the sleeping sites and the floristic and soil composition of latrines beneath them. Spider monkey activity at sleeping sites peaked at dawn and dusk and group composition varied by sex of the adults detected. The habitat-use of terrestrial fauna (vertebrates and dung beetles) differed between latrine sites and non-latrine controls, underpinned by species-specific changes in the relative abundance of several seed-dispersing species (such as paca and great curassow). Seedling density was higher in latrines than in non-latrine controls. Although most soil properties were similar between latrines and controls, potassium and manganese concentrations were different. These results suggest that spider monkey sleeping site fidelity leads to a hotspot of ecological activity in latrines and downstream impacts on rainforest floristic composition and diversity.
C1 [Whitworth, Andrew; Whittaker, Lawrence; Pillco Huarcaya, Ruthmery; Flatt, Eleanor; Morales, Marvin Lopez; Priego, Marina Garrido; Forsyth, Adrian] Osa Conservat, Conservat Sci Team, Washington, DC 20005 USA.
   [Whitworth, Andrew] Univ Glasgow, Inst Biodivers Anim Hlth & Comparat Med, Coll Med Vet & Life Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Whittaker, Lawrence] Imperial Coll London, Div Biol, Silwood Pk Campus, Ascot SL5 7PY, Berks, England.
   [Connor, Danielle; Beirne, Chris] Univ Exeter, Sch Bio Sci, Ctr Ecol & Conservat, Penryn Campus, Penryn TR10 9FE, Cornwall, England.
RP Whitworth, A (corresponding author), Osa Conservat, Conservat Sci Team, Washington, DC 20005 USA.; Whitworth, A (corresponding author), Univ Glasgow, Inst Biodivers Anim Hlth & Comparat Med, Coll Med Vet & Life Sci, Glasgow G12 8QQ, Lanark, Scotland.
EM andy.w.whitworth@gmail.com; LawrenceWhittaker@hotmail.co.uk;
   ruthp@osaconservation.org; eleanorflatt@osaconservation.org;
   marvinlopez@osaconservation.org; daniconnor1995@gmail.com;
   marinagarrido@osaconservation.org; adrianforsyth@gmail.com;
   c.w.beirne@gmail.com
OI Whitworth, Andrew/0000-0001-6197-996X
FU Margot Marsh Biodiversity Fund
FX This field research was funded by the Margot Marsh Biodiversity Fund.
CR Urrea-Galeano LA, 2019, BIOTROPICA, V51, P186, DOI 10.1111/btp.12631
   AHUMADA JA, 1992, INT J PRIMATOL, V13, P33, DOI 10.1007/BF02547726
   Altrichter M, 2001, REV BIOL TROP, V49, P1183
   Anderson JR, 1998, AM J PRIMATOL, V46, P63, DOI 10.1002/(SICI)1098-2345(1998)46:1<63::AID-AJP5>3.3.CO;2-F
   Andresen E, 2004, OECOLOGIA, V139, P45, DOI 10.1007/s00442-003-1480-4
   AQUINO R, 1986, AM J PRIMATOL, V11, P319, DOI 10.1002/ajp.1350110403
   Bates D., 2007, MANUSCR U WIS, V15, P1
   Bello C, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1501105
   Bogoni JA, 2019, ECOGRAPHY, V42, P1803, DOI 10.1111/ecog.04670
   Bowler MT, 2017, REMOTE SENS ECOL CON, V3, P146, DOI 10.1002/rse2.35
   Boza M.A., 1988, COSTA RICA NATL PARK
   Brodie JF, 2016, TRENDS ECOL EVOL, V31, P414, DOI 10.1016/j.tree.2016.03.019
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Busia L, 2017, ETHOLOGY, V123, P405, DOI 10.1111/eth.12609
   BUXTON AP, 1951, J ANIM ECOL, V20, P31, DOI 10.2307/1642
   Camargo-Sanabria AA, 2016, ACTA OECOL, V73, P45, DOI 10.1016/j.actao.2016.02.005
   Ceballos G, 2017, P NATL ACAD SCI USA, V114, pE6089, DOI 10.1073/pnas.1704949114
   Chanthorn W, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-46399-y
   CHAPMAN CA, 1989, AM J PRIMATOL, V18, P53, DOI 10.1002/ajp.1350180106
   CHAPMAN CA, 1989, OECOLOGIA, V79, P506, DOI 10.1007/BF00378668
   Chapman CA, 2013, INT J PRIMATOL, V34, P1, DOI 10.1007/s10764-012-9645-9
   CLARKE KR, 1993, AUST J ECOL, V18, P117, DOI 10.1111/j.1442-9993.1993.tb00438.x
   Culot L, 2018, INT J PRIMATOL, V39, P397, DOI 10.1007/s10764-018-0041-y
   Dew J.L., 2008, SPIDER MONKEYS BEHAV
   Dos Santos Neves N, 2010, AUSTRAL ECOL, V35, P549, DOI 10.1111/j.1442-9993.2009.02066.x
   Droscher I, 2014, BEHAV ECOL SOCIOBIOL, V68, P2043, DOI 10.1007/s00265-014-1810-z
   Fan PF, 2008, AM J PRIMATOL, V70, P153, DOI 10.1002/ajp.20470
   Feeley K, 2005, J TROP ECOL, V21, P99, DOI 10.1017/S0266467404001701
   Fragoso JMV, 2003, ECOLOGY, V84, P1998, DOI 10.1890/01-0621
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Gonzalez-Zamora A, 2009, AM J PRIMATOL, V71, P8, DOI [10.1002/ajp.20625, 10.1002/ajp.20753]
   Gonzalez-Zamora A, 2015, J TROP ECOL, V31, P305, DOI 10.1017/S026646741500022X
   Gonzalez-Zamora A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0089346
   Gonzalez-Zamora A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0046852
   Gregory T, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04112-x
   Griffiths HM, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.1634
   HARTSHORN G S, 1983, P423
   Hartwell KS, 2014, INT J PRIMATOL, V35, P425, DOI 10.1007/s10764-013-9746-0
   Herwitz S.R., 1981, REGENERATION SELECTE, V24
   Holdridge L. R., 1967, Life zone ecology.
   HOWDEN H F, 1975, Biotropica, V7, P77, DOI 10.2307/2989750
   Irwin MT, 2004, J MAMMAL, V85, P420, DOI 10.1644/1545-1542(2004)085<0420:LLOOLB>2.0.CO;2
   Lambert Joanna E., 2005, P137, DOI 10.1079/9780851998060.0137
   Larsen TH, 2005, BIOTROPICA, V37, P322, DOI 10.1111/j.1744-7429.2005.00042.x
   Lawson CR, 2012, BIOTROPICA, V44, P271, DOI 10.1111/j.1744-7429.2012.00871.x
   LAWTON JH, 1995, LINKING SPECIES & ECOSYSTEMS, P141
   Levi T, 2009, J APPL ECOL, V46, P804, DOI 10.1111/j.1365-2664.2009.01661.x
   Link A, 2006, J TROP ECOL, V22, P235, DOI 10.1017/S0266467405003081
   Jose-Dominguez JM, 2015, INT J PRIMATOL, V36, P948, DOI 10.1007/s10764-015-9865-x
   Marsh CJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073147
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Munoz-Delgado J, 2004, PHYSIOL BEHAV, V83, P107, DOI 10.1016/j.physbeh.2004.07.015
   Munoz-Delgado J, 2014, CHRONOBIOL INT, V31, P983, DOI 10.3109/07420528.2014.938813
   Nichols E, 2008, BIOL CONSERV, V141, P1461, DOI 10.1016/j.biocon.2008.04.011
   Oksanen J., 2007, VEGAN PACK COMMUN EC, V10, P631
   Oliveira-Santos LGR, 2008, J TROP ECOL, V24, P563, DOI 10.1017/S0266467408005324
   Bravo SP, 2012, ECOL RES, V27, P311, DOI 10.1007/s11284-011-0904-6
   Peres CA, 2016, P NATL ACAD SCI USA, V113, P892, DOI 10.1073/pnas.1516525113
   Huarcaya RP, 2020, ORYX, V54, P901, DOI 10.1017/S0030605318001096
   Pouvelle S, 2009, J TROP ECOL, V25, P239, DOI 10.1017/S0266467409005987
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
   Russo SE, 2004, ECOL LETT, V7, P1058, DOI 10.1111/j.1461-0248.2004.00668.x
   Scherbaum C, 2013, CURR ZOOL, V59, P125, DOI 10.1093/czoolo/59.1.125
   Spaan D, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3020034
   Tan CL, 2013, PRIMATES, V54, P1, DOI 10.1007/s10329-012-0318-2
   Velazquez-Vazquez G, 2015, INT J PRIMATOL, V36, P1154, DOI 10.1007/s10764-015-9883-8
   Wallace R.B., 2010, SPIDER MONKEYS, P138
   Wallace Robert B., 2001, Neotropical Primates, V9, P101
   Watmough SA, 2007, APPL GEOCHEM, V22, P1241, DOI 10.1016/j.apgeochem.2007.03.039
   Weghorst JA, 2007, PRIMATES, V48, P108, DOI 10.1007/s10329-006-0025-y
   Whitworth A, 2019, DIVERS DISTRIB, V25, P1166, DOI 10.1111/ddi.12930
   Whitworth A, 2018, CONDOR, V120, P852, DOI 10.1650/CONDOR-18-57.1
   Whitworth A, 2016, TROP CONSERV SCI, V9, P675, DOI 10.1177/194008291600900208
   Wright SJ, 2011, ECOLOGY, V92, P1616, DOI 10.1890/10-1558.1
NR 75
TC 1
Z9 2
U1 5
U2 11
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD DEC
PY 2019
VL 9
IS 12
AR 1052
DI 10.3390/ani9121052
PG 16
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Veterinary Sciences; Zoology
GA KB6XV
UT WOS:000506636400051
PM 31805694
OA gold, Green Published, Green Accepted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Wojcik, B
   Evans, BE
   Iehl, EE
   Barker, RE
   Wheeler, ME
   Peterson, BE
   Dohm, RI
   Mueller, MA
   Olson, LO
   Ederer, B
   Stewart, M
   Crimmins, S
   Pemble, K
   Van Stappen, J
   Olson, E
   Van Deelen, TR
AF Allen, Maximilian L.
   Wojcik, Beth
   Evans, Bryn E.
   Iehl, Emily E.
   Barker, Rachel E.
   Wheeler, Michael E.
   Peterson, Brittany E.
   Dohm, Regan I.
   Mueller, Marcus A.
   Olson, Lucas O.
   Ederer, Brittany
   Stewart, Margaret
   Crimmins, Shawn
   Pemble, Ken
   Van Stappen, Julie
   Olson, Erik
   Van Deelen, Timothy R.
TI Detection of Endangered American Martens (Martes americana) in Apostle
   Islands National Lakeshore, Wisconsin
SO AMERICAN MIDLAND NATURALIST
LA English
DT Article
ID CARNIVORES; MARKING
AB American martens (Martes americana) warrant concern in Wisconsin, U.S.A., for multiple reasons, including being the state's only endangered mammal and a clan animal of the Ojibwe tribes. American martens were once present throughout much of the state but were extirpated in the early 20th century through habitat loss and unregulated trapping. In the 1950s two reintroductions of martens to Stockton island of the Apostle Islands archipelago were considered failures, with the last confirmed sighting in the archipelago in 1969. In the decades since the Stockton Island reintroduction efforts; anecdotal reports of martens have surfaced throughout the archipelago. In 2014-2016 we deployed 91 camera traps on 13 of the 21 Apostle Islands to survey the archipelago's extant carnivore species. We detected American martens at 28 of 87 functioning camera trap sites on 5 of 13 monitored islands and documented the existence of American martens in APIS in Wisconsin for the first time its over 50 y. We suggest continued research to evaluate the status of the APIS population and its potential origins to guide future conservation efforts.
C1 [Allen, Maximilian L.; Wojcik, Beth; Evans, Bryn E.; Iehl, Emily E.; Barker, Rachel E.; Wheeler, Michael E.; Peterson, Brittany E.; Dohm, Regan I.; Mueller, Marcus A.; Olson, Lucas O.; Ederer, Brittany; Stewart, Margaret; Van Deelen, Timothy R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Crimmins, Shawn] Univ Wisconsin, Coll Nat Resources, 800 Reserve St, Stevens Point, WI 54481 USA.
   [Pemble, Ken; Van Stappen, Julie] Apostle Isl Natl Lakeshore, Planning & Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
   [Olson, Erik] Northland Coll, Nat Resources, 1411 Ellis Ave S, Ashland, WI 54806 USA.
RP Allen, ML (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM maximilian.allen@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU United States Department of the Interior (USDI) [P14AC01180]; Apostle
   Islands National Lakeshore [P14AC01180]; Great Lakes Northern Forest
   CESU [P14AC01180]; Northland College (Department of Natural Resources,
   Sigurd Olson Professorship in the Natural Sciences, Morris O. Ristvedt
   Professorship in the Natural Sciences); University of Wisconsin
   (Schorger fund); University of Wisconsin (Department of Forest and
   Wildlife Ecology); University of Wisconsin (Beers-Bascom Professorship
   in Conservation, College of Agriculture and Life Sciences)
FX This project was supported by the United States Department of the
   Interior (USDI), Apostle Islands National Lakeshore and the Great Lakes
   Northern Forest CESU (Cooperative Agreement P14AC01180), Northland
   College (Department of Natural Resources, Sigurd Olson Professorship in
   the Natural Sciences, Morris O. Ristvedt Professorship in the Natural
   Sciences) and the University of Wisconsin (Schorger fund, Department of
   Forest and Wildlife Ecology; Beers-Bascom Professorship in Conservation,
   College of Agriculture and Life Sciences). We thank the personnel from
   each group that contributed to this project, especially the numerous
   technicians from the National Park Service, and students from Northland
   College that provided assistance over the course of the project.
CR ALLEN M. L., 2017, SURVEY TECHNIQUES CO
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   BECANT J. L, 2002, NAT AREA J, V22, P180
   BiEscit J. C., 2008, PEOPLE PLACES HUMAN
   BRANDER R. B., 1978, APOSTLE ISLANDS NATL
   Buskirk S. W., 1994, SCI BASIS CONSERVING, P7
   Carlson JE, 2014, J WILDLIFE MANAGE, V78, P1499, DOI 10.1002/jwmg.785
   CRAVEN S R, 1987, Colonial Waterbirds, V10, P64, DOI 10.2307/1521232
   Harmsen BJ, 2010, J MAMMAL, V91, P1225, DOI 10.1644/09-MAMM-A-416.1
   Jackson H. H. T., 1961, MAMMALS OF WISCONSIN
   JoiwirAt H. C., 1956, MARTEN RELEASE STOCK
   Kohn B.E., 1987, 143 WISC DEP NAT RES
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   MAcARTHUR R. H, 1967, THEORY ISLAND BIOGEO, P206
   Manlick PJ, 2017, CONSERV LETT, V10, P178, DOI 10.1111/conl.12257
   Naing H, 2015, RAFFLES B ZOOL, V63, P376
   O'COSISI A. F., 2011, CAMERA TRAPS ANIMAL
   SNIFF U, 2015, GLKN200503
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilson EO, 2010, THEORY OF ISLAND BIOGEOGRAPHY REVISITED, P1
   WOODEOFD J. E., 2011, CONSERVATION MANAGEM
   Woodford JE, 2013, WILDLIFE SOC B, V37, P616, DOI 10.1002/wsb.291
NR 23
TC 2
Z9 2
U1 1
U2 24
PU AMER MIDLAND NATURALIST
PI NOTRE DAME
PA UNIV NOTRE DAME, BOX 369, ROOM 295 GLSC, NOTRE DAME, IN 46556 USA
SN 0003-0031
EI 1938-4238
J9 AM MIDL NAT
JI Am. Midl. Nat.
PD APR
PY 2018
VL 179
IS 2
BP 294
EP 298
PG 5
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA GB5WA
UT WOS:000429137400010
DA 2022-02-10
ER

PT J
AU Wang, BX
   Rocha, DG
   Abrahams, MI
   Antunes, AP
   Costa, HCM
   Goncalves, ALS
   Spironello, WR
   de Paula, MJ
   Peres, CA
   Pezzuti, J
   Ramalho, E
   Reis, ML
   Carvalho, E
   Rohe, F
   Macdonald, DW
   Tan, CKW
AF Wang, Bingxin
   Rocha, Daniel G.
   Abrahams, Mark I.
   Antunes, Andre P.
   Costa, Hugo C. M.
   Sousa Goncalves, Andre Luis
   Spironello, Wilson Roberto
   de Paula, Milton Jose
   Peres, Carlos A.
   Pezzuti, Juarez
   Ramalho, Emiliano
   Reis, Marcelo Lima
   Carvalho, Elildo, Jr.
   Rohe, Fabio
   Macdonald, David W.
   Tan, Cedric Kai Wei
TI Habitat use of the ocelot (Leopardus pardalis) in Brazilian Amazon
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE Brazilian Amazon; camera traps; mesopredator; occupancy; ocelot;
   restricted spatial regression
ID ATLANTIC FOREST; NEOFELIS-NEBULOSA; ACTIVITY PATTERNS; TEMPORAL
   ACTIVITY; TROPICAL FORESTS; PROTECTED AREAS; FELIS-PARDALIS;
   RAIN-FOREST; OCCUPANCY; CONSERVATION
AB Amazonia forest plays a major role in providing ecosystem services for human and sanctuaries for wildlife. However, ongoing deforestation and habitat fragmentation in the Brazilian Amazon has threatened both. The ocelot is an ecologically important mesopredator and a potential conservation ambassador species, yet there are no previous studies on its habitat preference and spatial patterns in this biome. From 2010 to 2017, twelve sites were surveyed, totaling 899 camera trap stations, the largest known dataset for this species. Using occupancy modeling incorporating spatial autocorrelation, we assessed habitat use for ocelot populations across the Brazilian Amazon. Our results revealed a positive sigmoidal correlation between remote-sensing derived metrics of forest cover, disjunct core area density, elevation, distance to roads, distance to settlements and habitat use, and that habitat use by ocelots was negatively associated with slope and distance to river/lake. These findings shed light on the regional scale habitat use of ocelots and indicate important species-habitat relationships, thus providing valuable information for conservation management and land-use planning.
C1 [Wang, Bingxin; Macdonald, David W.; Tan, Cedric Kai Wei] Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney, Oxon, England.
   [Wang, Bingxin] Chinese Acad Sci, Inst Bot, State Key Lab Vegetat & Environm Change, Beijing, Peoples R China.
   [Wang, Bingxin] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Rocha, Daniel G.] Univ Calif Davis, Dept Wildlife Fish & Conservat Biol, Grad Grp Ecol, Davis, CA 95616 USA.
   [Rocha, Daniel G.] Inst Dev Sustentavel Mamiraua, Grp Ecol & Conservacao Felinos Amazonia, Tefe, Brazil.
   [Abrahams, Mark I.] Bristol Zool Soc, Field Conservat & Sci Dept, Bristol, Avon, England.
   [Antunes, Andre P.] Redefauna Rede Pesquisa Biodiversidade Conservaca, Manaus, Amazonas, Brazil.
   [Costa, Hugo C. M.] Univ Estadual Santa Cruz, Programa Posgrad Ecol & Conservacao Biodiversidad, Ilheus, Brazil.
   [Sousa Goncalves, Andre Luis; Spironello, Wilson Roberto] Inst Nacl de Pesquisas da Amazonia, Grp Pesquisa Mamiferos Amazon, Manaus, Amazonas, Brazil.
   [de Paula, Milton Jose; Pezzuti, Juarez] Univ Para, Ctr Adv Amazon Studies, Altamira, Brazil.
   [de Paula, Milton Jose] Univ Fed Para, Programa Posgrad Ecol, Belem, Para, Brazil.
   [de Paula, Milton Jose] EMBRAPA Amazonia Oriental, Belem, Para, Brazil.
   [Peres, Carlos A.] Univ East Anglia, Cetre Ecol Evolut & Conservat, Sch Environm Sci, Norwich, Norfolk, England.
   [Ramalho, Emiliano] Inst Dev Sustentavel Mamiraua, Tefe, Brazil.
   [Reis, Marcelo Lima] ICMBio CNPq, Brasilia, DF, Brazil.
   [Carvalho, Elildo, Jr.] Inst Chico Mendes Conservacao Biodiversidade, Ctr Nacl Pesquisa & Conservacao Mamiferos Carnivo, Atibaia, Brazil.
   [Carvalho, Elildo, Jr.] Norwegian Univ Life Sci, Fac Ecol & Nat Resource Management, As, Norway.
   [Rohe, Fabio] INPA, Programa Posgrad Genet Conservacao & Biol Evolut, Manaus, Amazonas, Brazil.
   [Rohe, Fabio] Wildlife Conservat Soc Brazil, Amazon Program, Manaus, Amazonas, Brazil.
RP Tan, CKW (corresponding author), Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney, Oxon, England.
EM cedric.tan@zoo.ox.ac.uk
RI Peres, Carlos A./ABE-8361-2020; Carvalho, Elildo/R-8556-2019; Peres,
   Carlos A./B-1276-2013; Peres, Carlos Augusto/N-8275-2019; Pezzuti,
   Juarez/X-4061-2019; Antunes, Andre/Q-3236-2018
OI Carvalho, Elildo/0000-0003-4356-2954; Peres, Carlos
   A./0000-0002-1588-8765; Peres, Carlos Augusto/0000-0002-1588-8765;
   abrahams, mark/0000-0002-6424-187X; Tan, Cedric/0000-0001-6505-2467;
   Wang, Bingxin/0000-0002-7021-8625; Gomes da Rocha,
   Daniel/0000-0002-0100-3102; Antunes, Andre/0000-0003-4404-3486
FU CAPES/Doutorado Pleno no exterior [88881.128140/2016-01];
   Recanati-Kaplan Foundation; Tang Family Foundation; Gordon & Betty Moore
   FoundationGordon and Betty Moore Foundation; Instituto Nacional de
   Pesquisas da Amazonia; Programa Areas Protegidas da Amazonia
FX CAPES/Doutorado Pleno no exterior, Grant/Award Number:
   88881.128140/2016-01; Recanati-Kaplan Foundation; Tang Family
   Foundation; Gordon & Betty Moore Foundation; Instituto Nacional de
   Pesquisas da Amazonia; Programa Areas Protegidas da Amazonia
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Antunes AP, 2016, SCI ADV, V2, DOI 10.1126/sciadv.1600936
   Barbieri MM, 2004, ANN STAT, V32, P870, DOI 10.1214/009053604000000238
   Barto K, 2013, MUMIN MULTIMODEL INF
   Blake JG, 2016, J MAMMAL, V97, P455, DOI 10.1093/jmammal/gyv190
   Burnham KP, 2004, SOCIOL METHOD RES, V33, P261, DOI 10.1177/0049124104268644
   Costa HCM, 2018, PEERJ, V6, DOI 10.7717/peerj.5058
   Cove M. V., 2014, HYSTRIX, V25, P5049
   da Rocha DG, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154624
   de Oliveira Tadeu G., 2010, P559
   Di Bitetti MS, 2006, J ZOOL, V270, P153, DOI 10.1111/j.1469-7998.2006.00102.x
   Di Bitetti MS, 2008, J TROP ECOL, V24, P189, DOI 10.1017/S0266467408004847
   Di Bitetti MS, 2013, MAMM BIOL, V78, P21, DOI 10.1016/j.mambio.2012.08.006
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   DiMiceli C. M., 2011, ANN GLOBAL AUTOMATED, V65, P2000
   Droz M, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.051909
   Emmons L. H., 1998, ENVIRON CONSERV, V25, P175
   EMMONS LH, 1988, REV ECOL-TERRE VIE, V43, P133
   EMMONS LH, 1987, BEHAV ECOL SOCIOBIOL, V20, P271, DOI 10.1007/BF00292180
   Cuyckens GAE, 2014, ENDANGER SPECIES RES, V26, P167, DOI 10.3354/esr00640
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Geary WL, 2018, J APPL ECOL, V55, P1594, DOI 10.1111/1365-2664.13125
   Gibson L, 2011, NATURE, V478, P378, DOI 10.1038/nature10425
   Goncalves A. L. S., 2013, COMPOSITION OCCURREN
   Gonzalez--Borrajo N, 2017, MAMMAL REV, V47, P62, DOI 10.1111/mam.12081
   Grace JB, 2012, ECOSPHERE, V3, DOI 10.1890/ES12-00048.1
   Grassman LI, 2005, J MAMMAL, V86, P29, DOI 10.1644/1545-1542(2005)086&lt;0029:EOTSFI&gt;2.0.CO;2
   Haddad NM, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1500052
   Haidir Iding A., 2013, Cat News, V59, P7
   Haines AM, 2006, EUR J WILDLIFE RES, V52, P216, DOI 10.1007/s10344-006-0043-5
   Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1215904, 10.1126/science.1244693]
   Hearn AJ, 2019, ORYX, V53, P643, DOI 10.1017/S0030605317001065
   Hines JE, 2010, ECOL APPL, V20, P1456, DOI 10.1890/09-0321.1
   Hines JE., 2006, PRESENCE SOFTWARE ES
   Hodges JS, 2010, AM STAT, V64, P325, DOI 10.1198/tast.2010.10052
   Houghton RA, 2001, GLOBAL CHANGE BIOL, V7, P731, DOI 10.1046/j.1365-2486.2001.00426.x
   Hughes J, 2013, J R STAT SOC B, V75, P139, DOI 10.1111/j.1467-9868.2012.01041.x
   IBGE IBDEGEE, 2011, SIN CENS DEM 2010, P261
   Johnson D. S., 2015, STOCC R PACKAGE FITT
   Johnson DS, 2013, ECOLOGY, V94, P801, DOI 10.1890/12-0564.1
   Kalies EL, 2012, ECOL APPL, V22, P204, DOI 10.1890/11-0758.1
   Kolowski JM, 2010, BIOL CONSERV, V143, P917, DOI 10.1016/j.biocon.2009.12.039
   Kottek M, 2006, METEOROL Z, V15, P259, DOI 10.1127/0941-2948/2006/0130
   Laurance WF, 1998, ECOLOGY, V79, P2032, DOI [10.1890/0012-9658(1998)079[2032:RFFATD]2.0.CO;2, 10.1890/05-0064]
   Macdonald EA, 2017, GLOB ECOL CONSERV, V12, P204, DOI 10.1016/j.gecco.2017.11.006
   MacKenzie DI, 2004, J AGR BIOL ENVIR ST, V9, P300, DOI 10.1198/108571104X3361
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   Malhi Y, 2014, ANNU REV ENV RESOUR, V39, P125, DOI 10.1146/annurev-environ-030713-155141
   Massara RL, 2018, BIOTROPICA, V50, P125, DOI 10.1111/btp.12481
   Massara RL, 2016, J MAMMAL, V97, P1634, DOI 10.1093/jmammal/gyw129
   Massara RL, 2018, MAMM BIOL, V92, P86, DOI 10.1016/j.mambio.2018.04.009
   Massara RL, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141333
   Mazerolle M.J., 2017, R PACKAGE VERS, V2, P1, DOI DOI 10.1515/pacres-2017-0001
   McGarigal K., 2002, FRAGSTATS V3 SPATIAL
   Murray Julie L., 1997, Mammalian Species, V548, P1
   Newbold T, 2015, NATURE, V520, P45, DOI 10.1038/nature14324
   Noss RF, 1996, CONSERV BIOL, V10, P949, DOI 10.1046/j.1523-1739.1996.10040949.x
   Nowell K., 1996, WILD CATS STATUS SUR
   Otis D. L., 1978, WILDLIFE MONOGR, V62, P3, DOI DOI 10.2307/3830650
   Paviolo A., 2015, LEOPARDUS PARDALIS E
   Penido G, 2016, BIOTA NEOTROP, V16, DOI 10.1590/1676-0611-BN-2016-0168
   Penjor U, 2018, ECOL EVOL, V8, P4278, DOI 10.1002/ece3.3970
   PERES CA, 1994, BIOTROPICA, V26, P285, DOI 10.2307/2388849
   Poley LG, 2014, J BIOGEOGR, V41, P122, DOI 10.1111/jbi.12200
   Prange S, 2004, CAN J ZOOL, V82, P1804, DOI 10.1139/Z04-179
   Pratas-Santiago LP, 2016, J ZOOL, V299, P275, DOI 10.1111/jzo.12359
   Prentice RC, 2016, WETLAND BOOK 2 DISTR, P1
   Prugh LR, 2009, BIOSCIENCE, V59, P779, DOI 10.1525/bio.2009.59.9.9
   QGIS Development Team, 2017, QGIS GEOGR INF SYST
   R Core Team, 2019, R LANGUAGE ENV STAT
   Razzaghi M, 2013, J MOD APPL STAT METH, V12, P164, DOI 10.22237/jmasm/1367381880
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Robertson AL, 2010, GLOBAL CHANGE BIOL, V16, P3193, DOI 10.1111/j.1365-2486.2010.02314.x
   Rota CT, 2016, METHODS ECOL EVOL, V7, P1164, DOI 10.1111/2041-210X.12587
   Rota CT, 2009, J APPL ECOL, V46, P1173, DOI 10.1111/j.1365-2664.2009.01734.x
   SHUKLA J, 1990, SCIENCE, V247, P1322, DOI 10.1126/science.247.4948.1322
   Smith N.J.H., 1976, Oryx, V13, P362
   Tan CKW, 2017, BIOL CONSERV, V206, P65, DOI 10.1016/j.biocon.2016.12.012
   USGS, 2003, STRM DOC
   Vargas LEP, 2016, BIODIVERS CONSERV, V25, P739, DOI 10.1007/s10531-016-1089-7
   Vasconcelos P. G. de A., 2017, African Journal of Agricultural Research, V12, P169
   Vetter D, 2011, ECOGRAPHY, V34, P1, DOI 10.1111/j.1600-0587.2010.06453.x
   Wang E, 2002, STUD NEOTROP FAUNA E, V37, P207, DOI 10.1076/snfe.37.3.207.8564
NR 84
TC 7
Z9 7
U1 7
U2 25
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD MAY
PY 2019
VL 9
IS 9
BP 5049
EP 5062
DI 10.1002/ece3.5005
PG 14
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA HX7KR
UT WOS:000467584200002
PM 31110661
OA Green Accepted, Green Published, gold
DA 2022-02-10
ER

PT J
AU Reilly, ML
   Tobler, MW
   Sonderegger, DL
   Beier, P
AF Reilly, M. L.
   Tobler, M. W.
   Sonderegger, D. L.
   Beier, P.
TI Spatial and temporal response of wildlife to recreational activities in
   the San Francisco Bay ecoregion
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Outdoor recreation; Camera traps; Mammal habitat use; Diel activity
   patterns; Occupancy model; Bayesian analysis
ID ESTIMATING SITE OCCUPANCY; ACTIVITY PATTERNS; BEHAVIORAL-RESPONSES;
   STRESS-RESPONSE; CAMERA-TRAP; DISTURBANCE; CARNIVORES; MANAGEMENT;
   EXPOSURE; COYOTES
AB Non-motorized human recreation may displace animals from otherwise suitable habitat; in addition, animals may alter their activity patterns to reduce (or increase) interactions with recreationists. We investigated how hiking, mountain biking, equestrians, and recreationists with domestic dogs affected habitat use and diel activity patterns of ten species of medium and large-sized mammals in the San Francisco Bay ecoregion. We Used camera traps to quantify habitat use and activity patterns of wild mammals and human recreationists at 241 locations in 87 protected areas. We modeled habitat use with a multi-species occupancy model. Species habitat Use was most closely associated with environmental covariates such as landcover, precipitation, and elevation. Although recreation had less influence on habitat use, the presence of domestic dogs was negatively associated with habitat use of mountain lions and Virginia opossum. We also compared diel activity patterns of species at sites with no observed recreation to the activity patterns of species at sites with high (>= eight per day) levels of non-motorized recreation. Coyotes were more active at night and less active during the day in areas with high levels of recreation. Striped skunks were slightly more active later into the morning in areas that allowed human recreation. Smaller carnivores with nocturnal activity patterns may not be directly affected by recreational activities that are limited to daylight hours. We suggest that by maintaining habitat free of domestic dogs, and creating trail free buffers, land managers can manage recreation in a way that minimizes impacts to wildlife habitat and preserves the value of protected areas to people and wildlife. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Reilly, M. L.; Beier, P.] No Arizona Univ, Sch Forestry, Flagstaff, AZ USA.
   [Tobler, M. W.] San Diego Zoo Global, Inst Conserv Res Escondido, San Diego, CA USA.
   [Sonderegger, D. L.] No Arizona Univ, Dept Math & Stat, Flagstaff, AZ USA.
   [Reilly, M. L.] New Mexico Highlands Univ, Dept Biol & Chem, Las Vegas, NM 87701 USA.
RP Reilly, ML (corresponding author), New Mexico Highlands Univ, Dept Biol & Chem, Las Vegas, NM 87701 USA.
EM m1r326@nau.edu
OI Sonderegger, Derek/0000-0001-5151-8588; Tobler,
   Mathias/0000-0002-8587-0560
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation
   [2718]
FX E. Holldorf, C. Griffin, T. Batter, M. Grey, L. Lucore, N. Gengler, K.
   Lauger, S. Espinosa, A. Nickles, T. Volk, C. Miller, G. Pfau, A.
   Coconis, M. Sutton, K. Galbreath, B. Halliwell, C. Cooper assisted ably
   in the field work; Jeff Jenness developed GIS tools for the project. The
   Gordon and Betty Moore Foundation (Grant 2718) funded this work. We
   thank the agencies that allowed us to perform research on their lands
   and the biologists that helped coordinate permits for fieldwork.
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   BAOSC, 2011, CONS LANDS NETW SAN
   Barja I, 2007, J STEROID BIOCHEM, V104, P136, DOI 10.1016/j.jsbmb.2007.03.008
   Beyer HL, 2010, PHILOS T R SOC B, V365, P2245, DOI 10.1098/rstb.2010.0083
   Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
   Brown CL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040505
   Busch R. H., 2004, COUGAR ALMANAC COMPL, V24
   CASSIRER EF, 1992, WILDLIFE SOC B, V20, P375
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   Dickson BG, 2005, J WILDLIFE MANAGE, V69, P264, DOI 10.2193/0022-541X(2005)069&lt;0264:IOVTAR&gt;2.0.CO;2
   Dickson BG, 2002, J WILDLIFE MANAGE, V66, P1235, DOI 10.2307/3802956
   Dolton-Thorton N., 2015, BAY NATURE      0820
   Ellenberg U, 2007, GEN COMP ENDOCR, V152, P54, DOI 10.1016/j.ygcen.2007.02.022
   ESRI, 2010, ARCGLS REL 10 0
   Freeman C., 2015, COMMUNICATION
   Frid A, 2002, CONSERV ECOL, V6
   Gehring TM, 2003, BIOL CONSERV, V109, P283, DOI 10.1016/S0006-3207(02)00156-8
   George SL, 2006, BIOL CONSERV, V133, P107, DOI 10.1016/j.biocon.2006.05.024
   Gillogly Michael, 2015, COMMUNICATION
   Gu WD, 2004, BIOL CONSERV, V116, P195, DOI 10.1016/S0006-3207(03)00190-3
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Harmsen BJ, 2009, J MAMMAL, V90, P612, DOI 10.1644/08-MAMM-A-140R.1
   Jalkosky M. G., 1997, 1354 ARC WILDL SERV
   Karanth KK, 2009, J APPL ECOL, V46, P1189, DOI 10.1111/j.1365-2664.2009.01710.x
   Kays R., 2016, J APPL ECOL
   Kitchen AM, 2000, CAN J ZOOL, V78, P853, DOI 10.1139/cjz-78-5-853
   Krausman P. R., 2008, 281 FWP
   Krausman P. R., 1995, RMGTR264 USDA
   Krausman P. R., 1995, RMGTR264
   LaPoint S. D., 2014, ANIM CONSERV, V18
   Lenth BE, 2008, NAT AREA J, V28, P218, DOI 10.3375/0885-8608(2008)28[218:TEODOW]2.0.CO;2
   Linkie M., 2011, J ZOOL, P1
   Linkie M, 2007, BIOL CONSERV, V137, P20, DOI 10.1016/j.biocon.2007.01.016
   MACARTHUR RA, 1982, J WILDLIFE MANAGE, V46, P351, DOI 10.2307/3808646
   Mackay P., 2008, NONINVASIVE SURVEY M
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Markovchick-Nicholls L, 2008, CONSERV BIOL, V22, P99, DOI 10.1111/j.1523-1739.2007.00846.x
   McClennen N, 2001, AM MIDL NAT, V146, P27, DOI 10.1674/0003-0031(2001)146[0027:TEOSAA]2.0.CO;2
   Meredith M., 2014, OVERLAP ESTIMATES CO
   Mordecai RS, 2011, J APPL ECOL, V48, P56, DOI 10.1111/j.1365-2664.2010.01921.x
   Mullner A, 2004, BIOL CONSERV, V118, P549, DOI 10.1016/j.biocon.2003.10.003
   National Oceanic and Atmospheric Association, 1995, WR126 NWS
   O'Connell AF, 2006, J WILDLIFE MANAGE, V70, P1625, DOI 10.2193/0022-541X(2006)70[1625:ESOADP]2.0.CO;2
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Papouchis CM, 2001, J WILDLIFE MANAGE, V65, P573, DOI 10.2307/3803110
   Plummer M., 2017, JAGS PROGRAM ANAL BA
   Powell BF, 2006, G WRIGHT FORUM, V23, P50
   Rasmussen GSA, 2012, J ZOOL, V286, P232, DOI 10.1111/j.1469-7998.2011.00874.x
   Ray J., 2000, 15 WILDL CONS SOC
   Reed S. E., 2014, P1182112 WILDL CONS
   Reed SE, 2011, CONSERV BIOL, V25, P504, DOI 10.1111/j.1523-1739.2010.01641.x
   Reed SE, 2008, CONSERV LETT, V1, P146, DOI 10.1111/j.1755-263X.2008.00019.x
   Reilly M. L., 2016, ASSESSING PRED UNPUB
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Robert K., 2015, P ROYAL ACAD B, V282
   Royle A. J., 2008, HIERARCHICAL MODELIN, P101
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Savidge M., 2015, COMMUNICATION
   Sime C. A., 1999, WILDL SOC B, V8
   Su YS., 2015, R2JAGS USING R RUN J
   Taylor AR, 2003, ECOL APPL, V13, P951, DOI 10.1890/1051-0761(2003)13[951:WRTRAA]2.0.CO;2
   Tigas LA, 2002, BIOL CONSERV, V108, P299, DOI 10.1016/S0006-3207(02)00120-9
   Tobler M. T., 2012, CAMERABASE 1 6 ATRIU
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
   Tobler MW, 2009, J TROP ECOL, V25, P261, DOI 10.1017/S0266467409005896
   Tyre AJ, 2003, ECOL APPL, V13, P1790, DOI 10.1890/02-5078
   United States Census Bureau, AM FACT FIND
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilmers CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060590
   Yamaura Y, 2012, BIODIVERS CONSERV, V21, P1365, DOI 10.1007/s10531-012-0244-z
   Zaradic PA, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007367
   Zipkin EF, 2010, BIOL CONSERV, V143, P479, DOI 10.1016/j.biocon.2009.11.016
NR 74
TC 39
Z9 43
U1 8
U2 120
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD MAR
PY 2017
VL 207
BP 117
EP 126
DI 10.1016/j.biocon.2016.11.003
PG 10
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA EM8ZU
UT WOS:000395601400014
DA 2022-02-10
ER

PT C
AU Beery, S
   Van Horn, G
   Perona, P
AF Beery, Sara
   Van Horn, Grant
   Perona, Pietro
BE Ferrari, V
   Hebert, M
   Sminchisescu, C
   Weiss, Y
TI Recognition in Terra Incognita
SO COMPUTER VISION - ECCV 2018, PT XVI
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 15th European Conference on Computer Vision (ECCV)
CY SEP 08-14, 2018
CL Munich, GERMANY
DE Recognition; Transfer learning; Domain adaptation; Context; Dataset;
   Benchmark
AB It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https:// beerys.github.io/CaltechCameraTraps/)
C1 [Beery, Sara; Van Horn, Grant; Perona, Pietro] CALTECH, Pasadena, CA 91125 USA.
RP Beery, S (corresponding author), CALTECH, Pasadena, CA 91125 USA.
EM sbeery@caltech.edu; gvanhorn@caltech.edu; perona@caltech.edu
FU NSFGRFPNational Science Foundation (NSF)NSF - Office of the Director
   (OD) [1745301]; AWS Research Grant
FX We would like to thank the USGS and NPS for providing data. This work
   was supported by NSFGRFP Grant No. 1745301, the views are those of the
   authors and do not necessarily reflect the views of the NSF. Compute
   time was provided by an AWS Research Grant.
CR [Anonymous], 2017, IEEE ICC
   Babaee M., 2017, ARXIV170201731
   Benedek C, 2008, INT C PATT RECOG, P1686
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Busto P. P., 2017, IEEE INT C COMP VIS, V1
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Chen Y., 2017, ARXIV171111556
   Csurka G., 2017, ARXIV170205374
   Deng J., 2009, CVPR09
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151
   Giraldo-Zuluaga J.-H., 2017, VISUAL COMPUT, P1
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006
   He, 2017, 2017 IEEE INT S CIRC, P1, DOI DOI 10.1109/ISCAS.2017.8050762
   Hoffman J., 2016, ARXIV161202649
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krasin I., 2017, OPENIMAGES PUBLIC DA
   Kumar N, 2012, 12 EUR C COMP VIS EC
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin KH, 2014, IEEE IMAGE PROC, P1125, DOI 10.1109/ICIP.2014.7025224
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
   Miyake, 1982, COMPETITION COOPERAT, P267, DOI DOI 10.1007/978-3-642-46466-9_18
   Murphy GL., 2004, BIG BOOK CONCEPTS
   Nilsback M.E., 2006, P 2006 IEEE COMP SOC, P1447, DOI DOI 10.1109/CVPR.2006.42
   Norouzzadeh MS, 2017, ARXIV170305830
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peng XC, 2015, IEEE I CONF COMP VIS, P1278, DOI 10.1109/ICCV.2015.151
   Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29
   Poplin R., 2018, NAT BIOMED ENG, V1
   Raj A., 2015, ARXIV150705578
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Schaller RR, 1997, IEEE SPECTRUM, V34, P52, DOI 10.1109/6.591665
   Spain M, 2008, LECT NOTES COMPUT SC, V5302, P523, DOI 10.1007/978-3-540-88682-2_40
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Sun B., 2014, BMVC, V1, P3
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang K., 2012, ADV NEURAL INFORM PR, V25, P638
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   van Horn G., MERLIN BIRD ID SMART
   Van Horn G., 2018, COMPUT VIS PATTERN R
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Van Horn Grant, 2017, ARXIV170706642
   Van Horn Grant, 2017, ARXIV170901450
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Welinder P, 2013, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2013.419
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Xu JL, 2014, IEEE T PATTERN ANAL, V36, P2367, DOI 10.1109/TPAMI.2014.2327973
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhan Y, 2017, IEEE GEOSCI REMOTE S, V14, P1845, DOI 10.1109/LGRS.2017.2738149
   Zhang Y., 2017, P IEEE INT C COMP VI
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 57
TC 33
Z9 33
U1 3
U2 4
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-01270-0; 978-3-030-01269-4
J9 LECT NOTES COMPUT SC
PY 2018
VL 11220
BP 472
EP 489
DI 10.1007/978-3-030-01270-0_28
PG 18
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BQ5DJ
UT WOS:000603403700028
OA Green Submitted, Green Accepted
DA 2022-02-10
ER

PT J
AU Satter, CB
   Augustine, BC
   Harmsen, BJ
   Foster, RJ
   Sanchez, EE
   Wultsch, C
   Davis, ML
   Kelly, MJ
AF Satter, Christopher B.
   Augustine, Ben C.
   Harmsen, Bart J.
   Foster, Rebecca J.
   Sanchez, Emma E.
   Wultsch, Claudia
   Davis, Miranda L.
   Kelly, Marcella J.
TI Long-term monitoring of ocelot densities in Belize
SO JOURNAL OF WILDLIFE MANAGEMENT
LA English
DT Article
DE Belize; camera-trapping; density; multi-session models; ocelots;
   sex-specific models; spatially explicit capture-recapture
ID JAGUAR PANTHERA-ONCA; LEOPARDUS-PARDALIS; CAMERA-TRAPS; HABITAT USE;
   ACTIVITY PATTERNS; FOREST; POPULATION; DEFORESTATION; CONSERVATION;
   ABUNDANCE
AB Ocelots (Leopardus pardalis) are listed as least concern on the International Union for Conservation of Nature (IUCN) Red list of Threatened Species, yet we lack knowledge on basic demographic parameters across much of the ocelot's geographic range, including population density. We used camera-trapping methodology and spatially explicit capture-recapture (SECR) models with sex-specific detection function parameters to estimate ocelot densities across 7 field sites over 1 to 12 years (from data collected during 2002-2015) in Belize, Central America. Ocelot densities in the broadleaf rainforest sites ranged between 7.2 and 22.7 ocelots/100 km(2), whereas density in the pine (Pinus spp.) forest site was 0.9 ocelots/100 km(2). Applying an inverse-variance weighted average over all years for each broadleaf site increased precision and resulted in average density ranging from 8.5 to 13.0 ocelots/100 km(2). Males often had larger movement parameter estimates and higher detection probabilities at their activity centers than females. In most years, the sex ratio was not significantly different from 50:50, but the pooled sex ratio estimated using an inverse weighted average over all years indicated a female bias in 1 site, and a male bias in another. We did not detect any population trends as density estimates remained relatively constant over time; however, the power to detect such trends was generally low. Our SECR density estimates were lower but more precise than previous estimates and indicated population stability for ocelots in Belize. (c) 2018 The Authors. Journal of Wildlife Management published by Wiley Periodicals, Inc. on behalf of The Wildlife Society.
C1 [Satter, Christopher B.; Kelly, Marcella J.] Virginia Tech, Dept Fish & Wildlife Conservat, 310 W Campus Dr,Cheatham Hall, Blacksburg, VA 24061 USA.
   [Augustine, Ben C.] Cornell Univ, Atkinson Ctr Sustainable Future, G02 Fernow Hall, Ithaca, NY 14850 USA.
   [Augustine, Ben C.] Cornell Univ, Dept Nat Resources, G02 Fernow Hall, Ithaca, NY 14850 USA.
   [Harmsen, Bart J.; Foster, Rebecca J.; Sanchez, Emma E.] Panthera, 8 W 40th St,18th Floor, New York, NY 10018 USA.
   [Wultsch, Claudia] Amer Museum Nat Hist, Sackler Inst Comparat Genom, Cent Pk West & 79th St, New York, NY 10024 USA.
   [Davis, Miranda L.] Univ Connecticut, Ecol & Evolutionary Biol, 75 N Eagleville Rd,Unit 3043, Storrs, CT 06269 USA.
RP Satter, CB (corresponding author), Virginia Tech, Dept Fish & Wildlife Conservat, 310 W Campus Dr,Cheatham Hall, Blacksburg, VA 24061 USA.
EM chrissatter@vt.edu
RI Satter, Christopher/AAF-9152-2019; Kelly, Marcella J/B-4891-2011
OI FOSTER, REBECCA/0000-0002-6055-422X
FU National Science FoundationNational Science Foundation (NSF);
   Philadelphia Zoo; Virginia Tech; Wildlife Conservation Society; Panthera
FX We thank the Belize Forest Department, Program for Belize, Gallon Jug
   Estate, Yalbac Ranch and Cattle Company, Las Cuevas Research Station,
   Friends for Conservation and Development, Wildtracks, Belize Audubon
   Society, and Bull Run Farm (G. W. and M. Y. Headley) for permission to
   conduct this research. We thank local personnel such as N. "Chapal" Bol,
   and M. Aguilar for their local knowledge and expertise, and numerous
   volunteers and assistants (especially A. Dillon and T. Mc Namara) over
   the years who conducted field and lab work, from multiple universities,
   especially the University of Belize and Virginia Tech. We thank the
   National Science Foundation, the Philadelphia Zoo, Virginia Tech,
   Wildlife Conservation Society, and Panthera for funding over the years.
CR Abadi F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062636
   Aide TM, 2013, BIOTROPICA, V45, P262, DOI 10.1111/j.1744-7429.2012.00908.x
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155
   Gomez-Ramirez MA, 2017, ENDANGER SPECIES RES, V32, P471, DOI 10.3354/esr00828
   Bashir T, 2014, ACTA THERIOL, V59, P35, DOI 10.1007/s13364-013-0145-x
   BELETSKY L., 1999, BELIZE NO GUATEMALA
   Borenstein M, 2010, RES SYNTH METHODS, V1, P97, DOI 10.1002/jrsm.12
   Boulanger J, 2002, URSUS, V13, P137
   Carrillo S, 2002, DISTRIBUTION ECOLOGY
   Chandler RB, 2014, METHODS ECOL EVOL, V5, P1351, DOI 10.1111/2041-210X.12153
   Cherrington EA, 2010, FOREST COVER DEFORES
   Clabaugh J., 2017, BIODIVERSITY ENV RES
   da Rocha DG, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154624
   Davis M., 2008, THESIS
   Davis ML, 2011, ANIM CONSERV, V14, P56, DOI 10.1111/j.1469-1795.2010.00389.x
   de Oliveira Tadeu G., 2010, P559
   Di Bitetti MS, 2006, J ZOOL, V270, P153, DOI 10.1111/j.1469-7998.2006.00102.x
   Dillon A, 2008, J ZOOL, V275, P391, DOI 10.1111/j.1469-7998.2008.00452.x
   Dillon A, 2007, ORYX, V41, P469, DOI 10.1017/S0030605307000518
   Efford, 2011, SECR SPATIALLY EXPLI
   Efford M., 2018, SECR 3 1 SPATIALLY E
   Efford M. G., 2004, Animal Biodiversity and Conservation, V27, P217
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Emmons L., 1988, FIELD STUDY OCELOTS
   Foster RJ, 2016, ORYX, V50, P63, DOI 10.1017/S003060531400060X
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Gardner B, 2010, J WILDLIFE MANAGE, V74, P318, DOI 10.2193/2009-101
   Gerber BD, 2012, POPUL ECOL, V54, P43, DOI 10.1007/s10144-011-0276-3
   Grassman LI, 2005, J ZOOL, V266, P45, DOI 10.1017/S095283690500659X
   Haines AM, 2006, EUR J WILDLIFE RES, V52, P216, DOI 10.1007/s10344-006-0043-5
   Haines AM, 2006, ORYX, V40, P90, DOI 10.1017/S0030605306000044
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Harveson PM, 2004, WILDLIFE SOC B, V32, P948, DOI 10.2193/0091-7648(2004)032[0948:HUBOIS]2.0.CO;2
   Horne JS, 2009, SOUTHWEST NAT, V54, P119, DOI 10.1894/PS-49.1
   Hunter L., 2011, CARNIVORES WORLD
   Kellman M, 1997, J BIOGEOGR, V24, P23, DOI 10.1111/j.1365-2699.1997.tb00047.x
   Kelly M.J., 2014, ANAL 5 YEARS DATA RI
   Kolowski JM, 2010, BIOL CONSERV, V143, P917, DOI 10.1016/j.biocon.2009.12.039
   LEBRETON JD, 1992, ECOL MONOGR, V62, P67, DOI 10.2307/2937171
   LUDLOW ME, 1987, NATL GEOGR RES, V3, P447
   MacKenzie DI, 2005, ECOLOGY, V86, P1101, DOI 10.1890/04-1060
   Martinez-Hernandez A, 2015, ORYX, V49, P619, DOI 10.1017/S0030605313001452
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Miller CM, 2006, JAGUAR DENSITY FIREB
   Murray Julie L., 1997, Mammalian Species, V548, P1
   Nogueira SSC, 2011, BIODIVERS CONSERV, V20, P1385, DOI 10.1007/s10531-011-0047-7
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   Paviolo A, 2009, J MAMMAL, V90, P926, DOI 10.1644/08-MAMM-A-128.1
   Programme for Belize, 2008, FOR TIMB EXTR
   Reed JM, 2002, CONSERV BIOL, V16, P7, DOI 10.1046/j.1523-1739.2002.99419.x
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Salvador J, 2016, MAMMALIA, V80, P395, DOI 10.1515/mammalia-2014-0172
   Sandell M., 1989, P164
   Satter C. B., 2013, WILD FELID MONITOR, V9, P22
   Satter C. B., 2017, THESIS
   Schmid-Holmes S, 2001, BIOL CONSERV, V99, P293, DOI 10.1016/S0006-3207(00)00195-6
   Silveira L, 2010, ORYX, V44, P104, DOI 10.1017/S0030605309990433
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Soisalo MK, 2006, BIOL CONSERV, V129, P487, DOI 10.1016/j.biocon.2005.11.023
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Sunquist M., 2002, WILD CATS WORLD, DOI 10.1644/1545-1542(2004)0852.0.co;2
   Tobler MW, 2013, BIOL CONSERV, V159, P109, DOI 10.1016/j.biocon.2012.12.009
   White GC, 2005, WILDLIFE RES, V32, P211, DOI 10.1071/WR03123
   Wright A. C. S., 1959, VOLUME COLONIAL RES, V24
   Young CA, 2008, TROP CONSERV SCI, V1, P18, DOI 10.1177/194008290800100102
NR 67
TC 11
Z9 12
U1 1
U2 26
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0022-541X
EI 1937-2817
J9 J WILDLIFE MANAGE
JI J. Wildl. Manage.
PD FEB
PY 2019
VL 83
IS 2
BP 283
EP 294
DI 10.1002/jwmg.21598
PG 12
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA HH1NM
UT WOS:000455486900005
OA Green Published, hybrid
DA 2022-02-10
ER

PT J
AU Kazi, S
   Aurisch, Y
   Coulson, G
   Eldridge, MDB
   Irving, M
   Miller, KA
   Parrott, ML
AF Kazi, Sakib
   Aurisch, Yohanna
   Coulson, Graeme
   Eldridge, Mark D. B.
   Irving, Matt
   Miller, Kimberly A.
   Parrott, Marissa L.
TI Range extension of eastern wallaroo (Osphranter robustus robustus) in
   Victoria
SO AUSTRALIAN MAMMALOGY
LA English
DT Article; Early Access
DE alpine zone; boulderfield; camera; distribution; ecology; Macropodidae;
   marsupial; Osphranter robustus robustus
ID GREY-KANGAROO
AB The eastern wallaroo (Osphranter robustus robustus) is a large macropodid commonly found in New South Wales and Queensland, but rare in Victoria. Previously only known in north-east Victoria from a resident population near Suggan Buggan, and isolated records <50 km from the NSW border, we report camera trap observations of O. r. robustus from Mt Loch, near Hotham Heights. This represents the highest altitude observation of the species (similar to 1720 m above sea level), the furthest Victorian record from NSW, and a south-westerly range extension of 73 km.
C1 [Kazi, Sakib; Parrott, Marissa L.] Zoos Victoria, Wildlife Conservat & Sci, Parkville, Vic 3052, Australia.
   [Aurisch, Yohanna] Pk Victoria, Tawonga South, Vic 3698, Australia.
   [Coulson, Graeme] Univ Melbourne, Sch BioSci, Melbourne, Vic 3010, Australia.
   [Eldridge, Mark D. B.] Australian Museum, Australian Museum Res Inst, 1 William St, Sydney, NSW 2010, Australia.
   [Irving, Matt] Pk Victoria, Mt Buffalo, Vic 3740, Australia.
   [Miller, Kimberly A.] Zoos Victoria, Healesville Sanctuary, Badger Creek, Vic 3777, Australia.
RP Kazi, S (corresponding author), Zoos Victoria, Wildlife Conservat & Sci, Parkville, Vic 3052, Australia.
EM skazi@zoo.org.au
FU Dyson Bequest; Zoos Victoria Bushfire Emergency Wildlife Fund; WildArk;
   Aussie Ark; Global Wildlife Conservation
FX Funding was generously provided by the Dyson Bequest, Zoos Victoria
   Bushfire Emergency Wildlife Fund, WildArk, Aussie Ark and Global
   Wildlife Conservation, for Zoos Victoria's Mountain Pygmy-possum
   research, which led to this discovery.
CR Balland J, 2020, WILDLIFE RES, V47, P381, DOI 10.1071/WR19234
   DELWP, 2021, FLOR FAUN GUAR ACT 1
   JARMAN PJ, 1983, AUST WILDLIFE RES, V10, P33
   Menkhorst P. W., 1995, MAMMALS VICTORIA DIS, P142
   Mittermeier, 2015, HDB MAMMALS WORLD, P630
   Richardson BJ, 2019, AUST MAMMAL, V41, P65, DOI 10.1071/AM17032
   Taylor R.J., 1982, Australian Mammalogy, V5, P221
   TAYLOR RJ, 1983, AUST WILDLIFE RES, V10, P203
   TAYLOR RJ, 1984, J ANIM ECOL, V53, P65, DOI 10.2307/4342
   TAYLOR RJ, 1981, THESIS U NEW ENGLAND
NR 10
TC 0
Z9 0
U1 0
U2 0
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 0310-0049
EI 1836-7402
J9 AUST MAMMAL
JI Aust. Mammal.
DI 10.1071/AM21029
EA OCT 2021
PG 3
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA WO2HX
UT WOS:000712281800001
DA 2022-02-10
ER

PT J
AU Harper, LR
   Handley, LL
   Carpenter, AI
   Ghazali, M
   Di Muri, C
   Macgregor, CJ
   Logan, TW
   Law, A
   Breithaupt, T
   Read, DS
   McDevitt, AD
   Hanfling, B
AF Harper, Lynsey R.
   Handley, Lori Lawson
   Carpenter, Angus I.
   Ghazali, Muhammad
   Di Muri, Cristina
   Macgregor, Callum J.
   Logan, Thomas W.
   Law, Alan
   Breithaupt, Thomas
   Read, Daniel S.
   McDevitt, Allan D.
   Hanfling, Bernd
TI Environmental DNA (eDNA) metabarcoding of pond water as a tool to survey
   conservation and management priority mammals
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Camera traps; Field signs; Lentic; Monitoring; Semi-aquatic mammals;
   Terrestrial mammals
ID TERRESTRIAL MAMMALS; OTTER; SNOW
AB Environmental DNA (eDNA) metabarcoding can identify terrestrial taxa utilising aquatic habitats alongside aquatic communities, but terrestrial species' eDNA dynamics are understudied. We evaluated eDNA metabarcoding for monitoring semi-aquatic and terrestrial mammals, specifically nine species of conservation or management concern, and examined spatiotemporal variation in mammal eDNA signals. We hypothesised eDNA signals would be stronger for semi-aquatic than terrestrial mammals, and at sites where individuals exhibited behaviours. In captivity, we sampled waterbodies at points where behaviours were observed ('directed' sampling) and at equidistant intervals along the shoreline ('stratified' sampling). We surveyed natural ponds (N = 6) where focal species were present using stratified water sampling, camera traps, and field signs. eDNA samples were metabarcoded using vertebrate-specific primers. All focal species were detected in captivity. eDNA signal strength did not differ between directed and stratified samples across or within species, between semi-aquatic or terrestrial species, or according to behaviours. eDNA was evenly distributed in artificial waterbodies, but unevenly distributed in natural ponds. Survey methods deployed at natural ponds shared three species detections. Metabarcoding missed badger and red fox recorded by cameras and field signs, but detected small mammals these tools overlooked, e.g. water vole. Terrestrial mammal eDNA signals were weaker and detected less frequently than semi-aquatic mammal eDNA signals. eDNA metabarcoding could enhance mammal monitoring through large-scale, multi-species distribution assessment for priority and difficult to survey species, and provide early indication of range expansions or contractions. However, eDNA surveys need high spatiotemporal resolution and metabarcoding biases require further investigation before routine implementation.
C1 [Harper, Lynsey R.; Handley, Lori Lawson; Di Muri, Cristina; Logan, Thomas W.; Breithaupt, Thomas; Hanfling, Bernd] Univ Hull, Dept Biol & Marine Sci, Kingston Upon Hull HU6 7RX, N Humberside, England.
   [Harper, Lynsey R.] Univ Illinois, Prairie Res Inst, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
   [Carpenter, Angus I.] Wildwood Trust, Canterbury Rd, Herne Common CT6 7LQ, Herne Bay, England.
   [Ghazali, Muhammad] Edinburgh Zoo, Royal Zool Soc Scotland, 134 Corstorphine Rd, Edinburgh EH12 6TS, Midlothian, Scotland.
   [Macgregor, Callum J.] Univ York, Dept Biol, Wentworth Way, York YO10 5DD, N Yorkshire, England.
   [Law, Alan] Univ Stirling, Biol & Environm Sci, Stirling FK9 4LA, Scotland.
   [Read, Daniel S.] CEH, Benson Lane, Wallingford OX10 8BB, Oxon, England.
   [McDevitt, Allan D.] Univ Salford, Sch Sci Engn & Environm, Ecosyst & Environm res Ctr, Salford M5 4WT, Lancs, England.
RP Harper, LR (corresponding author), Univ Illinois, Prairie Res Inst, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
EM lynsey.harper2@gmail.com
RI Macgregor, Callum/Y-2563-2018; Read, Daniel/B-5446-2008; Harper, Lynsey
   R./AAQ-2705-2020; Carpenter, Angus/AAM-4652-2020; Di Muri,
   Cristina/ABF-5993-2021; Logan, Thomas W./AAV-3319-2021
OI Macgregor, Callum/0000-0001-8281-8284; Read, Daniel/0000-0001-8546-5154;
   Harper, Lynsey R./0000-0003-0923-1801; Carpenter,
   Angus/0000-0002-0262-9895; Di Muri, Cristina/0000-0003-4072-0662; Logan,
   Thomas W./0000-0001-6640-1489; Breithaupt, Thomas/0000-0003-4586-0998;
   Hanfling, Bernd/0000-0001-7630-9360; Law, Alan/0000-0001-5971-3214
FU University of Hull; Natural Environment Research Council as part of the
   UK-SCAPE programme [NE/R016429/1]
FX This work was funded by the University of Hull, and D.S.R was supported
   by the Natural Environment Research Council award number NE/R016429/1 as
   part of the UK-SCAPE programme delivering National Capability. We would
   like to thank Gill Murray-Dickson (National Museums Scotland) and Helen
   Semi (RZSS Edinburgh Zoo) for feedback on the study design, and Richard
   Griffiths (DICE, University of Kent) for support with filtration of
   water samples from Wildwood Trust. We are grateful to staff at Wildwood
   Trust and RZSS Highland Wildlife Park for assisting with eDNA sampling
   from animal enclosures. We thank Richard Hampshire and volunteers at
   Tophill Low Nature Reserve for assisting with camera trap deployment.
   Tim Kohler (Natural England) aPaul Ramsay kindly gave permission to
   sample at Thorne Moors and the Bamff Estate respectively.
CR Balint M, 2018, MOL ECOL RESOUR, V18, P1415, DOI 10.1111/1755-0998.12934
   Bland LM, 2015, CONSERV BIOL, V29, P250, DOI 10.1111/cobi.12372
   Bonesi L, 2004, OIKOS, V106, P509, DOI 10.1111/j.0030-1299.2004.13034.x
   Bronner I.F., 2009, CURRENT PROTOCOLS HU, V18
   Brooks ME, 2017, R J, V9, P378, DOI 10.32614/RJ-2017-066
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2018, PEERJ, V6, DOI 10.7717/peerj.5827
   Deiner K, 2017, MOL ECOL, V26, P5872, DOI 10.1111/mec.14350
   Evans NT, 2017, CAN J FISH AQUAT SCI, V74, P1362, DOI 10.1139/cjfas-2016-0306
   Franklin TW, 2019, BIOL CONSERV, V229, P50, DOI 10.1016/j.biocon.2018.11.006
   Gaughran A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191818
   Hanfling B., 2017, REV RECENT ADV GENET
   Hanfling B, 2016, MOL ECOL, V25, P3101, DOI 10.1111/mec.13660
   Harper LR, 2018, TESTING ECOLOGICAL H, DOI [10.1101/278309, DOI 10.1101/278309]
   Harper LR, 2018, ECOL EVOL, V8, P6330, DOI 10.1002/ece3.4013
   Harris S, 2004, MAMMAL REV, V34, P157, DOI 10.1046/j.0305-1838.2003.00030.x
   Ishige T, 2017, BIOL CONSERV, V210, P281, DOI 10.1016/j.biocon.2017.04.023
   Johnson H., 2019, ENV DNA, V1, P26, DOI [10.1002/edn3.5, DOI 10.1002/EDN3.5]
   Joint Nature Conservation Committee, 2018, UK BAP PRIOR TERR MA
   Kelly RP, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086175
   Kinoshita G, 2019, ZOOL SCI, V36, P198, DOI 10.2108/zs180172
   Kitson JJN, 2019, MOL ECOL, V28, P471, DOI 10.1111/mec.14518
   Klymus KE, 2017, DIVERSITY-BASEL, V9, DOI 10.3390/d9040054
   Lacoursiere-Roussel A, 2016, GENOME, V59, P991, DOI 10.1139/gen-2015-0218
   Leempoel K., 2019, BIORXIV, DOI [10.1101/634022, DOI 10.1101/634022]
   Lugg WH, 2018, METHODS ECOL EVOL, V9, P1049, DOI 10.1111/2041-210X.12951
   Massimino D, 2018, BIOL CONSERV, V226, P153, DOI 10.1016/j.biocon.2018.07.026
   Mathews F., 2018, REV POPULATION CONSE
   McDevitt, 2019, FISHING MAMMALS LAND, DOI [10.1101/629758, DOI 10.1101/629758]
   McKnight D.T., 2019, ENV DNA, V1, P14, DOI [10.1002/edn3.11, DOI 10.1002/EDN3.11]
   R Core Team, 2017, R LANG ENV STAT COMP
   Riaz T, 2011, NUCLEIC ACIDS RES, V39, DOI 10.1093/nar/gkr732
   Rodgers TW, 2015, CONSERV GENET RESOUR, V7, P693, DOI 10.1007/s12686-015-0478-7
   RuizOlmo J, 1997, ACTA THERIOL, V42, P259, DOI 10.4098/AT.arch.97-28
   Sadlier LMJ, 2004, MAMMAL REV, V34, P75, DOI 10.1046/j.0305-1838.2003.00029.x
   Sellers Graham S., 2018, Metabarcoding and Metagenomics, V2, pe24556, DOI 10.3897/mbmg.2.24556
   Sheehy E, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2017.2603
   Staley ZR, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32680-z
   Tessler M, 2018, SYST BIODIVERS, V16, P488, DOI 10.1080/14772000.2018.1433729
   Thomsen PF, 2015, BIOL CONSERV, V183, P4, DOI 10.1016/j.biocon.2014.11.019
   Thomsen PF, 2012, MOL ECOL, V21, P2565, DOI 10.1111/j.1365-294X.2011.05418.x
   Ushio M, 2017, MOL ECOL RESOUR, V17, pe63, DOI 10.1111/1755-0998.12690
   Valentini A, 2016, MOL ECOL, V25, P929, DOI 10.1111/mec.13428
   Visconti P, 2011, PHILOS T R SOC B, V366, P2693, DOI 10.1098/rstb.2011.0105
   Wickham H., 2016, GGPLOT2 ELEGANT GRAP, DOI [10.1007/978-3-319-24277-4, DOI 10.1007/978-3-319-24277-4_9]
   Williams KE, 2018, ECOL EVOL, V8, P688, DOI 10.1002/ece3.3698
   Zuur Alain F., 2009, P1
NR 47
TC 21
Z9 26
U1 10
U2 51
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD OCT
PY 2019
VL 238
AR 108225
DI 10.1016/j.biocon.2019.108225
PG 11
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA JO0BM
UT WOS:000497252700027
OA Green Submitted, Green Accepted
DA 2022-02-10
ER

PT J
AU Piel, AK
   Crunchant, A
   Knot, IE
   Chalmers, C
   Fergus, P
   Mulero-Pazmany, M
   Wich, SA
AF Piel, A. K.
   Crunchant, A.
   Knot, I. E.
   Chalmers, C.
   Fergus, P.
   Mulero-Pazmany, M.
   Wich, S. A.
TI Noninvasive Technologies for Primate Conservation in the 21st Century
SO INTERNATIONAL JOURNAL OF PRIMATOLOGY
LA English
DT Article; Early Access
DE Endangered; Methods; Monitoring; Remote Sensing; Tools
ID ESTIMATING ANIMAL DENSITY; CAMERA TRAPS; HABITAT USE; NATIONAL-PARK;
   LOUD CALLS; RESPIRATORY-DISEASE; ENDANGERED PRIMATE; WILDLIFE RESEARCH;
   REAL-TIME; OCCUPANCY
AB Observing and quantifying primate behavior in the wild is challenging. Human presence affects primate behavior and habituation of new, especially terrestrial, individuals is a time-intensive process that carries with it ethical and health concerns, especially during the recent pandemic when primates are at even greater risk than usual. As a result, wildlife researchers, including primatologists, have increasingly turned to new technologies to answer questions and provide important data related to primate conservation. Tools and methods should be chosen carefully to maximize and improve the data that will be used to answer the research questions. We review here the role of four indirect methods-camera traps, acoustic monitoring, drones, and portable field labs- and improvements in machine learning that offer rapid, reliable means of combing through large datasets that these methods generate. We describe key applications and limitations of each tool in primate conservation, and where we anticipate primate conservation technology moving forward in the coming years.
C1 [Piel, A. K.] UCL, Dept Anthropol, London, England.
   [Crunchant, A.; Chalmers, C.; Fergus, P.; Mulero-Pazmany, M.; Wich, S. A.] Liverpool John Moores Univ, Sch Biol & Environm Sci, Liverpool, Merseyside, England.
   [Knot, I. E.; Wich, S. A.] Univ Amsterdam, Inst Biodivers & Ecosyst Dynam, Amsterdam, Netherlands.
RP Piel, AK (corresponding author), UCL, Dept Anthropol, London, England.
EM a.piel@ucl.ac.uk
OI Knot, Ineke/0000-0001-9886-2597; Piel, Alexander/0000-0002-4674-537X;
   Wich, Serge/0000-0003-3954-5174; Chalmers, Carl/0000-0003-0822-1150
CR ADAMS W., 2007, CONSERV SOC, V5, P147, DOI DOI 10.2307/26392879
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Allan A. T. L., 2020, SCI ADV, P1
   Ancrenaz M, 2005, PLOS BIOL, V3, P30, DOI 10.1371/journal.pbio.0030003
   [Anonymous], 2011, QUANTIFYING SENSITIV, DOI [10.1111/j.2041-210X.2011.00094.x, DOI 10.1111/J.2041-210X.2011.00094.X]
   Apolo-Apolo OE, 2020, EUR J AGRON, V115, DOI 10.1016/j.eja.2020.126030
   Arandjelovic M, 2018, AM J PRIMATOL, V80, DOI 10.1002/ajp.22743
   Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Assmann JJ, 2019, J UNMANNED VEH SYST, V7, P54, DOI 10.1139/juvs-2018-0018
   August T., 2019, AUTONOMOUS DRONES AR
   Balantic C, 2019, ECOL APPL, V29, DOI 10.1002/eap.1854
   Baldi P, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.570862
   Barasona JA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115608
   Beehner JC, 2017, HORM BEHAV, V91, P68, DOI 10.1016/j.yhbeh.2017.03.003
   Berger-Tal O, 2018, CONSERV LETT, V11, DOI 10.1111/conl.12458
   Bessone M, 2020, J APPL ECOL, V57, P963, DOI 10.1111/1365-2664.13602
   Bianco MJ, 2019, J ACOUST SOC AM, V146, P3590, DOI 10.1121/1.5133944
   Blanco MB, 2020, CONSERV GENET, V21, P785, DOI 10.1007/s10592-020-01296-0
   Blumstein DT, 2011, J APPL ECOL, V48, P758, DOI 10.1111/j.1365-2664.2011.01993.x
   Bondi E., 2018, IJCAI INT JOINT C AR, P5814
   Bondi E, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P7741
   Bondi Elizabeth, 2019, P77
   Bonnin N, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2020017
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers DL, 2015, J AM STAT ASSOC, V110, P195, DOI 10.1080/01621459.2014.893884
   Bouchet H, 2012, J COMP PSYCHOL, V126, P45, DOI 10.1037/a0025018
   Brauer CL, 2016, WILDLIFE SOC B, V40, P140, DOI 10.1002/wsb.619
   Brede B, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071032
   Brittain S, 2020, CONSERV BIOL, V34, P925, DOI 10.1111/cobi.13464
   Brown AT, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232044
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Buckland ST, 2015, METH STAT ECOL, P1, DOI 10.1007/978-3-319-19219-2
   Buckland ST, 2010, INT J PRIMATOL, V31, P485, DOI 10.1007/s10764-010-9408-4
   Buehler P, 2019, ECOL INFORM, V50, P191, DOI 10.1016/j.ecoinf.2019.02.003
   Burke C, 2019, J UNMANNED VEH SYST, V7, P235, DOI 10.1139/juvs-2018-0035
   Burke C, 2018, PROC SPIE, V10709, DOI 10.1117/12.2311673
   Buxton RT, 2018, GLOB ECOL CONSERV, V16, DOI 10.1016/j.gecco.2018.e00493
   Campos-Cerqueira M, 2016, METHODS ECOL EVOL, V7, P1340, DOI 10.1111/2041-210X.12599
   Cardenosa D, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.39
   Carvalho F, 2020, ENVIRON CONSERV, V47, P113, DOI 10.1017/S0376892920000090
   Chabot D, 2019, BIOL CONSERV, V237, P125, DOI 10.1016/j.biocon.2019.06.022
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chalmers C., 2019, ARXIV191007360
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chang JJM, 2020, GENES-BASEL, V11, DOI 10.3390/genes11101121
   Chapman CA, 2005, EVOL ANTHROPOL, V14, P134, DOI 10.1002/evan.20068
   Chen DM, 2021, AM J PRIMATOL, V83, DOI 10.1002/ajp.23270
   Chen RL, 2019, ECOL EVOL, V9, P9453, DOI 10.1002/ece3.5410
   Clark DA, 2019, ARCT SCI, V5, P62, DOI 10.1139/as-2018-0013
   Clink DJ, 2019, BIOACOUSTICS, V28, P193, DOI 10.1080/09524622.2018.1426042
   Comer CE, 2014, WILDLIFE SOC B, V38, P103, DOI 10.1002/wsb.375
   Comstock KE, 2003, CONSERV BIOL, V17, P1840, DOI 10.1111/j.1523-1739.2003.00358.x
   Coulter LL, 2009, ENVIRON MONIT ASSESS, V152, P343, DOI 10.1007/s10661-008-0320-8
   Cove MV, 2013, TROP CONSERV SCI, V6, P781, DOI 10.1177/194008291300600606
   Crofoot MC, 2010, ANIM BEHAV, V80, P475, DOI 10.1016/j.anbehav.2010.06.006
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   D'Odorico P, 2020, NEW PHYTOL, V226, P1667, DOI 10.1111/nph.16488
   Damas Joana, 2020, bioRxiv, DOI [10.1073/pnas.2010146117, 10.1101/2020.04.16.045302]
   Dawson DK, 2009, J APPL ECOL, V46, P1201, DOI 10.1111/j.1365-2664.2009.01731.x
   Delgado RA, 2006, INT J PRIMATOL, V27, P5, DOI 10.1007/s10764-005-9001-4
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   Diaz-Delgado R, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010003
   Doran-Sheehy DM, 2007, AM J PRIMATOL, V69, P1354, DOI 10.1002/ajp.20442
   Efford M, 2004, OIKOS, V106, P598, DOI 10.1111/j.0030-1299.2004.13043.x
   Efford MG, 2009, ECOLOGY, V90, P2676, DOI 10.1890/08-1735.1
   Elsey RM, 2016, SOUTHEAST NAT, V15, P76, DOI 10.1656/058.015.0106
   Enari H, 2019, ECOL INDIC, V98, P753, DOI 10.1016/j.ecolind.2018.11.062
   Eriksson J, 2004, MOL ECOL, V13, P3425, DOI 10.1111/j.1365-294X.2004.02332.x
   Estrada A, 2018, PEERJ, V6, DOI 10.7717/peerj.4869
   Estrada A, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1600946
   Fang YH, 2020, PRIMATES, V61, P151, DOI 10.1007/s10329-019-00774-5
   Fauver JR, 2020, CELL, V181, P990, DOI 10.1016/j.cell.2020.04.021
   Frankham R., 2010, INTRO CONSERVATION G
   Fruth B, 2018, AM J PHYS ANTHROPOL, V166, P499, DOI 10.1002/ajpa.23373
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Garriga RM, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0215545
   Gazagne E, 2020, RAFFLES B ZOOL, V68, P735, DOI 10.26107/RBZ-2020-0085
   Gerber B, 2010, ORYX, V44, P219, DOI 10.1017/S0030605309991037
   Gilardi K. V, 1999, BEST PRACTICE GUIDEL
   Gillespie TR, 2020, NATURE, V579, P497, DOI 10.1038/d41586-020-00859-y
   Gogarten JF, 2020, MOL ECOL RESOUR, V20, P204, DOI 10.1111/1755-0998.13101
   Goordial J, 2017, FRONT MICROBIOL, V8, DOI 10.3389/fmicb.2017.02594
   Goossens B, 2006, PLOS BIOL, V4, P285, DOI 10.1371/journal.pbio.0040025
   Gowers GOF, 2019, GENES-BASEL, V10, DOI 10.3390/genes10110902
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   Groves CR, 2002, BIOSCIENCE, V52, P499, DOI 10.1641/0006-3568(2002)052[0499:PFBCPC]2.0.CO;2
   Grutzmacher KS, 2016, ECOHEALTH, V13, P499, DOI 10.1007/s10393-016-1144-6
   Guevara EE, 2018, CONSERV GENET RESOUR, V10, P119, DOI 10.1007/s12686-017-0758-5
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   Hambrecht L, 2019, BIOL CONSERV, V233, P109, DOI 10.1016/j.biocon.2019.02.017
   Hanya G, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0190631
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Heinicke S, 2015, METHODS ECOL EVOL, V6, P753, DOI 10.1111/2041-210X.12384
   Higham JP, 2016, HORM BEHAV, V84, P145, DOI 10.1016/j.yhbeh.2016.07.001
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   Hines D.I, 2017, OCCUPANCY ESTIMATION
   Hoban SM, 2013, CONSERV GENET RESOUR, V5, P593, DOI 10.1007/s12686-013-9859-y
   Hongo S, 2018, INT J PRIMATOL, V39, P27, DOI 10.1007/s10764-017-0007-5
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Isaac NJB, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000296
   Jack KM, 2008, AM J PRIMATOL, V70, P490, DOI 10.1002/ajp.20512
   Jain M, 2016, GENOME BIOL, V17, DOI 10.1186/s13059-016-1103-0
   Lopez JJ, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010010
   Johnson CL, 2020, ORYX, V54, P784, DOI 10.1017/S0030605319000851
   Johnson Sarah S, 2017, J Biomol Tech, V28, P2, DOI 10.7171/jbt.17-2801-009
   Joppa LN, 2017, NATURE, V552, P325, DOI 10.1038/d41586-017-08675-7
   Kalan AK, 2015, ECOL INDIC, V54, P217, DOI 10.1016/j.ecolind.2015.02.023
   Kalbitzer U, 2019, AFR J ECOL, V57, P190, DOI 10.1111/aje.12589
   Kappeler PM, 1998, AM J PRIMATOL, V46, P7
   Kauffman MJ, 2007, ORYX, V41, P70, DOI 10.1017/S0030605306001414
   Kays R, 2019, INT J REMOTE SENS, V40, P407, DOI 10.1080/01431161.2018.1523580
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Kidney D, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0155066
   Kimwele C. N., 2012, African Journal of Biotechnology, V11, P14276
   Klosterman S, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122852
   Knot IE, 2020, FRONT ECOL EVOL, V8, DOI 10.3389/fevo.2020.00100
   Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
   Kolowski JM, 2012, INT J PRIMATOL, V33, P958, DOI 10.1007/s10764-012-9627-y
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Koustubh Sharma, 2020, Ecological Solutions and Evidence, V1, DOI 10.1002/2688-8319.12033
   Krehenwinkel H, 2019, GENES-BASEL, V10, DOI 10.3390/genes10110858
   Krehenwinkel H, 2019, GIGASCIENCE, V8, DOI 10.1093/gigascience/giz006
   Krief S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0109925
   KUCERA TE, 2011, ANIMAL ECOLOGY METHO, P00009, DOI DOI 10.1007/978-4-431-99495-4_2
   Kwok R, 2019, NATURE, V567, P133, DOI 10.1038/d41586-019-00746-1
   Lehman SM, 2006, DEV PRIMATOL, P1
   Leyequien E, 2007, INT J APPL EARTH OBS, V9, P1, DOI 10.1016/j.jag.2006.08.002
   Lilly AA, 2002, INT J PRIMATOL, V23, P555, DOI 10.1023/A:1014969617036
   Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
   Loit K, 2019, APPL ENVIRON MICROB, V85, DOI 10.1128/AEM.01368-19
   Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Lu S, 2020, COMP SARS COV 2 INFE, DOI [10.1101/2020.04.08.031807, DOI 10.1101/2020.04.08.031807]
   Lynn MS, 2016, INTRODUCTION TO PRIMATE CONSERVATION, P53, DOI 10.1093/acprof:oso/9780198703389.003.0005
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   Marques TA, 2013, BIOL REV, V88, P287, DOI 10.1111/brv.12001
   Marques TA, 2009, J ACOUST SOC AM, V125, P1982, DOI 10.1121/1.3089590
   Marshall AJ, 2013, TECH ECOL CONSERVAT, P103
   Martin, 2012, ESTIMATING MINKE WHA, DOI DOI 10.1111/J.1748-7692.2011.00561.X
   Marvin DC, 2016, GLOB ECOL CONSERV, V7, P262, DOI 10.1016/j.gecco.2016.07.002
   Marx V, 2015, NAT METHODS, V12, P393, DOI 10.1038/nmeth.3369
   Massawe E. A., 2017, INT J ADV SMART SENS, V7, P1, DOI [10.5121/ ijassn.2017.7401., DOI 10.5121/IJASSN.2017.7401]
   Masters A, 2019, FORENSIC SCI INT, V301, P231, DOI 10.1016/j.forsciint.2019.05.041
   McDonald MA, 1999, J ACOUST SOC AM, V105, P2643, DOI 10.1121/1.426880
   McDowall IL, 2008, APPL HERPETOL, V5, P371, DOI 10.1163/157075408786532057
   McMahon BJ, 2014, EVOL APPL, V7, P999, DOI 10.1111/eva.12193
   Measey GJ, 2017, J APPL ECOL, V54, P894, DOI 10.1111/1365-2664.12810
   Medeiros K, 2019, INT J PRIMATOL, V40, P511, DOI 10.1007/s10764-019-00103-z
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Meek PD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110832
   Meijaard E, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027491
   Meijaard E, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0012042
   Melin Amanda D, 2020, bioRxiv, DOI 10.1101/2020.04.09.034967
   Menegon M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0184741
   Mitani JC, 1998, PRIMATES, V39, P171, DOI 10.1007/BF02557729
   Mondol S, 2015, CONSERV BIOL, V29, P556, DOI 10.1111/cobi.12393
   Moore JF, 2020, ANIM CONSERV, V23, P561, DOI 10.1111/acv.12569
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Morgan D, 2006, INT J PRIMATOL, V27, P147, DOI 10.1007/s10764-005-9013-0
   Mporas I, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10207379
   Mulero-Pazmany M, 2015, ECOL EVOL, V5, P4808, DOI 10.1002/ece3.1744
   Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   Negrey JD, 2019, EMERG MICROBES INFEC, V8, P139, DOI [10.1080/22221751.2018.1563456, 10.1080/22221751.201]
   Ni QY, 2018, PEERJ, V6, DOI 10.7717/peerj.6069
   NIJMAN V, 2017, AM J PRIMATOL, V79, DOI DOI 10.1002/AJP.22517
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Nowak K, 2014, BEHAV ECOL, V25, P1199, DOI 10.1093/beheco/aru110
   Nowak Maciej M., 2018, European Journal of Ecology, V4, P56, DOI 10.2478/eje-2018-0012
   Nunn CL, 2016, INTRODUCTION TO PRIMATE CONSERVATION, P157, DOI 10.1093/acprof:oso/9780198703389.003.0010
   O'Donoghue P, 2016, J APPL ECOL, V53, P5, DOI 10.1111/1365-2664.12452
   Oklander LI, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60569-3
   Olivares-Mendez MA, 2015, SENSORS-BASEL, V15, P31362, DOI 10.3390/s151229861
   Olson ER, 2012, ORYX, V46, P593, DOI 10.1017/S0030605312000488
   Ondei S, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2859
   Munnink BOB, 2020, NAT MED, V26, DOI 10.1038/s41591-020-0997-y
   Ouso DO, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-61600-3
   Pafco B, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24126-3
   Park JY, 2017, SCI REP-UK, V7, DOI [10.1038/srep43270, 10.1177/2158244016684912]
   Patrono LV, 2020, NAT MICROBIOL, V5, P955, DOI 10.1038/s41564-020-0706-0
   Pebsworth PA, 2014, INT J PRIMATOL, V35, P825, DOI 10.1007/s10764-014-9802-4
   PERES CA, 1994, BIOTROPICA, V26, P98, DOI 10.2307/2389114
   Pettorelli N, 2013, NORMALIZED DIFFERENCE VEGETATION INDEX, P1, DOI 10.1093/acprof:osobl/9780199693160.001.0001
   Piel AK, 2018, AM J PHYS ANTHROPOL, V166, P530, DOI 10.1002/ajpa.23609
   Piel AK, 2015, AM J PRIMATOL, V77, P1027, DOI 10.1002/ajp.22438
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Pomerantz A, 2018, GIGASCIENCE, V7, DOI 10.1093/gigascience/giy033
   Quick J, 2017, NAT PROTOC, V12, P1261, DOI 10.1038/nprot.2017.066
   Quick J, 2016, NATURE, V530, P228, DOI 10.1038/nature16996
   Ren W., 2020, FUNCTIONAL GENETIC A, DOI [10.1101/2020.04.22.046565, DOI 10.1101/2020.04.22.046565]
   Rodriguez A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0050336
   Rovero F., 2016, CAMERA TRAPPING WILD
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2016, REMOTE SENS ECOL CON, V2, P84, DOI 10.1002/rse2.17
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Sandbrook C, 2018, CONSERV SOC, V16, P493, DOI 10.4103/cs.cs_17_165
   Sarron J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10121900
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Seah A, 2020, GENES-BASEL, V11, DOI 10.3390/genes11040445
   Sebastian-Gonzalez E, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01224-130207
   Senthilnath J, 2016, BIOSYST ENG, V146, P16, DOI 10.1016/j.biosystemseng.2015.12.003
   Sousa-Lima RS, 2013, AQUAT MAMM, V39, P23, DOI 10.1578/AM.39.1.2013.23
   Spaan D, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3020034
   Spehar SN, 2017, INT J PRIMATOL, V38, P358, DOI 10.1007/s10764-017-9959-8
   Spillmann B, 2017, BIOACOUSTICS, V26, P109, DOI 10.1080/09524622.2016.1216802
   Spillmann B, 2015, AM J PRIMATOL, V77, P767, DOI 10.1002/ajp.22398
   Srivathsan A, 2019, BMC BIOL, V17, DOI 10.1186/s12915-019-0706-9
   Srivathsan A, 2016, FRONT ZOOL, V13, DOI 10.1186/s12983-016-0150-4
   Stevenson BC, 2015, METHODS ECOL EVOL, V6, P38, DOI 10.1111/2041-210X.12291
   STRIER KB, 1991, CONSERV BIOL, V5, P214, DOI 10.1111/j.1523-1739.1991.tb00126.x
   Szantoi Z, 2017, INT J REMOTE SENS, V38, P2231, DOI 10.1080/01431161.2017.1280638
   Tan TF, 2016, 2016 IEEE CONFERENCE ON SYSTEMS, PROCESS AND CONTROL (ICSPC), P37, DOI 10.1109/SPC.2016.7920700
   Teelen S, 2007, AM J PRIMATOL, V69, P1030, DOI 10.1002/ajp.20417
   Thompson ME, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.180840
   Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
   Van Andel AC, 2015, AM J PRIMATOL, V77, P1122, DOI 10.1002/ajp.22446
   Vigilant L, 2009, PRIMATES, V50, P105, DOI 10.1007/s10329-008-0124-z
   Voigt M, 2018, CURR BIOL, V28, P761, DOI 10.1016/j.cub.2018.01.053
   Wang YF, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P1401
   Wasser SK, 2004, P NATL ACAD SCI USA, V101, P14847, DOI 10.1073/pnas.0403170101
   Wearn OR, 2019, ROY SOC OPEN SCI, V6, DOI 10.1098/rsos.181748
   Welbourne DJ, 2019, ANIMALS-BASEL, V9, DOI 10.3390/ani9060388
   Whitworth A, 2016, TROP CONSERV SCI, V9, P675, DOI 10.1177/194008291600900208
   Whytock RC, 2017, METHODS ECOL EVOL, V8, P308, DOI 10.1111/2041-210X.12678
   Wich S. A., 2018, NEW GEOSPATIAL APPRO, P121
   Wich SA, 2018, CONSERVATION DRONES: MAPPING AND MONITORING BIODIVERSITY, P1, DOI 10.1093/oso/9780198787617.001.0001
   Wich SA, 2002, BEHAV ECOL SOCIOBIOL, V52, P474, DOI 10.1007/s00265-002-0541-8
   WICH SA, 2016, PRIMATE CONSERVATION, P00001, DOI DOI 10.1093/ACPROF:OSO/9780198703389.003.0001
   Wich S, 2016, J UNMANNED VEH SYST, V4, P45, DOI 10.1139/juvs-2015-0015
   Wijers M, 2021, BIOACOUSTICS, V30, P41, DOI 10.1080/09524622.2019.1685408
   Williamson E. A., 2003, FIELD LAB METHODS PR, P33
   Wilson AM, 2017, AUK, V134, P350, DOI 10.1642/AUK-16-216.1
   Zak AA, 2017, INT J PRIMATOL, V38, P224, DOI 10.1007/s10764-016-9945-6
   Zhang H, 2020, GLOB ECOL CONSERV, V23, DOI 10.1016/j.gecco.2020.e01101
   ZIMMERMANN E, 1993, ETHOLOGY, V93, P211
NR 241
TC 0
Z9 0
U1 3
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0164-0291
EI 1573-8604
J9 INT J PRIMATOL
JI Int. J. Primatol.
DI 10.1007/s10764-021-00245-z
EA OCT 2021
PG 35
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA WK3QY
UT WOS:000709644700001
OA hybrid, Green Accepted, Green Published
DA 2022-02-10
ER

PT J
AU Nickel, BA
   Suraci, JP
   Allen, ML
   Wilmers, CC
AF Nickel, Barry A.
   Suraci, Justin P.
   Allen, Maximilian L.
   Wilmers, Christopher C.
TI Human presence and human footprint have non-equivalent effects on
   wildlife spatiotemporal habitat use
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Camera trap; Community ecology; Occupancy model; Wildlife nocturnality;
   Anthropogenic disturbance; Recreation ecology
ID ESTIMATING SITE OCCUPANCY; LARGE CARNIVORES; MODEL SELECTION; PROTECTED
   AREAS; RECREATION; URBAN; DISTURBANCE; LANDSCAPE; ECOLOGY; FEAR
AB Human impacts on wildlife stem from both our footprint on the landscape and the presence of people in wildlife habitat. Each may influence wildlife at very different spatial and temporal scales, yet efforts to disentangle these two classes of anthropogenic disturbance in their effects on wildlife have remained limited, as have efforts to predict the spatial extent of human presence and its impacts independently of human footprint. We used camera trap data from a 1400-km(2) grid spanning wildlands and residential development in central California to compare the effects of human presence (human detections on camera) and footprint (building density) on mammalian predators. We then developed a model predicting the spatial extent of human presence and its impacts across the broader landscape. Occupancy modeling and temporal activity analyses showed that human presence and footprint had non-equivalent and often opposing effects on wildlife. Larger predators (pumas Puma concolor, bobcats Lynx rufus, coyotes Canis latrans) were less active where human footprint was high but avoided high human presence temporally rather than spatially. Smaller predators (striped skunks Mephitis mephitis, Virginia opossums Didelphis virginiana) preferred developed areas but exhibited reduced activity where human presence was high. A spatial model, based on readily available landscape covariates (parking lots, trails, topography), performed well in predicting human activity outside of developed areas, and revealed high human presence even in remote protected areas that provide otherwise intact wildlife habitat. This work highlights the need to integrate multiple disturbance types when evaluating the impacts of anthropogenic activity on wildlife.
C1 [Nickel, Barry A.; Suraci, Justin P.; Wilmers, Christopher C.] Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Environm Studies Dept, Santa Cruz, CA 95064 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, Prairie Res Inst, Champaign, IL 61820 USA.
RP Suraci, JP (corresponding author), Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Environm Studies Dept, Santa Cruz, CA 95064 USA.
EM justin.suraci@gmail.com
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Gordon and Betty Moore FoundationGordon and Betty Moore Foundation;
   National Science FoundationNational Science Foundation (NSF) [1255913,
   0963022]; Blue Foundation; Peninsula Open Space Trust; Resources Legacy
   Fund
FX We thank P. Houghtaling, R. King, and K. Briner for help in the field,
   A. Nisi for data management assistances, and multiple undergraduate
   volunteers for help scoring camera trap images. We are grateful to the
   many Santa Cruz Mountains property owners who provided access to their
   land. Funding was provided by the Gordon and Betty Moore Foundation,
   National Science Foundation (grants 1255913 and 0963022 to CCW), the
   Blue Foundation, Peninsula Open Space Trust and Resources Legacy Fund.
CR Balmford A, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002074
   Bateman PW, 2012, J ZOOL, V287, P1, DOI 10.1111/j.1469-7998.2011.00887.x
   Beckmann JP, 2003, J ZOOL, V261, P207, DOI 10.1017/S0952836903004126
   Boydston EE, 2003, ANIM CONSERV, V6, P207, DOI 10.1017/S1367943003003263
   Broms KM, 2016, ECOLOGY, V97, P1759, DOI 10.1890/15-1471.1
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Burton AC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038007
   Carter NH, 2012, P NATL ACAD SCI USA, V109, P15360, DOI 10.1073/pnas.1210490109
   Clinchy M, 2016, BEHAV ECOL, V27, P1826, DOI 10.1093/beheco/arw117
   Cordell H.K., 2008, INT J WILDERNESS, V14, P7
   Creel S, 2008, ANIM BEHAV, V76, P1139, DOI 10.1016/j.anbehav.2008.07.006
   Darimont CT, 2015, SCIENCE, V349, P858, DOI 10.1126/science.aac4249
   Diaz-Ruiz F, 2016, J ZOOL, V298, P128, DOI 10.1111/jzo.12294
   Dickie M, 2017, J APPL ECOL, V54, P253, DOI 10.1111/1365-2664.12732
   Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
   Droge E, 2017, NAT ECOL EVOL, V1, P1123, DOI 10.1038/s41559-017-0220-9
   Efford MG, 2012, ECOSPHERE, V3, DOI 10.1890/ES11-00308.1
   Fischer J, 2007, GLOBAL ECOL BIOGEOGR, V16, P265, DOI 10.1111/j.1466-8238.2007.00287.x
   Frid A, 2002, CONSERV ECOL, V6
   Gaynor KM, 2018, SCIENCE, V360, P1232, DOI 10.1126/science.aar7121
   Gelman A, 2008, STAT MED, V27, P2865, DOI 10.1002/sim.3107
   Gutzwiller KJ, 2017, FRONT ECOL ENVIRON, V15, P517, DOI 10.1002/fee.1631
   Hansen AJ, 2005, ECOL APPL, V15, P1893, DOI 10.1890/05-5221
   Hobbs N. T., 2015, BAYESIAN MODELS STAT
   Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
   Kohl MT, 2018, ECOL MONOGR, V88, P638, DOI 10.1002/ecm.1313
   Ladle A, 2017, METHODS ECOL EVOL, V8, P329, DOI 10.1111/2041-210X.12660
   Larson CL, 2018, LANDSCAPE URBAN PLAN, V175, P62, DOI 10.1016/j.landurbplan.2018.03.009
   Larson CL, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167259
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   Martin J, 2016, J STAT COMPUT SIM, V86, P3777, DOI 10.1080/00949655.2016.1186166
   Martinuzzi S, 2015, NRS8 USDA FOR SERV
   Monz CA, 2013, FRONT ECOL ENVIRON, V11, P441, DOI 10.1890/120358
   Muhly TB, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017050
   Neilson EW, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2092
   Nix JH, 2018, BEHAV PROCESS, V146, P16, DOI 10.1016/j.beproc.2017.11.002
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Ordiz A, 2011, OECOLOGIA, V166, P59, DOI 10.1007/s00442-011-1920-5
   Patten MA, 2018, BIOL CONSERV, V218, P233, DOI 10.1016/j.biocon.2017.12.033
   R Core Team, 2019, R LANG ENV STAT COMP
   Radeloff VC, 2005, ECOL APPL, V15, P799, DOI 10.1890/04-1413
   Radeloff VC, 2010, P NATL ACAD SCI USA, V107, P940, DOI 10.1073/pnas.0911131107
   Reed SE, 2011, CONSERV BIOL, V25, P504, DOI 10.1111/j.1523-1739.2010.01641.x
   Reed SE, 2008, CONSERV LETT, V1, P146, DOI 10.1111/j.1755-263X.2008.00019.x
   Reilly ML, 2017, BIOL CONSERV, V207, P117, DOI 10.1016/j.biocon.2016.11.003
   Riley SPD, 2006, J WILDLIFE MANAGE, V70, P1425, DOI 10.2193/0022-541X(2006)70[1425:SEOBAG]2.0.CO;2
   Rossi SD, 2015, APPL GEOGR, V63, P77, DOI 10.1016/j.apgeog.2015.06.008
   Royle J.A., 2008, HEIRARCHICAL MODELS
   Smith JA, 2019, LANDSCAPE URBAN PLAN, V183, P50, DOI 10.1016/j.landurbplan.2018.11.003
   Smith JA, 2018, OIKOS, V127, P890, DOI 10.1111/oik.04592
   Smith JA, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2017.0433
   Suraci JP, 2019, ECOL LETT, V22, P1578, DOI 10.1111/ele.13344
   Suraci JP, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2644
   Tablado Z, 2017, BIOL REV, V92, P216, DOI 10.1111/brv.12224
   Tucker MA, 2018, SCIENCE, V359, P466, DOI 10.1126/science.aam9712
   Venter O, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.67
   VUONG QH, 1989, ECONOMETRICA, V57, P307, DOI 10.2307/1912557
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilmers CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060590
NR 61
TC 27
Z9 30
U1 14
U2 45
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD JAN
PY 2020
VL 241
AR 108383
DI 10.1016/j.biocon.2019.108383
PG 11
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KT0IV
UT WOS:000518695100021
DA 2022-02-10
ER

PT J
AU Wysong, ML
   Gregory, P
   Watson, AWT
   Woolley, LA
   Parker, CW
AF Wysong, Michael L.
   Gregory, Pius
   Watson, Alexander W. T.
   Woolley, Leigh-Ann
   Parker, Christopher W.
CA Yawuru Country Managers
   Yawuru Country Managers
   Karajarri Rangers
   Karajarri Rangers
   Nyikina Mangala Rangers
TI Cross-cultural collaboration leads to greater understanding of the rare
   Spectacled Hare-wallaby in the west Kimberley, Western Australia
SO ECOLOGICAL MANAGEMENT & RESTORATION
LA English
DT Article
DE Camera traps; cross-cultural partnerships; Dampier Peninsula; Indigenous
   knowledge; Indigenous Protected Area; Indigenous rangers; Yawuru
ID KNOWLEDGE; OPPORTUNITIES; CONSERVATION
AB Cross-cultural collaboration between Yawuru Country Managers (Rangers) and WWF-Australia ecologists led to new detections of the Spectacled Hare-wallaby (SHW), (Lagorchestes conspicillatus) in the west Kimberley region of Western Australia where it was presumed to be locally extirpated. This collaboration relied on the expertise of the Yawuru Country Managers to select specific locations for targeted field surveys and resulted in the confirmation of SHW on the Yawuru IPA for the first time in a decade. Subsequent remote camera trap surveys over a larger area included collaboration with two additional neighbouring Indigenous ranger groups, Karrajarri and Nyikina Mangala. These surveys investigated the spatial and temporal relationship between SHW and other mammals which may threaten (e.g., feral Cat [Felis catus], Dingo [Canis familiaris dingo]) or compete (e.g., Agile Wallaby [Macropus agilis]; Cattle [Bos taurus]) with them. We found a negative relationship between SHW and cat activity, suggesting that cats may limit the activity or abundance of SHW. Temporal portioning was evident between SHW and both Cattle and Agile Wallaby suggesting that SHW may avoid times when these species are most active. Further, we found a negative relationship between SHW occurrence and distance to fire scar edge burnt in current or previous fire season. This edge habitat is likely important to SHW because they may require recently burnt areas to forage and dense unburnt areas to shelter. This project highlights the benefits of cross-cultural research and monitoring partnerships with Indigenous rangers as active observers and managers of their traditional lands.
C1 [Wysong, Michael L.; Gregory, Pius; Yawuru Country Managers; Yawuru Country Managers] Nyamba Buru Yawuru, Environm Serv Unit, 55 Reid Rd, Cable Beach, WA 6726, Australia.
   [Wysong, Michael L.] Charles Darwin Univ, Res Inst Environm & Livelihoods, Ellengowan Dr, Casurina, NT 0810, Australia.
   [Watson, Alexander W. T.; Woolley, Leigh-Ann] World Wide Fund Nat Australia, Sydney, NSW, Australia.
   [Parker, Christopher W.] Nyamba Buru Yawuru, Cable Beach, WA, Australia.
   [Karajarri Rangers; Karajarri Rangers] Karajarri Tradit Lands Assoc, Off 2,Broome Lotteries House,Lot 642 Cable Beach, Broome, WA 6725, Australia.
   [Nyikina Mangala Rangers] Walalakoo Aboriginal Corp, 70A Stanley St, Derby, WA 6728, Australia.
RP Wysong, ML (corresponding author), Nyamba Buru Yawuru, Environm Serv Unit, 55 Reid Rd, Cable Beach, WA 6726, Australia.
EM mlwysong@gmail.com; alexander.watson@australianwildlife.org;
   LWoolley@wwf.org.au; chris@spectrumecology.com.au
FU Lotterywest [421008692]; Australian Government's National Indigenous
   Australians Agency; Indigenous Land and Sea Corporation
FX We acknowledge and respect the Yawuru, Karajarri and Nykina Traditional
   Owners past, present and future on whose lands this project took place.
   Staff from WWF-Australia (Jessica Koleck, Jessica Chapman, Ellie Boyle,
   Hamsini Bijlani and Tanya Vernes) assisted on field surveys. Support for
   field surveys on Unallocated Crown Lands was provided by the Department
   of Biodiversity, Conservation and Attractions Broome office. We
   specifically thank Nyikina Mangala ranger coordinator Damien Giles and
   Karajarri ranger coordinator Ewan Noakes for assistance with logistics
   and fieldwork. Support for field surveys was also provided by the Nyamba
   Buru Yawuru Environmental Services IPA team including Nathan Kay, and
   former Yawuru Country Managers Johani Mamid and Jacob Corpus. We also
   thank Steve Reynolds of Environs Kimberley for his early assistance on
   the project. We gratefully acknowledge funding from Lotterywest to
   WWF-Australia (grant number 421008692). The project was also made
   possible by funding for the Yawuru IPA and Country Manager programs from
   the Australian Government's National Indigenous Australians Agency and
   the Indigenous Land and Sea Corporation. The authors declare no conflict
   of interest with respect to the submission of this manuscript.
CR Anonymous, 2014, GUARDIAN 1123
   Atlas of Living Australia, 2021, SPEC PAG, DOI [10.26197/ala.3321258f-c4e0-489c-921c-ff0e5a57c11b, DOI 10.26197/ALA.3321258F-C4E0-489C-921C-FF0E5A57C11B]
   Austin B.J., 2017, GUIDELINES COLLABORA
   Australian Government Department of Agriculture Water and Environment, 2021, IND PROT AR
   Australian Government Department of Sustainability Environment Water Population and Communities, 2011, SURV GUID AUSTR THRE
   Berkes F, 2009, J ENVIRON MANAGE, V90, P1692, DOI 10.1016/j.jenvman.2008.12.001
   Bohensky EL, 2013, ECOL SOC, V18, DOI 10.5751/ES-05846-180320
   Bohensky EL, 2011, ECOL SOC, V16, DOI 10.5751/ES-04342-160406
   Brook LA, 2012, J APPL ECOL, V49, P1278, DOI 10.1111/j.1365-2664.2012.02207.x
   Burbidge A., 1995, MAMMALS AUSTR, P313
   BURBIDGE AA, 1988, AUST WILDLIFE RES, V15, P9
   Dawson SJ, 2018, AUSTRAL ECOL, V43, P159, DOI 10.1111/aec.12553
   Eldridge DJ, 2016, ECOL APPL, V26, P1273, DOI 10.1890/15-1234
   Ens E., 2012, PEOPLE COUNTRY VITAL, P45
   Ens EJ, 2015, BIOL CONSERV, V181, P133, DOI 10.1016/j.biocon.2014.11.008
   Ens Emilie J., 2012, Ecological Management & Restoration, V13, P100, DOI 10.1111/j.1442-8903.2011.00634.x
   Godden L, 2016, RESTOR ECOL, V24, P692, DOI 10.1111/rec.12394
   INGLEBY S, 1992, WILDLIFE RES, V19, P721, DOI 10.1071/WR9920721
   INGLEBY S, 1991, WILDLIFE RES, V18, P501, DOI 10.1071/WR9910501
   Kenneally K.F., 1996, BROOME
   Legge S, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.52
   Legge Sarah, 2011, Ecological Management & Restoration, V12, P84, DOI 10.1111/j.1442-8903.2011.00595.x
   Letnic M, 2010, BIOL REV, V85, P501, DOI 10.1111/j.1469-185X.2009.00113.x
   McGregor H, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133915
   McGregor HW, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0109097
   McKemey M, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12030995
   Mistry J, 2016, SCIENCE, V352, P1274, DOI 10.1126/science.aaf1160
   Paltridge R, 2020, WILDLIFE RES, V47, P709, DOI 10.1071/WR20035
   Parkins K, 2019, FOREST ECOL MANAG, V451, DOI 10.1016/j.foreco.2019.05.013
   Prober SM, 2011, ECOL SOC, V16
   Radford IJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130721
   Radford IJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092341
   Reed G, 2021, CONSERV BIOL, V35, P179, DOI 10.1111/cobi.13532
   Reid A.M., 2019, GRASS FIRE KANGAROOS
   Reid AM, 2020, AUSTRAL ECOL, V45, P529, DOI 10.1111/aec.12860
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Russell-Smith J, 2003, INT J WILDLAND FIRE, V12, P283, DOI 10.1071/WF03015
   Sheil D, 2004, TRENDS ECOL EVOL, V19, P634, DOI 10.1016/j.tree.2004.09.019
   Smyth D., 2015, PARKS, V21, P73, DOI DOI 10.2305/IUCN.CH.2014.PARKS-21-2DS.EN
   Tengo M, 2014, AMBIO, V43, P579, DOI 10.1007/s13280-014-0501-3
   Vigilante T, 2017, LAND-BASEL, V6, DOI 10.3390/land6040068
   Ward-Fear G, 2019, CONSERV LETT, V12, DOI 10.1111/conl.12643
   Winter J., 2016, IUCN RED LIST THREAT
   Wiseman ND, 2016, GEOGR RES-AUST, V54, P52, DOI 10.1111/1745-5871.12150
   Woinarski J. C., 2014, ACTION PLAN AUSTR MA
   WWF-Australia, 2011, STRAT PLAN 2011 2016
   YRMTBC Yawuru Registered Native Title Body Corporate, 2014, WAL NAG BIRR BUR YAW
NR 47
TC 2
Z9 2
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1442-7001
EI 1442-8903
J9 ECOL MANAG RESTOR
JI Ecol. Manag. Restor.
PD JAN
PY 2022
VL 23
SU 1
BP 139
EP 149
DI 10.1111/emr.12524
PG 11
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA YO9YD
UT WOS:000748287400017
DA 2022-02-10
ER

PT J
AU Haryono, M
   Rahmat, UM
   Daryan, M
   Raharja, AS
   Muhtarom, A
   Firdaus, AY
   Rohaeti, A
   Subchiyatin, I
   Nugraheni, A
   Khairani, KO
   Kartina
AF Haryono, Mohamad
   Rahmat, Ujang Mamat
   Daryan, Muhiban
   Raharja, Agung Suci
   Muhtarom, Aom
   Firdaus, Asep Yayus
   Rohaeti, Ai
   Subchiyatin, Irma
   Nugraheni, Amila
   Khairani, Kurnia Oktalina
   Kartina
TI Monitoring of the Javan rhino population in Ujung Kulon National Park,
   Java
SO PACHYDERM
LA English
DT Article
ID SEX-RATIO
AB A monitoring project of the Javan rhino was conducted so as to understand the extent to which the growth of this population has succeeded. Monitoring was conducted by making use of camera traps, which were strategically placed by using a stratified sampling method based on the area of concentration of Javan rhino. The population size of Javan rhino in 20:13 was a minimal 58 individuals consisting of 8 calves and 50 sub adults or adults with a sex ratio of 35 males: 23 females. The birth rate was recorded at 13.79% while the mortality rate was 3.45%. We also recorded 4 new calves in 2013.
C1 [Haryono, Mohamad; Daryan, Muhiban; Raharja, Agung Suci; Muhtarom, Aom; Firdaus, Asep Yayus; Rohaeti, Ai; Subchiyatin, Irma; Nugraheni, Amila] Ujung Kulon Natl Pk Author, Pandeglang 42264, Indonesia.
   [Rahmat, Ujang Mamat] Minist Forestry, Directorate Gen Forest Protect & Nat Conservat, Directorate Biodivers Conservat, Jakarta, Indonesia.
   [Khairani, Kurnia Oktalina] Cornell Univ, Ithaca, NY 14850 USA.
   [Kartina] Tirtayasa Univ, Fac Agr, Serang 42121, Indonesia.
RP Nugraheni, A (corresponding author), Ujung Kulon Natl Pk Author, Jl P Kemerdekaan 51 Labuan, Pandeglang 42264, Indonesia.
EM amilan_tnuk@yahoo.com
RI Dr. Abid Muhtarom, S.Pd/AAL-7677-2021
OI Dr. Abid Muhtarom, S.Pd/0000-0002-2781-8086
CR Amman H., 1985, THESIS
   Colles A, 2009, ECOL LETT, V12, P849, DOI 10.1111/j.1461-0248.2009.01336.x
   Ewen JG, 2011, J ANIM ECOL, V80, P448, DOI 10.1111/j.1365-2656.2010.01774.x
   Griffiths M., 1993, JAVAN RHINO UJUNG KU
   Hariyadi ARS, 2011, PACHYDERM, P90
   Hoogenyerf A, 1970, UDJUNG KULON LAND LA
   Kemenhut (KementerianKehutanan), 1999, PER PEM NOM 7 TENT P
   Krebs CJ, 2006, MAMMALS ECOLOGICAL C
   Kusuma IH, 2008, MEDIA KONSERVASI, V13, P59
   Lyons JE, 2008, J WILDLIFE MANAGE, V72, P1683, DOI 10.2193/2008-141
   Muntasib E. K. S. H., 2002, THESIS
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P233, DOI 10.1007/978-4-431-99495-4_13
   Rahmat U. M., 2007, THESIS
   Rosenfeld CS, 2004, BIOL REPROD, V71, P1063, DOI 10.1095/biolreprod.104.030890
   Schenkel R, 1969, ACTA TROPICA SPARATU, V26
   Stokes EJ, 2011, MONITORING WILDLIFE
   Trolliet F, 2014, AGRON SOC ENV, V18, P446
   Ujung Kulon National Park, 2010, LAP SENS BAD JAW RHI
   Van Strien N. J., 2008, RHINOCEROS SONDAICUS
   Wedekind C., 2012, TOPICS CONSERVATION
NR 20
TC 1
Z9 1
U1 2
U2 30
PU IUCN-SSC ASIAN ELEPHANT SPECIALIST GROUP
PI RAJAGIRIYA
PA C/O JAYANTHA JAYEWARDENE, BIODIVERSITY & ELEPHANT CONSERVATION TRUST,
   RAJAGIRIYA GARDENS, NAWALA RD, RAJAGIRIYA, 615-32, SRI LANKA
SN 1026-2881
J9 PACHYDERM
JI Pachyderm
PD JUN-JUL
PY 2014
IS 56
BP 82
EP 86
PG 5
WC Biodiversity Conservation; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Zoology
GA CY4SY
UT WOS:000366399800008
DA 2022-02-10
ER

PT C
AU Dhillon, A
   Verma, GK
AF Dhillon, Anamika
   Verma, Gyanendra K.
BE Tiwary, US
TI Wild Animal Detection from Highly Cluttered Forest Images Using Deep
   Residual Networks
SO INTELLIGENT HUMAN COMPUTER INTERACTION
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 10th International Conference on Intelligent Human Computer Interaction
   (IHCI)
CY DEC 07-09, 2018
CL Allahabad, INDIA
SP Indian Inst Informat Technol Allahabad
DE Wild animal detection; DCNN feature extractor; Ensemble tree; KNN;
   Natural scenes; SVM
AB Wild animal detection is a dynamic research field since last decades. The videos acquired from camera-trap comprises of scenes that are cluttered that poses a challenge for detection of the wild animal. In this paper, we proposed a deep learning based system to detect wild animal from highly cluttered natural forest images. We have utilized Deep Residual Network (ResNet) for features extraction from cluttered forest images. These features are feed to classification through some of the best in class machine learning techniques, to be specific Support Vector Machine, K-Nearest Neighbor and Ensemble Tree. Our outcomes demonstrate that our detection system through ResNet outperforms compare to existing systems reported in the literature.
C1 [Dhillon, Anamika; Verma, Gyanendra K.] Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
RP Verma, GK (corresponding author), Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
EM dhillon.anamika2390@gmail.com; gyanendra@nitkkr.ac.in
RI Verma, Gyanendra/A-1013-2016
OI Verma, Gyanendra/0000-0003-2567-3730
CR Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chauhan A. K., 2013, INT J ADV RES COMPUT, V3
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Joshi Ritesh, 2012, International Journal of Ecosystem, V2, P44
   Kays R., 2014, P N AM CONSERVATION, P80
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Rakibe Rupali S., 2013, INT J SCI RES PUBLIC, V3, P2250
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176
   Szegedy C., 2013, ADV NEURAL INF PROCE, V26, P2553, DOI DOI 10.5555/2999792.2999897
   Szegedy C., 2014, ARXIV14121441
   Tilak S., 2011, INT J RES REV WIRELE, V1, P1929
   Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
   Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
NR 16
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-04021-5; 978-3-030-04020-8
J9 LECT NOTES COMPUT SC
PY 2018
VL 11278
BP 230
EP 238
DI 10.1007/978-3-030-04021-5_21
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ4GA
UT WOS:000589562600021
DA 2022-02-10
ER

PT J
AU Krofel, M
   Juznic, D
   Allen, ML
AF Krofel, Miha
   Juznic, Damjan
   Allen, Maximilian L.
TI Scavenging and carcass caching behavior by European wildcat (Felis
   silvestris)
SO ECOLOGICAL RESEARCH
LA English
DT Article
DE camera traps; felids; food caching; interference competition; scavengers
AB While scavenging has been repeatedly reported for several felid species, surprisingly little information is available on scavenging behavior of the European wildcat (Felis silvestris). To fill this knowledge gap, we used camera traps to document scavenging behavior at the 48 experimentally set deer carcasses at random locations throughout the year. We recorded European wildcats scavenging on 38% of the eight carcasses set in winter and on none set in the other parts of the year. Wildcats fed on two carcasses for extended periods (up to 22 days) with an average of 3.3 visits per day and 7.8-h interval between the visits. We recorded scavenging throughout the day, but analysis indicated a crepuscular pattern. We also recorded caching behavior on 7% of the visits (n = 105), when wildcats used leaves or snow to partly or completely cover the carcasses. Beside wildcats, 12 other vertebrate species of scavengers were recorded at the carcasses. We recorded agonistic interaction with European badger (Meles meles) and despite its smaller size, the wildcat managed to defend the carcass. The extensive feeding, frequent caching behavior and active defense from scavengers indicate that the wildcats recognized the ungulate carcasses as an important food source in winter and that scavenging could be a neglected aspect of the European wildcat ecology. We also suggest that caching behavior could be regularly used by the European wildcat when feeding on larger carcasses, but was likely previously missed due to limited research effort to record scavenging and caching behavior.
C1 [Krofel, Miha; Juznic, Damjan] Univ Ljubljana, Biotech Fac, Dept Forestry, Ljubljana, Slovenia.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
RP Krofel, M (corresponding author), Univ Ljubljana, Biotech Fac, Dept Forestry, Ljubljana, Slovenia.
EM miha.krofel@bf.uni-lj.si
OI Krofel, Miha/0000-0002-2010-5219
FU Javna Agencija za Raziskovalno Dejavnost RSSlovenian Research Agency -
   Slovenia [N1-0163, P4-0059]
FX Javna Agencija za Raziskovalno Dejavnost RS, Grant/Award Numbers:
   N1-0163, P4-0059
CR Allen ML, 2016, ECOLOGY, V97, P1905, DOI 10.1002/ecy.1462
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Apostolico F, 2016, MAMMAL RES, V61, P109, DOI 10.1007/s13364-015-0255-8
   Balme GA, 2017, J ANIM ECOL, V86, P634, DOI 10.1111/1365-2656.12654
   Bang P, 2001, ANIMAL TRACKS SIGNS
   Bischoff-Mattson Z, 2009, WEST N AM NATURALIST, V69, P343, DOI 10.3398/064.069.0308
   Brighten Alex Leigh, 2019, Journal of Threatened Taxa, V11, P13492, DOI 10.11609/jott.4445.11.4.13492-13496
   Careau V, 2007, BEHAV ECOL SOCIOBIOL, V62, P87, DOI 10.1007/s00265-007-0441-z
   Cirovic D, 2016, BIOL CONSERV, V199, P51, DOI 10.1016/j.biocon.2016.04.027
   de Ruiter DJ, 2001, AFR J ECOL, V39, P396, DOI 10.1046/j.1365-2028.2001.00320.x
   Herbst M., 2009, THESIS U PRETORIA PR
   Hunter Luke, 2015, P1
   Inagaki A, 2020, ECOL EVOL, V10, P1223, DOI 10.1002/ece3.5976
   Kitchener A.C., 2017, Cat News, P3
   Koike, 2020, URSUS
   Krofel, 2016, STOPINJE SLEDOVI ZIV
   Krofel M, 2021, ECOL RES, V36, P556, DOI 10.1111/1440-1703.12211
   Krofel M, 2019, FOLIA ZOOL, V68, P274, DOI 10.25225/fozo.037.2019
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Krofel Miha, 2011, Acrocephalus, V32, P45, DOI 10.2478/v10100-011-0003-3
   MACDONALD DW, 1976, Z TIERPSYCHOL, V42, P170
   Meredith M., 2017, OVERVIEW OVERLAP PAC
   Molinari J.A., 2000, RAUBTIERE WERK HDB B
   Peers Michael J.L., 2018, Northwestern Naturalist, V99, P232
   Renard Aurelie, 2015, Mammalian Species, P78, DOI 10.1093/mspecies/sev008
   Ruiz-Villar H, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-01413-x
   Sebastian-Gonzalez E, 2019, GLOBAL CHANGE BIOL, V25, P3005, DOI 10.1111/gcb.14708
   Sebastian-Gonzalez E, 2020, ECOGRAPHY, V43, P1143, DOI 10.1111/ecog.05083
   Selva N, 2005, CAN J ZOOL, V83, P1590, DOI 10.1139/Z05-158
   Selva N, 2003, ECOSCIENCE, V10, P303, DOI 10.1080/11956860.2003.11682778
   Shackelford, 2017, ENCY ANIMAL COGNITIO, P1, DOI DOI 10.1007/978-3-319-47829-6_444-1
   Sliwa Alexander, 1994, Zoologische Garten, V64, P83
   SONERUD GA, 1986, HOLARCTIC ECOL, V9, P33
   Van Valkenburgh B, 1989, CARNIVORE BEHAV ECOL, V1, P410, DOI DOI 10.1007/978-1-4757-4716-4
   Wilmers CC, 2003, ECOL LETT, V6, P996, DOI 10.1046/j.1461-0248.2003.00522.x
   Wilson EE, 2011, TRENDS ECOL EVOL, V26, P129, DOI 10.1016/j.tree.2010.12.011
NR 36
TC 2
Z9 2
U1 1
U2 4
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0912-3814
EI 1440-1703
J9 ECOL RES
JI Ecol. Res.
PD MAY
PY 2021
VL 36
IS 3
BP 556
EP 561
DI 10.1111/1440-1703.12211
EA FEB 2021
PG 6
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SD0QN
UT WOS:000623346000001
DA 2022-02-10
ER

PT J
AU Clapham, M
   Miller, E
   Nguyen, M
   Darimont, CT
AF Clapham, Melanie
   Miller, Ed
   Nguyen, Mary
   Darimont, Chris T.
TI Automated facial recognition for wildlife that lack unique markings: A
   deep learning approach for brown bears
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE deep learning; face recognition; grizzly bear; individual ID; machine
   learning; wildlife monitoring
ID CAMERA TRAPS; REIDENTIFICATION; INDIVIDUALS; NETWORKS
AB Emerging technologies support a new era of applied wildlife research, generating data on scales from individuals to populations. Computer vision methods can process large datasets generated through image-based techniques by automating the detection and identification of species and individuals. With the exception of primates, however, there are no objective visual methods of individual identification for species that lack unique and consistent body markings. We apply deep learning approaches of facial recognition using object detection, landmark detection, a similarity comparison network, and an support vector machine-based classifier to identify individuals in a representative species, the brown bear Ursus arctos. Our open-source application, BearID, detects a bear's face in an image, rotates and extracts the face, creates an "embedding" for the face, and uses the embedding to classify the individual. We trained and tested the application using labeled images of 132 known individuals collected from British Columbia, Canada, and Alaska, USA. Based on 4,674 images, with an 80/20% split for training and testing, respectively, we achieved a facial detection (ability to find a face) average precision of 0.98 and an individual classification (ability to identify the individual) accuracy of 83.9%. BearID and its annotated source code provide a replicable methodology for applying deep learning methods of facial recognition applicable to many other species that lack distinguishing markings. Further analyses of performance should focus on the influence of certain parameters on recognition accuracy, such as age and body size. Combining BearID with camera trapping could facilitate fine-scale behavioral research such as individual spatiotemporal activity patterns, and a cost-effective method of population monitoring through mark-recapture studies, with implications for species and landscape conservation and management. Applications to practical conservation include identifying problem individuals in human-wildlife conflicts, and evaluating the intrapopulation variation in efficacy of conservation strategies, such as wildlife crossings.
C1 [Clapham, Melanie; Miller, Ed; Nguyen, Mary] BearID Project, Sooke, BC, Canada.
   [Clapham, Melanie; Darimont, Chris T.] Univ Victoria, Dept Geog, 3800 Finnerty Rd, Victoria, BC V8P 5C2, Canada.
   [Darimont, Chris T.] Raincoast Conservat Fdn, Bella Bella, BC, Canada.
RP Clapham, M (corresponding author), Univ Victoria, Dept Geog, 3800 Finnerty Rd, Victoria, BC V8P 5C2, Canada.
EM melanie@understandingbears.com
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)CGIAR [CRDPJ
   523329-18]
FX Natural Sciences and Engineering Research Council of Canada, Grant/Award
   Number: CRDPJ 523329-18
CR Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Chen P, 2020, ECOL EVOL, V10, P3561, DOI 10.1002/ece3.6152
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Clapham M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0035404
   Clutton-Brock T, 2010, TRENDS ECOL EVOL, V25, P562, DOI 10.1016/j.tree.2010.08.002
   Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
   Dalal N., 2021, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Deb D., 2018, 2018 IEEE 9 INT C BI, P1, DOI DOI 10.1109/BTAS.2018.8698538
   Dexter CE, 2018, AUST MAMMAL, V40, P67, DOI 10.1071/AM16043
   Ernst A., 2011, 8 IEEE INT C ADV VID, P279, DOI DOI 10.1109/AVSS.2011.6027337
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   HAN B, 2018, ADV NEUR IN, V31
   Hertel AG, 2017, BEHAV ECOL, V28, P1524, DOI 10.1093/beheco/arx122
   Hilderbrand GV, 1999, CAN J ZOOL, V77, P132, DOI 10.1139/cjz-77-1-132
   Huang G.B., 2007, LABELED FACES WILD D
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   King D. E., 2015, 150200046 ARXIV
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Loos A, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P116, DOI 10.1109/ISM.2012.30
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Moreira TP, 2017, MULTIMED TOOLS APPL, V76, P15325, DOI 10.1007/s11042-016-3824-1
   Rashmi P., 2017, INT J RECENT TRENDS, V3, P207, DOI [10.23883/IJRTER.2017.3215.TXUQG, DOI 10.23883/IJRTER.2017.3215.TXUQG]
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Russell R., 1983, BEARS THEIR BIOL MAN, P174, DOI DOI 10.2307/3872535
   Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Swan GJF, 2017, TRENDS ECOL EVOL, V32, P518, DOI 10.1016/j.tree.2017.03.011
   Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Witham CL, 2018, J NEUROSCI METH, V300, P157, DOI 10.1016/j.jneumeth.2017.07.020
NR 37
TC 4
Z9 4
U1 2
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD DEC
PY 2020
VL 10
IS 23
BP 12883
EP 12892
DI 10.1002/ece3.6840
EA NOV 2020
PG 10
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA OZ9EC
UT WOS:000585885000001
PM 33304501
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Allen, ML
   Wang, SD
   Olson, LO
   Li, Q
   Krofel, M
AF Allen, Maximilian L.
   Wang, Shaodong
   Olson, Lucas O.
   Li, Qing
   Krofel, Miha
TI Counting cats for conservation: seasonal estimates of leopard density
   and drivers of distribution in the Serengeti
SO BIODIVERSITY AND CONSERVATION
LA English
DT Article
DE Competition; Leopard; Panthera pardus; Population density; Predator-prey
   dynamics; Seasonal variation; Spatially explicit capture recapture
ID PANTHERA-PARDUS; CAPTURE-RECAPTURE; LARGE CARNIVORES; ACTIVITY PATTERNS;
   LUANGWA VALLEY; NATIONAL-PARK; SOUTH-AFRICA; MANAGEMENT; MODELS; LIONS
AB Large carnivore conservation is important for ecosystem integrity and understanding drivers of their abundance is essential to guide conservation efforts. Leopard (Panthera pardus) populations are in a general state of decline, although local studies demonstrated large variation in their population trends and density estimates vary widely across their range. We used spatially-explicit capture-recapture models for unmarked populations with camera trap data from a citizen science project to estimate previously-unknown leopard population densities in Serengeti National Park, Tanzania, and determine potential biological drivers of their abundance and distribution. We estimated leopard densities, at 5.41 (95% CrI = 2.23-9.26) and 5.72 (95% CrI = 2.44-9.55) individuals/100 km(2), in the dry and wet season, respectively, which confirmed Serengeti National Park as one of the strongholds of this species in Africa. In contrast to abundance estimates, we found that drivers of leopard abundance and distribution varied among the dry and wet seasons, and were primarily affected by interactions with other larger carnivores and cover. The underlying driver of leopard distribution may be the dynamic prey availability which shifts between seasons, leading to an avoidance of dominant carnivores when prey availability is low in the dry season but an association with dominant carnivores when prey availability is high in the wet season. As efforts to conserve large carnivore populations increase worldwide, our results highlight the benefits of using data from citizen science projects, including large camera-trapping surveys, to estimate local carnivore abundances. Using a Bayesian framework allows of estimation of population density, but it is also important to understand the factors that dictate their distribution across the year to inform conservation efforts.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Wang, Shaodong; Li, Qing] Iowa State Univ, Dept Ind & Mfg Syst Engn, 3004 Black Engn Bldg,2529 Union Dr, Ames, IA 50011 USA.
   [Olson, Lucas O.] Univ Wisconsin, 1630 Linden Dr, Madison, WI 53706 USA.
   [Krofel, Miha] Univ Ljubljana, Biotech Fac, Dept Forestry, Vecna Pot 83, Ljubljana 1000, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X; Li, Qing/0000-0002-3069-3878
FU Slovenian Research AgencySlovenian Research Agency - Slovenia [P4-0059]
FX We thank the Snapshot Serengeti team (www.snapshotserengeti.org) for
   generously providing the data for this study, and Ali Swanson
   specifically for comments on earlier drafts that greatly improved the
   manuscript. We thank the Illinois Natural History Survey, University of
   Illinois and Slovenian Research Agency (Grant P4-0059) for funding.
CR Allen ML, 2019, MAMMALIA, V83, P552, DOI 10.1515/mammalia-2017-0162
   Allen ML, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30988-4
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Anderson D.R., 2003, MODEL SELECTION MULT
   Athreya V, 2011, CONSERV BIOL, V25, P133, DOI 10.1111/j.1523-1739.2010.01599.x
   Balme G, 2019, POPUL ECOL, V61, P256, DOI 10.1002/1438-390X.1023
   Balme GA, 2017, BEHAV ECOL, V28, P1348, DOI 10.1093/beheco/arx098
   Balme GA, 2017, J ANIM ECOL, V86, P634, DOI 10.1111/1365-2656.12654
   Balme GA, 2009, BIOL CONSERV, V142, P2681, DOI 10.1016/j.biocon.2009.06.020
   BORNER M, 1987, OECOLOGIA, V73, P32, DOI 10.1007/BF00376974
   Broekhuis F, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0153875
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chapman S, 2010, S AFR J WILDL RES, V40, P114, DOI 10.3957/056.040.0202
   Chapron G, 2014, SCIENCE, V346, P1517, DOI 10.1126/science.1257553
   Dalerum F, 2008, BIODIVERS CONSERV, V17, P2939, DOI 10.1007/s10531-008-9406-4
   Datta A, 2008, BIOL CONSERV, V141, P1429, DOI 10.1016/j.biocon.2008.02.022
   Davis CL, 2018, ECOL LETT, V21, P1401, DOI 10.1111/ele.13124
   Devens C, 2018, AFR J ECOL, V56, P850, DOI 10.1111/aje.12512
   du Preez B, 2015, ANIM BEHAV, V100, P22, DOI 10.1016/j.anbehav.2014.10.025
   Durant SM, 2017, P NATL ACAD SCI USA, V114, P528, DOI 10.1073/pnas.1611122114
   Durant SM, 1998, J ANIM ECOL, V67, P370, DOI 10.1046/j.1365-2656.1998.00202.x
   Gray TNE, 2012, J WILDLIFE MANAGE, V76, P163, DOI 10.1002/jwmg.230
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Gelman A., 1992, STAT SCI, V7, P457, DOI [10.1214/ss/1177011136., 10.1214/ss/1177011136, DOI 10.1214/SS/1177011136]
   Gese EM, 2001, CONSERV BIOL SER, V5, P372
   Grey JNC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082832
   Hamilton P. H., 1981, LEOPARD PANTHERA PAR
   Harihar A, 2011, J APPL ECOL, V48, P806, DOI 10.1111/j.1365-2664.2011.01981.x
   Harmsen BJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179505
   Hayward MW, 2007, ORYX, V41, P205, DOI 10.1017/S0030605307001767
   Hayward MW, 2006, J ZOOL, V270, P298, DOI 10.1111/j.1469-7998.2006.00139.x
   Hayward MW, 2007, BIOL CONSERV, V139, P219, DOI 10.1016/j.biocon.2007.06.018
   Hopcraft JGC, 2005, J ANIM ECOL, V74, P559, DOI 10.1111/j.1365-2656.2005.00955.x
   Jacobson AP, 2016, PEERJ, V4, DOI 10.7717/peerj.1974
   Kissui BM, 2008, ANIM CONSERV, V11, P422, DOI 10.1111/j.1469-1795.2008.00199.x
   Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   KRUUK H., 1967, MAMMALIA, V31, P1, DOI 10.1515/mamm.1967.31.1.1
   Lamichhane BR, 2019, BIODIVERS CONSERV, V28, P1473, DOI 10.1007/s10531-019-01737-4
   Lassen KG, 2005, RETROVIROLOGY, V2, DOI 10.1186/1742-4690-2-S1-S74
   Marker LL, 2005, S AFR J WILDL RES, V35, P105
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   Mduma SAR, 1999, J ANIM ECOL, V68, P1101, DOI 10.1046/j.1365-2656.1999.00352.x
   Mizutani F, 1998, J ZOOL, V244, P269, DOI 10.1111/j.1469-7998.1998.tb00031.x
   Mkonyi FJ, 2018, AFR J ECOL, V56, P972, DOI 10.1111/aje.12528
   Mysterud A, 2010, J APPL ECOL, V47, P920, DOI 10.1111/j.1365-2664.2010.01836.x
   NORTON-GRIFFITHS M, 1975, East African Wildlife Journal, V13, P347, DOI 10.1111/j.1365-2028.1975.tb00144.x
   O'Brien TG, 2011, ECOL APPL, V21, P2908, DOI 10.1890/10-2284.1
   Packer C, 2011, CONSERV BIOL, V25, P142, DOI 10.1111/j.1523-1739.2010.01576.x
   Packer C, 2003, ECOL LETT, V6, P797, DOI 10.1046/j.1461-0248.2003.00500.x
   Packer C, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005941
   Power R. J., 2002, Koedoe, V45, P67
   Qi JZ, 2015, BIOL CONSERV, V191, P258, DOI 10.1016/j.biocon.2015.06.034
   R Core Team, 2018, R R PROJ STAT COMP
   Ray-Brambach RR, 2018, MAMM BIOL, V92, P102, DOI 10.1016/j.mambio.2017.11.002
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Rich LN, 2019, BIOL CONSERV, V233, P12, DOI 10.1016/j.biocon.2019.02.018
   Rich LN, 2014, J MAMMAL, V95, P382, DOI 10.1644/13-MAMM-A-126
   Rich LN, 2013, J WILDLIFE MANAGE, V77, P1280, DOI 10.1002/jwmg.562
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Rosenblatt E, 2016, ECOL EVOL, V6, P3772, DOI 10.1002/ece3.2155
   Rovero F., 2016, CAMERA TRAPPING WILD, P1
   Royle JA, 2014, SPATIAL CAPTURE-RECAPTURE, P1
   Schaller G. B., 1976, SERENGETI LION STUDY
   Sinclair ARE, 2007, CONSERV BIOL, V21, P580, DOI 10.1111/j.1523-1739.2007.00699.x
   Sinclair ARE, 1995, SERENGETI
   Stein AB, 2020, IUCN RED LIST THREAT
   Stein AB, 2015, AFR J WILDL RES, V45, P247, DOI 10.3957/056.045.0247
   Steinmetz R, 2013, BIOL CONSERV, V163, P68, DOI 10.1016/j.biocon.2012.12.016
   Strampelli P, 2020, ORYX, V54, P405, DOI 10.1017/S0030605318000121
   Strampelli P, 2018, MAMM BIOL, V89, P14, DOI 10.1016/j.mambio.2017.12.003
   Swanepoel LH, 2015, WILDLIFE BIOL, V21, P263, DOI 10.2981/wlb.00108
   Swanson A, 2016, ECOL EVOL, V6, P8534, DOI 10.1002/ece3.2569
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swanson A, 2014, J ANIM ECOL, V83, P1418, DOI 10.1111/1365-2656.12231
   Vanak AT, 2013, ECOLOGY, V94, P2619, DOI 10.1890/13-0217.1
   Veldhuis MP, 2019, SCIENCE, V363, P1424, DOI 10.1126/science.aav0564
   Zeileis A, 2008, J STAT SOFTW, V27, P1, DOI 10.18637/jss.v027.i08
NR 79
TC 3
Z9 3
U1 4
U2 19
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0960-3115
EI 1572-9710
J9 BIODIVERS CONSERV
JI Biodivers. Conserv.
PD NOV
PY 2020
VL 29
IS 13
BP 3591
EP 3608
DI 10.1007/s10531-020-02039-w
EA AUG 2020
PG 18
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA OC3UE
UT WOS:000561289700001
OA Green Published
DA 2022-02-10
ER

PT J
AU Richardson, ML
AF Richardson, Matthew L.
TI DAILY AND MONTHLY ACTIVITY OF BROWN BEARS (URSUS ARCTOS) NEAR A PROPOSED
   INDUSTRIAL PROJECT IN COASTAL BRITISH COLUMBIA
SO WESTERN NORTH AMERICAN NATURALIST
LA English
DT Article
ID ACTIVITY PATTERNS; GRIZZLY BEARS; SALMON; HABITUATION; BEHAVIOR; HUMANS;
   BLACK
AB The Kitimat Liquefied Natural Gas (KLNG) Plant is proposed for construction adjacent to Bish Creek (Kitimat, British Columbia, Canada). Bish Creek is a corridor for brown bears (Ursus arctos), and 8 camera traps were deployed along the creek for 1442 trapping days in 2014 to determine baseline activity of brown bears. Brown bear activity varied across weeks, peaking particularly in July and September. Within a 24-h day, bears were commonly photographed during hours 5, 6, and 21 and uncommonly photographed during the 3 hours preceding noon and a 4-h period in the afternoon. However, the time of day that bears were photographed varied across seasons; bears were more commonly photographed during the day in July and at night in September. Understanding this change in activity across seasons will inform management of bear resources and human activities on-site to avoid human-bear interactions.
C1 [Richardson, Matthew L.] Smithsonian Conservat Biol Inst, Ctr Conservat & Sustainabil, Washington, DC 20560 USA.
   [Richardson, Matthew L.] Univ Dist Columbia, Urban Sustainabil & Environm Sci, Coll Agr, Washington, DC 20008 USA.
RP Richardson, ML (corresponding author), Smithsonian Conservat Biol Inst, Ctr Conservat & Sustainabil, Washington, DC 20560 USA.
EM matthew.richardson@udc.edu
FU Kitimat LNG Operating General Partnership; Smithsonian
   InstitutionSmithsonian Institution
FX I thank Jonquil Crosby, Mike Stekelenburg, and Al Hummel for field
   assistance, Danny Aiuto for database management, Sulema Castro for
   logistical support, Rob Heibein, Sheryl Maruca, and Maria Hartley for
   constructive discussions and site access, Ashley Myers for help creating
   Figure 1, and Alfonso Alonso and Francisco Dallmeier for help
   administering the research program. Funding was provided by the Kitimat
   LNG Operating General Partnership through an independent research
   agreement with the Smithsonian Institution.
CR Fortin JK, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0141983
   Fortin JK, 2013, J MAMMAL, V94, P833, DOI 10.1644/12-MAMM-A-238.1
   GARD R, 1971, J WILDLIFE MANAGE, V35, P193, DOI 10.2307/3799591
   Gende SM, 2001, OECOLOGIA, V127, P372, DOI 10.1007/s004420000590
   HERRERO S., 2002, BEAR ATTACKS THEIR C
   JOPE KL, 1985, WILDLIFE SOC B, V13, P32
   Kaczensky P, 2006, J ZOOL, V269, P474, DOI 10.1111/j.1469-7998.2006.00114.x
   Klinka DR, 2002, CAN J ZOOL, V80, P1317, DOI 10.1139/Z02-123
   Machutchon AG, 1998, URSUS-SERIES, V10, P539
   MCLELLAN BN, 1988, J APPL ECOL, V25, P451, DOI 10.2307/2403836
   McLellan ML, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0117734
   Munro RHM, 2006, J MAMMAL, V87, P1112, DOI 10.1644/05-MAMM-A-410R3.1
   Nevin OT, 2005, BIOL CONSERV, V121, P611, DOI 10.1016/j.biocon.2004.06.011
   Olson TL, 1998, URSUS-SERIES, V10, P547
   Ordiz A, 2013, J APPL ECOL, V50, P306, DOI 10.1111/1365-2664.12047
   SAS Institute, 2008, SAS STAT US GUID PER
   Schwartz CC, 2010, J WILDLIFE MANAGE, V74, P1628, DOI 10.2193/2009-571
   Smith TS, 2005, URSUS, V16, P1, DOI 10.2192/1537-6176(2005)016[0001:ABBHAH]2.0.CO;2
   Wheat RE, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1408
NR 19
TC 2
Z9 2
U1 1
U2 9
PU BRIGHAM YOUNG UNIV
PI PROVO
PA 290 LIFE SCIENCE MUSEUM, PROVO, UT 84602 USA
SN 1527-0904
EI 1944-8341
J9 WEST N AM NATURALIST
JI West. North Am. Naturalist
PD MAR
PY 2017
VL 77
IS 1
BP 118
EP 123
DI 10.3398/064.077.0113
PG 6
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA EX9KV
UT WOS:000403577800012
OA Green Published
DA 2022-02-10
ER

PT C
AU Schneider, S
   Taylor, GW
   Kremer, SC
AF Schneider, Stefan
   Taylor, Graham W.
   Kremer, Stefan C.
GP IEEE
TI Similarity Learning Networks for Animal Individual Re-Identification -
   Beyond the Capabilities of a Human Observer
SO 2020 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION WORKSHOPS
   (WACVW)
SE IEEE Winter Conference on Applications of Computer Vision Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CY MAR 01-05, 2020
CL Snowmass, CO
SP IEEE, IEEE Comp Soc, CVF
AB Deep learning has become the standard methodology to approach computer vision tasks when large amounts of labeled data are available. One area where traditional deep learning approaches fail to perform is one-shot learning tasks where a model must correctly classify a new category after seeing only one example. One such domain is animal re-identification, an application of computer vision which can be used globally as a method to automate species population estimates from camera trap images. Our work demonstrates both the application of similarity comparison networks to animal re-identification, as well as the capabilities of deep convolutional neural networks to generalize across domains. Few studies have considered animal reidentification methods across species. Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean average precision (mAP)@1 and mAP@5. We consider five data sets corresponding to five different species: humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart for all species. Without any species-specific modifications, our results demonstrate that similarity comparison networks can reach a performance level beyond that of humans for the task of animal re-identification. The ability for researchers to re-identify an animal individual upon re-encounter is fundamental for addressing a broad range of questions in the study of population dynamics and community/behavioural ecology. Our expectation is that similarity comparison networks are the beginning of a major trend that could stand to revolutionize animal reidentification from camera trap data.
C1 [Schneider, Stefan; Kremer, Stefan C.] Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
   [Taylor, Graham W.] Univ Guelph, Sch Engn, Vector Inst Artificial Intelligence, Guelph, ON, Canada.
RP Schneider, S (corresponding author), Univ Guelph, Sch Comp Sci, Guelph, ON, Canada.
EM sschne01@uoguelph.ca; gwtaylor@uoguelph.ca; skremer@uoguelph.ca
OI Kremer, Stefan C./0000-0002-3667-4379
CR Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   BOUMA S, 2018, INT CONF IMAG VIS, pNI317
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Carter SJB, 2014, J EXP MAR BIOL ECOL, V452, P105, DOI 10.1016/j.jembe.2013.12.010
   Deb D., 2018, ARXIV180408790
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hermans A., 2017, ARXIV ARXIV170307737
   Hiby L., 2009, BIOL LETT, prsbl
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Krebs C. J., 1989, TECH REP
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2006, 2006 IEEE COMPUTER S, V2, P1735
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li S., 2019, ARXIV190605586
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Martinel N, 2015, IEEE T PATTERN ANAL, V37, P1656, DOI 10.1109/TPAMI.2014.2377748
   Meek P.D., 2013, Wildlife Biology in Practice, V9, P7
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Norouzzadeh M. S., 2017, ARXIV170305830V5
   Parham J, 2018, IEEE WINT CONF APPL, P1075, DOI 10.1109/WACV.2018.00123
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schneider J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0205043
   Schneider S., C COMP ROB VIS
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
NR 36
TC 9
Z9 9
U1 1
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2572-4398
BN 978-1-7281-7162-3
J9 IEEE WINT CONF APPL
PY 2020
BP 44
EP 52
PG 9
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ3RU
UT WOS:000587895300007
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Gonzalez, LF
   Montes, GA
   Puig, E
   Johnson, S
   Mengersen, K
   Gaston, KJ
AF Gonzalez, Luis F.
   Montes, Glen A.
   Puig, Eduard
   Johnson, Sandra
   Mengersen, Kerrie
   Gaston, Kevin J.
TI Unmanned Aerial Vehicles (UAVs) and Artificial Intelligence
   Revolutionizing Wildlife Monitoring and Conservation
SO SENSORS
LA English
DT Article
DE Unmanned Aerial Vehicle (UAV); wildlife monitoring; artificial
   intelligence; thermal imaging; robotics; conservation; automatic
   classification; koala; deer; wild pigs; dingo; conservation
AB Surveying threatened and invasive species to obtain accurate population estimates is an important but challenging task that requires a considerable investment in time and resources. Estimates using existing ground-based monitoring techniques, such as camera traps and surveys performed on foot, are known to be resource intensive, potentially inaccurate and imprecise, and difficult to validate. Recent developments in unmanned aerial vehicles (UAV), artificial intelligence and miniaturized thermal imaging systems represent a new opportunity for wildlife experts to inexpensively survey relatively large areas. The system presented in this paper includes thermal image acquisition as well as a video processing pipeline to perform object detection, classification and tracking of wildlife in forest or open areas. The system is tested on thermal video data from ground based and test flight footage, and is found to be able to detect all the target wildlife located in the surveyed area. The system is flexible in that the user can readily define the types of objects to classify and the object characteristics that should be considered during classification.
C1 [Gonzalez, Luis F.; Montes, Glen A.; Puig, Eduard] Queensland Univ Technol, ARCAA, 2 George St, Brisbane, Qld 4000, Australia.
   [Johnson, Sandra; Mengersen, Kerrie] Queensland Univ Technol, ARC Ctr Excellence Math & Stat Frontiers ACEMS, 2 George St, Brisbane, Qld 4000, Australia.
   [Gaston, Kevin J.] Univ Exeter, Environm & Sustainabil Inst, Penryn TR10 9EZ, Cornwall, England.
RP Gonzalez, LF (corresponding author), Queensland Univ Technol, ARCAA, 2 George St, Brisbane, Qld 4000, Australia.
EM felipe.gonzalez@qut.edu.au; glen.montes@mail.escuelaing.edu.co;
   eduard.puiggarcia@qut.edu.au; sandra.johnson@qut.edu.au;
   k.mengersen@qut.edu.au; k.j.gaston@exeter.ac.uk
RI Gonzalez, Felipe/G-7661-2011
OI Gonzalez, Felipe/0000-0002-4342-3682; Mengersen,
   Kerrie/0000-0001-8625-9168; Gaston, Kevin J./0000-0002-7235-7928
CR Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150
   [Anonymous], IM WATCH WILDL VID F
   [Anonymous], KANG BOX INFR VID FI
   Bevan Elizabeth, 2015, Marine Turtle Newsletter, P19
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chabot D, 2012, WATERBIRDS, V35, P170, DOI 10.1675/063.035.0119
   Christiansen P, 2014, SENSORS-BASEL, V14, P13778, DOI 10.3390/s140813778
   Cork Lennon, 2007, IEEE Aerospace and Electronic Systems Magazine, V22, P29, DOI 10.1109/MAES.2007.4408524
   Cristescu R. H., 2015, SCI REP, V5, P1
   Ditmer MA, 2015, CURR BIOL, V25, P2278, DOI 10.1016/j.cub.2015.07.024
   DJI, S800 EVO
   Dos Santos Gilberto Antonio Marcon, 2014, 2014 IEEE 11th International Conference on Mobile Ad-Hoc and Sensor Systems (MASS), P761, DOI 10.1109/MASS.2014.48
   Gaston KJ, 2009, J APPL ECOL, V46, P1, DOI 10.1111/j.1365-2664.2008.01596.x
   Hodgson A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079556
   Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
   Korner F., 2010, P 2010 IEEE RSJ INT
   Leonardo M., 2013, P 2013 ASME IEEE INT
   Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
   Mulero-Pazmany M., 2015, ECOL EVOL
   Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
   Murray JV, 2008, BIOL CONSERV, V141, P7, DOI 10.1016/j.biocon.2007.07.020
   Schaub M, 2007, CONSERV BIOL, V21, P945, DOI 10.1111/j.1523-1739.2007.00743.x
   Senate Environment and Communications References Committee, COMPL INQ 2010 2013
   Shumway N, 2015, ENVIRON SCI POLICY, V54, P297, DOI 10.1016/j.envsci.2015.07.024
   Soriano P., 2009, INT S ROBOT, V40, P239
   van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
   Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
   Vincent JB, 2015, FRONT ECOL ENVIRON, V13, P74, DOI 10.1890/15.WB.002
   Williams B.P., 2014, P 14 AIAA AV TECHN I
   Witmer GW, 2005, WILDLIFE RES, V32, P259, DOI 10.1071/WR04003
NR 31
TC 181
Z9 191
U1 8
U2 174
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JAN
PY 2016
VL 16
IS 1
AR 97
DI 10.3390/s16010097
PG 18
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA DE5OC
UT WOS:000370679800078
PM 26784196
OA Green Published, Green Submitted, gold
DA 2022-02-10
ER

PT J
AU Zitzmann, F
   Reich, M
   Schaarschmidt, F
AF Zitzmann, Felix
   Reich, Michael
   Schaarschmidt, Frank
TI Potential of small-scale and structurally diverse short-rotation coppice
   as habitat for large and medium-sized mammals
SO BIOLOGIA
LA English
DT Article
DE Perennial woody biomass crops; Bioenergy; Biodiversity; Wildlife; Game;
   Camera trapping
AB We surveyed occurrence and activity of large and medium-sized mammals on three experimental short-rotation coppice (SRC) and three afforestations by camera trapping. Both habitat types were surveyed simultaneously in spring. Additional wintertime surveys were performed on the SRC to consider seasonal aspects of habitat utilisation. In spring, SRC and afforestations were predominantly used by the same species. European hare (Lepus europaeus) and roe deer (Capreolus capreolus) were the most active species across all sites. Additionally, the European rabbit (Oryctolagus cuniculus) showed intense activity on one SRC site. Activity of carnivorous and omnivorous species was comparatively low in both habitat types, but even lower on the SRC. The only forest-associated species (European badger Meles meles), detected on all afforestations, was absent from the SRC. In winter, the surveyed SRC were used by the same species as in spring. Most species showed similar activity on the SRC in both seasons. We conclude that small-scale and structurally diverse SRC provide suitable habitat, in different seasons, especially for herbivorous mammals associated with farmland and forest-ecotones rather than forest species. The extent to which our results can be generalised to large-scale commercial SRC is unclear. However, the results indicate that SRC can be managed in a manner compatible with wildlife and may then have a habitat function for mammals comparable to that of young afforestations. Creation of within-plantation heterogeneity can be a suitable measure to improve habitat quality and should, therefore, be considered in the design and management of SRC.
C1 [Zitzmann, Felix; Reich, Michael] Leibniz Univ Hannover, Inst Environm Planning, Herrenhauser Str 2, D-30419 Hannover, Germany.
   [Schaarschmidt, Frank] Leibniz Univ Hannover, Inst Cell Biol & Biophys, Biostat Dept, Herrenhauser Str 2, D-30419 Hannover, Germany.
RP Zitzmann, F (corresponding author), Leibniz Univ Hannover, Inst Environm Planning, Herrenhauser Str 2, D-30419 Hannover, Germany.
EM zitzmann@umwelt.uni-hannover.de
OI Zitzmann, Felix/0000-0002-7732-1860; Reich, Michael/0000-0001-5401-1993
FU Lower Saxony Ministry of Food, Agriculture and Consumer Protection
   (Niedersachsisches Ministerium fur Ernahrung, Landwirtschaft und
   Verbraucherschutz) [105.2-3234/1-13-4]
FX Open Access funding enabled and organized by Projekt DEAL. This work was
   supported by the Lower Saxony Ministry of Food, Agriculture and Consumer
   Protection (Niedersachsisches Ministerium fur Ernahrung, Landwirtschaft
   und Verbraucherschutz, ML), Grant No. 105.2-3234/1-13-4.
CR Armenteros JA, 2021, INTEGR ZOOL, V16, P226, DOI 10.1111/1749-4877.12496
   Baum S, 2009, LANDBAUFORSCH VOLK, V59, P163
   Bergstrom R, 2002, BIOMASS BIOENERG, V23, P27, DOI 10.1016/S0961-9534(02)00027-2
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Campbell SP, 2012, BIOMASS BIOENERG, V47, P342, DOI 10.1016/j.biombioe.2012.09.026
   CHRISTIAN DP, 1994, BIOMASS BIOENERG, V6, P31, DOI 10.1016/0961-9534(94)90082-5
   Christian DP, 1997, BIOMASS BIOENERG, V12, P35, DOI 10.1016/S0961-9534(96)00062-1
   Christian DP, 1998, BIOMASS BIOENERG, V14, P395, DOI 10.1016/S0961-9534(97)10076-9
   Christian DP, 1997, J WILDLIFE MANAGE, V61, P171, DOI 10.2307/3802426
   Dauber J, 2010, GCB BIOENERGY, V2, P289, DOI 10.1111/j.1757-1707.2010.01058.x
   Desiree JI, 2014, GCB BIOENERGY, V6, P183, DOI 10.1111/gcbb.12067
   Dimitriou I., 2015, SUSTAINABLE SHORT RO
   Don A, 2012, GCB BIOENERGY, V4, P372, DOI 10.1111/j.1757-1707.2011.01116.x
   Eggers J, 2009, GCB BIOENERGY, V1, P18, DOI 10.1111/j.1757-1707.2009.01002.x
   Englund O, 2020, WIRES ENERGY ENVIRON, V9, DOI 10.1002/wene.375
   Everaars J, 2014, GCB BIOENERGY, V6, P252, DOI 10.1111/gcbb.12135
   Fletcher RJ, 2011, FRONT ECOL ENVIRON, V9, P161, DOI 10.1890/090091
   Gepp N, 2015, NATURSCHUTZ LANDSCHA, V48, P287
   Giordano M, 2009, HYSTRIX, V20, P127
   GORANSSON G, 1994, BIOMASS BIOENERG, V6, P49, DOI 10.1016/0961-9534(94)90084-1
   Gruss Holger, 2011, Naturschutz und Landschaftsplanung, V43, P197
   GUSTAFSSON L, 1987, FOREST ECOL MANAG, V21, P141, DOI 10.1016/0378-1127(87)90078-8
   Halekoh U, 2006, J STAT SOFTW, V15, P1, DOI 10.18637/jss.v015.i02
   Hanowski JM, 1997, CONSERV BIOL, V11, P936, DOI 10.1046/j.1523-1739.1997.96173.x
   Hilbe J., 2013, GEN ESTIMATING EQUAT, Vsecond
   Huston MA, 2003, J ENVIRON MANAGE, V67, P77, DOI 10.1016/S0301-4797(02)00190-1
   Keuling O, 2011, EUR J WILDLIFE RES, V57, P95, DOI 10.1007/s10344-010-0403-z
   Meehan TD, 2010, P NATL ACAD SCI USA, V107, P18533, DOI 10.1073/pnas.1008475107
   Moser BW, 2002, NORTHWEST SCI, V76, P158
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Petrovan SO, 2017, EUR J WILDLIFE RES, V63, DOI 10.1007/s10344-017-1106-5
   R Core Team, 2019, R VERS 361 R LANG EN
   Robertson GP, 2008, SCIENCE, V322, P49, DOI 10.1126/science.1161525
   Rowe RL, 2011, BIOMASS BIOENERG, V35, P325, DOI 10.1016/j.biombioe.2010.08.046
   Rowe RL, 2009, RENEW SUST ENERG REV, V13, P271, DOI 10.1016/j.rser.2007.07.008
   SAGE RB, 1994, BIOMASS BIOENERG, V6, P41, DOI 10.1016/0961-9534(94)90083-3
   Sage RB, 1998, BIOMASS BIOENERG, V15, P39, DOI 10.1016/S0961-9534(97)10055-1
   Sage R, 2006, IBIS, V148, P184, DOI 10.1111/j.1474-919X.2006.00522.x
   Sauerbrei R, 2014, GCB BIOENERGY, V6, P265, DOI 10.1111/gcbb.12146
   Schulz U., 2008, COTTBUSER SCHRIFTEN, V6, P167
   Schulz U, 2009, LANDBAUFORSCH VOLK, V59, P171
   Smith RK, 2004, J APPL ECOL, V41, P1092, DOI 10.1111/j.0021-8901.2004.00976.x
   Taylor G., 2016, PERENNIAL BIOMASS CR, V319, P3, DOI [10.1007/978-3-319-44530-4_1, DOI 10.1007/978-3-319-44530-4_1]
   Vanbeveren SPP, 2019, RENEW SUST ENERG REV, V111, P34, DOI 10.1016/j.rser.2019.05.012
   Vaughan N, 2003, J APPL ECOL, V40, P163, DOI 10.1046/j.1365-2664.2003.00784.x
   Weih M, 2003, BASIC APPL ECOL, V4, P149, DOI 10.1078/1439-1791-00157
NR 46
TC 3
Z9 3
U1 3
U2 4
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0006-3088
EI 1336-9563
J9 BIOLOGIA
JI Biologia
PD AUG
PY 2021
VL 76
IS 8
BP 2195
EP 2206
DI 10.1007/s11756-021-00686-0
EA MAR 2021
PG 12
WC Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Life Sciences & Biomedicine - Other Topics
GA UC8NN
UT WOS:000623914400002
OA hybrid
DA 2022-02-10
ER

PT J
AU Gonzalez-Gallina, A
   Perez-Garduza, F
   Iglesias-Hernandez, JA
   Oliveras-De Ita, A
   Vazquez-Zuniga, O
   Chacon-Hernandez, A
   Hidalgo-Mihart, MG
AF Gonzalez-Gallina, Alberto
   Perez-Garduza, Freddy
   Iglesias-Hernandez, Jesus A.
   Oliveras-De Ita, Adan
   Vazquez-Zuniga, Octavio
   Chacon-Hernandez, Andres
   Hidalgo-Mihart, Mircea G.
TI A Novel Item, Black Vultures (Coragyps atratus) Used as Food by a Jaguar
   (Panthera onca) in Quintana Roo, Mexico
SO AMERICAN MIDLAND NATURALIST
LA English
DT Article
ID PUMAS; PATTERNS; SITES; DIET
AB Using a GPS satellite collar, we tracked a jaguar close to Playa del Carmen city in Quintana Roo, Mexico from Jan. to Dec. 2013. We observed the jaguar recurrently used a cenote located near the Playa del Carmen city landfill. We searched on two occasions for potential prey items killed by the jaguar and also set two camera traps on the cenote area. We found the carcasses of two black vultures probably eaten by the jaguar, and we also obtained photographic evidence of the jaguar with a black vulture in its mouth. The photo, along with other evidence, reveals the potential significance of vultures as prey for this endangered species in areas where, due to subsistence hunting and urban expansion, jaguar's usual prey species have low abundances.
C1 [Gonzalez-Gallina, Alberto] Inst Ecol AC Red Ambiente & Sustentabilidad, Xalapa 91070, Veracruz, Mexico.
   [Perez-Garduza, Freddy; Hidalgo-Mihart, Mircea G.] Univ Juarez Autonoma Tabasco, Div Acad Ciencias Biol, Km 0-5 Carretera Villahermosa Cardenas, Villahermosa 86039, Tabasco, Mexico.
   [Iglesias-Hernandez, Jesus A.; Oliveras-De Ita, Adan; Vazquez-Zuniga, Octavio; Chacon-Hernandez, Andres] SA DE CV, SEGA, Del Benito Juarez 03230, DF, Mexico.
RP Hidalgo-Mihart, MG (corresponding author), Univ Juarez Autonoma Tabasco, Div Acad Ciencias Biol, Km 0-5 Carretera Villahermosa Cardenas, Villahermosa 86039, Tabasco, Mexico.
EM mhidalgo@yahoo.com
OI Hidalgo Mihart, Mircea G/0000-0002-8779-6886
FU Ingenieros Civiles Asociados; CONACYTConsejo Nacional de Ciencia y
   Tecnologia (CONACyT) [335814/232663]
FX This paper was possible thanks to the financial support granted to
   Sistemas Estrategicos para la Gestion Ambiental SEGA S. A. de C.V. by
   Ingenieros Civiles Asociados from their Infraestructure division (ICAi)
   through Consorcio del Mayab, within the wildlife monitoring for the
   highway Project entitled "Ramales Cedral-Tintal,Tintal-Playa del Carmen
   con una longitud de 54 km en el estado de Quintana Roo, Mexico.'' The
   Division Academica de Ciencias Biologicas de la Universidad Juarez
   Autonoma de Tabasco (DACBiol UJAT), granted logistical support for the
   success of this Project. Capture, management and collaring of the
   jaguars was under the capture permit code SGPA/DGVS/9611/12 Oct. 15th of
   2012 granted to Mircea Gabriel Hidalgo Mihart on behalf of the Direccion
   General de Vida Silvestre-SEMARNAT-Mexico. We thank to Erica Strand for
   revising this manuscript. We thank CONACYT for graduate studies
   scholarship number 335814/232663 awarded to A. Gonzalez Gallina who is
   studying at the Instituto de Ecologia A. C. (INECOL).
CR BEN-SHAHAR R., 1999, TRAIL WILD ENCOUNTER
   Campbell M. ON., 2015, VULTURES THEIR EVOLU
   CAMPOS-CAMARA B.L., 2007, PROCESOS URBANIZACIO
   Carrillo E, 2009, J TROP ECOL, V25, P563, DOI 10.1017/S0266467409990137
   CASO A., 2008, IUCN RED LIST THREAT, V2014
   Cassaigne I, 2016, SOUTHWEST NAT, V61, P125
   Cavalcanti SMC, 2010, J MAMMAL, V91, P722, DOI 10.1644/09-MAMM-A-171.1
   COMISION NACIONAL DE AREAS NATURALES PROTEGIDAS, 2009, PROGR ACC CONS ESP P
   CRAWSHAW PG, 1991, J ZOOL, V223, P357, DOI 10.1111/j.1469-7998.1991.tb04770.x
   CRUZ E. G, 2011, CONSERVACI ESTUDIO J, P81
   Da Silveira R, 2010, J HERPETOL, V44, P418, DOI 10.1670/08-340.1
   De Oliveira T. G., 2002, JAGUAR NUEVO MILENIO, P265
   Dupuy Rada Juan Manuel, 2007, Invest. Geog, P104
   ELBROCH M., 2001, BIRD TRACKS SIGN GUI
   ELLIS E. A., 2015, EVALUATION MAPEO DET
   Ferguson-Lees J., 2006, RAPTORS WORLD
   Foster RJ, 2010, J ZOOL, V280, P309, DOI 10.1111/j.1469-7998.2009.00663.x
   Gese EM, 2016, WILDLIFE RES, V43, P130, DOI 10.1071/WR15196
   Gonzalez CAL, 2002, MAMMALIA, V66, P603
   GONZALEZ-GALLINA A., 2017, HOMERANGE MALE JAGUA
   GRUBE G. E., 1953, WILSON B, V65, P119
   HERNANDEZ-SAN MARTIN A.D, 2015, NAT AREA J, V35, P308
   Howell S.N.G., 1995, GUIDE BIRDS MEXICO N
   HOWES PAUL GRISWOLD, 1926, BIRD LORE, V28, P175
   INIGO E., 1999, BIODIVERSITAS CONABI, V22, P1
   INSTITLTO NACIONAL DE ESTADISTICA Y GEOGRAFIA, 2013, AN EST QUINT ROO 201
   Jackson J.A., 1983, P245
   Logan KA, 1999, WILDLIFE SOC B, V27, P201
   MACKINNON R., 2005, AVES RESERVAS PEN IN
   Newsome TM, 2015, GLOBAL ECOL BIOGEOGR, V24, P1, DOI 10.1111/geb.12236
   Novaes WG, 2013, ZOOLOGIA-CURITIBA, V30, P607, DOI 10.1590/S1984-46702013005000014
   Polisar J, 2003, BIOL CONSERV, V109, P297, DOI 10.1016/S0006-3207(02)00157-X
   REMOLINA-SUAREZ J. F., 2014, COMISION NACL AREAS
   Rodriguez-Soto C, 2011, DIVERS DISTRIB, V17, P350, DOI 10.1111/j.1472-4642.2010.00740.x
   Salom-Perez R, 2007, ORYX, V41, P51, DOI 10.1017/S0030605307001615
   Santos-Fita D, 2012, J ETHNOBIOL ETHNOMED, V8, DOI 10.1186/1746-4269-8-38
   Scott W. E. D., 1892, AUK, V9, P120, DOI [10.2307/4067933, DOI 10.2307/4067933]
   SECRETARIA DEL MEDIO AMBIENTE Y RECURSOS NATURALES, 2010, NORM OFIC MEX NOM 05
   Sikes RS, 2011, J MAMMAL, V92, P235, DOI 10.1644/10-MAMM-F-355.1
NR 39
TC 0
Z9 0
U1 1
U2 27
PU AMER MIDLAND NATURALIST
PI NOTRE DAME
PA UNIV NOTRE DAME, BOX 369, ROOM 295 GLSC, NOTRE DAME, IN 46556 USA
SN 0003-0031
EI 1938-4238
J9 AM MIDL NAT
JI Am. Midl. Nat.
PD JUL
PY 2017
VL 178
IS 1
BP 158
EP 164
DI 10.1674/0003-0031-178.1.158
PG 7
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA FA6RB
UT WOS:000405570500014
DA 2022-02-10
ER

PT J
AU Szenicer, A
   Reinwald, M
   Moseley, B
   Nissen-Meyer, T
   Muteti, ZM
   Oduor, S
   McDermott-Roberts, A
   Baydin, AG
   Mortimer, B
AF Szenicer, Alexandre
   Reinwald, Michael
   Moseley, Ben
   Nissen-Meyer, Tarje
   Muteti, Zachary Mutinda
   Oduor, Sandy
   McDermott-Roberts, Alex
   Baydin, Atilim G.
   Mortimer, Beth
TI Seismic savanna: machine learning for classifying wildlife and
   behaviours using ground-based vibration field recordings
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article; Early Access
DE African elephant; machine learning; seismic waves; wildlife monitoring
ID ELEPHANTS; AUGMENTATION; DISCRIMINATION
AB We develop a machine learning approach to detect and discriminate elephants from other species, and to recognise important behaviours such as running and rumbling, based only on seismic data generated by the animals. We demonstrate our approach using data acquired in the Kenyan savanna, consisting of 8000 h seismic recordings and 250 k camera trap pictures. Our classifiers, different convolutional neural networks trained on seismograms and spectrograms, achieved 80%-90% balanced accuracy in detecting elephants up to 100 m away, and over 90% balanced accuracy in recognising running and rumbling behaviours from the seismic data. We release the dataset used in this study: SeisSavanna represents a unique collection of seismic signals with the associated wildlife species and behaviour. Our results suggest that seismic data offer substantial benefits for monitoring wildlife, and we propose to further develop our methods using dense arrays that could result in a seismic shift for wildlife monitoring.
C1 [Szenicer, Alexandre; Nissen-Meyer, Tarje] Univ Oxford, Dept Earth Sci, South Parks Rd, Oxford OX1 3AN, England.
   [Reinwald, Michael; McDermott-Roberts, Alex; Mortimer, Beth] Univ Oxford, Dept Zool, Oxford, England.
   [Moseley, Ben; Baydin, Atilim G.] Univ Oxford, Dept Comp Sci, Oxford, England.
   [Muteti, Zachary Mutinda; Oduor, Sandy] Mpala Res Ctr, Nanyuki, Kenya.
   [Baydin, Atilim G.] Univ Oxford, Dept Engn Sci, Oxford, England.
RP Nissen-Meyer, T (corresponding author), Univ Oxford, Dept Earth Sci, South Parks Rd, Oxford OX1 3AN, England.
EM meyer@earth.ox.ac.uk
RI Szenicer, Alexandre/N-9921-2016
OI Szenicer, Alexandre/0000-0002-4829-5739
FU National GeographicNational Geographic Society [NGS50019R-18]; Royal
   SocietyRoyal Society of LondonEuropean Commission [URF R1 191033]; John
   Fell Oxford University Press Research Fund; Royal Commission for the
   Exhibition of 1851; Centre for Doctoral Training in Autonomous
   Intelligent Machines and Systems at the University of Oxford, Oxford,
   UK; UK Engineering and Physical Sciences Research CouncilUK Research &
   Innovation (UKRI)Engineering & Physical Sciences Research Council
   (EPSRC); EPSRC/MURI grantUK Research & Innovation (UKRI)Engineering &
   Physical Sciences Research Council (EPSRC) [EP/N019474/1]; Lawrence
   Berkeley National Lab
FX We would like to express our gratitude to all the staff at the Mpala
   Research Centre for assisting with fieldwork operations and for creating
   a warm and welcoming environment, in particular Dino Martins and Cosmas
   Nzomo. We thank Frank Pope and staff at Save the Elephants for help
   getting research permits and Kenyan Wildlife Service affiliations, and
   Paula Koelemeijer for help with sensor deployment. A. Szenicer thanks
   the anonymous donor of his PhD grant. We thank National Geographic
   (NGS50019R-18), Royal Society (URF R1 191033), John Fell Oxford
   University Press Research Fund, Royal Commission for the Exhibition of
   1851 for funding. This research has been supported by the Centre for
   Doctoral Training in Autonomous Intelligent Machines and Systems at the
   University of Oxford, Oxford, UK, and the UK Engineering and Physical
   Sciences Research Council. A. G. Baydin is supported by EPSRC/MURI grant
   EP/N019474/1 and by Lawrence Berkeley National Lab.
CR Barnosky AD, 2004, SCIENCE, V306, P70, DOI 10.1126/science.1101476
   Bellwood DR, 2004, NATURE, V429, P827, DOI 10.1038/nature02691
   Bianco MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50381-z
   Blake S, 2001, AFR J ECOL, V39, P178, DOI 10.1046/j.1365-2028.2001.00296.x
   Brown T., 2020, ARXIV 200514165, V33, P1877
   Ceballos G, 2020, P NATL ACAD SCI USA, V117, P13596, DOI 10.1073/pnas.1922686117
   Chase MJ, 2016, PEERJ, V4, DOI 10.7717/peerj.2354
   Clemente J, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP 2019), P417, DOI 10.1109/SMARTCOMP.2019.00081
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Douglas-Hamilton I., 1987, Oryx, V6, P11
   Duporge I, 2021, REMOTE SENS ECOL CON, V7, P369, DOI 10.1002/rse2.195
   DZIEWONSKI AM, 1977, J GEOPHYS RES, V82, P239, DOI 10.1029/JB082i002p00239
   Falcin A, 2021, J VOLCANOL GEOTH RES, V411, DOI 10.1016/j.jvolgeores.2020.107151
   Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013
   Galanti Valeria, 2000, Hystrix, V11, P27
   Gobush K., 2021, LOXODONTA AFRICANA I
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hosseini K, 2020, GEOPHYS J INT, V220, P96, DOI 10.1093/gji/ggz394
   Iandola F. N., 2017, ARXIV PREPRINT ARXIV
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   Lamb O.D., 2021, FRONTIERS CONSERVATI, V1, DOI [10.3389/fcosc.2020.630967, DOI 10.3389/FCOSC.2020.630967]
   Leonid TT, 2021, J AMB INTEL HUM COMP, V12, P5269, DOI 10.1007/s12652-020-02005-y
   Li ZF, 2018, GEOPHYS RES LETT, V45, P4773, DOI 10.1029/2018GL077870
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mangewa LJ, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11216116
   Meier MA, 2019, J GEOPHYS RES-SOL EA, V124, P788, DOI 10.1029/2018JB016661
   Mortimer B, 2018, CURR BIOL, V28, pR547, DOI 10.1016/j.cub.2018.03.062
   Moseley B, 2020, SOLID EARTH, V11, P1527, DOI 10.5194/se-11-1527-2020
   Mousavi SM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17591-w
   Mousavi SM, 2020, GEOPHYS RES LETT, V47, DOI 10.1029/2019GL085976
   Ngene SM, 2010, AFR J ECOL, V48, P386, DOI 10.1111/j.1365-2028.2009.01125.x
   Palanisamy K., 2020, ARXIV200711154
   Poole Joyce H., 2011, P125
   Randler C, 2018, ECOL EVOL, V8, P7151, DOI 10.1002/ece3.4240
   Reinwald M, 2021, J R SOC INTERFACE, V18, DOI 10.1098/rsif.2021.0264
   Ren L, 2008, J R SOC INTERFACE, V5, P195, DOI 10.1098/rsif.2007.1095
   REW R, 1990, IEEE COMPUT GRAPH, V10, P76, DOI 10.1109/38.56302
   Rost S, 2002, REV GEOPHYS, V40, DOI 10.1029/2000RG000100
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Scheele B, 2019, SCIENCE, V363, P1459, DOI 10.1126/science.aav0379
   Senior AW, 2020, NATURE, V577, P706, DOI 10.1038/s41586-019-1923-7
   Shamout FE, 2020, IEEE J BIOMED HEALTH, V24, P437, DOI 10.1109/JBHI.2019.2937803
   Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Smit J, 2019, ORYX, V53, P368, DOI 10.1017/S0030605317000345
   Springenberg J. T., 2014, ARXIV14126806
   Stephens M, 1988, ALV VIS C, P147, DOI [10.5244/C.2.23, DOI 10.5244/C.2.23]
   Sugumar SJ, 2013, CURR SCI INDIA, V104, P1515
   Sukumar R., 2003, LIVING ELEPHANTS EVO
   Szenicer A, 2020, GEOPHYS J INT, V223, P1247, DOI 10.1093/gji/ggaa349
   Szenicer A, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw6548
   Thouless C., 2016, OCCASIONAL PAPER SER
   Wasser SK, 2008, CONSERV BIOL, V22, P1065, DOI 10.1111/j.1523-1739.2008.01012.x
   Wood JD, 2005, J APPL ECOL, V42, P587, DOI 10.1111/j.1365-2664.2005.01044.x
   Zhu WQ, 2020, ADV GEOPHYS, V61, P151, DOI 10.1016/bs.agph.2020.07.003
NR 55
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
DI 10.1002/rse2.242
EA NOV 2021
PG 15
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA WU0OM
UT WOS:000716254000001
OA gold
DA 2022-02-10
ER

PT J
AU Overton, CT
   Casazza, ML
   Connelly, D
   Gardner, S
AF Overton, Cory T.
   Casazza, Michael L.
   Connelly, Daniel
   Gardner, Scott
TI Gambel's Quail Survey Variability and Implications for Survey Design in
   the Mojave Desert
SO WILDLIFE SOCIETY BULLETIN
LA English
DT Article
DE acoustic recording; Breeding Bird Survey; California; call-count;
   Callipepla gambelii; Gambel's quail; Mojave; statistical power; survey
ID INDEXES
AB Careful design of a wildlife population monitoring strategy is necessary to obtain accurate and precise results whether the purpose of the survey is development of habitat suitability models, to estimate abundance, or assess site occupancy. Important characteristics to consider in survey design are sources of elevated variability, particularly within-subject variability, which increases the amount of data needed to achieve statistical certainty either in terms of population trend analysis, hypothesis testing, or statistical power. However, alternative objectives, such as associating counts with habitat characteristics, may benefit from increased variation among counts when differences covary with habitat measures. This difference can result in competing needs when developing survey protocols. We investigated the relative precision of differing gamebird monitoring protocols to identify methods with the greatest statistical efficiency. We assessed call-count transects using standard Breeding Bird Survey protocols (Passive call-counts) and modified by including longer survey periods and call playback (Active call-counts), autonomous recording units with supervised call detection (ARU-recorded calls), camera traps, and roadside covey-counts for Gambel's quail (Callipepla gambelii) in the Mojave Desert (CA, USA) during the spring of 2016. Active call-counts had the lowest within-site variation relative to estimated population index values, but Passive call-count transects may be more efficient for some purposes because more survey stations can be completed within a single survey timeframe. The ARU-recorded calls may provide a suitable alternative despite larger sample size needs, especially for occupancy surveys because multiple units can be deployed concurrently. The ultimate sample size required will depend on specific study objectives and scope of interest, but camera traps and breeding-season covey counts are not likely to meet objectives in desert environments. (c) 2020 The Wildlife Society.
C1 [Overton, Cory T.; Casazza, Michael L.] US Geol Survey, Western Ecol Res Ctr, Dixon, CA 95620 USA.
   [Connelly, Daniel] Pheasants Forever, Granite Bay, CA 95746 USA.
   [Gardner, Scott] Calif Dept Fish & Wildlife, Wildlife Branch, Sacramento, CA 95811 USA.
RP Overton, CT (corresponding author), US Geol Survey, Western Ecol Res Ctr, Dixon, CA 95620 USA.
EM coverton@usgs.gov
OI Overton, Cory/0000-0002-5060-7447; casazza, Mike/0000-0002-5636-735X
FU State of California; Pheasants Forever; U.S. Geological SurveyUnited
   States Geological Survey
FX We thank N. Young, H. Pavisich, and A. Merritt for conducting surveys
   and M. Ricca, T. Bui, and L. Parker for administrative assistance during
   the project. We thank J. Weigand for assistance in developing the
   project scope and incorporating design and process features relating to
   the Bureau of Land Management Assessment, Inventory, and Monitoring
   assessment procedures. We are also indebted to T. La Doux and the
   University of California Natural Reserve System, Granite Mountains
   Desert Research Center, and T. Anderson and staff at the Sonny
   Bono-Salton Sea National Wildlife Refuge for housing and logistical
   support during field activities. This research was funded with support
   from the State of California, Pheasants Forever, and the U.S. Geological
   Survey. Research permits for field activities conducted within the
   Mojave National Preserve were provided by the National Park Service and
   we thank N. Darby for his assistance in this regard and in helping us to
   locate wildlife guzzlers. Special thanks to Dr. W. Thogmartin and C.
   Smith for their insightful reviews of this manuscript. Any use of trade,
   firm, or product names is for descriptive purposes only and does not
   imply endorsement by the U.S. Government.
CR Anderson DR, 2003, WILDLIFE SOC B, V31, P288
   Bioacoustics Research Program, 2014, RAV PRO INT SOUND AN
   Casazza ML, 2005, WILDLIFE SOC B, V33, P606, DOI 10.2193/0091-7648(2005)33[606:EOCPIF]2.0.CO;2
   CLARK WAV, 1976, GEOGR ANAL, V8, P428
   Delehanty DJ, 2004, WILDLIFE SOC B, V32, P588, DOI 10.2193/0091-7648(2004)32[588:FTFMQF]2.0.CO;2
   England A. S., 1993, BIRDS N AM, DOI [10.2173/bna.71, DOI 10.2173/BNA.71]
   Jakob C, 2010, EUR J WILDLIFE RES, V56, P907, DOI 10.1007/s10344-010-0388-7
   Johnson DH, 2008, J WILDLIFE MANAGE, V72, P857, DOI 10.2193/2007-294
   Koops HV, 2015, LECT NOTES COMPUT SC, V9283, P261, DOI 10.1007/978-3-319-24027-5_26
   Link WA, 1998, ECOL APPL, V8, P258, DOI 10.2307/2641065
   Lynch James F., 1995, U S Forest Service General Technical Report PSW, V149, P1
   Overton C. T., 2019, COMP GAMBELS QUAIL S, DOI [10.5066/P9SVPK0N, DOI 10.5066/P9SVPK0N]
   RAMSEY FL, 1997, STAT SLEUTH COURSE M
   Rollins D., 2009, GAMEBIRD 2006 QUAIL, P210
   Rollins D., 2005, COUNTING QUAIL B 617
   Rusk JP, 2007, J WILDLIFE MANAGE, V71, P1336, DOI 10.2193/2006-071
   Sauer J. R., 2017, N AM BREEDING BIRD S
   Thompson W., 2004, SAMPLING RARE ELUSIV
   Wagner F. H., 2009, ARID LAND ECOSYSTEMS, V2, P125
NR 19
TC 0
Z9 0
U1 1
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1938-5463
J9 WILDLIFE SOC B
JI Wildl. Soc. Bull.
PD SEP
PY 2020
VL 44
IS 3
BP 493
EP 501
DI 10.1002/wsb.1105
EA JUL 2020
PG 9
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA NV0LS
UT WOS:000544614900001
DA 2022-02-10
ER

PT J
AU Ishige, T
   Miya, M
   Ushio, M
   Sado, T
   Ushioda, M
   Maebashi, K
   Yonechi, R
   Lagan, P
   Matsubayashi, H
AF Ishige, Taichiro
   Miya, Masaki
   Ushio, Masayuki
   Sado, Tetsuya
   Ushioda, Masaharu
   Maebashi, Kaori
   Yonechi, Risako
   Lagan, Peter
   Matsubayashi, Hisashi
TI Tropical-forest mammals as detected by environmental DNA at natural
   saltlicks in Borneo
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Borneo; Endangered species; Environmental DNA; Natural saltlick; NGS
ID CLOUDED LEOPARD; NEOFELIS-NEBULOSA; LICKS; CONSERVATION; ORANGUTANS;
   PYGMAEUS
AB Although tropical forests are among the most species-rich ecosystems on earth, 42% of mammal species in tropical forests are endangered because of overhunting and/or unsustainable exploitation. Camera-trap surveys have shown that natural saltlicks can be used to determine mammalian fauna, especially medium to large endangered species in tropical forests; establishment of camera traps, however, is time and effort intensive. Furthermore, the photographic range and detectable size of species are often restricted. Environmental DNA (eDNA) metabarcoding is a powerful approach that might provide a better way to study terrestrial animals in tropical forests. In this study, we examined whether eDNA from natural saltlicks comprehensively represented species composition in a Bornean tropical forest. We collected 100-150-mL water samples from natural saltlicks in Sabah, Malaysian Borneo. We constructed amplicon libraries for MiSeq sequencing using eDNA extracted from the water samples. Six endangered species were detected using this method, including Bomean orangutan (Pongo pygtnaeus), Bomean banteng (Bos javanicus lowt), Asian elephant (Elephas maximus), Sunda pangolin (Manis javanica), sambar deer (Rosa unicolor) and bearded pig (Sus barbatus). However, most small and minor species were not detected, with low sequence identity (80-96%). Therefore, we propose that more species of tropical forest mammals should have their sequences deposited in DNA databases. This study is the first to report the endangered mammals of a tropical forest detected using eDNA from natural saltlicks.
C1 [Ishige, Taichiro] Tokyo Univ Agr, NODAI Genome Res Ctr, 1-1-1 Sakuragaoka, Tokyo 1568502, Japan.
   [Miya, Masaki; Sado, Tetsuya] Nat Hist Museum & Inst, Dept Ecol & Environm Sci, Chuo Ku, 955-2 Aoba Cho, Chiba 2608682, Japan.
   [Ushio, Masayuki] Kyoto Univ, Ctr Ecol Res, 2-509-3 Hirano, Otsu, Shiga 5202113, Japan.
   [Ushio, Masayuki] Japan Sci & Technol Agcy, PRESTO, 4-1-8 Honcho, Kawaguchi, Saitama 3320012, Japan.
   [Ushioda, Masaharu; Maebashi, Kaori; Yonechi, Risako; Matsubayashi, Hisashi] Tokyo Univ Agr, Fac Agr, 1737 Funako, Atsugi, Kanagawa 2430034, Japan.
   [Lagan, Peter] Sabah Forestry Dept, Locked Bag 68, Sandakan 90009, Sabah, Malaysia.
RP Matsubayashi, H (corresponding author), Tokyo Univ Agr, Fac Agr, 1737 Funako, Atsugi, Kanagawa 2430034, Japan.
EM hm155202@nodai.ac.jp
RI Miya, Masaki/O-2664-2019; Ushio, Masayuki/F-8667-2010
OI Ushio, Masayuki/0000-0003-4831-7181; Miya, Masaki/0000-0002-9791-9886
FU Japan Society for the Promotion of Science (JSPS) Core-to-Core
   ProgramMinistry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT)Japan Society for the Promotion of Science; Advanced
   Research Networks (Wildlife Research Centre of Kyoto University);
   Ministry of Education, Culture, Sports, Science and Technology
   (MEXT)Ministry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT) [S1311017]; Japan Science and Technology Agency (JST) CREST
   Program [JPMJCR13A2]
FX This research conducted a collaborative research with Sabah Forestry
   Department and supported by the Japan Society for the Promotion of
   Science (JSPS) Core-to-Core Program, A. Advanced Research Networks
   (Wildlife Research Centre of Kyoto University), the Ministry of
   Education, Culture, Sports, Science and Technology (MEXT; S1311017) and
   the Japan Science and Technology Agency (JST) CREST Program
   (JPMJCR13A2).
CR Ampeng A, 2016, EUR J WILDLIFE RES, V62, P147, DOI 10.1007/s10344-015-0983-8
   Blake J. G., 2013, ANIMAL CONSERVATION, V16, P420
   Buckley-Beason VA, 2006, CURR BIOL, V16, P2371, DOI 10.1016/j.cub.2006.08.066
   Camacho C, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-421
   Christiansen P, 2008, J MAMMAL, V89, P1435, DOI 10.1644/08-MAMM-A-013.1
   Edgar RC, 2010, BIOINFORMATICS, V26, P2460, DOI 10.1093/bioinformatics/btq461
   IUCN, 2016, IUCN RED LIST THREAT
   Kitchener AC, 2006, CURR BIOL, V16, P2377, DOI 10.1016/j.cub.2006.10.066
   Loken B, 2013, AM J PRIMATOL, V75, P1129, DOI 10.1002/ajp.22174
   Magoc T, 2011, BIOINFORMATICS, V27, P2957, DOI 10.1093/bioinformatics/btr507
   Matsubayashi H, 2007, ECOL RES, V22, P742, DOI 10.1007/s11284-006-0313-4
   Matsubayashi H, 2008, TROPICS, V17, P81
   Matsubayashi H, 2011, RAFFLES B ZOOL, V59, P109
   Matsuda I, 2015, ECOL RES, V30, P191, DOI 10.1007/s11284-014-1219-1
   Miya M, 2015, ROY SOC OPEN SCI, V2, DOI 10.1098/rsos.150088
   Miya M, 2016, JOVE-J VIS EXP, DOI 10.3791/54741
   Thomsen PF, 2015, BIOL CONSERV, V183, P4, DOI 10.1016/j.biocon.2014.11.019
   Timmins R., 2015, IUCN RED LIST THREAT, V2015
   Ushio M., 2016, BIORXIV
   Wilkie DS, 2011, ANN NY ACAD SCI, V1223, P120, DOI 10.1111/j.1749-6632.2010.05908.x
   Wilting A, 2007, FRONT ZOOL, V4, DOI 10.1186/1742-9994-4-15
   Wilting A, 2011, MOL PHYLOGENET EVOL, V58, P317, DOI 10.1016/j.ympev.2010.11.007
NR 22
TC 29
Z9 30
U1 4
U2 92
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD JUN
PY 2017
VL 210
BP 281
EP 285
DI 10.1016/j.biocon.2017.04.023
PN A
PG 5
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA FB1CP
UT WOS:000405881600030
DA 2022-02-10
ER

PT J
AU Allen, ML
   Sibarani, MC
   Utoyo, L
   Krofel, M
AF Allen, M. L.
   Sibarani, M. C.
   Utoyo, L.
   Krofel, M.
TI Terrestrial mammal community richness and temporal overlap between
   tigers and other carnivores in Bukit Barisan Selatan National Park,
   Sumatra
SO ANIMAL BIODIVERSITY AND CONSERVATION
LA English
DT Article
DE Activity patterns; Carnivores; Conservation; Interspecific interactions;
   Panthera tigris sumatrae
ID PANTHERA-TIGRIS-SUMATRAE; ACTIVITY PATTERNS; RAIN-FOREST; TROPICAL
   FOREST; CONSERVATION; BIODIVERSITY; OCCUPANCY; JAGUAR; PUMA; PREY
AB Terrestrial mammal community richness and temporal overlap between tigers and other carnivores in Bukit Barisan Selatan National Park, Sumatra. Rapid and widespread biodiversity losses around the world make it important to survey and monitor endangered species, especially in biodiversity hotspots. Bukit Barisan Selatan National Park (BBSNP) is one of the largest conserved areas on the island of Sumatra, and is important for the conservation of many threatened species. Sumatran tigers (Panthera tigris sumatrae) are critically endangered and serve as an umbrella species for conservation, but may also affect the activity and distribution of other carnivores. We deployed camera traps for 8 years in an area of Bukit Barisan Selatan National Park (BBSNP) with little human activity to document the local terrestrial mammal community and investigate tiger spatial and temporal overlap with other carnivore species. We detected 39 mammal species including Sumatran tiger and several other threatened mammals. Annual species richness averaged 21.5 (range 19-24) mammals, and remained stable over time. The mammal order significantly affected annual detection of species and the number of cameras where a species was detected, while species conservation status did not. Tigers exhibited a diurnal activity pattern, and had the highest temporal overlap with marbled cats (Pardofelis marmorata), dholes (Cuon alpinus), and Malayan sun bears (Helarctos malayanus), but little overlap with other carnivores. These findings suggest that some smaller carnivores might be adjusting temporal activity to avoid tigers or mesocarnivores. The stable trends in richness of terrestrial mammal species show that BBSNP remains an important hotspot for the conservation of biodiversity.
C1 [Allen, M. L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Sibarani, M. C.; Utoyo, L.] Wildlife Conservat Soc Indonesia Program, Jalan Tampomas Ujung 35, Bogor 16151, West Java, Indonesia.
   [Krofel, M.] Univ Ljubljana, Dept Forestry, Biotech Fac, Vecna Pot 83, SI-1000 Ljubljana, Slovenia.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Conservation International; Missouri Botanical Garden; Smithsonian
   InstitutionSmithsonian Institution; Wildlife Conservation Society;
   Gordon and Betty Moore FoundationGordon and Betty Moore Foundation;
   Illinois Natural History Survey
FX All data in this publication are available through the Tropical Ecology
   Assessment and Monitoring (TEAM) Network, a collaboration between
   Conservation International, the Missouri Botanical Garden, the
   Smithsonian Institution, and the Wildlife Conservation Society. The work
   was partially funded by these institutions, the Gordon and Betty Moore
   Foundation, and the Illinois Natural History Survey. Monitoring
   activities were managed by the Wildlife Conservation Society in
   collaboration with the Bukit Barisan Selatan National Park and the
   Ministry of Environment and Forestry, Republic of Indonesia. We thank
   all the field staff and forest rangers involved in the camera trap
   deployment, and W. Marthy and F. R. Affandi for field coordination.
CR Allen M. L., 2019, ORYX, V53, P54
   Allen ML, 2018, MAMM BIOL, V89, P90, DOI 10.1016/j.mambio.2018.01.001
   Allen ML, 2017, J ETHOL, V35, P13, DOI 10.1007/s10164-016-0492-6
   Balme GA, 2017, BEHAV ECOL, V28, P1348, DOI 10.1093/beheco/arx098
   Burton A, 2019, FRONT ECOL ENVIRON, V17, P300, DOI 10.1002/fee.2053
   Chapin FS, 1998, BIOSCIENCE, V48, P45, DOI 10.2307/1313227
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Fitzgerald Christopher S., 2002, Mammalian Species, V696, P1, DOI 10.1644/1545-1410(2002)696<0001:HM>2.0.CO;2
   Foster VC, 2013, BIOTROPICA, V45, P373, DOI 10.1111/btp.12021
   Gopal R, 2010, ORYX, V44, P383, DOI 10.1017/S0030605310000529
   Grassman LI, 2005, J MAMMAL, V86, P29, DOI 10.1644/1545-1542(2005)086&lt;0029:EOTSFI&gt;2.0.CO;2
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   Herrera H, 2018, REV BIOL TROP, V66, P1559, DOI 10.15517/rbt.v66i4.32895
   Hunter Luke, 2015, P1
   Johnson A, 2009, ORYX, V43, P626, DOI 10.1017/S0030605309990238
   Karanth KU, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2016.1860
   Karanth KU, 2010, TIGERS OF THE WORLD: THE SCIENCE, POLITICS, AND CONSERVATION OF PANTHERA TIGRIS, 2ND EDITION, P241
   Kawanishi K, 2004, BIOL CONSERV, V120, P329, DOI 10.1016/j.biocon.2004.03.005
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Linkie M., 2008, IUCN RED LIST THREAT, V8235
   Linkie M, 2008, CONSERV BIOL, V22, P683, DOI 10.1111/j.1523-1739.2008.00906.x
   Linkie M, 2007, BIOL CONSERV, V137, P20, DOI 10.1016/j.biocon.2007.01.016
   Lynam AJ, 2013, RAFFLES B ZOOL, V61, P407
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   Myers N, 2000, NATURE, V403, P853, DOI 10.1038/35002501
   Nichols JD, 2007, ECOLOGY, V88, P1395, DOI 10.1890/06-1474
   O'Brien TG, 2008, ANIM CONSERV, V11, P179, DOI 10.1111/j.1469-1795.2008.00178.x
   O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
   O'Brien Timothy G., 1996, Oryx, V30, P207
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Pattanavibool A, 2015, IUCN RED LIST THREAT
   PIMM SL, 1995, SCIENCE, V269, P347, DOI 10.1126/science.269.5222.347
   Pusparini W, 2018, ORYX, V52, P25, DOI 10.1017/S0030605317001144
   R Core Team, 2016, R LANG ENV STAT COMP
   Rich LN, 2016, J APPL ECOL, V53, P1225, DOI 10.1111/1365-2664.12650
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Romero-Munoz A, 2010, J TROP ECOL, V26, P303, DOI 10.1017/S0266467410000052
   Rovero F., 2016, CAMERA TRAPPING WILD
   Seidensticker J.C. IV., 1973, Wildlife Monogr, VNo. 35, P1
   Seidensticker J, 2010, INTEGR ZOOL, V5, P285, DOI 10.1111/j.1749-4877.2010.00214.x
   Sibarani MC, 2019, J APPL ECOL, V56, P1220, DOI 10.1111/1365-2664.13360
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tobler MW, 2008, ANIM CONSERV, V11, P187, DOI 10.1111/j.1469-1795.2008.00181.x
   vanSchaik CP, 1996, BIOTROPICA, V28, P105, DOI 10.2307/2388775
   Walston J, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000485
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wibisono HT, 2009, ORYX, V43, P634, DOI 10.1017/S003060530999055X
NR 51
TC 7
Z9 7
U1 1
U2 5
PU MUSEU DE CIENCIES NATURALS-ZOOLOGIA
PI BARCELONA
PA PASSEIG PICASSO S-N, PARC CIUTADELLA, BARCELONA, E-08003, SPAIN
SN 1578-665X
EI 2014-928X
J9 ANIM BIODIV CONSERV
JI Anim. Biodivers. Conserv.
PY 2020
VL 43
IS 1
BP 97
EP 107
DI 10.32800/abc.2020.43.0097
PG 11
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA LS2KV
UT WOS:000536219200010
OA gold
DA 2022-02-10
ER

PT J
AU Santoro, S
   Perez, I
   Gegundez-Arias, ME
   Calzada, J
AF Santoro, Simone
   Perez, Isaac
   Gegundez-Arias, Manuel Emilio
   Calzada, Javier
TI Camera traps and artificial intelligence for monitoring invasive species
   and emerging diseases
SO ECOLOGICAL INFORMATICS
LA English
DT Editorial Material
DE Camera trapping; Artificial intelligence; Biological invasions
C1 [Santoro, Simone; Gegundez-Arias, Manuel Emilio; Calzada, Javier] Univ Huelva, Fac Ciencias Experiment, Dept Ciencias Integradas, Huelva 21007, Spain.
   [Perez, Isaac] Univ Huelva, Dept Tecnol Informaci prime, Escuela Tecnica Super Ingn, Huelva 21007, Spain.
   [Perez, Isaac; Gegundez-Arias, Manuel Emilio] Univ Huelva, Dept Sistemas Vision Predicci Optimizac Control, Centro Cientifico Tecnol Huelva, Huelva 21007, Spain.
RP Santoro, S (corresponding author), Univ Huelva, Fac Ciencias Experiment, Dept Ciencias Integradas, Huelva 21007, Spain.
EM simone.santoro@dci.uhu.es
RI SANTORO, SIMONE/B-2162-2015
OI SANTORO, SIMONE/0000-0003-0986-3278
FU The Fundacion Biodiversidad del Ministerio para la Transicion Ecologica
   y el Reto Demografico (MITECO)
FX The Fundacion Biodiversidad del Ministerio para la Transicion Ecologica
   y el Reto Demografico (MITECO) funded the research reported in this
   manuscript (Project AI-CENSUS) .
CR Crowl TA, 2008, FRONT ECOL ENVIRON, V6, P238, DOI 10.1890/070151
   Dunn AM, 2015, TRENDS PARASITOL, V31, P189, DOI 10.1016/j.pt.2014.12.003
   Hughes J, 2013, BIOL CONSERV, V157, P341, DOI 10.1016/j.biocon.2012.07.005
   Lowe S., 2000, 100 WORLDS WORST INV
   Marchetti M.P., 2013, INVASION ECOLOGY
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Sandino J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18040944
   Scott DM, 2020, URBAN ECOSYST, V23, P1127, DOI 10.1007/s11252-020-00985-5
NR 8
TC 0
Z9 0
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAR
PY 2022
VL 67
AR 101491
DI 10.1016/j.ecoinf.2021.101491
PG 2
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA XM8PN
UT WOS:000729081800008
OA Bronze
DA 2022-02-10
ER

PT C
AU Koh, PW
   Sagawa, S
   Marklund, H
   Xie, SM
   Zhang, M
   Balsubramani, A
   Hu, WH
   Yasunaga, M
   Phillips, RL
   Gao, I
   Lee, T
   David, E
   Stavness, I
   Guo, W
   Earnshaw, BA
   Haque, IS
   Beery, S
   Leskovec, J
   Kundaje, A
   Pierson, E
   Levine, S
   Finn, C
   Liang, P
AF Koh, Pang Wei
   Sagawa, Shiori
   Marklund, Henrik
   Xie, Sang Michael
   Zhang, Marvin
   Balsubramani, Akshay
   Hu, Weihua
   Yasunaga, Michihiro
   Phillips, Richard Lanas
   Gao, Irena
   Lee, Tony
   David, Etienne
   Stavness, Ian
   Guo, Wei
   Earnshaw, Berton A.
   Haque, Imran S.
   Beery, Sara
   Leskovec, Jure
   Kundaje, Anshul
   Pierson, Emma
   Levine, Sergey
   Finn, Chelsea
   Liang, Percy
BE Meila, M
   Zhang, T
TI WILDS: A Benchmark of in-the-Wild Distribution Shifts
SO INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 139
SE Proceedings of Machine Learning Research
LA English
DT Proceedings Paper
CT International Conference on Machine Learning (ICML)
CY JUL 18-24, 2021
CL ELECTR NETWORK
ID RACIAL DISPARITIES; CLASSIFICATION; ART; IMAGERY; CANCER; HEALTH; DRIFT
AB Distribution shifts-where the training distribution differs from the test distribution-can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an opensource package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.
C1 [Koh, Pang Wei; Sagawa, Shiori; Marklund, Henrik; Xie, Sang Michael; Balsubramani, Akshay; Hu, Weihua; Yasunaga, Michihiro; Gao, Irena; Lee, Tony; Leskovec, Jure; Kundaje, Anshul; Finn, Chelsea; Liang, Percy] Stanford, Stanford, CA 94305 USA.
   [Zhang, Marvin; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA USA.
   [Phillips, Richard Lanas; Pierson, Emma] Cornell, Ithaca, NY USA.
   [David, Etienne] INRAE, Paris, France.
   [Stavness, Ian] USask, Saskatoon, SK, Canada.
   [Guo, Wei] UTokyo, Tokyo, Japan.
   [Earnshaw, Berton A.; Haque, Imran S.] Recursion, Cambridge, MA USA.
   [Beery, Sara] CALTECH, Pasadena, CA 91125 USA.
   [Pierson, Emma] Microsoft Res, Redmond, WA USA.
RP Koh, PW; Sagawa, S; Liang, P (corresponding author), Stanford, Stanford, CA 94305 USA.
EM pangwei@cs.stanford.edu; ssagawa@cs.stanford.edu; pliang@cs.stanford.edu
OI Kundaje, Anshul/0000-0003-3084-2287
FU Open Philanthropy Project Award; NSFNational Science Foundation (NSF)
   [OAC-1835598, OAC-1934578, CCF-1918940, IIS-2030477, 1805310]; Herbert
   Kunzel Stanford Graduate Fellowship; Dr. Tech. Marcus Wallenberg
   Foundation for Education in International Industrial Entrepreneurship;
   CIFARCanadian Institute for Advanced Research (CIFAR); GoogleGoogle
   Incorporated; NDSEG Graduate Fellowships; Funai Overseas Scholarship;
   Masason Foundation Fellowship; NSF Graduate Research FellowshipNational
   Science Foundation (NSF); DARPAUnited States Department of
   DefenseDefense Advanced Research Projects Agency (DARPA)
   [N660011924033]; ARO [W911NF-16-1-0342, W911NF-16-1-0171]; Stanford Data
   Science Initiative; Wu Tsai Neurosciences Institute; Chan Zuckerberg
   Biohub; Amazon; JPMorgan Chase; Docomo; Hitachi; JD.com; KDDIKDDI
   Corporation; NVIDIA; Dell; Toshiba; UnitedHealth Group
FX This project was funded by an Open Philanthropy Project Award and NSF
   Award Grant No. 1805310. Shiori Sagawa was supported by the Herbert
   Kunzel Stanford Graduate Fellowship. Henrik Marklund was supported by
   the Dr. Tech. Marcus Wallenberg Foundation for Education in
   International Industrial Entrepreneurship, CIFAR, and Google. Sang
   Michael Xie and Marvin Zhang were supported by NDSEG Graduate
   Fellowships. Weihua Hu was supported by the Funai Overseas Scholarship
   and the Masason Foundation Fellowship. Sara Beery was supported by an
   NSF Graduate Research Fellowship and is a PIMCO Fellow in Data Science.
   Jure Leskovec is a Chan Zuckerberg Biohub investigator. Chelsea Finn is
   a CIFAR Fellow in the Learning in Machines and Brains Program.; We also
   gratefully acknowledge the support of DARPA under Nos. N660011924033
   (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP);
   NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940
   (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu
   Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan
   Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, and
   UnitedHealth Group.
CR Abelson B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1563, DOI 10.1145/2623330.2623335
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Aguet F, 2020, SCIENCE, V369, P1318, DOI 10.1126/science.aaz1776
   Ahadi A., 2015, P 11 ANN INT C INT C, P121, DOI [10.1145/2787622.2787717, DOI 10.1145/2787622.2787717]
   Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
   Aich S, 2018, IEEE WINT CONF APPL, P323, DOI 10.1109/WACV.2018.00042
   AlBadawy EA, 2018, MED PHYS, V45, P1150, DOI 10.1002/mp.12752
   Alexandari AM, 2020, PR MACH LEARN RES, V119
   Allamanis M., 2017, ARXIV170507867
   Allamanis M, 2015, 2015 10TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE 2015) PROCEEDINGS, P38, DOI 10.1145/2786805.2786849
   AMORIM LA, 2018, ASS COMPUTATIONAL LI, P229, DOI DOI 10.1109/WSCAD.2018.00043
   Nguyen AT, 2015, 2015 IEEE/ACM 37TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, VOL 1, P858, DOI 10.1109/ICSE.2015.336
   [Anonymous], 2020, BBC
   [Anonymous], 2019, ARXIV191007113
   Ardila R, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P4218
   Arjovsky Martin, 2019, ARXIV190702893
   Attene-Ramos MS, 2013, DRUG DISCOV TODAY, V18, P716, DOI 10.1016/j.drudis.2013.05.015
   Avsec Z., 2019, BIORXIV, DOI [10.1101/737981, DOI 10.1101/737981]
   Ayalew Tewodros W., 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P330, DOI 10.1007/978-3-030-65414-6_23
   Azizzadenesheli Kamyar, 2019, INT C LEARN REPR
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Badgeley MA, 2019, NPJ DIGIT MED, V2, DOI 10.1038/s41746-019-0105-1
   Balaji Y., 2018, NEURIPS, P1006
   Bandi P, 2019, IEEE T MED IMAGING, V38, P550, DOI 10.1109/TMI.2018.2867350
   Barbu A, 2019, ADV NEUR IN, V32
   Bartlett PL, 2008, J MACH LEARN RES, V9, P1823
   Baumann T, 2019, LANG RESOUR EVAL, V53, P303, DOI 10.1007/s10579-017-9410-y
   Beck AH, 2011, SCI TRANSL MED, V3, DOI 10.1126/scitranslmed.3002564
   Becke AD, 2014, J CHEM PHYS, V140, DOI 10.1063/1.4869598
   Beede E, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376718
   Beery S., 2020, ARXIV200410340
   Beery S., 2019, ARXIV190706772
   Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
   Bejnordi BE, 2017, JAMA-J AM MED ASSOC, V318, P2199, DOI 10.1001/jama.2017.14585
   Bellamy D., 2020, ABS201001149
   Bellemare MG, 2020, NATURE, V588, P77, DOI 10.1038/s41586-020-2939-8
   Ben-David S., 2006, NIPS, V19, P137
   Bender E. M., 2018, T ASSOC COMPUT LING, V6, P587, DOI [10.1162/tacl_a_00041, DOI 10.1162/TACL_A_00041]
   BenTaieb A, 2018, IEEE T MED IMAGING, V37, P792, DOI 10.1109/TMI.2017.2781228
   Berman G., 2018, ETHICAL CONSIDERATIO
   Beyene AA, 2015, KNOWL INF SYST, V44, P177, DOI 10.1007/s10115-014-0756-9
   Blanchard G., 2011, ADV NEURAL INF PROCE, P1
   Blitzer J., 2007, P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Blodgett SL, 2016, P 2016 C EMP METH, P1119, DOI DOI 10.18653/V1/D16-1120
   Blodgett SL, 2017, P WORKSH FAIRN ACC T
   Blumenstock J, 2015, SCIENCE, V350, P1073, DOI 10.1126/science.aac4420
   Blundell, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387
   Board Editorial, 2016, NY TIMES
   Bohacek RS, 1996, MED RES REV, V16, P3, DOI 10.1002/(SICI)1098-1128(199601)16:1<3::AID-MED1>3.3.CO;2-D
   Borkan D., 2019, ARXIV190302088
   Borkan D, 2019, COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 ), P491, DOI 10.1145/3308560.3317593
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Boutros M, 2015, CELL, V163, P1314, DOI 10.1016/j.cell.2015.11.007
   Bray MA, 2016, NAT PROTOC, V11, P1757, DOI 10.1038/nprot.2016.105
   Broach JR, 1996, NATURE, V384, P14
   Broussard M., 2020, NY TIMES
   Bruch M, 2009, 7TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING, P213, DOI 10.1145/1595696.1595728
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Bug D, 2017, LECT NOTES COMPUT SC, V10553, P135, DOI 10.1007/978-3-319-67558-9_16
   Bunel Rudy, 2018, INT C LEARN REPR ICL
   Buolamwini J., 2018, P C FAIRN ACC TRANSP, P77
   BURKE L, 2016, LANCET GLOB HEALTH, V4, DOI DOI 10.3389/FCELL.2016.00103
   Byrd J, 2019, PR MACH LEARN RES, V97
   Caicedo JC, 2018, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2018.00970
   Caicedo JC, 2017, NAT METHODS, V14, P849, DOI [10.1038/NMETH.4397, 10.1038/nmeth.4397]
   Caldas S., 2018, LEAF BENCHMARK FEDER
   Campanella G, 2019, NAT MED, V25, P1301, DOI 10.1038/s41591-019-0508-1
   Cao KD, 2019, ADV NEUR IN, V32
   Cao Kaidi, 2020, ARXIV200615766
   CARLUCCI FM, 2019, COMPUTER VISION PATT, P2224, DOI DOI 10.1109/CVPR.2019.00233
   Cavalli-Sforza V., 2020, INT C ED DAT MIN, P257
   Chanussot L., 2020, ARXIV201009990
   Chen I.Y., 2020, ANN REV BIOMEDICAL D
   Chen Irene Y, 2019, AMA J Ethics, V21, pE167, DOI 10.1001/amajethics.2019.167
   Chen VS, 2019, ADV NEUR IN, V32
   Ching T, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0387
   Christie G, 2018, PROC CVPR IEEE, P6172, DOI 10.1109/CVPR.2018.00646
   Chung JS, 2018, INTERSPEECH, P1086
   Clark JH, 2020, T ASSOC COMPUT LING, V8, P454, DOI 10.1162/tacl_a_00317
   Codella Noel, 2019, ARXIV190203368
   Conneau Alexis, 2018, ARXIV180905053
   Consortium H., 2019, NATURE, V574
   Corbett-Davies S., 2018, MEASURE MISMEASURE F
   Corbett-Davies S., 2016, WASH POST
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   CORDELLA LP, 1995, IEEE T NEURAL NETWOR, V6, P1140, DOI 10.1109/72.410358
   Courtiol P, 2019, NAT MED, V25, P1519, DOI 10.1038/s41591-019-0583-3
   Croce F., 2020, ARXIV201009670
   Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Cynthia Dwork, 2012, P 3 INN THEOR COMP C, P214
   D'Amour A., 2020, ARXIV PREPRINT ARXIV
   D'Amour A, 2020, FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P525, DOI 10.1145/3351095.3372878
   Dai DX, 2018, IEEE INT C INTELL TR, P3819, DOI 10.1109/ITSC.2018.8569387
   David E., 2021, GLOBAL WHEAT HEAD DA
   David E, 2020, PLANT PHENOMICS, V2020, DOI 10.34133/2020/3521852
   Davis SE, 2017, J AM MED INFORM ASSN, V24, P1052, DOI 10.1093/jamia/ocx030
   DeGrave Alex J, 2020, medRxiv, DOI 10.1101/2020.09.13.20193565
   Delangue Clement, 2019, ARXIV PREPRINT ARXIV
   Desmarais MC, 2012, USER MODEL USER-ADAP, V22, P9, DOI 10.1007/s11257-011-9106-8
   DEVKOTA P, 2018, EMPIRICAL METHODS NA, P2799
   DigitalGlobe N., 2016, SPAC
   Dill KA, 2012, SCIENCE, V338, P1042, DOI 10.1126/science.1219021
   Dixon L, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P67, DOI 10.1145/3278721.3278729
   Djolonga Josip, 2020, ARXIV200708558
   Dodge S., 2017, 2017 26 INT C COMP C, P1, DOI DOI 10.1109/ICCCN.2017.8038465
   Dou Q., 2019, NEURIPS, P6447
   Dreccer MF, 2019, PLANT SCI, V282, P73, DOI 10.1016/j.plantsci.2018.06.008
   Dressel J, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aao5580
   Duchi J., 2020, ARXIV200713982
   Duchi JC, 2021, ANN STAT, V49, P1378, DOI 10.1214/20-AOS2004
   Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247
   Echeverri CJ, 2006, NAT REV GENET, V7, P373, DOI 10.1038/nrg1836
   El-Yaniv R., 2018, INT C LEARN REPR ICL
   Elvidge CD, 2009, COMPUT GEOSCI-UK, V35, P1652, DOI 10.1016/j.cageo.2009.01.009
   Eraslan G, 2019, NAT REV GENET, V20, P389, DOI 10.1038/s41576-019-0122-6
   Espey J., 2015, SUSTAINABLE DEV SOLU
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   FAN QC, 2018, ADV NEURAL INFORM PR, P3539
   Fan Z, 2018, IEEE J-STARS, V11, P876, DOI 10.1109/JSTARS.2018.2793849
   Fang C, 2013, IEEE I CONF COMP VIS, P1657, DOI 10.1109/ICCV.2013.208
   Feng J., 2019, ARXIV190605473
   Filmer D., 2011, DEMOGRAPHY, V49
   Franks Christine, 2015, P 37 INT C SOFTW ENG
   Fuentes A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17092022
   Futoma J, 2020, LANCET DIGIT HEALTH, V2, pE489, DOI 10.1016/S2589-7500(20)30186-2
   Gal Y, 2016, PR MACH LEARN RES, V48
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Garg Saurabh, 2020, ADV NEUR IN, V33
   Gebru T., 2018, DATASHEETS DATASETS
   Geifman Y, 2019, PR MACH LEARN RES, V97
   Geifman Y, 2017, ADV NEUR IN, V30
   Geirhos R, 2020, ABS200407780 ARXIV
   Geirhos R., 2018, NEURIPS, P7538, DOI DOI 10.5555/3327757.3327854
   Geirhos Robert, 2018, ARXIV PREPRINT ARXIV
   Gelman A, 2007, J AM STAT ASSOC, V102, P813, DOI 10.1198/016214506000001040
   Geva Mor, 2019, P 2019 C EMP METH NA, P1161
   Gibson C.C, BIORXIV, DOI [10.1101/2020.08.02.233064, DOI 10.1101/2020.08.02.233064]
   GILMER J, 2017, INT C MACH LEARN ICM, V70
   Godinez W. J., 2018, BIORXIV
   Goel K., 2020, ARXIV200806775
   Goel S, 2016, ANN APPL STAT, V10, P365, DOI 10.1214/15-AOAS897
   Gogoll D, 2020, IEEE INT C INT ROBOT, P2636, DOI 10.1109/IROS45743.2020.9341277
   Goh WWB, 2017, TRENDS BIOTECHNOL, V35, P498, DOI 10.1016/j.tibtech.2017.02.012
   Goodfellow I., 2015, P 3 INT C LEARN REPR
   Graetz N, 2018, NATURE, V555, P48, DOI 10.1038/nature25761
   Grooten M., 2020, LIVING PLANET REPORT
   Gulrajani Ishaan, 2020, ARXIV200701434
   Guo J., 2018, ARXIV PREPRINT ARXIV, P4694
   Gupta A, 2018, ADV NEUR IN, V31
   Gurcan Metin N, 2009, IEEE Rev Biomed Eng, V2, P147, DOI 10.1109/RBME.2009.2034865
   Halpern Y., 2020, NEURIPS 18 COMPETITI, P155
   Han X., 2020, P 2020 C EMP METH NA, P7732, DOI [10.18653/v1/2020.emnlp-main.622, DOI 10.18653/V1/2020.EMNLP-MAIN.622]
   Hand DJ, 2006, STAT SCI, V21, P1, DOI 10.1214/088342306000000060
   Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1215904, 10.1126/science.1244693]
   Harrill Joshua, 2019, Current Opinion in Toxicology, V15, P64, DOI 10.1016/j.cotox.2019.05.004
   Hashimoto TB, 2018, PR MACH LEARN RES, V80
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He Luheng, 2019, P 2019 C N AM CHAPT
   He Y, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107383
   Heinze-Deml C., 2017, ARXIV171011469
   Hellendoorn VJ, 2019, PROC INT CONF SOFTW, P960, DOI 10.1109/ICSE.2019.00101
   Henderson BE, 2012, NAT REV CANCER, V12, P648, DOI 10.1038/nrc3341
   Hendrycks D., 2020, ARXIV200616241
   Hendrycks D., 2020, ARXIV191111132
   Hendrycks D, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2744
   Hendrycks Dan, 2017, 5 INT C LEARN REPR I
   Hendrycks Dan, 2019, 7 INT C LEARN REPR I
   Ho JWK, 2014, NATURE, V512, P449, DOI 10.1038/nature13415
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hovy D, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P591
   Hu JJ, 2020, PR MACH LEARN RES, V119
   Hu W., 2020, NEURIPS
   Hu WH, 2018, PR MACH LEARN RES, V80
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang J., 2020, P IEEE CVF C COMP VI, P13075
   Hughes JP, 2011, BRIT J PHARMACOL, V162, P1239, DOI 10.1111/j.1476-5381.2010.01127.x
   Husain H., 2019, ARXIV190909436
   Jaganathan K, 2019, CELL, V176, P535, DOI 10.1016/j.cell.2018.12.015
   Jean N, 2018, ADV NEUR IN, V31
   Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894
   Jin W., 2020, ARXIV200603908
   Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Jones E., 2021, INT C LEARN REPR ICL, P2021
   Jorgensen A., 2015, P WORKSH NOIS US GEN, P9
   Jumper J., 2020, 14 CRITICAL ASSESSME
   Kahn G., 2020, ARXIV200205700
   Kallus N, 2018, PR MACH LEARN RES, V80
   Kamath Amita, 2020, ACL
   Kearns M, 2018, PR MACH LEARN RES, V80
   Keilwagen J, 2019, GENOME BIOL, V20, DOI 10.1186/s13059-018-1614-y
   Kelley DR, 2016, GENOME RES, V26, P990, DOI 10.1101/gr.200535.115
   Kim J. H., 2016, INCORPORATING SPATIA
   Kim Najoung, 2020, P 2020 C EMP METH NA, P9087
   Kim S, 2016, NUCLEIC ACIDS RES, V44, pD1202, DOI 10.1093/nar/gkv951
   Koenecke A, 2020, P NATL ACAD SCI USA, V117, P7684, DOI 10.1073/pnas.1915768117
   Koh PW, 2020, PR MACH LEARN RES, V119
   Kompa B., 2020, ARXIV201003039
   Komura D, 2018, COMPUT STRUCT BIOTEC, V16, P34, DOI 10.1016/j.csbj.2018.01.001
   Kulal S, 2019, ADV NEUR IN, V32
   Kulkarni C, 2015, UNDERST INNOV, P131, DOI 10.1007/978-3-319-06823-7_9
   Kulkarni Chinmay E., 2014, P 1 ACM C LEARN SCAL, P99, DOI 10.1145/2556325.2566238
   Kumar A, 2020, PR MACH LEARN RES, V119
   Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248
   Kuznichov D, 2019, IEEE COMPUT SOC CONF, P2580, DOI 10.1109/CVPRW.2019.00314
   Lake B, 2018, PR MACH LEARN RES, V80
   Lample Guillaume, 2019, ADV NEURAL INFORM PR
   Landrum G., 2006, RDKIT OPEN SOURCE CH
   LANGMEAD B, 2010, NAT REV GENET, V11, DOI DOI 10.1186/GB-2010-11-8-R83
   Larrazabal AJ, 2020, P NATL ACAD SCI USA, V117, P12592, DOI 10.1073/pnas.1919012117
   Latessa E.J., 2010, FEDERAL PROBATION, V74, P16
   Lau RYK, 2014, DECIS SUPPORT SYST, V65, P80, DOI 10.1016/j.dss.2014.05.005
   LeCun Y, 1998, MNIST DATABASE HANDW
   Levine, 2017, 2017 IEEE INT C ROB, P3389, DOI DOI 10.1109/ICRA.2017.7989385
   Li D, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li H.M.Y.T. Yongya, 2019, BIORXIV
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li HY, 2019, GENOME RES, V29, P281, DOI 10.1101/gr.237156.118
   Li T., 2019, ARXIV PREPRINT ARXIV
   Li Y, 2017, P 5 INT C LEARN REPR
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Liang Shiyu, 2018, 6 INT C LEARN REPR I
   Libbrecht MW, 2015, NAT REV GENET, V16, P321, DOI 10.1038/nrg3920
   Lin ZY, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz0652
   Lipton Z. C., 2018, ARXIV PREPRINT ARXIV
   Lipton Zachary, 2019, INT C LEARN REPR ICL
   Liu LT, 2018, PR MACH LEARN RES, V80
   Liu Y, 2017, ARXIV170302442
   Ljosa V, 2012, NAT METHODS, V9, P637, DOI 10.1038/nmeth.2083
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Loshchilov Ilya, 2019, ARXIV171105101
   Lu S., 2021, ARXIV210204664V2
   Lum K, 2016, SIGNIFICANCE, V13, P14, DOI DOI 10.1111/J.1740-9713.2016.00960.X
   Lum K., 2019, MEASURES FAIRNESS NE, P21
   Lyu J, 2019, NATURE, V566, P224, DOI 10.1038/s41586-019-0917-9
   Macarron R, 2011, NAT REV DRUG DISCOV, V10, P188, DOI 10.1038/nrd3368
   Macenko M, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO TO MACRO, VOLS 1 AND 2, P1107, DOI 10.1109/ISBI.2009.5193250
   Madec S, 2019, AGR FOREST METEOROL, V264, P225, DOI 10.1016/j.agrformet.2018.10.013
   Malloy BA, 2017, INT SYMP EMP SOFTWAR, P314, DOI 10.1109/ESEM.2017.45
   Mansour Y., 2009, ADV NEURAL INF PROCE, P1041
   Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556
   Mattu, 2016, PROPUBLICA
   McCloskey K, 2020, J MED CHEM, V63, P8857, DOI 10.1021/acs.jmedchem.0c00452
   McCoy R.T., 2019, ARXIV PREPRINT ARXIV
   McCoy R. Thomas, 2019, ARXIV191102969
   McKinney SM, 2020, NATURE, V577, P89, DOI 10.1038/s41586-019-1799-6
   Mehrabi N., 2019, ARXIV190809635CS
   Meinshausen N, 2015, ANN STAT, V43, P1801, DOI 10.1214/15-AOS1325
   Michael Ando D, 2017, BIORXIV, DOI [10.1101/161422, DOI 10.1101/161422]
   Miller J., 2020, ARXIV200414444
   Mirowski Piotr, 2017, ICLR
   Moore JE, 2020, NATURE, V583, P699, DOI 10.1038/s41586-020-2493-4
   MOULT J, 1995, PROTEINS, V23, pR2, DOI 10.1002/prot.340230303
   Nekoto Wilhelmina, 2020, FINDINGS ASS COMPUTA, P2144
   Nestor B, 2019, FEATURE ROBUSTNESS N
   Newman D, 2007, UCI MACHINE LEARNING
   Ng, 2019, RADIOLOGY, DOI [DOI 10.1148/RADIOL.2020201160, 10.1148/radiol.2020201160]
   Ni J., 2019, P 2019 C EMP METH NA, P188, DOI DOI 10.18653/V1/D19-1018
   NITA M, 2010, 2010 ACM IEEE 32 INT, V0001, P00205
   Noor Abdisalan M, 2008, Popul Health Metr, V6, P5, DOI 10.1186/1478-7954-6-5
   Norouzzadeh M. S., 2019, ARXIV191009716
   Nygaard V, 2016, BIOSTATISTICS, V17, P29, DOI 10.1093/biostatistics/kxv027
   Obermeyer Z, 2019, SCIENCE, V366, P447, DOI 10.1126/science.aax2342
   Oren Y., 2019, EMPIRICAL METHODS NA
   Osgood-Zimmerman A, 2018, NATURE, V555, P41, DOI 10.1038/nature25760
   Ovadia Y, 2019, ADV NEUR IN, V32
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Parham J., 2017, ASS ADVANCEMENT ARTI, DOI [10.1016/j.wasman.2010.12.019, DOI 10.1016/J.WASMAN.2010.12.019.]
   Parker HS, 2012, STAT APPL GENET MOL, V11, DOI 10.1515/1544-6115.1766
   Patro GK, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1194, DOI 10.1145/3366423.3380196
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Peng XB, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Perelman L, 2014, ASSESS WRIT, V21, P104, DOI 10.1016/j.asw.2014.05.001
   Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Phillips, 2020, ARXIV200706199 CORR
   Piech C., 2013, ED DATA MINING
   Pierson E, 2018, PR MACH LEARN RES, V84
   Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026
   Pipal KA, 2012, N AM J FISH MANAGE, V32, P880, DOI 10.1080/02755947.2012.697096
   Price WN, 2019, NAT MED, V25, P37, DOI 10.1038/s41591-018-0272-7
   Proksch S., 2016, 2016 31 IEEE ACM INT
   Proksch S, 2015, ACM T SOFTW ENG METH, V25, DOI 10.1145/2744200
   Quang D, 2019, METHODS, V166, P40, DOI 10.1016/j.ymeth.2019.03.020
   Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Raychev V, 2016, ACM SIGPLAN NOTICES, V51, P731, DOI 10.1145/3022671.2984041
   Raychev V, 2014, ACM SIGPLAN NOTICES, V49, P419, DOI [10.1145/2666356.2594321, 10.1145/2594291.2594321]
   Re C., 2019, ARXIV190905372
   Recht B, 2019, PR MACH LEARN RES, V97
   Reiner RC, 2018, NEW ENGL J MED, V379, P1128, DOI 10.1056/NEJMoa1716766
   Reker D., 2020, DRUG DISCOV TODAY
   Ren SQ, 2015, ADV NEUR IN, V28
   Reynolds M, 2020, PLANT SCI, V295, DOI 10.1016/j.plantsci.2019.110396
   Ribeiro Marco Tulio, 2020, ARXIV PREPRINT ARXIV, P4902, DOI 10.18653/v1/2020.acl-main.442
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Rigaki M, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P70, DOI 10.1109/SPW.2018.00019
   Robbes Romain, 2008, 2008 23rd IEEE/ACM International Conference on Automated Software Engineering, P317, DOI 10.1109/ASE.2008.42
   Rolf E, 2020, PR MACH LEARN RES, V108, P1759
   ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352
   Rosenfeld A., 2018, ARXIV180803305
   Ruszczynski, 2014, LECT STOCHASTIC PROG
   Sadeghi F, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII
   Sadeghi-Tehran P, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0253-8
   SAEED U, 2017, ELIFE, V6, DOI DOI 10.1186/S40035-017-0076-6
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446
   Sagawa S., 2020, PROCEEDINGS OF THE 3, P8346
   Sagawa S, 2020, INT C LEARN REPR
   Sahn DE, 2003, REV INCOME WEALTH, P463
   Sanh Victor, 2019, EMC2
   Santurkar Shibani, 2020, BREEDS BENCHMARKS SU, V8
   Sap M, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1668
   Schneider S., 2020, ARXIV200712808
   Seyyed-Kalantari L., 2020, CHEXCLUSION FAIRNESS
   Shakoor N, 2017, CURR OPIN PLANT BIOL, V38, P184, DOI 10.1016/j.pbi.2017.05.006
   Shankar Shreya, 2017, ARXIV PREPRINT ARXIV
   Shankar V., 2019, ARXIV190602168
   Shen J, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P4058
   Shermis MD, 2014, ASSESS WRIT, V20, P53, DOI 10.1016/j.asw.2013.04.001
   Shetty R, 2019, PROC CVPR IEEE, P8210, DOI 10.1109/CVPR.2019.00841
   Shi Y, 2016, PLOS ONE, V11, P1, DOI DOI 10.1371/J0URNAL.P0NE.0157259
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
   Shoichet BK, 2004, NATURE, V432, P862, DOI 10.1038/nature03197
   Slack D., 2019, ARXIV190809092CSSTAT
   SOHONI NS, 2020, ADV NEURAL INFORM PR, V33
   Soneson C, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0100335
   Song, 2019, INT C LEARN REPR
   Srivastava D, 2020, BBA-GENE REGUL MECH, V1863, DOI 10.1016/j.bbagrm.2019.194443
   SRIVASTAVA M, 2020, INT C MACH LEARN, P109
   Sterling T, 2015, J CHEM INF MODEL, V55, P2324, DOI 10.1021/acs.jcim.5b00559
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Subbaswamy A., 2020, ARXIV201015100
   Sun BC, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Sun Pei, 2020, P IEEE CVF C COMP VI
   Sun Yu, 2020, INT C MACH LEARN ICM, P9229
   Svyatkovskiy A, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2727
   Swinney DC, 2011, NAT REV DRUG DISCOV, V10, P507, DOI 10.1038/nrd3480
   Tabak G, 2020, PEERJ, V8, DOI 10.7717/peerj.8594
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Taghipour K., 2016, P 2016 C EMP METH NA, P1882, DOI [10.18653/v1/D16-1193, DOI 10.18653/V1/D16-1193]
   Taori Rohan, 2020, ARXIV200700644, V33
   Tatman R., 2017, P 1 ACL WORKSH ETH N, P53
   Taylor J., 2019, INT C LEARN REPR ICL
   Taylor MJ, 2021, J AM SOC MASS SPECTR, V32, P872, DOI 10.1021/jasms.0c00439
   Tellez D, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101544
   Tellez D, 2018, IEEE T MED IMAGING, V37, P2126, DOI 10.1109/TMI.2018.2820199
   Temel D, 2018, 2018 17TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P137, DOI 10.1109/ICMLA.2018.00028
   Thorp KR, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111682
   Tiecke TG, 2017, MAPPING WORLD POPULA
   Tobin Joshua, 2017, ARXIV170306907
   Toda Y, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/9237136
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tuschl T, 2001, CHEMBIOCHEM, V2, P239, DOI 10.1002/1439-7633(20010401)2:4<239::AID-CBIC239>3.3.CO;2-I
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Ubbens Jordan R., 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P391, DOI 10.1007/978-3-030-65414-6_27
   Uzkent Burak, 2020, P IEEE CVF C COMP VI
   Vasic M., 2019, ARXIV PREPRINT ARXIV
   Vatnehol S, 2018, ICES J MAR SCI, V75, P1803, DOI 10.1093/icesjms/fsy029
   Veeling BS, 2018, LECT NOTES COMPUT SC, V11071, P210, DOI 10.1007/978-3-030-00934-2_24
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Veta M, 2019, MED IMAGE ANAL, V54, P111, DOI 10.1016/j.media.2019.02.012
   Veta M, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161286
   Volpi R., 2018, ADV NEURAL INFORM PR
   Wang A, 2019, ADV NEUR IN, V32
   Wang Alex, 2019, 7 INT C LEARNING REP
   Wang Dong, 2020, ARXIV200610726
   Wang Haohan, 2019, NEURIPS, P10506
   Wang SL, 2017, IEEE I CONF COMP VIS, P3028, DOI 10.1109/ICCV.2017.327
   Wang S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020207
   Ward D, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103009
   Wearn OR, 2017, WWF CONSERVATION TEC, V1
   Weinberger S., 2015, SPEECH ACCENT ARCHIV
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weinstein JN, 2013, NAT GENET, V45, P1113, DOI 10.1038/ng.2764
   West R., 2014, T ASSOC COMPUT LING, V2, P297, DOI [10.1162/tacl_a_00184, DOI 10.1162/TACL_A_00184]
   Weston Jason, 2017, INT C LEARN REPR ICL
   Widmer G, 1996, MACH LEARN, V23, P69, DOI 10.1007/BF00116900
   Williams JJ, 2016, PROCEEDINGS OF THE THIRD (2016) ACM CONFERENCE ON LEARNING @ SCALE (L@S 2016), P379, DOI 10.1145/2876034.2876042
   Wilson Benjamin, 2019, ARXIV190211097
   Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758
   Wu MK, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P782
   Wu YF, 2019, PR MACH LEARN RES, V97
   Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a
   Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489
   Xiao Kai Y., 2020, ARXIV200609994
   Xie M, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3929
   Xie S. M., 2020, IN N OUT PRETRAINING
   Xiong HP, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0537-2
   Xu K., 2018, ARXIV PREPRINT ARXIV
   Yang Y., 2019, ASIAN BUS MANAG, P1
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270
   Yasunaga Michihiro, 2020, ARXIV PREPRINT ARXIV
   Yeh C, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16185-w
   You J., 2017, 31 AAAI C ART INT SA
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yuval N., 2011, NIPS WORKSH DEEP LEA
   Zafar M. B., 2017, ADV NEURAL INFORM PR, P229
   Zech JR, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002683
   Zemel Richard, 2020, ARXIV PREPRINT ARXIV
   Zeng J., 2018, MIAM BEH FIN C, P1
   ZHANG L, 2013, INT C MACH LEARN, P819
   Zhang M., 2020, ARXIV200702931
   Zhao Jieyu, 2018, P 2018 C N AM CHAPT, V2, P15
   Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/NMETH.3547, 10.1038/nmeth.3547]
   Zhou Xiang, 2020, P 2020 C EMP METH NA, P8215
   Zhou YX, 2014, NATURE, V509, P487, DOI 10.1038/nature13166
   Zitnick C.L., 2020, ARXIV201009435CONDMA
NR 413
TC 4
Z9 4
U1 1
U2 1
PU JMLR-JOURNAL MACHINE LEARNING RESEARCH
PI SAN DIEGO
PA 1269 LAW ST, SAN DIEGO, CA, UNITED STATES
SN 2640-3498
J9 PR MACH LEARN RES
PY 2021
VL 139
PG 28
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS0JP
UT WOS:000683104605062
DA 2022-02-10
ER

PT J
AU Rumelt, RB
   Basto, A
   Roncal, CM
AF Rumelt, Reid B.
   Basto, Arianna
   Roncal, Carla Mere
TI Automated audio recording as a means of surveying tinamous (Tinamidae)
   in the Peruvian Amazon
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE bioacoustics; bird biology; machine learning; Neotropics; Peru; tinamous
ID ATLANTIC FOREST; HABITAT; EBIRD
AB The use of machine learning technologies to process large quantities of remotely collected audio data is a powerful emerging research tool in ecology and conservation. We applied these methods to a field study of tinamou (Tinamidae) biology in Madre de Dios, Peru, a region expected to have high levels of interspecies competition and niche partitioning as a result of high tinamou alpha diversity. We used autonomous recording units to gather environmental audio over a period of several months at lowland rainforest sites in the Los Amigos Conservation Concession and developed a Convolutional Neural Network-based data processing pipeline to detect tinamou vocalizations in the dataset. The classified acoustic event data are comparable to similar metrics derived from an ongoing camera trapping survey at the same site, and it should be possible to combine the two datasets for future explorations of the target species' niche space parameters. Here, we provide an overview of the methodology used in the data collection and processing pipeline, offer general suggestions for processing large amounts of environmental audio data, and demonstrate how data collected in this manner can be used to answer questions about bird biology.
C1 [Rumelt, Reid B.] Cornell Univ, Coll Agr & Life Sci, Ithaca, NY 14853 USA.
   [Basto, Arianna] Colorado State Univ, Warner Coll Nat Resources, Ft Collins, CO 80523 USA.
   [Roncal, Carla Mere] Univ Florida, Sch Forest Fisheries & Geomat Sci, Gainesville, FL USA.
RP Rumelt, RB (corresponding author), Cornell Univ, Coll Agr & Life Sci, Ithaca, NY 14853 USA.
EM rbr73@cornell.edu
OI Rumelt, Reid/0000-0003-3551-0599
FU Amazon Conservation Association
FX Amazon Conservation Association
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Acevedo MA, 2006, WILDLIFE SOC B, V34, P211, DOI 10.2193/0091-7648(2006)34[211:UADRSA]2.0.CO;2
   Bertelli S, 2002, BIOL J LINN SOC, V77, P423, DOI 10.1046/j.1095-8312.2002.00112.x
   Brandes TS, 2008, BIRD CONSERV INT, V18, pS163, DOI 10.1017/S0959270908000415
   de Juana E, 2020, BIRDS OF THE WORLD, DOI [10.2173/bow.blctin1.01, DOI 10.2173/BOW.BLCTIN1.01]
   Ding J, 2016, IEEE GEOSCI REMOTE S, V13, P364, DOI 10.1109/LGRS.2015.2513754
   dos Anjos L, 2006, BIOTROPICA, V38, P229, DOI 10.1111/j.1744-7429.2006.00122.x
   eBird, 2017, EBIRD ONL DAT BIRD D
   Fink D. T., 2020, EBIRD STATUS TRENDS EBIRD STATUS TRENDS, DOI [10.2173/ebirdst.2018, DOI 10.2173/EBIRDST.2018]
   Guerta RS, 2014, ORNITOL NEOTROP, V25, P73
   Hussein H, 2017, WORKING NOTES CLEF
   Joly A., 2019, WORKING NOTES CLEF 2, V2380
   Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
   Kotsiantis SB, 2007, FRONT ARTIF INTEL AP, V160, P3
   LANDAU HJ, 1967, PR INST ELECTR ELECT, V55, P1701, DOI 10.1109/PROC.1967.5962
   Larsen TH, 2006, COLEOPTS BULL, V60, P315, DOI 10.1649/0010-065X(2006)60[315:ETAHSB]2.0.CO;2
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Nogueira F., 2014, BAYESIAN OPTIMIZATIO
   OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
   Perez-Granados C, 2020, BIOTROPICA, V52, P165, DOI 10.1111/btp.12742
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Reich BJ, 2018, METHODS ECOL EVOL, V9, P1626, DOI 10.1111/2041-210X.13002
   Roncal CM, 2019, J FIELD ORNITHOL, V90, P203, DOI 10.1111/jofo.12299
   Royle JA, 2003, ECOLOGY, V84, P777, DOI 10.1890/0012-9658(2003)084[0777:EAFRPA]2.0.CO;2
   Dias LCS, 2016, WILSON J ORNITHOL, V128, P885
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
   Sullivan BL, 2014, BIOL CONSERV, V169, P31, DOI 10.1016/j.biocon.2013.11.003
   Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006
   Thornton DH, 2012, ORYX, V46, P567, DOI 10.1017/S0030605311001451
NR 29
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD OCT
PY 2021
VL 11
IS 19
BP 13518
EP 13531
DI 10.1002/ece3.8078
EA SEP 2021
PG 14
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA WC5VO
UT WOS:000692685400001
PM 34646487
OA Green Submitted, gold, Green Published
DA 2022-02-10
ER

PT J
AU van Pinxteren, BOCM
   Sirianni, G
   Gratton, P
   Despres-Einspenner, ML
   Egas, M
   Kuhl, H
   Lapuente, J
   Meier, AC
   Janmaat, KRL
AF van Pinxteren, Bryndan O. C. M.
   Sirianni, Giulia
   Gratton, Paolo
   Despres-Einspenner, Marie-Lyne
   Egas, Martijn
   Kuehl, Hjalmar
   Lapuente, Juan
   Meier, Amelia C.
   Janmaat, Karline R. L.
TI Sooty mangabeys scavenge on nuts cracked by chimpanzees and red river
   hogsAn investigation of inter-specific interactions around tropical nut
   trees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE auditory cues; community ecology; interspecific interactions;
   nut-cracking; scavenging; tropics
ID TAI-NATIONAL-PARK; RAIN-FOREST MAMMALS; PREDATION-RISK; TOOL USE;
   HUNTING BEHAVIOR; SPATIAL MEMORY; VIGILANCE; HAMMERS; AGGREGATION;
   MACAQUES
AB Carrion scavenging is a well-studied phenomenon, but virtually nothing is known about scavenging on plant material, especially on remnants of cracked nuts. Just like meat, the insides of hard-shelled nuts are high in energetic value, and both foods are difficult to acquire. In the Tai forest, chimpanzees (Pan troglodytes) and red river hogs (Potamochoerus porcus) crack nuts by using tools or strong jaws, respectively. In this study, previously collected non-invasive camera trap data were used to investigate scavenging by sooty mangabeys (Cercocebus atys), two species of Guinea fowl (Agelestres meleagrides; Guttera verreauxi), and squirrels (Scrunidae spp.) on the nut remnants cracked by chimpanzees and red river hogs. We investigated how scavengers located nut remnants, by analyzing their visiting behavior in relation to known nut-cracking events. Furthermore, since mangabeys are infrequently preyed upon by chimpanzees, we investigated whether they perceive an increase in predation risk when approaching nut remnants. In total, 190 nut-cracking events were observed in four different areas of Tai National Park, Ivory Coast. We could confirm that mangabeys scavenged on the nuts cracked by chimpanzees and hogs and that this enabled them to access food source that would not be accessible otherwise. We furthermore found that mangabeys, but not the other species, were more likely to visit nut-cracking sites after nut-cracking activities than before, and discuss the potential strategies that the monkeys could have used to locate nut remnants. In addition, mangabeys showed elevated levels of vigilance at the chimpanzee nut-cracking sites compared with other foraging sites, suggesting that they perceived elevated danger at these sites. Scavenging on remnants of cracked nuts is a hitherto understudied type of foraging behavior that could be widespread in nature and increases the complexity of community ecology in tropical rainforests.
   By use of camera trap videos it was confirmed that mangabeys scavenge on the nut remnants cracked by chimpanzees and red river hogs. Squirrels and two types of guinea fowl might scavenge on these nut remnants but this could not be seen clearly. Looking at the visitation rate before and after nut cracking events it was found that the possible scavenging species were more present at the nut cracking sites after a nut cracking event took place. It was found that mangabeys have an increase in vigilance behavior at chimpanzee nut cracking sites compared with outside these chimpanzee nut cracking sites, indicating that the mangabeys perceive a higher risk at these nut cracking sites.
C1 [van Pinxteren, Bryndan O. C. M.; Egas, Martijn; Janmaat, Karline R. L.] Univ Amsterdam, Dept Evolutionary & Populat Biol, Inst Biodivers & Ecosyst Dynam, Sci Pk 904, NL-1090 GE Amsterdam, Netherlands.
   [Sirianni, Giulia; Gratton, Paolo; Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar; Lapuente, Juan; Meier, Amelia C.; Janmaat, Karline R. L.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Kuehl, Hjalmar] Halle Jena Leipzig, German Ctr Integrat Biodivers Res, Leipzig, Germany.
   [Meier, Amelia C.] Duke Univ, Nicholas Sch Environm, Durham, NC 27708 USA.
RP van Pinxteren, BOCM (corresponding author), Univ Amsterdam, Dept Evolutionary & Populat Biol, Inst Biodivers & Ecosyst Dynam, Sci Pk 904, NL-1090 GE Amsterdam, Netherlands.
EM bocmvpinxteren@gmail.com
RI Lapuente, Juan/ABG-3912-2021
OI Lapuente, Juan/0000-0002-6783-5585; Sirianni,
   Giulia/0000-0003-4589-0345; Gratton, Paolo/0000-0001-8464-4062
FU Stichting Fonds Doctor Catharine van Tussenbroek; Dobberke Stichting
   voor Vergelijkende Psychology; Stichting Kronendak; Leakey Foundation;
   Lucy Burger Stichting; Koninklijke Nederlandse Akademie van
   Wetenschappen; Wenner-Gren Foundation; Centre for Forest Research
FX Stichting Fonds Doctor Catharine van Tussenbroek; Dobberke Stichting
   voor Vergelijkende Psychology; Stichting Kronendak; The Leakey
   Foundation; Lucy Burger Stichting; Koninklijke Nederlandse Akademie van
   Wetenschappen; Wenner-Gren Foundation; Centre for Forest Research
CR Abernethy K., 1999, WILDLIFE CONSERVATIO, V102, P50
   Atwood TC, 2008, ANIM BEHAV, V75, P753, DOI 10.1016/j.anbehav.2007.08.024
   Ban SD, 2016, ANIM BEHAV, V118, P135, DOI 10.1016/j.anbehav.2016.05.014
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bergmuller R., 1998, THESIS
   Boback SM, 2007, COMP BIOCHEM PHYS A, V148, P651, DOI 10.1016/j.cbpa.2007.08.014
   BOESCH C, 1983, BEHAVIOUR, V83, P265, DOI 10.1163/156853983X00192
   BOESCH C, 1989, AM J PHYS ANTHROPOL, V78, P547, DOI 10.1002/ajpa.1330780410
   BOESCH C, 1990, FOLIA PRIMATOL, V54, P86, DOI 10.1159/000156428
   Boesch C, 2000, CHIMPANZEES TAI FORE
   Bridges A. S., 2011, BEHAV ACTIVITY PATTE
   De Moraes BLC, 2014, AM J PRIMATOL, V76, P967, DOI 10.1002/ajp.22286
   Campos FA, 2014, BEHAV ECOL, V25, P477, DOI 10.1093/beheco/aru005
   Canale GR, 2009, AM J PRIMATOL, V71, P366, DOI 10.1002/ajp.20648
   Cowlishaw G, 1998, BEHAVIOUR, V135, P431, DOI 10.1163/156853998793066203
   Craigie ID, 2010, BIOL CONSERV, V143, P2221, DOI 10.1016/j.biocon.2010.06.007
   del Hoyo J, 2014, HBW BIRDLIFE INT ILL, V1
   Dermody BJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024635
   Desbordes, 2013, PRIMATES WORLD ILLUS
   Despres-Einspenner ML, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22647
   DeVault TL, 2003, OIKOS, V102, P225, DOI 10.1034/j.1600-0706.2003.12378.x
   Dobson A.J., 2008, INTRO GEN LINEAR MOD, Vthird
   Dupuch A, 2014, BEHAV ECOL SOCIOBIOL, V68, P299, DOI 10.1007/s00265-013-1645-z
   Estienne V, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22672
   Estrada A, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1600946
   Favreau FR, 2010, P ROY SOC B-BIOL SCI, V277, P2089, DOI 10.1098/rspb.2009.2337
   Forstmeier W, 2011, BEHAV ECOL SOCIOBIOL, V65, P47, DOI 10.1007/s00265-010-1038-5
   Francis I.S., 1992, Bird Conservation International, V2, P25
   Gessner J, 2014, AFR J ECOL, V52, P59, DOI 10.1111/aje.12084
   Gone Bi Z, 2007, THESIS
   Gumert MD, 2009, AM J PRIMATOL, V71, P594, DOI 10.1002/ajp.20694
   Herbinger I, 2001, INT J PRIMATOL, V22, P143, DOI 10.1023/A:1005663212997
   Hoppe-Dominik B, 2011, AFR J ECOL, V49, P450, DOI 10.1111/j.1365-2028.2011.01277.x
   Houston D.C., 1985, Ornithological Monographs, P856
   Hoyo J. D., 1994, NEW WORLD VULTURES G
   Janmaat KRL, 2006, ANIM BEHAV, V72, P797, DOI 10.1016/j.anbehav.2005.12.009
   Janmaat KRL, 2013, ANIM BEHAV, V86, P1183, DOI 10.1016/j.anbehav.2013.09.021
   Janmaat KRL, 2013, ANIM COGN, V16, P851, DOI 10.1007/s10071-013-0617-z
   Janmaat KRL, 2006, THESIS
   Junker J, 2012, DIVERS DISTRIB, V18, P1077, DOI 10.1111/ddi.12005
   Kalan AK, 2015, ANIM BEHAV, V101, P1, DOI 10.1016/j.anbehav.2014.12.011
   Kouakou CY, 2011, J TROP ECOL, V27, P621, DOI 10.1017/S0266467411000423
   Kuhl HS, 2016, SCI REP-UK, V6, DOI 10.1038/srep22219
   Kuhl HS, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0035610
   Laundre J.W., 2010, OPEN ECOL J, V3, P1
   Laurance WF, 2006, CONSERV BIOL, V20, P1251, DOI 10.1111/j.1523-1739.2006.00420.x
   Leslie David M. Jr., 2015, Mammalian Species, P15, DOI 10.1093/mspecies/sev002
   Leus K., 2013, MAMMALS OF AFRICA, VVI, P35
   LIMA SL, 1985, ANIM BEHAV, V33, P155, DOI 10.1016/S0003-3472(85)80129-9
   Luncz LV, 2017, INT J PRIMATOL, V38, P872, DOI 10.1007/s10764-017-9985-6
   Malbrant R., 1949, MAMMALIAN SPECIES, V47, P15
   Martin P., 2007, MEASURING BEHAV INTR, V3rd
   McGraw W. S., 2007, MONKEYS TAI FOREST A, V51
   McGraw WS, 2011, AM J PHYS ANTHROPOL, V144, P140, DOI 10.1002/ajpa.21402
   Mittermeier R. A., 2013, PRIMATES
   Moupela C, 2014, TROP ECOL, V55, P327
   N'Goran P. K., 2013, International Journal of Innovation and Applied Studies, V3, P326
   Nowak K, 2014, BEHAV ECOL, V25, P1199, DOI 10.1093/beheco/aru110
   Oduro W., 1989, THESIS
   Periquet S, 2012, BEHAV ECOL, V23, P970, DOI 10.1093/beheco/ars060
   R Core Development Team, 2013, R LANG ENV STAT COMP
   Range F, 2004, ETHOLOGY, V110, P301, DOI 10.1111/j.1439-0310.2004.00973.x
   Rosevear D. R., 1969, NATURAL HIST
   Rowe N., 2016, ALL WORLDS PRIMATES
   Shettleworth S.J., 2010, COGNITION EVOLUTION, V2nd Edn
   Shultz S, 2002, P ROY SOC B-BIOL SCI, V269, P1797, DOI 10.1098/rspb.2002.2098
   Sirianni G, 2018, ANIM COGN, V21, P109, DOI 10.1007/s10071-017-1144-0
   Sirianni G, 2015, ANIM BEHAV, V100, P152, DOI 10.1016/j.anbehav.2014.11.022
   Smith AC, 2004, BEHAV ECOL SOCIOBIOL, V56, P18, DOI 10.1007/s00265-003-0753-6
   Taylor C. A., 2015, THESIS
   Treves A, 2001, BEHAV ECOL SOCIOBIOL, V50, P90, DOI 10.1007/s002650100328
   Vanthomme H, 2013, CONSERV BIOL, V27, P281, DOI 10.1111/cobi.12017
   Visalberghi E, 2015, PHILOS T R SOC B, V370, DOI 10.1098/rstb.2014.0351
   Watts DP, 2002, INT J PRIMATOL, V23, P1, DOI 10.1023/A:1013270606320
   WHITE LJT, 1994, J ANIM ECOL, V63, P499, DOI 10.2307/5217
   Wikenros C, 2014, J MAMMAL, V95, P862, DOI 10.1644/13-MAMM-A-125
   Wittiger L, 2013, BEHAV ECOL SOCIOBIOL, V67, P1097, DOI 10.1007/s00265-013-1534-5
   Zuberbuehler Klaus, 2010, P64
NR 78
TC 1
Z9 1
U1 0
U2 5
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD AUG
PY 2018
VL 80
IS 8
AR e22895
DI 10.1002/ajp.22895
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Zoology
GA GT7AN
UT WOS:000444672400007
PM 30024029
OA Green Published, hybrid
DA 2022-02-10
ER

PT J
AU Guo, YH
   Rothfus, TA
   Ashour, AS
   Si, L
   Du, CL
   Ting, TF
AF Guo, Yanhui
   Rothfus, Thomas A.
   Ashour, Amira S.
   Si, Lei
   Du, Chunlai
   Ting, Tih-Fen
TI Varied channels region proposal and classification network for wildlife
   image classification under complex environment
SO IET IMAGE PROCESSING
LA English
DT Article
DE feature extraction; object recognition; object detection; learning
   (artificial intelligence); neural nets; image segmentation; image
   classification; convolution; cameras; varied channels region proposal;
   classification network; wildlife image classification; deep
   convolutional neural network; automatic wildlife animal classification;
   camera trapped images; different aims; background images; region
   proposal component; region candidates; classification component;
   animals; potential animal regions; low contrast animal images; object
   detection network; faster region convolutional neural network
AB A varied channels region proposal and classification network (VCRPCN) is developed based on a deep convolutional neural network (DCNN) and the characteristics of the animals appearing for automatic wildlife animal classification in camera trapped images, the architecture of the network is improved by feeding different channels into different components of the network to accomplish different aims, i.e. the animal images and their background images are employed in the region proposal component to extract region candidates for the animal's location, and the animal images combined with the region candidates are fed into the classification component to identify their categories. This novel architecture considers changes to the image due to the animals' appearances, and identifies potential animal regions in images and extracts their local features to describe and classify them. Five hundred low contrast animal images have been collected. All images have low contrast due to being acquired during the night. Cross-validation is employed to statistically measure the performance of the proposed algorithm. The experimental results demonstrate that in comparison with the well-known object detection network, faster R-CNN, the proposed VCRPCN achieved higher accuracy with the same dataset and training configuration with an average accuracy improvement of 21%.
C1 [Guo, Yanhui; Si, Lei] Univ Illinois, Dept Comp Sci, Springfield, IL 62703 USA.
   [Rothfus, Thomas A.] Univ Illinois, Therkildsen Field Stn Emiquon, Springfield, IL USA.
   [Rothfus, Thomas A.; Ting, Tih-Fen] Univ Illinois, Dept Environm Studies, Springfield, IL USA.
   [Ashour, Amira S.] Tanta Univ, Dept Elect & Elect Commun Engn, Tanta, Egypt.
   [Du, Chunlai] North China Univ Technol, Sch Comp Sci, Beijing, Peoples R China.
RP Guo, YH (corresponding author), Univ Illinois, Dept Comp Sci, Springfield, IL 62703 USA.
EM yguo56@uis.edu
RI ; Guo, Yanhui/L-3267-2013
OI Si, Lei/0000-0002-1886-0362; Guo, Yanhui/0000-0003-1814-9682; Ashour,
   Amira/0000-0003-3217-6185
CR Borji A, 2014, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2014.22
   Bridle J. S., 1990, NEUROCOMPUTING, V4, P227, DOI DOI 10.1007/978-3-642-76153-9_28
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
   Darrell, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Kays R., 2010, INT J RES REV WIREL, V1, P19
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI [10.1007/s11263-019-01204-1, 10.1007/978-3-030-01264-9_45]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Matuska S, 2016, RADIOENGINEERING, V25, P161, DOI 10.13164/re.2016.0161
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   REDMON J, 2016, PROC CVPR IEEE, P779, DOI DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vitousek PM, 1997, SCIENCE, V277, P494, DOI 10.1126/science.277.5325.494
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Yu X., 2013, J IMAGE VIDEO PROCES, P561
   Zhang S, 2015, IEEE SENS J, V15, P2679, DOI 10.1109/JSEN.2014.2382174
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
NR 32
TC 2
Z9 2
U1 5
U2 23
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1751-9659
EI 1751-9667
J9 IET IMAGE PROCESS
JI IET Image Process.
PD MAR 27
PY 2020
VL 14
IS 4
BP 585
EP 591
DI 10.1049/iet-ipr.2019.1042
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA KW1WN
UT WOS:000520961700001
DA 2022-02-10
ER

PT J
AU Molyneux, J
   Pavey, CR
   James, AI
   Carthew, SM
AF Molyneux, J.
   Pavey, C. R.
   James, A. I.
   Carthew, S. M.
TI The efficacy of monitoring techniques for detecting small mammals and
   reptiles in arid environments
SO WILDLIFE RESEARCH
LA English
DT Article
ID OCCUPANCY-ABUNDANCE RELATIONSHIP; ESTIMATING SITE OCCUPANCY; FIRE-DRIVEN
   SUCCESSION; ULURU-NATIONAL-PARK; CAMERA-TRAPS; POPULATION-DYNAMICS;
   DASYCERCUS-BLYTHI; CENTRAL AUSTRALIA; DIGITAL CAMERAS; RAINFALL
AB Context. Accurate surveying and monitoring of biodiversity provides essential baseline data for developing and implementing effective environmental management strategies. Land managers in arid zones face the challenge of managing vast, remote landscapes that support numerous cryptic species that are difficult to detect and monitor. Although researchers and land managers are using an increasingly wider variety of monitoring techniques to detect and monitor species, little is known of the relative effectiveness and comparative costs of these techniques.
   Aims. The present study simultaneously assessed the efficacy of three popular monitoring techniques utilised in the spinifex sand plains of arid Australia, namely, live trapping, sign surveys and passive infrared (PIR)-camera trapping.
   Methods. We explored variations in capture rates and species richness for each technique and compared initial and on-going costs of the techniques over time.
   Key results. Sign surveys detected the greatest number of species and groups overall. Detectability of small mammals and reptiles, as a target group, was greater using PIR cameras, although the probability of detection by each technique varied among specific species. PIR cameras were initially the most expensive technique; however, the low ongoing costs of maintaining cameras in the field meant that they became the most cost effective after eight survey periods.
   Conclusions. Each of the techniques tested here showed biases towards the detection of specific groups or species in the spinifex sand-plain habitat of Australia. Regardless, PIR cameras performed better at detecting the greatest diversity of target species and financially over time.
   Implications. To accurately survey species across vast areas and climate variations, studies often extend over long time periods. Many long-term studies would be likely to benefit financially from the increased deployment of PIR cameras alongside or in place of live trapping surveys, with little impact on the ability to monitor the presence of most species in the region.
C1 [Molyneux, J.; Carthew, S. M.] Charles Darwin Univ, Res Inst Environm & Livelihoods, Darwin, NT 0909, Australia.
   [Pavey, C. R.] CSIRO, Land & Water, PMB 44, Winnellie, NT 0822, Australia.
   [James, A. I.] Australian Wildlife Conservancy, Mornington, WA 6221, Australia.
RP Molyneux, J (corresponding author), Charles Darwin Univ, Res Inst Environm & Livelihoods, Darwin, NT 0909, Australia.
EM jmolyneux.ecology@gmail.com
RI Pavey, Chris/AAT-6315-2020; Pavey, Chris/D-7209-2011
OI Pavey, Chris/0000-0003-2162-8019; 
FU Margaret Middleton Fund; ANZ Trustees Foundation; Australian Geographic
   Society; Northern Territory Government; Schultz Foundation; Wildlife
   Preservation of Australia; Norman Wettenhall Foundation; Charles Darwin
   University Ethics Committee [A12024]
FX This research was funded by the Margaret Middleton Fund, ANZ Trustees
   Foundation, Australian Geographic Society, Northern Territory
   Government, Schultz Foundation, Wildlife Preservation of Australia and
   the Norman Wettenhall Foundation. The Australian Wildlife Conservancy
   and in particular Newhaven managers, Danae Moore and Josef Schofield, is
   thanked for their invaluable support and guidance in the field. We also
   thank Greg Fyfe, Peter Nunn and Anthony Molyneux for their expertise in
   identifying species in PIR camera photos and the numerous volunteers who
   aided in field surveys. All survey procedures were approved by the
   Charles Darwin University Ethics Committee (approval no. A12024).
CR Alagaili AN, 2014, MAMM BIOL, V79, P195, DOI 10.1016/j.mambio.2013.10.001
   Anderson K. A., 2002, MODEL SELECTION MULT
   Australian Wildlife Conservancy [AWC], 2016, ANN FAUN TRAPP NEWH
   Baillie J.E.M., 2004, IUCN RED LIST THREAT
   Barea-Azcon JM, 2007, BIODIVERS CONSERV, V16, P1213, DOI 10.1007/s10531-006-9114-x
   Bennison K, 2014, AUST MAMMAL, V36, P184, DOI 10.1071/AM13015
   Benshemesh J, 2014, J MAMMAL, V95, P1054, DOI 10.1644/14-MAMM-A-051
   Boitani L, 2000, RES TECHNIQUES ANIMA
   Bolker B, 2012, GLMMADMB GEN LINEAR
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Catling PC, 1997, WILDLIFE RES, V24, P417, DOI 10.1071/WR96073
   Charles Darwin University [CDU], 2014, SAL GUID OFF HUM RES
   Davies MJ, 2014, AUST MAMMAL, V36, P103, DOI 10.1071/AM13017
   De Bondi N, 2010, WILDLIFE RES, V37, P456, DOI 10.1071/WR10046
   Department of National Parks Sport and Racing, 2016, CAMP FEES
   Dickman CR, 2011, J MAMMAL, V92, P1193, DOI 10.1644/10-MAMM-S-329.1
   Dickman CR, 2010, J MAMMAL, V91, P798, DOI 10.1644/09-MAMM-S-205.1
   Dickman CR, 1999, WILDLIFE RES, V26, P389, DOI 10.1071/WR97057
   Dickman CR, 2001, WILDLIFE RES, V28, P493, DOI 10.1071/WR00023
   Fairfax RJ, 2014, CURR ISSUES TOUR, V17, P72, DOI 10.1080/13683500.2012.714749
   Fancourt BA, 2014, AUST MAMMAL, V36, P247, DOI 10.1071/AM14004
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Garden JG, 2007, WILDLIFE RES, V34, P218, DOI 10.1071/WR06111
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Greenville AC, 2014, OECOLOGIA, V175, P1349, DOI 10.1007/s00442-014-2977-8
   Greenville AC, 2013, AUSTRAL ECOL, V38, P754, DOI 10.1111/aec.12033
   Greenville AC, 2012, ECOL EVOL, V2, P2645, DOI 10.1002/ece3.377
   Halls Creek Travel & Tourism, 2016, ACC E KIMB
   Hamel S, 2013, METHODS ECOL EVOL, V4, P105, DOI 10.1111/j.2041-210x.2012.00262.x
   HECK KL, 1975, ECOLOGY, V56, P1459, DOI 10.2307/1934716
   Henderson, 2000, ECOLOGICAL METHODS
   Hothorn T, 2008, BIOMETRICAL J, V50, P346, DOI 10.1002/bimj.200810425
   Hui C, 2009, ECOL APPL, V19, P2038, DOI 10.1890/08-2236.1
   Ieno, 2012, ZERO INFLATED MODELS
   Jones Clyde, 1996, P115
   Karanth KU, 2011, J APPL ECOL, V48, P1048, DOI 10.1111/j.1365-2664.2011.02002.x
   Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
   KENDALL KC, 1992, ECOL APPL, V2, P422, DOI 10.2307/1941877
   Letnic M, 2011, J MAMMAL, V92, P1210, DOI 10.1644/10-MAMM-S-229.1
   Lyra-Jorge MC, 2008, EUR J WILDLIFE RES, V54, P739, DOI 10.1007/s10344-008-0205-8
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   Masters P, 1996, WILDLIFE RES, V23, P39, DOI 10.1071/WR9960039
   MASTERS P, 1993, WILDLIFE RES, V20, P803, DOI 10.1071/WR9930803
   Masters P, 2012, WILDLIFE RES, V39, P419, DOI 10.1071/WR11156
   Masters Pip, 1998, Australian Mammalogy, V20, P403
   McAlpin S, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0019041
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McGrath Tim, 2012, Herpetological Review, V43, P249
   Meek P., 2014, CAMERA TRAPPING WILD
   Milstead WB, 2007, J MAMMAL, V88, P1532, DOI 10.1644/16-MAMM-A-407R.1
   Molyneux J., 2017, FAUNA ASSEMBLAGES SP
   Moseby KE, 2009, TALES SAND GUIDE IDE
   National Health and Medical Research Council, 2015, GUID CAR US AUSTR NA
   NICHOLS JD, 1983, J MAMMAL, V64, P253, DOI 10.2307/1380555
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   Paull DJ, 2011, WILDLIFE RES, V38, P188, DOI 10.1071/WR10203
   Pavey CR, 2008, J MAMMAL, V89, P674, DOI 10.1644/07-MAMM-A-168R.1
   Pavey CR, 2011, AUST J ZOOL, V59, P156, DOI 10.1071/ZO11052
   Perry R. A., 1979, INT BIOL PROGRAMME
   PIANKA ER, 1969, ECOLOGY, V50, P1012, DOI 10.2307/1936893
   Price-Rees SJ, 2013, AUSTRAL ECOL, V38, P493, DOI 10.1111/j.1442-9993.2012.02439.x
   R Core Team, 2015, R LANG ENV STAT COMP
   Scott DM, 2006, BIOL CONSERV, V127, P72, DOI 10.1016/j.biocon.2005.07.014
   Scroggie MP, 2009, J ZOOL, V277, P214, DOI 10.1111/j.1469-7998.2008.00528.x
   Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
   Smith MS, 2008, RANGELAND J, V30, P15, DOI 10.1071/RJ07052
   Southgate R, 2005, WILDLIFE RES, V32, P43, DOI 10.1071/WR03087
   Stanley TR, 2005, J WILDLIFE MANAGE, V69, P874, DOI 10.2193/0022-541X(2005)069[0874:ESOAAU]2.0.CO;2
   STEWART AP, 1979, AUST WILDLIFE RES, V6, P165, DOI 10.1071/WR9790165
   Sun CC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088025
   Thompson GG, 2007, WILDLIFE RES, V34, P491, DOI 10.1071/WR06081
   Thrifty Car Rentals, 2016, CAR HIR
   Watson M., 2007, SURVEY SO MARSUPIAL
   Webb NF, 2012, WILDLIFE SOC B, V36, P240, DOI 10.1002/wsb.140
   Welbourne DJ, 2015, WILDLIFE RES, V42, P414, DOI 10.1071/WR15054
   Wiewel AS, 2007, J MAMMAL, V88, P250, DOI 10.1644/06-MAMM-A-098R1.1
   Zuur Alain F., 2009, P1
NR 78
TC 8
Z9 9
U1 4
U2 38
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
PY 2017
VL 44
IS 6-7
BP 534
EP 545
DI 10.1071/WR17017
PG 12
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA FQ4SS
UT WOS:000418348700010
DA 2022-02-10
ER

PT J
AU Allen, ML
   Farmer, MJ
   Clare, JDJ
   Olson, ER
   Van Stappen, J
   Van Deelen, TR
AF Allen, M. L.
   Farmer, M. J.
   Clare, J. D. J.
   Olson, E. R.
   Van Stappen, J.
   Van Deelen, T. R.
TI Is there anybody out there? Occupancy of the carnivore guild in a
   temperate archipelago
SO COMMUNITY ECOLOGY
LA English
DT Article
DE Apostle Islands; Camera traps; Carnivore; Community; Distribution;
   Island biogeography; Occupancy; Species richness
ID RANGE CONTRACTIONS; PREDATION RISK; ICE COVER; WOLVES; BEHAVIOR;
   ECOLOGY; MARKING; CATS; LAKE; FEAR
AB Carnivores are important components of ecological communities with wide-ranging effects that vary with carnivore size, natural history, and hunting tactics. Researchers and managers should strive to understand both the presence and distribution of carnivores within their local environment. We studied the carnivore guild in the Apostle Islands, where the distribution and occupancy of carnivores was largely unknown. We monitored 19 islands with 160 functioning camera traps from 2014-2017, from which we collected 203,385 photographs across 49,280 trap nights. We documented 7,291 total wildlife events with 1,970 carnivore events, and detected 10 of the 12 terrestrial carnivores found in Wisconsin. Detection rates for species were generally higher in summer than winter, except for coyotes (Canis latrans) and red foxes (Vulpes vulpes). Finite-sample occupancy estimates for carnivores varied across islands, with mean estimated occupancy across islands varying from a high of 0.73 for black bears to a low of 0.21 for gray wolves. Of the potential island biogeography explanatory variables for carnivore occupancy we considered, island size was the most important, followed by distance to the mainland, and then inter-island distance. We estimated that terrestrial carnivore species varied in the number of islands they were detected on from 1 island for gray wolves to 13 islands for black bears. Estimated carnivore richness across islands (i.e., the number of carnivores occupying an island) also varied substantively from 1 species on Michigan Island to 10 species on Stockton Island. Island size and connectivity between islands appear important for the persistence of the carnivore community in the Apostle Islands.
C1 [Allen, M. L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Farmer, M. J.; Clare, J. D. J.; Van Deelen, T. R.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Olson, E. R.] Northland Coll, Nat Resources, 1411 Ellis Ave S, Ashland, WI 54806 USA.
   [Van Stappen, J.] Apostle Isl Natl Lakeshore, Planning & Resource Management, 415 Washington Ave, Bayfield, WI 54814 USA.
RP Farmer, MJ (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM mjmorales@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Apostle Islands National Lakeshore (GLNF CESU) [P14AC01180]; Northland
   College (Department of Natural Resources); Northland College (Sigurd
   Olson Professorship in the Natural Sciences); Northland College (Morris
   O. Ristvedt Professorship in the Natural Sciences); NASA Earth and Space
   Science Fellowship [NNX16AO61H]; University of Wisconsin (Schorger fund,
   Department of Forest and Wildlife Ecology); University of Wisconsin
   (Beers-Bascom Professorship in Conservation)
FX This project was supported by the Apostle Islands National Lakeshore
   (GLNF CESU Agreement P14AC01180), Northland College (Department of
   Natural Resources; Sigurd Olson Professorship in the Natural Sciences;
   Morris O. Ristvedt Professorship in the Natural Sciences), NASA Earth
   and Space Science Fellowship (Grant number NNX16AO61H), and the
   University of Wisconsin (Schorger fund, Department of Forest and
   Wildlife Ecology; Beers-Bascom Professorship in Conservation). We thank
   the personnel from each group that contributed to this project,
   especially APIS staff and volunteers, graduate students from the Van
   Deelen lab at the University of Wisconsin - Madison, and students from
   Northland College. The support and cooperative spirit between these
   three groups was key to the success of this project. The authors declare
   that they have no conflict of interest.)
CR Adams JR, 2011, P ROY SOC B-BIOL SCI, V278, P3336, DOI 10.1098/rspb.2011.0261
   Allen ML, 2018, AM MIDL NAT, V179, P294
   Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Allen ML, 2016, ECOLOGY, V97, P1905, DOI 10.1002/ecy.1462
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Allen ML, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102257
   Altendorf KB, 2001, J MAMMAL, V82, P430, DOI 10.1644/1545-1542(2001)082&lt;0430:AEOPRO&gt;2.0.CO;2
   Anderson M. K., 2003, Ecological Restoration, V21, P269, DOI 10.3368/er.21.4.269
   [Anonymous], 2018, LIST OF MAMM
   Atwood TC, 2007, J WILDLIFE MANAGE, V71, P1098, DOI 10.2193/2006-102
   Belant JL, 2005, URSUS, V16, P85, DOI 10.2192/1537-6176(2005)016[0085:ABBPSA]2.0.CO;2
   Berger J., 2013, LARGE CARNIVORES CON
   Black KM, 2019, J WILDLIFE MANAGE, V83, P158, DOI 10.1002/jwmg.21556
   Brown JS, 1999, J MAMMAL, V80, P385, DOI 10.2307/1383287
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Busch J. C., 2008, HIST RESOURCE STUDY
   Cederholm CJ, 1999, FISHERIES, V24, P6, DOI 10.1577/1548-8446(1999)024&lt;0006:PSC&gt;2.0.CO;2
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Courchamp F, 1999, J ANIM ECOL, V68, P282, DOI 10.1046/j.1365-2656.1999.00285.x
   CRAVEN S R, 1987, Colonial Waterbirds, V10, P64, DOI 10.2307/1521232
   Efford MG, 2012, ECOSPHERE, V3, DOI 10.1890/ES11-00308.1
   Estes JA, 1996, WILDLIFE SOC B, V24, P390
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Gelman A., 1992, STAT SCI, V7, P457, DOI [10.1214/ss/1177011136., 10.1214/ss/1177011136, DOI 10.1214/SS/1177011136]
   Gelman A, 2008, ANN APPL STAT, V2, P1360, DOI 10.1214/08-AOAS191
   Harmsen BJ, 2010, J MAMMAL, V91, P1225, DOI 10.1644/09-MAMM-A-416.1
   Harrison RL, 2015, WEST N AM NATURALIST, V75, P387, DOI 10.3398/064.075.0409
   Howk F, 2009, J GREAT LAKES RES, V35, P159, DOI 10.1016/j.jglr.2008.11.002
   HUNTER MD, 1992, ECOLOGY, V73, P724
   Jackson H. H. T., 1920, Journal of Mammalogy, V1, DOI 10.2307/1373741
   Judziewicz E.J., 1993, MICHIGAN BOT, V32, P43
   Kellert, 1997, VALUE LIFE BIOL DIVE
   Kellner K., 2015, JAGSUI WRAPPER RJAGS
   Krofel M, 2012, BEHAV ECOL SOCIOBIOL, V66, P1297, DOI 10.1007/s00265-012-1384-6
   Laliberte AS, 2004, BIOSCIENCE, V54, P123, DOI 10.1641/0006-3568(2004)054[0123:RCONAC]2.0.CO;2
   Lariviere Serge, 2001, Mammalian Species, V647, P1, DOI 10.1644/1545-1410(2001)647<0001:UA>2.0.CO;2
   Lesmeister DB, 2015, WILDLIFE MONOGR, V191, P1, DOI 10.1002/wmon.1015
   Licht Daniel S., 2015, Canadian Field-Naturalist, V129, P139
   Lomolino M.V., 1988, P185
   MAC ARTHUR ROBERT H., 1967
   MacKenzie D. I., 2006, OCCUPANCY ESTIMATION
   Magnuson JJ, 2000, SCIENCE, V289, P1743, DOI 10.1126/science.289.5485.1743
   Mallick B., 1998, SANKHYA, V60, P65, DOI DOI 10.1186/1471-2105-12-186
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   National Centers for Environmental Information (NCEI), CLIM DAT ONL DAT DIS
   Newsome TM, 2016, MAMMAL REV, V46, P255, DOI 10.1111/mam.12067
   O'Hara RB, 2009, BAYESIAN ANAL, V4, P85, DOI 10.1214/09-BA403
   Okello MM, 2008, TOURISM MANAGE, V29, P751, DOI 10.1016/j.tourman.2007.08.003
   Pauli JN, 2005, NORTHEAST NAT, V12, P245, DOI 10.1656/1092-6194(2005)012[0245:EFLSCI]2.0.CO;2
   Plummer M., 2003, P 3 INT WORKSH DISTR, P125
   Popescu VD, 2014, ECOL EVOL, V4, P933, DOI 10.1002/ece3.997
   Prugh LR, 2009, BIOSCIENCE, V59, P779, DOI 10.1525/bio.2009.59.9.9
   R Core Team, 2018, STATS PACK LANG ENV
   Rich LN, 2017, J ZOOL, V303, P90, DOI 10.1111/jzo.12470
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Ripple WJ, 2004, BIOSCIENCE, V54, P755, DOI 10.1641/0006-3568(2004)054[0755:WATEOF]2.0.CO;2
   Sivy KJ, 2017, AM NAT, V190, P663, DOI 10.1086/693996
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wilson EO, 2010, THEORY OF ISLAND BIOGEOGRAPHY REVISITED, P1
   Wilton CM, 2015, URSUS, V26, P53, DOI 10.2192/URSUS-D-15-00008.1
   Wolf C, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.170052
NR 61
TC 8
Z9 8
U1 0
U2 21
PU AKADEMIAI KIADO ZRT
PI BUDAPEST
PA BUDAFOKI UT 187-189-A-3, H-1117 BUDAPEST, HUNGARY
SN 1585-8553
EI 1588-2756
J9 COMMUNITY ECOL
JI Community Ecol.
PD DEC
PY 2018
VL 19
IS 3
BP 272
EP 280
DI 10.1556/168.2018.19.3.8
PG 9
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HF7XL
UT WOS:000454455200008
OA Green Accepted
DA 2022-02-10
ER

PT J
AU Lennon, RJ
   Peach, WJ
   Dunn, JC
   Shore, RF
   Pereira, MG
   Sleep, D
   Dodd, S
   Wheatley, CJ
   Arnold, KE
   Brown, CD
AF Lennon, Rosie J.
   Peach, Will J.
   Dunn, Jenny C.
   Shore, Richard F.
   Pereira, M. Gloria
   Sleep, Darren
   Dodd, Steve
   Wheatley, Christopher J.
   Arnold, Kathryn E.
   Brown, Colin D.
TI From seeds to plasma: Confirmed exposure of multiple farmland bird
   species to clothianidin during sowing of winter cereals
SO SCIENCE OF THE TOTAL ENVIRONMENT
LA English
DT Article
DE Agriculture; Insecticide; Clothianidin; Systemic; Sub-lethal effects;
   Ecological monitoring
ID PITUITARY-THYROID AXIS; NEONICOTINOID INSECTICIDES; ENVIRONMENTAL RISKS;
   POTENTIAL EXPOSURE; WILDLIFE BIRD; TREATED SEED; PESTICIDE; RESIDUES;
   DITHIOCARBAMATE; AVAILABILITY
AB Neonicotinoids are the largest group of systemic insecticides worldwide and are most commonly applied as agricultural seed treatments. However, little is known about the extent to which farmland birds are exposed to these compounds during standard agricultural practices. This study uses winter cereal, treated with the neonicotinoid clothianidin, as a test system to examine patterns of exposure in farmland birds during a typical sowing period. The availability of neonicotinoid-treated seed was recorded post-sowing at 39 fields (25 farms), and camera traps were used to monitor seed consumption by wild birds in situ. The concentration of clothianidin in treated seeds and crop seedlings was measured via liquid chromatography-tandem mass spectrometry, and avian blood samples were collected from 11 species of farmland bird from a further six capture sites to quantify the prevalence and level of clothianidin exposure associated with seed treatments. Neonicotinoid-treated seeds were found on the soil surface at all but one of the fields surveyed at an average density of 2.8 seeds/m(2). The concentration of clothianidin in seeds varied around the target application rate, whilst crop seedlings contained on average 5.9% of the clothianidin measured in seeds. Exposure was confirmed in 32% of bird species observed in treated fields and 50% of individual birds post-sowing; the median concentration recorded in positive samples was 12 ng/mL. Results here provide clear evidence that a variety of farmland birds are subject to neonicotinoid exposure following normal agricultural sowing of neonicotinoid-treated cereal seed. Furthermore, the widespread availability of seeds at the soil surface was identified as a primary source of exposure. Overall, these data are likely to have global implications for bird species and current agricultural policies where neonicotinoids are in use, and may be pertinent to any future risk assessments for systemic insecticide seed treatments. Crown Copyright (C) 2020 Published by Elsevier B.V.
C1 [Lennon, Rosie J.; Wheatley, Christopher J.; Arnold, Kathryn E.; Brown, Colin D.] Univ York, Dept Environm & Geog, York, N Yorkshire, England.
   [Peach, Will J.; Dodd, Steve] Royal Soc Protect Birds, Sandy, Beds, England.
   [Dunn, Jenny C.] Univ Lincoln, Sch Life Sci, Joseph Banks Labs, Lincoln, England.
   [Shore, Richard F.; Pereira, M. Gloria; Sleep, Darren] Lancaster Environm Ctr, UK Ctr Ecol & Hydrol, Lancaster, England.
   [Wheatley, Christopher J.] Univ York, Dept Biol, York, N Yorkshire, England.
RP Lennon, RJ (corresponding author), Univ York, Dept Environm & Geog, York, N Yorkshire, England.
EM rjl529@york.ac.uk
OI Wheatley, Christopher/0000-0002-8550-2450; Dunn,
   Jenny/0000-0002-6277-2781
FU Natural Environment Research Council (NERC) as part of the ACCE Doctoral
   Training PartnershipUK Research & Innovation (UKRI)Natural Environment
   Research Council (NERC) [NE/L002450/1]; Royal Society for the Protection
   of Birds (RSPB)
FX The authors thank all landowners who gave permission for data to be
   collected; RSPB staff and Isobel Wright (University of Lincoln) who
   facilitated site access; Kerry Skelhorn, Will Kirby, Andy Bradbury and
   Derek Gruar of the RSPB who collected field samples in East Anglia,
   alongside SD and JCD; Nigel Butcher who helped with camera trap
   development; and Stuart Britton and Vivien Hartwell who facilitated
   sample collection in Lincolnshire. Research was funded by the Natural
   Environment Research Council (NERC; https://nerc.ukri.org/) as part of
   the ACCE (https://acce.shef.ac.uk/) Doctoral Training Partnership (grant
   number NE/L002450/1), and the Royal Society for the Protection of Birds
   (RSPB; https://www.rspb.org.uk/) who acted as a CASE partner for this
   project.
CR Abu Zeid EH, 2019, ECOTOX ENVIRON SAFE, V167, P60, DOI 10.1016/j.ecoenv.2018.09.121
   Addy-Orduna LM, 2018, SCI TOTAL ENVIRON, V10, P1216
   Alford A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173836
   Balfour NJ, 2016, AGR ECOSYST ENVIRON, V215, P85, DOI 10.1016/j.agee.2015.09.020
   Bass C., 2018, CURR BIOL, pR761
   Bayer Crop Science UK, 2019, REDG DET LAB SEED TA
   Bean TG, 2019, ENVIRON SCI TECHNOL, V53, P3888, DOI 10.1021/acs.est.8b07062
   Boatman ND, 2004, IBIS, V146, P131, DOI 10.1111/j.1474-919X.2004.00347.x
   Botias C, 2016, SCI TOTAL ENVIRON, V566, P269, DOI 10.1016/j.scitotenv.2016.05.065
   Bro E, 2016, ENVIRON SCI POLLUT R, V23, P9559, DOI 10.1007/s11356-016-6093-7
   Brooks ME, 2017, R J, V9, P378, DOI 10.32614/RJ-2017-066
   Byholm P, 2018, SCI TOTAL ENVIRON, V639, P929, DOI 10.1016/j.scitotenv.2018.05.185
   Cox C., 2001, Journal of Pesticide Reform, V21, P15
   de Snoo GR, 2004, PEST MANAG SCI, V60, P501, DOI 10.1002/ps.824
   Donald PF, 2001, P ROY SOC B-BIOL SCI, V268, P25, DOI 10.1098/rspb.2000.1325
   Eng ML, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-15446-x
   Ertl HM, 2018, WILDLIFE SOC B, V42, P649, DOI 10.1002/wsb.921
   European Food Safety Authority, 2006, IN RISK ASS PROV RAP, V3, P793
   European Food Safety Authority, 2010, EFSA J, V7
   Goulson D, 2013, J APPL ECOL, V50, P977, DOI 10.1111/1365-2664.12111
   Hao CY, 2018, SCI TOTAL ENVIRON, V644, P1080, DOI 10.1016/j.scitotenv.2018.06.317
   Holland JM, 2006, ANN APPL BIOL, V148, P49, DOI 10.1111/j.1744-7348.2006.00039.x
   Humann-Guilleminot S, 2019, J APPL ECOL, V56, P1502, DOI 10.1111/1365-2664.13392
   Humann-Guilleminot S, 2019, SCI TOTAL ENVIRON, V660, P1091, DOI 10.1016/j.scitotenv.2019.01.068
   Li Y, 2018, ENVIRON SCI POLLUT R, V25, P31318, DOI 10.1007/s11356-018-3121-9
   Li Y, 2018, ECOTOX ENVIRON SAFE, V164, P690, DOI 10.1016/j.ecoenv.2018.08.082
   Lopez-Antia A, 2016, J APPL ECOL, V53, P1373, DOI 10.1111/1365-2664.12668
   Lopez-Antia A, 2013, ECOTOXICOLOGY, V22, P125, DOI 10.1007/s10646-012-1009-x
   Magnusson B., 2012, HDB CALCULATION MEAS
   McGee S, 2018, PEERJ, V6, DOI 10.7717/peerj.5880
   Meier U., 1997, GROWTH STAGES MONO D
   Met Office, 2012, MET OFF INT DAT ARCH
   Millot F, 2017, ENVIRON SCI POLLUT R, V24, P5469, DOI 10.1007/s11356-016-8272-y
   Mineau P., 2013, IMPACT NATIONS MOSTW
   Mohanty B, 2017, REPROD TOXICOL, V71, P32, DOI 10.1016/j.reprotox.2017.04.006
   Newton, 1979, POPULATION ECOLOGY R
   Pandey SP, 2017, NEUROTOXICOLOGY, V60, P16, DOI 10.1016/j.neuro.2017.02.010
   Pandey SP, 2015, CHEMOSPHERE, V122, P227, DOI 10.1016/j.chemosphere.2014.11.061
   Pietravalle, 2013, PESTICIDE USAGE SURV
   Pisa LW, 2015, ENVIRON SCI POLLUT R, V22, P68, DOI 10.1007/s11356-014-3471-x
   Prosser P, 2005, ECOTOXICOLOGY, V14, P679, DOI 10.1007/s10646-005-0018-4
   Prosser P., 2001, PROJECT PN0907 POTEN
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Radolinski J, 2019, CHEMOSPHERE, V222, P445, DOI 10.1016/j.chemosphere.2019.01.150
   Radolinski J, 2018, SCI TOTAL ENVIRON, V618, P561, DOI 10.1016/j.scitotenv.2017.11.031
   Rawi S., 2019, METAB BRAIN DIS, P1
   Redfern CP., 2001, RINGERS MANUAL
   Robinson RA, 2005, BTO RES REPORT 407
   Roy CL, 2019, SCI TOTAL ENVIRON, V682, P271, DOI 10.1016/j.scitotenv.2019.05.010
   Simon-Delso N, 2015, ENVIRON SCI POLLUT R, V22, P5, DOI 10.1007/s11356-014-3470-y
   Siriwardena GM, 2008, IBIS, V150, P585, DOI 10.1111/j.1474-919X.2008.00828.x
   Stanton RL, 2018, AGR ECOSYST ENVIRON, V254, P244, DOI 10.1016/j.agee.2017.11.028
   Taliansky-Chamudis A, 2017, SCI TOTAL ENVIRON, V595, P93, DOI 10.1016/j.scitotenv.2017.03.246
   Tomizawa M, 2005, ANNU REV PHARMACOL, V45, P247, DOI 10.1146/annurev.pharmtox.45.120403.095930
   Tomizawa M, 2000, J AGR FOOD CHEM, V48, P6016, DOI 10.1021/jf000873c
   Turaga U, 2016, ENVIRON TOXICOL CHEM, V35, P1511, DOI 10.1002/etc.3305
   Wood TJ, 2017, ENVIRON SCI POLLUT R, V24, P17285, DOI 10.1007/s11356-017-9240-x
   [No title captured]
NR 58
TC 11
Z9 11
U1 7
U2 21
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0048-9697
EI 1879-1026
J9 SCI TOTAL ENVIRON
JI Sci. Total Environ.
PD JUN 25
PY 2020
VL 723
AR 138056
DI 10.1016/j.scitotenv.2020.138056
PG 11
WC Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA LR7SU
UT WOS:000535897200003
PM 32224397
OA hybrid, Green Accepted
DA 2022-02-10
ER

PT J
AU Huaranca, JC
   Villalba, ML
   Negroes, N
   Jimenez, JE
   Macdonald, DW
   Pacheco, LF
AF Carlos Huaranca, Juan
   Lilian Villalba, Ma
   Negroes, Nuno
   Jimenez, Jaime E.
   Macdonald, David W.
   Pacheco, Luis F.
TI Density and activity patterns of Andean cat and pampas cat (Leopardus
   jacobita and L. colocolo) in the Bolivian Altiplano
SO WILDLIFE RESEARCH
LA English
DT Article
DE camera-trapping; endangered species; generalist species; overlap;
   spatially explicit capture-recapture; specialist species
ID POPULATION-DENSITY; MICROHABITAT USE; BOBCATS; PREY; CARNIVORES;
   MORTALITY; SURVIVAL; ECOLOGY; FOREST; FOXES
AB ContextUnderstanding the factors that determine the distribution and abundance of species is an important aim of ecology and prerequisite for conservation. The Andean cat (Leopardus jacobita) and the pampas cat (L. colocolo) are two of the least studied felids. Both are threatened, of similar size and live sympatrically in the Andes of Argentina, Bolivia, Chile, and Peru. AimsWe aimed at estimating the population densities of the Andean cat and pampas cat in two continuous areas and to analyse the activity patterns of these two species and that of mountain vizcacha (Lagidium viscacia), the main prey of the Andean cat. MethodsWe used camera traps to evaluate the density of both felid species using the space explicit capture recapture (SECR) framework and the overlap in their activity patterns with that of mountain vizcacha, using the kernel-density estimator in two contiguous areas in the Bolivian Altiplano, at Muro-Amaya and at Micani, both within the Ciudad de Piedra region. Key resultsAndean cat density was estimated at 6.45 individuals per 100km(2) in Muro-Amaya and 6.91 individuals per 100km(2) in Micani, whereas the density of the pampas cat was 5.31 individuals per 100km(2) and 8.99 individuals per 100km(2) respectively. The Andean cat was mainly nocturnal, whereas the pampas cat was cathemeral. The activity of the mountain vizcacha overlapped less with that of its specialised predator, the Andean cat, than with that of the pampas cat. ConclusionsIn line with our predictions, the Andean cat, considered a more specialised nocturnal hunter, particularly of mountain vizcacha, had lower population densities than did the more generalist pampas cat. ImplicationsLow population densities, as compared with theoretical expectations, pose an additional conservation problem for these felids, in an area such as the high Andes.
C1 [Carlos Huaranca, Juan; Jimenez, Jaime E.] Univ Los Lagos, Programa Doctorado Ciencias Menc Conservac & Mane, Ave Fushlocher 1305, Osorno, Chile.
   [Carlos Huaranca, Juan; Lilian Villalba, Ma] Andean Cat Alliance, La Paz, Bolivia.
   [Carlos Huaranca, Juan] Univ Mayor San Simon, Ctr Biodiversidad & Genet, Calle Sucre Frente Parque La Torre, Cochabamba, Bolivia.
   [Negroes, Nuno] Univ Aveiro, CESAM, Campus Univ Santiago, P-3810193 Aveiro, Portugal.
   [Negroes, Nuno] Univ Aveiro, Dept Biol, Campus Univ Santiago, P-3810193 Aveiro, Portugal.
   [Jimenez, Jaime E.] Univ North Texas, Coll Sci, Adv Environm Res Inst, Dept Biol Sci, Denton, TX 76203 USA.
   [Macdonald, David W.] Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney House,Abingdon Rd, Abingdon OX13 5QL, Oxon, England.
   [Pacheco, Luis F.] Univ Mayor San Andres, Carrera Biol, Inst Ecol, Colecc Boliviana Fauna, Calle 27 Cota Cota, La Paz, Bolivia.
   [Pacheco, Luis F.] BIOTA, Ctr Estudios Biol Teor & Aplicada, Ave Las Retamas 15,Entre Calles 34 & 35, La Paz, Bolivia.
RP Huaranca, JC (corresponding author), Univ Los Lagos, Programa Doctorado Ciencias Menc Conservac & Mane, Ave Fushlocher 1305, Osorno, Chile.; Huaranca, JC (corresponding author), Andean Cat Alliance, La Paz, Bolivia.; Huaranca, JC (corresponding author), Univ Mayor San Simon, Ctr Biodiversidad & Genet, Calle Sucre Frente Parque La Torre, Cochabamba, Bolivia.
EM jchuaranca@gmail.com
RI Jimenez, Jaime E./AAO-7531-2020; Huaranca, Juan Carlos/AAH-8050-2020;
   Soares, Nuno/E-4348-2015
OI Huaranca, Juan Carlos/0000-0001-9747-4568; Soares,
   Nuno/0000-0003-1315-6648
FU Wildlife Conservation Network; Small Cats Action Fund (Panthera
   Foundation); Mohamed bin Zayed Species Conservation Fund [11053162];
   Iniciativa de Especies Amenazadas Becas 'Werner Hanagarth' (Fundacion
   Puma); Andean Cat Alliance; ESRI Conservation Program; Idea Wild;
   Instituto de Ecologi'a at Universidad Mayor de San Andres; Wildlife
   Conservation Society of Bolivia
FX We acknowledge the collaboration of indigenous authorities of Ayllu
   Pahaza, and the Municipality of Calacoto. This research was funded by
   Wildlife Conservation Network, Small Cats Action Fund (Panthera
   Foundation), Mohamed bin Zayed Species Conservation Fund (Project:
   11053162), and Iniciativa de Especies Amenazadas Becas 'Werner
   Hanagarth' (Fundacion Puma). We also want to thank the support granted
   by the Andean Cat Alliance, ESRI Conservation Program, Idea Wild,
   Instituto de Ecologi ' a at Universidad Mayor de San Andres, and the
   Wildlife Conservation Society of Bolivia. Our thanks also go to
   Alejandra Torrez for her constant support. Herminio Ticona, Jim
   Sanderson, Miguel Saavedra, Mauricio Penaranda, Yony Quinones, and
   Gregorio Roque helped during fieldwork. We are grateful to Susan Walker
   for helpful comments on a previous version of this manuscript and two
   anonymous reviewers provided useful comments to the previous version of
   this manuscript.
CR Arispe Rosario, 2007, Cat News, V46, P36
   Balme GA, 2009, J WILDLIFE MANAGE, V73, P433, DOI 10.2193/2007-368
   Blankenship TL, 2006, WILDLIFE BIOL, V12, P297, DOI 10.2981/0909-6396(2006)12[297:CSACMB]2.0.CO;2
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Carbone C, 2002, SCIENCE, V295, P2273, DOI 10.1126/science.1067994
   Caruso N, 2012, ANN ZOOL FENN, V49, P181, DOI 10.5735/086.049.0306
   Cossios D., 2007, MANUAL METOLOGIAS RE
   Delibes-Mateos M, 2014, MAMM BIOL, V79, P393, DOI 10.1016/j.mambio.2014.04.006
   Efford MG, 2014, METHODS ECOL EVOL, V5, P599, DOI 10.1111/2041-210X.12169
   Efford MG, 2009, ENVIRON ECOL STAT SE, V3, P255, DOI 10.1007/978-0-387-78151-8_11
   Fajardo Ursula, 2014, Rev. peru biol., V21, P061, DOI 10.15381/rpb.v21i1.8248
   Foster VC, 2013, BIOTROPICA, V45, P373, DOI 10.1111/btp.12021
   Gardner B, 2010, ECOLOGY, V91, P3376, DOI 10.1890/09-0804.1
   Gobierno Autonomo Municipal de Calacoto, 2008, PLAN DES MUN MUN CAL
   Haines AM, 2004, ACTA THERIOL, V49, P349, DOI 10.1007/BF03192533
   Huaranca J. C., 2011, OPCIONES ESTABLECIMI
   HUARANCA JC, 2013, CAT NEWS, V58, P4
   Ivan JS, 2013, ECOLOGY, V94, P817, DOI 10.1890/12-0102.1
   Jimenez JE, 1996, REV CHIL HIST NAT, V69, P113
   Jones M, 2000, J CHEM ECOL, V26, P455, DOI 10.1023/A:1005417707588
   Kelt DA, 2001, AM NAT, V157, P637, DOI 10.1086/320621
   Kolowski JM, 2002, J WILDLIFE MANAGE, V66, P822, DOI 10.2307/3803146
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Lopez G, 2014, EUR J WILDLIFE RES, V60, P359, DOI 10.1007/s10344-013-0794-8
   Lucherini M., 2016, IUCN RED LIST THREAT, DOI 10.2305/IUCN.UK.2016-1.RLTS.T6929A85324366.en
   Lucherini M, 2008, MAMMALIA, V72, P95, DOI 10.1515/MAMM.2008.018
   Lucherini M, 2009, J MAMMAL, V90, P1404, DOI 10.1644/09-MAMM-A-002R.1
   Macdonald D.W., 2010, BIOL CONSERVATION WI
   Marino J, 2011, DIVERS DISTRIB, V17, P311, DOI 10.1111/j.1472-4642.2011.00744.x
   Marino Jorgelina, 2010, P581
   MCNAB BK, 1963, AM NAT, V97, P133, DOI 10.1086/282264
   Napolitano C, 2008, MOL ECOL, V17, P678, DOI 10.1111/j.1365-294X.2007.03606.x
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   Nowell K., 1996, WILD CATS STATUS SUR
   Pereira L. J. A, 2009, THESIS
   R Core Team, 2017, R LANG ENV STAT COMP
   Reppucci J, 2011, J MAMMAL, V92, P140, DOI 10.1644/10-MAMM-A-053.1
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Riley SPD, 2006, J WILDLIFE MANAGE, V70, P1425, DOI 10.2193/0022-541X(2006)70[1425:SEOBAG]2.0.CO;2
   Ross J, 2013, J ZOOL, V290, P96, DOI 10.1111/jzo.12018
   Royle JA, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P163, DOI 10.1007/978-4-431-99495-4_10
   Royle JA, 2009, J APPL ECOL, V46, P118, DOI 10.1111/j.1365-2664.2008.01578.x
   Santini L, 2018, GLOBAL ECOL BIOGEOGR, V27, P968, DOI 10.1111/geb.12758
   Santos F, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0213671
   SCHOENER TW, 1974, SCIENCE, V185, P27, DOI 10.1126/science.185.4145.27
   Silva M, 2001, GLOBAL ECOL BIOGEOGR, V10, P469, DOI 10.1046/j.1466-822x.2001.00261.x
   Sollmann R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034575
   Suraci JP, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170255
   TARIFA T., 2001, REV BOLIVIANA ECOLOG, V9, P29
   Tellaeche C. G., 2015, THESIS
   Teran J., 2012, EVALUACION VEGETACIO
   Theuerkauf J, 2003, J MAMMAL, V84, P243, DOI 10.1644/1545-1542(2003)084<0243:DPADOW>2.0.CO;2
   Torrico J. O, 2009, THESIS
   Villalba L., 2016, IUCN RED LIST THREAT
   Villalba M. L., 2009, LIBRO ROJO FAUNA SIL, P525
   Villalba M. L., 2009, LIBRO ROJO FAUNA SIL, P451
   Villalba M. L., 2009, 10 INT MAMM C, P187
   Villalba M. L., 2009, 10 INT MAMM C, P113
   Viscarra M. E., 2008, THESIS
   Walker RS, 2007, J MAMMAL, V88, P519, DOI 10.1644/06-MAMM-A-172R.1
   Yensen E, 2000, MAMMALIAN SPECIES, V611, P1, DOI [10.1644/1545-1410(2000)644<0001:OJ>2.0.CO;2, DOI 10.1644/1545-1410(2000)644<0001:OJ>2.0.CO;2]
   Zanon-Martinez JI, 2016, WILDLIFE RES, V43, P449, DOI 10.1071/WR16056
NR 62
TC 4
Z9 5
U1 1
U2 21
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
PD FEB
PY 2020
VL 47
IS 1
BP 68
EP 76
DI 10.1071/WR19053
PG 9
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA KL2AI
UT WOS:000513231100007
DA 2022-02-10
ER

PT J
AU Simon, JA
   Chancel, E
   Hubert, P
   Aubert, D
   Villena, I
   Gilot-Fromont, E
   Poulle, ML
AF Simon, Julie Alice
   Chancel, Eva
   Hubert, Pauline
   Aubert, Dominique
   Villena, Isabelle
   Gilot-Fromont, Emmanuelle
   Poulle, Marie-Lazarine
TI Pattern of latrine use by domestic cats on dairy farms and the
   implications for Toxoplasma gondii transmission
SO VETERINARY PARASITOLOGY
LA English
DT Article
DE Felis silvestris cams; Video trap; Cat faeces; Toxoplasmosis;
   Intermediate hosts
ID SPATIAL-DISTRIBUTION; PARASITIC ZOONOSES; DEFECATION SITES; CAMERA
   TRAPS; SWINE FARMS; HOME-RANGE; OOCYSTS; SEROPREVALENCE; WILDLIFE; SOIL
AB Toxoplasma gondii is the parasite responsible for toxoplasmosis, a highly prevalent zoonosis that affects humans and warm-blooded animals. Faeces of infected cats can contain millions of T. gondii oocysts, which remain infectious in the environment for months. Sites repeatedly used by cats for defecation (latrines') are recognised as hotspots of T. gondii soil contamination, but this contamination varies from one latrine to another. To understand this spatial heterogeneity, camera traps were deployed in 39 cat latrines on three dairy farms with high density cat populations and programmed to record visits during sixteen 10-day sessions, rotating between three farms over a period of a year. Generalized Linear Mixed Models were used to test the effects of cat sexual maturity, latrine location and season on the number of cat faeces deposited and on the number of cats defecating per latrine, as determined from the analysis of 41,282 video recordings. Sexually immature cats defecated 6.60 fold (95% CI = [2.87-15.25]) more often in latrines located close to a feeding site than in other latrines. This pattern was also observed for mature males (odds ratio [OR] = 9.42, 95% CI = [3.29-26.91]), especially during winter, but not for mature females (OR = 1.77, 95% CI = [0.80-3.94]). The number of defecating cats was also 2.67-fold (95% Cl = [1.66-4.30], P < 0.001) higher in latrines located close to a feeding point than in those located far from it, regardless of cat category and season. Visits by intermediate T. gondii hosts (micromammals, birds and others) were also recorded. Out of the 39 latrines, 30 (76.92%) were visited by at least one intermediate host during the study period, and some latrines were highly frequented (up to 8.74 visits/day on average). These results provide evidence that the location of food resources in dairy farms influences the latrine use pattern by cats. Highly frequented latrines can be of high risk of T. gondii infection for definitive and intermediate hosts.
C1 [Simon, Julie Alice; Aubert, Dominique; Villena, Isabelle; Poulle, Marie-Lazarine] Univ Reims, UFR Med, SFR Cap Sante, Lab Parasitol Mycol EA Escape 7510, 51 Rue Cognacg Jay, F-51095 Reims, France.
   [Simon, Julie Alice; Hubert, Pauline; Poulle, Marie-Lazarine] Univ Reims, CERFE, 5 Rue Heronniere, F-08240 Boult Aux Bois, France.
   [Chancel, Eva; Gilot-Fromont, Emmanuelle] VetAgro Sup, Campus Vet Lyon,1 Ave Bourgelat, F-69280 Marcy Letoile, France.
   [Hubert, Pauline] Faune Act, 6 Rue Jardin Gascon, F-08240 Boult Aux Bois, France.
   [Gilot-Fromont, Emmanuelle] Univ Claude Bernard Lyon 1, UMR CNRS Lab Biometrie & Biol Evolut 5558, 43 Bd 11 Novembre 1918, F-69622 Villeurbanne, France.
RP Simon, JA (corresponding author), Univ Reims, UFR Med, SFR Cap Sante, Lab Parasitol Mycol EA Escape 7510, 51 Rue Cognacg Jay, F-51095 Reims, France.
EM julie.rabeisensimon@gmail.com; chancel_eva@hotmail.fr;
   pauline_hubert@hotmail.fr; daubert@chu-reims.fr; ivillena@chu-reims.fr;
   emmanuelle.gilotfromont@vetagro-sup.fr;
   marie-lazarine.poulle@univ-reims.fr
RI Aubert, dominique/AAH-5854-2019
OI Gilot-Fromont, Emmanuelle/0000-0003-0011-7519
FU Agence de l'Environnement et de la Maitrise de l'Energie (ADEME);
   Ministry of Higher Education, Research and Innovation, France
FX Part of this work was financially supported by the Agence de
   l'Environnement et de la Maitrise de l'Energie (ADEME) in the framework
   of the "L'animal, le Sol, l'Eau: Dynamique de la Contamination
   Environnementale par la toxoplasmose" project (AFSSET-ADEME, 2010-2014).
   Julie Alice Simon was also supported by a grant from the Ministry of
   Higher Education, Research and Innovation, France.
CR Afonso E, 2008, INT J PARASITOL, V38, P1017, DOI 10.1016/j.ijpara.2008.01.004
   Afonso Eve, 2013, International Journal for Parasitology Parasites and Wildlife, V2, P278, DOI 10.1016/j.ijppaw.2013.09.006
   Anderson K. A., 2002, MODEL SELECTION MULT
   Baneth G., 2016, J COMP PATHOL, V155, pS54
   Barratt DG, 1997, ECOGRAPHY, V20, P271, DOI 10.1111/j.1600-0587.1997.tb00371.x
   Bastien M, 2018, FOLIA PARASIT, V65, DOI 10.14411/fp.2018.002
   Boyer K, 2011, CLIN INFECT DIS, V53, P1081, DOI 10.1093/cid/cir667
   Bradshaw JWS, 2012, BEHAV DOMESTIC CAT
   Buesching CD, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1328
   Conrad PA, 2005, INT J PARASITOL, V35, P1155, DOI 10.1016/j.ijpara.2005.07.002
   Courchamp F, 2000, WILDLIFE RES, V27, P603, DOI 10.1071/WR99049
   Dabritz HA, 2010, ZOONOSES PUBLIC HLTH, V57, P34, DOI 10.1111/j.1863-2378.2009.01273.x
   del Hoyo J., 1992, HDB BIRDS WORLD, V1
   Di Cerbo AR, 2008, HELMINTHOLOGIA, V45, P13, DOI 10.2478/s11687-008-0002-7
   Dubey JP, 2006, VET PARASITOL, V140, P69, DOI 10.1016/j.vetpar.2006.03.018
   DUBEY JP, 1995, J PARASITOL, V81, P723, DOI 10.2307/3283961
   Dubey JP., 2010, TOXOPLASMOSIS ANIMAL
   Elizondo EC, 2016, WILDLIFE BIOL, V22, P246, DOI 10.2981/wlb.00237
   Ferreira JP, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025970
   Forin-Wiart MA, 2014, EUR J WILDLIFE RES, V60, P665, DOI 10.1007/s10344-014-0833-0
   Gauss CBL, 2003, J PARASITOL, V89, P1067, DOI 10.1645/GE-114
   Germain E, 2008, J ZOOL, V276, P195, DOI 10.1111/j.1469-7998.2008.00479.x
   Gilot-Fromont E, 2009, VET PARASITOL, V161, P36, DOI 10.1016/j.vetpar.2008.12.004
   Gilot-Fromont E., 2012, TOXOPLASMOSIS RECENT, P1
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Goszczynski J, 2009, FOLIA ZOOL, V58, P363
   Gotteland C, 2014, INT J HEALTH GEOGR, P13
   Gotteland C, 2014, VET PARASITOL, V205, P629, DOI 10.1016/j.vetpar.2014.08.003
   Guislain MH, 2007, PARASITE, V14, P299, DOI 10.1051/parasite/2007144299
   Herrmann DC, 2010, INT J PARASITOL, V40, P285, DOI 10.1016/j.ijpara.2009.08.001
   Hill D, 2002, CLIN MICROBIOL INFEC, V8, P634, DOI 10.1046/j.1469-0691.2002.00485.x
   Horn JA, 2011, J WILDLIFE MANAGE, V75, P1177, DOI 10.1002/jwmg.145
   Ishida Y, 1998, J ETHOL, V16, P15, DOI 10.1007/BF02896349
   IZAWA M, 1982, Japanese Journal of Ecology, V32, P373
   Jordan NR, 2007, ANIM BEHAV, V73, P613, DOI 10.1016/j.anbehav.2006.06.010
   Kitts-Morgan SE, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120513
   Knapp J, 2018, INT J PARASITOL, V48, P937, DOI [10.1016/j.ijpara.2018.05.007, 10.1016/j.ijpara.2018.]
   Krauze-Gryz D, 2017, URBAN ECOSYST, V20, P945, DOI 10.1007/s11252-016-0634-1
   Kukielka E, 2013, PREV VET MED, V112, P213, DOI 10.1016/j.prevetmed.2013.08.008
   Lehmann T, 2003, INFECT GENET EVOL, V3, P135, DOI 10.1016/S1567-1348(03)00067-4
   Lelu M, 2012, APPL ENVIRON MICROB, V78, P5127, DOI 10.1128/AEM.00246-12
   LIBERG O, 1982, ACTA THERIOL, V27, P115, DOI 10.4098/AT.arch.82-9
   Liberg Olof, 2000, P119
   Lindsay DS, 2009, J PARASITOL, V95, P1019, DOI 10.1645/GE-1919.1
   Macdonald D.W., 1985, P619
   Macdonald David W., 2000, P95
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Milkovic M, 2009, AREA, V41, P310, DOI 10.1111/j.1475-4762.2008.00865.x
   Munoz-Zanzi CA, 2010, EMERG INFECT DIS, V16, P1591, DOI 10.3201/eid1610.091674
   Page LK, 2009, EMERG INFECT DIS, V15, P1530, DOI 10.3201/eid1509.090128
   Page LK, 1998, AM MIDL NAT, V140, P180, DOI 10.1674/0003-0031(1998)140[0180:RLSAIP]2.0.CO;2
   Polley L, 2005, INT J PARASITOL, V35, P1279, DOI 10.1016/j.ijpara.2005.07.003
   R Core Team, 2014, R LANG ENV STAT COMP
   Raoul F, 2015, VET PARASITOL, V213, P162, DOI 10.1016/j.vetpar.2015.07.034
   Richomme C, 2010, EPIDEMIOL INFECT, V138, P1257, DOI 10.1017/S0950268810000117
   Robertson ID, 2002, MICROBES INFECT, V4, P867, DOI 10.1016/S1286-4579(02)01607-6
   Rodgers TW, 2015, MAMM BIOL, V80, P380, DOI 10.1016/j.mambio.2015.05.004
   Rovero Francesco, 2010, Abc Taxa, V8, P100
   Say L, 1999, P ROY SOC B-BIOL SCI, V266, P2071, DOI 10.1098/rspb.1999.0889
   Schares G, 2016, INT J PARASITOL, V46, P263, DOI 10.1016/j.ijpara.2015.12.006
   Simon JA, 2017, INT J PARASITOL, V47, P357, DOI 10.1016/j.ijpara.2017.01.004
   Simon JA, 2018, PARASITE VECTOR, V11, DOI 10.1186/s13071-018-2834-4
   Soler Lucía, 2009, Mastozool. neotrop., V16, P485
   Tenter AM, 2000, INT J PARASITOL, V30, P1217, DOI 10.1016/S0020-7519(00)00124-7
   Traversa D, 2014, PARASITE VECTOR, V7, DOI 10.1186/1756-3305-7-67
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Turner D.C., 2013, DOMESTIC CAT BIOL IT
   VanWormer E, 2013, COMP IMMUNOL MICROB, V36, P217, DOI 10.1016/j.cimid.2012.10.006
   WEIGEL RM, 1995, J PARASITOL, V81, P736, DOI 10.2307/3283964
   Zuur Alain F., 2009, P1
NR 70
TC 0
Z9 0
U1 2
U2 23
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0304-4017
EI 1873-2550
J9 VET PARASITOL
JI Vet. Parasitol.
PD SEP
PY 2019
VL 273
BP 112
EP 121
DI 10.1016/j.vetpar.2019.08.001
PG 10
WC Parasitology; Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Parasitology; Veterinary Sciences
GA JA1IK
UT WOS:000487570500018
PM 31476666
OA Bronze
DA 2022-02-10
ER

PT J
AU Howe, EJ
   Buckland, ST
   Despres-Einspenner, ML
   Kuhl, HS
AF Howe, Eric J.
   Buckland, Stephen T.
   Despres-Einspenner, Marie-Lyne
   Kuehl, Hjalmar S.
TI Model selection with overdispersed distance sampling data
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE animal abundance; camera trapping; cue counting; distance sampling;
   model selection; overdispersion; QAIC
ID TRANSECT SURVEYS; SPATIAL MODELS; INFERENCE
AB Distance sampling (DS) is a widely used framework for estimating animal abundance. DS models assume that observations of distances to animals are independent. Non-independent observations introduce overdispersion, causing model selection criteria such as AIC or AIC(c) to favour overly complex models, with adverse effects on accuracy and precision. We describe, and evaluate via simulation and with real data, estimators of an overdispersion factor (c), and associated adjusted model selection criteria (QAIC) for use with overdispersed DS data. In other contexts, a single value of c<^> is calculated from the "global" model, that is the most highly parameterised model in the candidate set, and used to calculate QAIC for all models in the set; the resulting QAIC values, and associated Delta QAIC values and QAIC weights, are comparable across the entire set. Candidate models of the DS detection function include models with different general forms (e.g. half-normal, hazard rate, uniform), so it may not be possible to identify a single global model. We therefore propose a two-step model selection procedure by which QAIC is used to select among models with the same general form, and then a goodness-of-fit statistic is used to select among models with different forms. A drawback of this approach is that QAIC values are not comparable across all models in the candidate set. Relative to AIC, QAIC and the two-step model selection procedure avoided overfitting and improved the accuracy and precision of densities estimated from simulated data. When applied to six real datasets, adjusted criteria and procedures selected either the same model as AIC or a model that yielded a more accurate density estimate in five cases, and a model that yielded a less accurate estimate in one case. Many DS surveys yield overdispersed data, including cue counting surveys of songbirds and cetaceans, surveys of social species including primates, and camera-trapping surveys. Methods that adjust for overdispersion during the model selection stage of DS analyses therefore address a conspicuous gap in the DS analytical framework as applied to species of conservation concern.
C1 [Howe, Eric J.; Buckland, Stephen T.] Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.
   [Howe, Eric J.] Trent Univ, Wildlife Res & Monitoring Sect, Ontario Minist Nat Resources & Forestry, DNA Bldg, Peterborough, ON, Canada.
   [Despres-Einspenner, Marie-Lyne; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Leipzig, Germany.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Leipzig, Germany.
RP Howe, EJ (corresponding author), Univ St Andrews, Ctr Res Ecol & Environm Modelling, St Andrews, Fife, Scotland.; Howe, EJ (corresponding author), Trent Univ, Wildlife Res & Monitoring Sect, Ontario Minist Nat Resources & Forestry, DNA Bldg, Peterborough, ON, Canada.
EM ejh20@st-andrews.ac.uk
RI Buckland, Stephen T/A-1998-2012
OI Buckland, Stephen/0000-0002-9939-709X
FU Robert Bosch Foundation; Max Planck SocietyMax Planck Society;
   University of St AndrewsDeutsche KrebshilfeHelmholtz Association;
   Ministere de l'Enseignement Superieur et de la Recherche Scientifique
FX We thank the Robert Bosch Foundation, the Max Planck Society and the
   University of St Andrews for funding, the Ministere de l'Enseignement
   Superieur et de la Recherche Scientifique and the Ministere de
   l'Environnement et des Eaux et Forets in Cote d'Ivoire for permission to
   conduct field research in Tai National Park, Dr. Roman Wittig for
   permitting data collection in the area of the Tai Chimpanzee Project,
   and Dr. Joeseph Nocera for informal discussions about model selection in
   the presence of overdispersion.
CR Akaikei H., 1973, 2 INT S INFORM THEOR, P267
   Anderson D.R., 2002, MODEL SELECTION MULT
   Borchers DL, 2002, ESTIMATING ANIMAL AB, DOI [DOI 10.1007/978-1-4471-3708-5, 10.1007/978-1-4471-3708-5]
   Buckland S.T., 2001, pi
   BUCKLAND ST, 1984, BIOMETRICS, V40, P811, DOI 10.2307/2530926
   Buckland ST., 2004, ADV DISTANCE SAMPLIN
   Buckland ST, 2006, AUK, V123, P345, DOI 10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Burnham KP, 2001, WILDLIFE RES, V28, P111, DOI 10.1071/WR99107
   Cox D.R., 1989, ANAL BINARY DATA, V2nd ed.
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Hedley SL, 2004, J AGR BIOL ENVIR ST, V9, P181, DOI 10.1198/1085711043578
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Johnson DS, 2010, BIOMETRICS, V66, P310, DOI 10.1111/j.1541-0420.2009.01265.x
   Johnson JB, 2004, TRENDS ECOL EVOL, V19, P101, DOI 10.1016/j.tree.2003.10.013
   LEBRETON JD, 1992, ECOL MONOGR, V62, P67, DOI 10.2307/2937171
   LIANG KY, 1993, BIOMETRICS, V49, P623, DOI 10.2307/2532575
   Marques TA, 2007, AUK, V124, P1229, DOI 10.1642/0004-8038(2007)124[1229:IEOBDU]2.0.CO;2
   Miller DL, 2013, METHODS ECOL EVOL, V4, P1001, DOI 10.1111/2041-210X.12105
   R Core Team, 2017, R LANG ENV STAT COMP
NR 20
TC 7
Z9 7
U1 3
U2 18
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JAN
PY 2019
VL 10
IS 1
BP 38
EP 47
DI 10.1111/2041-210X.13082
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HK2NU
UT WOS:000457750600004
OA Green Accepted, Bronze
DA 2022-02-10
ER

PT J
AU Christin, S
   Hervet, E
   Lecomte, N
AF Christin, Sylvain
   Hervet, Eric
   Lecomte, Nicolas
TI Applications for deep learning in ecology
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Review
DE artificial intelligence; automatic monitoring; deep learning; ecology;
   neural network; pattern recognition
ID NEURAL-NETWORKS; OPPORTUNITIES; EVOLUTION; DYNAMICS; TRACKING; TOOLS
AB A lot of hype has recently been generated around deep learning, a novel group of artificial intelligence approaches able to break accuracy records in pattern recognition. Over the course of just a few years, deep learning has revolutionized several research fields such as bioinformatics and medicine with its flexibility and ability to process large and complex datasets. As ecological datasets are becoming larger and more complex, we believe these methods can be useful to ecologists as well. In this paper, we review existing implementations and show that deep learning has been used successfully to identify species, classify animal behaviour and estimate biodiversity in large datasets like camera-trap images, audio recordings and videos. We demonstrate that deep learning can be beneficial to most ecological disciplines, including applied contexts, such as management and conservation. We also identify common questions about how and when to use deep learning, such as what are the steps required to create a deep learning network, which tools are available to help, and what are the requirements in terms of data and computer power. We provide guidelines, recommendations and useful resources, including a reference flowchart to help ecologists get started with deep learning. We argue that at a time when automatic monitoring of populations and ecosystems generates a vast amount of data that cannot be effectively processed by humans anymore, deep learning could become a powerful reference tool for ecologists.
C1 [Christin, Sylvain; Lecomte, Nicolas] Univ Moncton, Dept Biol, Canada Res Chair Polar & Boreal Ecol, Moncton, NB, Canada.
   [Hervet, Eric] Univ Moncton, Dept Comp Sci, Moncton, NB, Canada.
RP Lecomte, N (corresponding author), Univ Moncton, Dept Biol, Canada Res Chair Polar & Boreal Ecol, Moncton, NB, Canada.
EM nicolas.lecomte@umoncton.ca
RI Christin, Sylvain/AAS-8910-2020
OI Lecomte, Nicolas/0000-0002-8473-5375
FU Canada Research Chair in Polar and Boreal Ecology; New Brunswick
   Innovation Fund; Polar Knowledge Canada
FX Canada Research Chair in Polar and Boreal Ecology; New Brunswick
   Innovation Fund; Polar Knowledge Canada
CR Abrams J.F., 2018, 483222 BIORXIV, DOI [10.1101/483222, DOI 10.1101/483222]
   Asner GP, 2018, BIOL CONSERV, V217, P289, DOI 10.1016/j.biocon.2017.10.020
   Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
   Beijbom O., 2015, ARXIV151004811CS
   Browning E, 2018, METHODS ECOL EVOL, V9, P681, DOI 10.1111/2041-210X.12926
   Candela L, 2015, J ASSOC INF SCI TECH, V66, P1747, DOI 10.1002/asi.23358
   Cantrell B, 2017, TRENDS ECOL EVOL, V32, P156, DOI 10.1016/j.tree.2016.12.004
   Carey CC, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2753
   Chen D., 2016, ARXIV160909353CSQBIO
   Chollet F., 2016, ARXIV161002357CS
   Chon TS, 2001, ECOL MODEL, V146, P181, DOI 10.1016/S0304-3800(01)00305-2
   Christin S., 2019, APPL DEEP LEARNING E
   Cutler DR, 2007, ECOLOGY, V88, P2783, DOI 10.1890/07-0539.1
   Desjardins-Proulx P., 2017, 089771 BIORXIV, DOI [10.1101/089771, DOI 10.1101/089771]
   Di Minin E, 2018, NAT ECOL EVOL, V2, P406, DOI 10.1038/s41559-018-0466-x
   Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
   Dobrescu A., 2017, LEVERAGING MULTIPLE
   Douarre C., 2016, DEEP LEARNING BASED
   Drake JM, 2006, J APPL ECOL, V43, P424, DOI 10.1111/j.1365-2664.2006.01141.x
   Dugan P. J., 2016, ARXIV160500972CS
   Ellis EC, 2015, ECOL MONOGR, V85, P287, DOI 10.1890/14-2274.1
   Fairbrass AJ, 2019, METHODS ECOL EVOL, V10, P186, DOI 10.1111/2041-210X.13114
   Fernandez S, 2007, LECT NOTES COMPUT SC, V4669, P220
   Giuffrida M. V., 2017, ARIGAN SYNTHETIC ARA
   Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
   Guegan, 2012, ARTIFICIAL NEURONAL
   Guirado E., 2018, 443671 BIORXIV, DOI [10.1101/443671, DOI 10.1101/443671]
   Hart AG, 2018, METHODS ECOL EVOL, V9, P2194, DOI 10.1111/2041-210X.13063
   Heaton JB, 2017, APPL STOCH MODEL BUS, V33, P3, DOI 10.1002/asmb.2209
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jeong KS, 2001, ECOL MODEL, V146, P115, DOI 10.1016/S0304-3800(01)00300-3
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7_24
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kalin U., 2018, 441733 BIORXIV, DOI [10.1101/441733, DOI 10.1101/441733]
   Kiskin I, 2020, NEURAL COMPUT APPL, V32, P915, DOI 10.1007/s00521-018-3626-7
   Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kroodsma DA, 2018, SCIENCE, V359, P904, DOI 10.1126/science.aao5646
   Lample Guillaume, 2017, 31 AAAI C ART INT
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee H, 2019, ECOL INDIC, V96, P505, DOI 10.1016/j.ecolind.2018.08.035
   Lek S, 1996, ECOL MODEL, V90, P39, DOI 10.1016/0304-3800(95)00142-5
   Li K., 2017, RECURRENT NEURAL NET
   Lowndes JSS, 2017, NAT ECOL EVOL, V1, DOI 10.1038/s41559-017-0160
   Mac Aodha O, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005995
   Marcus G., 2018, ARXIV180100631CSSTAT
   Min S, 2017, BRIEF BIOINFORM, V18, P851, DOI 10.1093/bib/bbw068
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Namin ST, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0333-4
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Olden JD, 2008, Q REV BIOL, V83, P171, DOI 10.1086/587826
   Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5
   Potamitis I, 2015, ECOL INFORM, V26, P6, DOI 10.1016/j.ecoinf.2015.01.002
   Qiao M., 2018, 467878 BIORXIV, DOI [10.1101/467878, DOI 10.1101/467878]
   Recknagel F, 2001, ECOL MODEL, V146, P303, DOI 10.1016/S0304-3800(01)00316-7
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Ryan MJ, 2000, BRAIN BEHAV EVOLUT, V56, P45, DOI 10.1159/000006677
   Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8
   Salamon J, 2017, INT CONF ACOUST SPEE, P141, DOI 10.1109/ICASSP.2017.7952134
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schneider S., 2018, ARXIV180310842CS
   Sevilla A., 2017, WORKING NOTES CLEF 2
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516-044442, 10.1146/annurev-bioeng-071516044442]
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Song Q, 2017, NEUROCOMPUTING, V226, P16, DOI 10.1016/j.neucom.2016.11.018
   STOCKWELL DRB, 1992, MATH COMPUT SIMULAT, V33, P385, DOI 10.1016/0378-4754(92)90126-2
   Sutskever I., 2014, ARXIV14093215CS
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Turesson H. K., 2016, 079566 BIORXIV
   Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
   Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
   Wachtmeister CA, 2000, BEHAV ECOL, V11, P405, DOI 10.1093/beheco/11.4.405
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
   Wild B., 2018, ARXIV180204557CS
   Wilson G, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005510
   Xu R, 2018, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.02235
   Younis S, 2018, BOT LETT, V165, P377, DOI 10.1080/23818107.2018.1446357
NR 81
TC 89
Z9 91
U1 56
U2 147
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD OCT
PY 2019
VL 10
IS 10
BP 1632
EP 1644
DI 10.1111/2041-210X.13256
EA JUL 2019
PG 13
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA JB0UB
UT WOS:000488345800001
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Luna, N
   Varela, AI
   Brokordt, K
   Luna-Jorquera, G
AF Luna, Nicolas
   Varela, Andrea, I
   Brokordt, Katherina
   Luna-Jorquera, Guillermo
TI Assessing Potential Predation Risk by Introduced Predators on Unattended
   Eggs in the Red-Tailed Tropicbird, Phaethon rubricauda, on Rapa Nui
   (Easter Island)
SO TROPICAL CONSERVATION SCIENCE
LA English
DT Article
DE breeding seabirds; new breeders; invasive species; subtropical oceanic
   islands; southeastern Pacific Ocean
ID INVASIVE RATS; BIRD EGGS; COLONIES; SEABIRDS; BIOLOGY; SEA
AB Anthropogenic impact has been heavy in remote oceanic islands, including the introduction of alien species, having negative effects on native seabirds. The isolated and subtropical Rapa Nui (Easter Island) is one of the few known breeding sites of the red-tailed tropicbird, Phaethon rubricauda in Chile (southeastern Pacific Ocean) where is listed as vulnerable. A relatively new breeding colony is found in the Rano Raraku volcano, where human-introduced species are present. We used hen eggs as a proxy for red-tailed tropicbird eggs to assess potential predation risk on unattended eggs. Each experimental egg was monitored by camera traps during 6 days. Three predatory species were identified on the records: the Brown rat Rattus norvegicus, the Polynesian rat Rattus exulans, and the raptor Chimango Caracara Phalcoboenus chimango. The most frequent species were the Rattus spp .A total of 45 predatory visits were recorded with a total time of 1.7 h, accounting for the 0.3% of the experimental time. Within this time of visits, all the potential predators spent time in both interacting activities (trying to prey on) and no-interacting activities with the experimental eggs. Only a Brown rat was able to prey on one of the eggs. Our results suggest that these invasive species are a low threat for unattended red-tailed tropicbird eggs at Rano Raraku, Rapa Nui. However, future research is needed to determine the potential negative effects over unattended red-tailed tropicbird nestlings that are easier for these predators to handle compared with an egg.
C1 [Luna, Nicolas; Varela, Andrea, I; Luna-Jorquera, Guillermo] Univ Catolica Norte, Dept Biol Marina, Millennium Nucleus Ecol & Sustainable Management, Coquimbo, Chile.
   [Luna, Nicolas] Univ Catolica Norte, Fac Ciencias Mar, Programa Magister Ciencias Mar Menc Recursos Cost, Coquimbo, Chile.
   [Brokordt, Katherina] Univ Catolica Norte, Lab Fisiol & Genet Marina, Coquimbo, Chile.
   [Brokordt, Katherina; Luna-Jorquera, Guillermo] Univ Catolica Norte, Ctr Estudios Avanzados Zonas Aridas, Coquimbo, Chile.
RP Varela, AI (corresponding author), Univ Catolica Norte, Sede Coquimbo, Larrondo 1281, Coquimbo 1781421, Chile.
EM and.vrl@gmail.com
OI Guillermo, Luna-Jorquera/0000-0003-4274-7025; Varela, Andrea
   I/0000-0002-9752-0816
FU FONDECYTComision Nacional de Investigacion Cientifica y Tecnologica
   (CONICYT)CONICYT FONDECYT [3160324]; CONICYTComision Nacional de
   Investigacion Cientifica y Tecnologica (CONICYT) [22161894]; Millenium
   Nucleus of Ecology and Sustainable Management of Oceanic Islands
   (ESMOI); Ministry of Economy, Development and Tourism (Chile);
   Universidad Catolica del Norte, Coquimbo, Chile
FX The author(s) disclosed receipt of the following financial support for
   the research, authorship, and/or publication of this article: Funding
   for this project was provided by a postdoctoral research grant awarded
   to A. I. Varela (FONDECYT No 3160324), by a MSc scholarship (CONICYT No
   22161894) awarded to N. Luna, and by the Millenium Nucleus of Ecology
   and Sustainable Management of Oceanic Islands (ESMOI), a Scientific
   Initiative supported by the Ministry of Economy, Development and Tourism
   (Chile). Funding for publication was provided by Universidad Catolica
   del Norte, Coquimbo, Chile.
CR Aguirre J.E., 2009, Boletin Chileno de Ornitologia, V15, P44
   Anderson A, 2002, WORLD ARCHAEOL, V33, P375, DOI 10.1080/00438240120107431
   Biondi LM, 2015, ANIM COGN, V18, P139, DOI 10.1007/s10071-014-0785-5
   BirdLife International, 2017, SPEC FACTSH PHAETH R
   Boland CRJ, 2004, IBIS, V146, P687, DOI 10.1111/j.1474-919x.2004.00310.x
   Bolton M, 2014, POLAR BIOL, V37, P1659, DOI 10.1007/s00300-014-1554-2
   del Hoyo J., 1992, HDB BIRDS WORLD, V1
   Duron Q, 2017, CURR ZOOL, V63, P583, DOI 10.1093/cz/zox009
   Dutson G, 2010, ACTION PLAN AUSTR BI
   FLEET RR, 1972, AUK, V89, P651
   Flores M., 2017, THESIS
   Flores M, 2017, PAC SCI, V71, P149, DOI 10.2984/71.2.4
   Friard O, 2016, METHODS ECOL EVOL, V7, P1325, DOI 10.1111/2041-210X.12584
   Furness RW, 1987, SEABIRD ECOLOGY
   Gaskin C. P., 2011, SEABIRDS KERMADEC RE
   Harper GA, 2015, GLOB ECOL CONSERV, V3, P607, DOI 10.1016/j.gecco.2015.02.010
   Hatfield JS, 2012, CONSERV BIOL, V26, P667, DOI 10.1111/j.1523-1739.2012.01853.x
   Varela AI, 2018, EMU, V118, P381, DOI 10.1080/01584197.2018.1464372
   Jaramillo A., 2008, B CHIL ORNITOL, V14, P8
   Jones HP, 2008, CONSERV BIOL, V22, P16, DOI 10.1111/j.1523-1739.2007.00859.x
   Krajick K, 2005, SCIENCE, V310, P1410, DOI 10.1126/science.310.5753.1410
   Marin Manuel, 2010, Boletin del Museo Nacional de Historia Natural Chile, V59, P75
   Ministry of the Environment Chile, 2017, 14 PROC WILD SPEC CL
   Nice M. M., 1962, T LINNEAN SOC N Y, V13
   Prieto J, 2003, BIODIVERS CONSERV, V12, P2477, DOI 10.1023/A:1025825924678
   QGIS Development Team, 2017, QGIS GEOGR INF SYST
   R Core Team, 2017, R LANG ENV STAT COMP
   Richards C, 2011, WORLD ARCHAEOL, V43, P191, DOI 10.1080/00438243.2011.579483
   SCHLATTER RP, 1987, ISLAS OCEANICAS CHIL, P271
   Schreiber B. A., 2009, BIRDS N AM, DOI [10.2173/bna.43, DOI 10.2173/BNA.43]
   Schreiber EA, 1996, COLON WATERBIRD, V19, P45, DOI 10.2307/1521806
   Simeone A, 2012, J ORNITHOL, V153, P1079, DOI 10.1007/s10336-012-0837-z
   STEADMAN DW, 1995, SCIENCE, V267, P1123, DOI 10.1126/science.267.5201.1123
   Vanderwerf Eric A., 2014, Marine Ornithology, V42, P73
   Zarzoso-Lacoste D, 2011, J ZOOL, V285, P188, DOI 10.1111/j.1469-7998.2011.00828.x
NR 35
TC 7
Z9 7
U1 2
U2 14
PU SAGE PUBLICATIONS INC
PI THOUSAND OAKS
PA 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN 1940-0829
J9 TROP CONSERV SCI
JI Trop. Conserv. Sci.
PD JUL 3
PY 2018
VL 11
DI 10.1177/1940082918785079
PG 8
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA GL8YX
UT WOS:000437516300001
OA gold
DA 2022-02-10
ER

PT C
AU Brust, CA
   Burghardt, T
   Groenenberg, M
   Kading, C
   Kuhl, HS
   Manguette, ML
   Denzler, J
AF Brust, Clemens-Alexander
   Burghardt, Tilo
   Groenenberg, Milou
   Kaeding, Christoph
   Kuehl, Hjalmar S.
   Manguette, Marie L.
   Denzler, Joachim
GP IEEE
TI Towards Automated Visual Monitoring of Individual Gorillas in the Wild
SO 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT 16th IEEE International Conference on Computer Vision (ICCV)
CY OCT 22-29, 2017
CL Venice, ITALY
SP IEEE, IEEE Comp Soc
ID CAMERA TRAPS; FACE RECOGNITION; LIFE-HISTORY; HABITAT USE; GREAT APES;
   MBELI-BAI; NEST; CONSERVATION; CHIMPANZEE; ABUNDANCE
AB In this paper we report on the context and evaluation of a system for an automatic interpretation of sightings of individual western lowland gorillas (Gorilla gorilla gorilla) as captured in facial field photography in the wild. This effort aligns with a growing need for effective and integrated monitoring approaches for assessing the status of biodiversity at high spatio-temporal scales. Manual field photography and the utilisation of autonomous camera traps have already transformed the way ecological surveys are conducted. In principle, many environments can now be monitored continuously, and with a higher spatio-temporal resolution than ever before. Yet, the manual effort required to process photographic data to derive relevant information delimits any large scale application of this methodology.
   The described system applies existing computer vision techniques including deep convolutional neural networks to cover the tasks of detection and localisation, as well as individual identification of gorillas in a practically relevant setup. We evaluate the approach on a relatively large and challenging data corpus of 12,765 field images of 147 individual gorillas with image-level labels (i.e. missing bounding boxes) photographed at Mbeli Bai at the Nouabal-Ndoki National Park, Republic of Congo. Results indicate a facial detection rate of 90.8% AP and an individual identification accuracy for ranking within the Top 5 set of 80.3%. We conclude that, whilst keeping the human in the loop is critical, this result is practically relevant as it exemplifies model transferability and has the potential to assist manual identification efforts. We argue further that there is significant need towards integrating computer vision deeper into ecological sampling methodologies and field practice to move the discipline forward and open up new research horizons.
C1 [Brust, Clemens-Alexander; Kaeding, Christoph; Denzler, Joachim] Friedrich Schiller Univ Jena, Comp Vis Grp, Jena, Germany.
   [Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
   [Groenenberg, Milou; Manguette, Marie L.] Wildlife Conservat Soc, Congo Program, Mbeli Bai Study, Kodigehalli, Karnataka, India.
   [Groenenberg, Milou] Wildlife Conservat Soc, Global Conservat Program, Bronx, NY USA.
   [Kaeding, Christoph; Denzler, Joachim] Michael Stifel Ctr Jena, Jena, Germany.
   [Kuehl, Hjalmar S.; Manguette, Marie L.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Kuehl, Hjalmar S.; Denzler, Joachim] German Ctr Integrat Biodivers Res iDiv, Halle Jena Leipzig, Germany.
RP Brust, CA (corresponding author), Friedrich Schiller Univ Jena, Comp Vis Grp, Jena, Germany.
OI Brust, Clemens-Alexander/0000-0001-5419-1998
FU German Research Foundation (DFG)German Research Foundation (DFG) [DE
   735/101]
FX This research was partly supported by grant DE 735/101 of the German
   Research Foundation (DFG). We thank the Ministry of Forest Economy and
   Environment and the Ministry of Scientific Research in the Republic of
   Congo for permission to work in the Nouabale-Ndoki National Park. We are
   grateful to the Wildlife Conservation Society's Congo Program for
   crucial logistical and administrative support. We are indebted to all
   research assistants who contributed to the datasets of the Mbeli Bai
   Study, in particular, Jana Robeyst, Davy Ekouoth, Barbara Hendus, and
   Vidrige Kandza. We are grateful for the financial support provided by
   the funders of the study. The contents of this publication are the sole
   responsibility of its authors and can in no way be taken to reflect the
   views of the funders.
CR Alexander L., 2012, ACM INT WORKSH MULT, P19
   Arandjelovic M, 2010, BIOL CONSERV, V143, P1780, DOI 10.1016/j.biocon.2010.04.030
   Boyer-Ontl KM, 2014, INT J PRIMATOL, V35, P881, DOI 10.1007/s10764-014-9783-3
   Branson S., 2014, BRIT MACH VIS C BMVC
   Breuer T., 2008, THESIS
   Breuer T, 2009, AM J PRIMATOL, V71, P106, DOI 10.1002/ajp.20628
   Brust C.-A., 2015, C COMP VIS THEOR APP
   Buckland S.T., 2001, pi
   Caillaud D, 2006, CURR BIOL, V16, pR489, DOI 10.1016/j.cub.2006.06.017
   Carreira J, 2015, IEEE T PATTERN ANAL, V37, P1177, DOI 10.1109/TPAMI.2014.2361137
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Crunchant AS, 2017, AM J PRIMATOL, V79, DOI 10.1002/ajp.22627
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Freytag A, 2014, LECT NOTES COMPUT SC, V8753, P144, DOI 10.1007/978-3-319-11752-2_12
   Galvis N, 2014, INT J PRIMATOL, V35, P908, DOI 10.1007/s10764-014-9791-3
   Girshick R., 2014, C COMP VIS PATT REC
   Goring C, 2014, PROC CVPR IEEE, P2489, DOI 10.1109/CVPR.2014.319
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   He XF, 2004, ADV NEUR IN, V16, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Head JS, 2012, J TROP ECOL, V28, P571, DOI 10.1017/S0266467412000612
   Howe E. J., 2017, METHODS ECOLOGY EVOL
   Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
   Irani R, 2015, IEEE COMPUT SOC CONF
   Jacobson SK, 1998, CONSERV BIOL, V12, P263, DOI 10.1046/j.1523-1739.1998.97235.x
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kding C., 2016, EUR S ART NEUR NETW
   Khosla A., 2011, C COMP VIS PATT REC
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kingma D. P., 2014, INT C LEARN REPR ICL
   Klailova M, 2012, FOLIA PRIMATOL, V83, P312, DOI 10.1159/000342143
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kuhl H., 2008, BEST PRACTICE GUIDEL
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Laing SE, 2003, J APPL ECOL, V40, P1102, DOI 10.1111/j.1365-2664.2003.00861.x
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu X., 2016, ARXIV160306765
   Long J., 2015, C COMP VIS PATT REC C COMP VIS PATT REC
   Loos A, 2011, EUR SIGNAL PR CONF, P922
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Maffei L, 2005, J TROP ECOL, V21, P349, DOI 10.1017/S0266467405002397
   Manly BF, 2010, HDB CAPTURE RECAPTUR
   McDonald-Madden E, 2008, J APPL ECOL, V45, P1630, DOI 10.1111/j.1365-2664.2008.01553.x
   Mehlman PT, 2002, INT J PRIMATOL, V23, P1257, DOI 10.1023/A:1021126920753
   Miura S., 1997, MALAYAN NATURE J MAL
   Morgan D, 2006, INT J PRIMATOL, V27, P147, DOI 10.1007/s10764-005-9013-0
   Nakashima Y, 2013, AM J PRIMATOL, V75, P1220, DOI 10.1002/ajp.22185
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
   Parkhi O. M., 2015, BRIT MACH VIS C BMVC
   Parnell RJ, 2002, AM J PRIMATOL, V56, P193, DOI 10.1002/ajp.1074
   Pebsworth PA, 2014, INT J PRIMATOL, V35, P825, DOI 10.1007/s10764-014-9802-4
   Prasad S, 2010, ECOL RES, V25, P225, DOI 10.1007/s11284-009-0650-1
   Redmon J., 2016, ARXIV161208242
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Robbins AM, 2016, ROY SOC OPEN SCI, V3, DOI 10.1098/rsos.160533
   Rodner E., 2015, C COMP VIS PATT REC
   Rodner E., 2016, ARXIV PREPRINT ARXIV
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Roy J, 2014, BIOL CONSERV, V180, P249, DOI 10.1016/j.biocon.2014.10.011
   Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
   Simon  Marcel, 2017, ARXIV170500487
   Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163
   Steinmetz R, 2014, J APPL ECOL, V51, P1469, DOI 10.1111/1365-2664.12239
   Stokes EJ, 2003, BEHAV ECOL SOCIOBIOL, V54, P329, DOI 10.1007/s00265-003-0630-3
   Stokes EJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010294
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   TUTIN CEG, 1984, AM J PRIMATOL, V6, P313, DOI 10.1002/ajp.1350060403
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vie J.-C., 2009, WILDLIFE CHANGING WO
   Wah C., 2011, TECHNICAL REPORT
   Walsh PD, 2005, ECOL APPL, V15, P1342, DOI 10.1890/03-5283
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Zhang N., 2014, EUR C COMP VIS ECCV
   Zhou K, 2016, DESTECH TRANS COMP
NR 86
TC 20
Z9 21
U1 0
U2 8
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2473-9936
BN 978-1-5386-1034-3
J9 IEEE INT CONF COMP V
PY 2017
BP 2820
EP 2830
DI 10.1109/ICCVW.2017.333
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4OB
UT WOS:000425239602105
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Doherty, TS
   Hall, ML
   Parkhurst, B
   Westcott, V
AF Doherty, Tim S.
   Hall, Michelle L.
   Parkhurst, Ben
   Westcott, Vanessa
TI Experimentally testing the response of feral cats and their prey to
   poison baiting
SO WILDLIFE RESEARCH
LA English
DT Article; Early Access
DE cat baiting; dynamic occupancy model; impact evaluation; invasive
   predator; lethal control; pest control
ID NEW-SOUTH-WALES; FELIS-CATUS; HOME-RANGE; RED FOXES; R PACKAGE;
   AUSTRALIA; MOVEMENTS; IMPACTS; MAMMALS; ISLAND
AB Context. Feral cats, Felis catus, have caused the decline and extinction of many species worldwide, particularly on islands and in Australia where native species are generally naive to the threat of this introduced predator. Effectively reducing cat populations to protect wildlife is challenging because cats have a cryptic nature, high reproductive rate and strong reinvasion ability.
   Aims. We experimentally tested the response of feral cats and their native prey to an Eradicat (R) poison baiting program at a conservation reserve.
   Methods. Baits were distributed by hand along roads and tracks every 50 m (similar to 10 baits km(-2)). We used camera traps to monitor the response of cats to baiting using a repeated before-after, control-impact design over 6 years. We also measured introduced rabbit, Oryctolagus cuniculus, activity by using sand pads and small mammal and reptile captures by using pitfall trapping.
   Key results. Dynamic occupancy modelling showed only modest effects of baiting on cats in 2 of 6 years, with occupancy in the baited area decreasing from 54% to 19% in 2014 (-35%) and from 89% to 63% in 2017 (-26%). Baiting effectiveness was not related to antecedent rainfall or prey availability. Bait availability was reduced by non-target interference; 73% of 41 monitored baits were removed by non-target species. We found no evidence for persistent changes in small mammal or reptile capture rates in the baited area relative to the unbaited area over the life of the project.
   Conclusions. Relatively low baiting density and non-target interference with baits are likely to have reduced baiting efficacy. Further testing and refinement of ground baiting is needed, including trialling higher baiting densities and/or frequencies.
C1 [Doherty, Tim S.] Univ Sydney, Sch Life & Environm Sci, Camperdown, NSW 2006, Australia.
   [Doherty, Tim S.] Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, 221 Burwood Highway, Burwood, Vic 3125, Australia.
   [Hall, Michelle L.; Parkhurst, Ben; Westcott, Vanessa] Bush Heritage Australia, 1-395 Collins St, Melbourne, Vic 3000, Australia.
   [Hall, Michelle L.] Univ Western Australia, Sch Biol Sci, 35 Stirling Highway, Perth, WA 6009, Australia.
   [Hall, Michelle L.] Univ Melbourne, Sch BioSci, Melbourne, Vic 3010, Australia.
RP Doherty, TS (corresponding author), Univ Sydney, Sch Life & Environm Sci, Camperdown, NSW 2006, Australia.; Doherty, TS (corresponding author), Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, 221 Burwood Highway, Burwood, Vic 3125, Australia.
EM tim.doherty@sydney.edu.au
RI Doherty, Tim S./G-9354-2015; Hall, Michelle/A-1904-2010
OI Doherty, Tim S./0000-0001-7745-0251; Hall, Michelle/0000-0002-1263-8314
FU Earthwatch Institute Australia; Bush Heritage Australia; Edith Cowan
   University; Deakin University; Australian Research CouncilAustralian
   Research Council [DE200100157]
FX Financial support for this project was provided by Earthwatch Institute
   Australia, Bush Heritage Australia and Edith Cowan University. T. S.
   Doherty was supported by Edith Cowan University, Deakin University and
   the Australian Research Council (DE200100157) over the life of the
   project.
CR Algar D., 2007, Conservation Science Western Australia, V6, P109
   Algar Dave, 2011, Conservation Science Western Australia, V8, P367
   Allen BL, 2014, ENVIRON SCI POLLUT R, V21, P2178, DOI 10.1007/s11356-013-2118-7
   Allsop Sinead E., 2017, Pacific Conservation Biology, V23, P240, DOI 10.1071/PC17006
   Barton K, 2019, MUMIN MULTIMODEL INF
   Bell L., 2011, FIELD TRIAL COMP BAI
   Bengsen AJ, 2016, J ZOOL, V298, P112, DOI 10.1111/jzo.12290
   Bengsen AJ, 2012, WILDLIFE RES, V39, P258, DOI 10.1071/WR11097
   Berry O, 2013, WILDLIFE RES, V40, P615, DOI 10.1071/WR13073
   BLOOMER JP, 1992, BIOL CONSERV, V60, P211, DOI 10.1016/0006-3207(92)91253-O
   Bonnaud E, 2011, BIOL INVASIONS, V13, P581, DOI 10.1007/s10530-010-9851-3
   Bureau of Meteorology, 2020, CLIMATE DATA
   Burrows N., 2018, AERIAL GROUND BAITIN
   Burrows ND, 2003, J ARID ENVIRON, V55, P691, DOI 10.1016/S0140-1963(02)00317-8
   Christensen Per E. S., 2013, Ecological Management & Restoration, V14, P47, DOI 10.1111/emr.12025
   Christie AP, 2019, J APPL ECOL, V56, P2742, DOI 10.1111/1365-2664.13499
   Coates Terry D., 2008, Australian Mammalogy, V30, P51
   Comer S, 2020, WILDLIFE RES, V47, P762, DOI 10.1071/WR19217
   Comer S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23495-z
   Doherty T. S., 2015, THESIS E COWAN U PER
   Doherty TS, 2017, MAMMAL REV, V47, P83, DOI 10.1111/mam.12080
   Doherty TS, 2016, P NATL ACAD SCI USA, V113, P11261, DOI 10.1073/pnas.1602480113
   Doherty TS, 2015, AUST MAMMAL, V37, P219, DOI 10.1071/AM14038
   Doherty TS, 2015, ECOL MANAG RESTOR, V16, P124, DOI 10.1111/emr.12158
   Doherty TS, 2015, INT J WILDLAND FIRE, V24, P534, DOI 10.1071/WF14115
   Edwards GP, 2001, AUSTRAL ECOL, V26, P93, DOI 10.1111/j.1442-9993.2001.01091.pp.x
   Fancourt BA, 2021, J ENVIRON MANAGE, V280, DOI 10.1016/j.jenvman.2020.111691
   Fisher P, 2015, APPL ANIM BEHAV SCI, V173, P88, DOI 10.1016/j.applanim.2014.09.010
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Friend JA, 2020, WILDLIFE RES, V47, P747, DOI 10.1071/WR19087
   Geyle H. M., 2020, Ecological Solutions and Evidence, V1, DOI 10.1002/2688-8319.12018
   HARDEN RH, 1985, AUST WILDLIFE RES, V12, P25
   Hilmer S., 2010, THESIS GOETHE U FRAN
   Hohnen R, 2020, WILDLIFE RES, V47, P547, DOI 10.1071/WR19056
   Hone J, 2010, J APPL ECOL, V47, P507, DOI 10.1111/j.1365-2664.2010.01812.x
   Johnston M. J., 2014, FIELD EFFICACY CURIO
   JONES E, 1982, AUST WILDLIFE RES, V9, P409
   Lazenby BT, 2014, WILDLIFE RES, V41, P407, DOI 10.1071/WR14030
   Leahy L, 2015, WILDLIFE RES, V42, P705, DOI 10.1071/WR15011
   Legge S, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.52
   Legge S, 2018, WILDLIFE RES, V45, P627, DOI 10.1071/WR17172
   Leo BT, 2018, PAC SCI, V72, P57, DOI 10.2984/72.1.4
   Letnic M, 2010, BIOL REV, V85, P501, DOI 10.1111/j.1469-185X.2009.00113.x
   Little, 2013, FIELD ASSESSMENT CUR
   Lohr CA, 2020, SCI TOTAL ENVIRON, V720, DOI 10.1016/j.scitotenv.2020.137631
   Loss SR, 2017, FRONT ECOL ENVIRON, V15, P502, DOI 10.1002/fee.1633
   MacKenzie D. I., 2018, OCCUPANCY ESTIMATION
   McGregor H, 2020, BIOL INVASIONS, V22, P799, DOI 10.1007/s10530-019-02131-5
   Medina FM, 2011, GLOBAL CHANGE BIOL, V17, P3503, DOI 10.1111/j.1365-2486.2011.02464.x
   Molsher R, 2005, WILDLIFE RES, V32, P587, DOI 10.1071/WR04093
   Moseby KE, 2011, WILDLIFE RES, V38, P350, DOI 10.1071/WR10236
   Moseby KE, 2011, WILDLIFE RES, V38, P338, DOI 10.1071/WR10235
   Moseby KE, 2009, AUSTRAL ECOL, V34, P156, DOI 10.1111/j.1442-9993.2008.01916.x
   Newsome TM, 2013, ECOGRAPHY, V36, P914, DOI 10.1111/j.1600-0587.2013.00056.x
   Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
   Nogales M, 2013, BIOSCIENCE, V63, P804, DOI 10.1525/bio.2013.63.10.7
   Norbury GL, 2015, BIOL CONSERV, V191, P409, DOI 10.1016/j.biocon.2015.07.031
   Palmer R, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251304
   R Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
   Reddiex B, 2006, WILDLIFE RES, V33, P711, DOI 10.1071/WR05103
   Richards J., 2010, SUSTAINED INTRODUCED
   Robley Alan, 2010, Australian Mammalogy, V32, P23, DOI 10.1071/AM09030
   Roshier DA, 2020, WILDLIFE RES, V47, P570, DOI 10.1071/WR19153
   Ruscoe WA, 2011, ECOL LETT, V14, P1035, DOI 10.1111/j.1461-0248.2011.01673.x
   Salo P, 2007, P ROY SOC B-BIOL SCI, V274, P1237, DOI 10.1098/rspb.2006.0444
   Shionosaki K, 2015, WILDLIFE RES, V42, P343, DOI 10.1071/WR14161
   Short J, 1997, WILDLIFE RES, V24, P703, DOI 10.1071/WR96071
   Stewart A., 2019, CENTRAL ROCK RAT MON
   Stobo-Wilson AM, 2020, WILDLIFE RES, V47, P720, DOI 10.1071/WR19237
   Walsh JC, 2012, ANIM CONSERV, V15, P319, DOI 10.1111/j.1469-1795.2012.00537.x
   Weston MA, 2009, LANDSCAPE URBAN PLAN, V89, P98, DOI 10.1016/j.landurbplan.2008.10.009
   Woinarski J.C., 2019, CATS AUSTR COMPANION
   Woinarski JCZ, 2015, P NATL ACAD SCI USA, V112, P4531, DOI 10.1073/pnas.1417301112
   Wysong ML, 2020, WILDLIFE RES, V47, P557, DOI 10.1071/WR19175
   Wysong ML, 2020, MOV ECOL, V8, DOI 10.1186/s40462-020-00203-z
NR 75
TC 0
Z9 0
U1 4
U2 4
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
DI 10.1071/WR21008
EA AUG 2021
PG 10
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA UC5OS
UT WOS:000686575300001
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Elbroch, LM
   Wittmer, HU
AF Allen, Maximilian L.
   Elbroch, L. Mark
   Wittmer, Heiko U.
TI Can't bear the competition: Energetic losses from kleptoparasitism by a
   dominant scavenger may alter foraging behaviors of an apex predator
SO BASIC AND APPLIED ECOLOGY
LA English
DT Article
DE kleptoparasitism; Predation; Puma concolor; scavenging; Ursus americanus
AB The interspecific interactions of apex predators are integral to the function of ecological communities, but most studies have focused on understanding their top down effects. Kleptoparasitism (the stealing of procured food) by dominant scavengers can have negative effects on populations and behaviors of apex predators. We captured 7 pumas (Puma concolor) and fitted them with GPS collars to investigate potential kill sites (n = 352), some of which we monitored with camera traps (n = 58). We analyzed whether observed kleptoparasitism by American black bears (Ursus americanus) affected puma energetics and foraging behavior. We found that black bears were the most frequent scavenger of puma kills (72.4%), and we documented bears scavenging puma kills during every month. The top model for bear detection of puma kills included prey size, temperature, and canopy cover, with bears more likely to scavenge from adult black-tailed deer (Odocoileus hemionus columbianus) carcasses in warmer temperatures and under dense canopy cover. When black bear scavenging occurred, pumas spent 22% less time at their kill and incurred energetic losses. In response, pumas shortened their inter-kill intervals by 1.3 days thus increasing their kill rates. Our results demonstrate how a dominant scavenger directly mediates the foraging behavior of an apex predator. These results suggest that community interactions do not necessarily start at the top in top-down systems, and the effects of predators on prey populations can only be understood within their respective ecological communities. (C) 2021 Gesellschaft fur Okologie. Published by Elsevier GmbH. All rights reserved.
C1 [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
   [Elbroch, L. Mark] Panthera, 8 West 40th St,18th Floor, New York, NY 10018 USA.
   [Wittmer, Heiko U.] Victoria Univ Wellington, Sch Biol Sci, POB 600, Wellington 6140, New Zealand.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
RI Wittmer, Heiko U/D-4172-2015
OI Wittmer, Heiko U/0000-0002-8861-188X
FU California Department of Fish and Wildlife; University of California at
   DavisUniversity of California System; California Deer Association;
   Victoria University of Wellington tuition scholarship
FX The California Department of Fish and Wildlife, the University of
   California at Davis, and the California Deer Association generously
   provided funding for the project. M. Allen was supported by a Victoria
   University of Wellington tuition scholarship. We thank B. Millsap, C.
   Wiley and D. Tichenor for their expertise and help in capturing pumas;
   and D. Casady, J. Golla, B. Evans, and many others for their help on the
   project. Constructive feedback from two anonymous reviewers improved
   previous drafts of our manuscript.
CR Ackerman B. B., 1982, THESIS
   Allen ML, 2015, CALIF FISH GAME, V101, P51
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Anderson K. A., 2002, MODEL SELECTION MULT
   Bacon MM, 2011, WILDLIFE SOC B, V35, P409, DOI 10.1002/wsb.85
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Carbone C, 1997, J ANIM ECOL, V66, P318, DOI 10.2307/5978
   Clark DA, 2014, J WILDLIFE MANAGE, V78, P1161, DOI 10.1002/jwmg.760
   Cristescu B., 2019, J ZOOL, V309, P259
   Cristescu B., 2020, BIIOGEOGRAPHICAL ECO, DOI [10.1101/2020.10.04.325779, DOI 10.1101/2020.10.04.325779]
   Cross PC, 2016, ECOLOGY, V97, P1938, DOI 10.1890/15-1346.1
   Danvir R.E., 1981, Encyclia, V58, P50
   De Roos AM, 2008, P NATL ACAD SCI USA, V105, P13930, DOI 10.1073/pnas.0803834105
   DeLong JP, 2015, AM NAT, V185, P354, DOI 10.1086/679735
   Elbroch LM, 2017, BIOL CONSERV, V215, P123, DOI 10.1016/j.biocon.2017.08.026
   Elbroch LM, 2015, BEHAV ECOL, V26, P247, DOI 10.1093/beheco/aru189
   Elbroch LM, 2014, ECOSPHERE, V5, DOI 10.1890/ES13-00373.1
   ESTES JA, 1974, SCIENCE, V185, P1058, DOI 10.1126/science.185.4156.1058
   Forrester TD, 2019, WILDLIFE BIOL, DOI 10.2981/wlb.00510
   Hayward MW, 2006, J ZOOL, V270, P298, DOI 10.1111/j.1469-7998.2006.00139.x
   Heffelfinger J., 2010, ARIZONA GAME FISH DE
   Helldin JO, 2007, WILDLIFE BIOL, V13, P475, DOI 10.2981/0909-6396(2007)13[475:CIRFVV]2.0.CO;2
   Jameson E.W., 2004, MAMMALS CALIFORNIA
   Knopff KH, 2010, J WILDLIFE MANAGE, V74, P1435, DOI 10.2193/2009-314
   Krofel M, 2016, BIOL CONSERV, V197, P40, DOI 10.1016/j.biocon.2016.02.019
   Krofel M, 2012, BEHAV ECOL SOCIOBIOL, V66, P1297, DOI 10.1007/s00265-012-1384-6
   Low W. A., 1963, Journal of Wildlife Management, V27, P466, DOI 10.2307/3798521
   Marescot L, 2015, POPUL ECOL, V57, P185, DOI 10.1007/s10144-014-0456-z
   MCLAREN BE, 1994, SCIENCE, V266, P1555, DOI 10.1126/science.266.5190.1555
   Metz MC, 2012, J ANIM ECOL, V81, P553, DOI 10.1111/j.1365-2656.2011.01945.x
   PARKER KL, 1993, CAN J ZOOL, V71, P1397, DOI 10.1139/z93-193
   Pinheiro J., 2013, NMLE LINEAR NONLINEA
   R Core Team, 2017, R LANG ENV STAT COMP
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Ripple WJ, 2001, BIOL CONSERV, V102, P227, DOI 10.1016/S0006-3207(01)00107-0
   Ruth TK, 2011, J WILDLIFE MANAGE, V75, P1381, DOI 10.1002/jwmg.190
   Ruth Toni K., 2010, P163
   Sibley D. A., 2005, SIBLEY FIELD GUIDE B
   Wilckens DT, 2016, J MAMMAL, V97, P373, DOI 10.1093/jmammal/gyv183
   Wilmers CC, 2003, J ANIM ECOL, V72, P909, DOI 10.1046/j.1365-2656.2003.00766.x
NR 40
TC 1
Z9 1
U1 4
U2 8
PU ELSEVIER GMBH
PI MUNICH
PA HACKERBRUCKE 6, 80335 MUNICH, GERMANY
SN 1439-1791
EI 1618-0089
J9 BASIC APPL ECOL
JI Basic Appl. Ecol.
PD MAR
PY 2021
VL 51
BP 1
EP 10
DI 10.1016/j.baae.2021.01.011
PG 10
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QL3JK
UT WOS:000620976100001
DA 2022-02-10
ER

PT J
AU Phosri, K
   Tantipisanuh, N
   Chutipong, W
   Gore, ML
   Giordano, AJ
   Ngoprasert, D
AF Phosri, Kitipat
   Tantipisanuh, Naruemon
   Chutipong, Wanlop
   Gore, Meredith L.
   Giordano, Anthony J.
   Ngoprasert, Dusit
TI Fishing cats in an anthropogenic landscape: A multi-method assessment of
   local population status and threats
SO GLOBAL ECOLOGY AND CONSERVATION
LA English
DT Article
DE Camera-trapping; Human dimensions; Interviews; Prionailurus viverrinus;
   Spatially explicit capture-recapture; Thailand
ID PRIONAILURUS-VIVERRINUS BENNETT; 1833 CARNIVORA FELIDAE; CONSERVATION;
   WILDLIFE; BEAR; ECOLOGY
AB Fishing cat populations appear to have declined significantly in recent years due to the loss and fragmentation of inland and coastal wetland habitats. Moreover, there are still large gaps in data on population and density estimates, and threat evaluation, which are vital for conservation assessments. This research aimed to help fill these critical knowledge gaps. Our study is the first density estimate for fishing cat from mainland Southeast Asia. We conducted a camera-trap survey and used a spatially-explicit capture-recapture analytical framework to estimate the abundance of fishing cats in and around Khao Sam Roi Yot area (KSRY), which hosts an isolated, threatened population of the felid in Thailand. We also conducted interviews among adjacent communities to better understand local perspectives toward fishing cats, conflict with local people, and as a consequence of both, anthropogenic threats to the population. Over 6966 trap-days, we identified at least 33 individual adult cats and based on our top model (g0 similar to bk, sigma similar to h2), we estimated fishing cat density to be 18 +/- SE 6 individuals/100 km(2) (95% CI 10 - 33). Among 80 interviewees, we recorded 25 incidents of conflict, most relating to raids on poultry (n = 18) and damage to fishing gear in pursuit of fish (n = 5). Land use type, land use change, and human activity, did not significantly affect fishing cat density and movements. Our findings further suggest that a proposed tax policy governing land use may force landowners to convert suitable fishing cat habitat to unsuitable areas, resulting in the loss of up to 30% of existing suitable habitat from our study area. We also found that local communities would support either an exemption for landowners not wishing to develop suitable fishing cat habitat, and/or an additional policies that incentivize the maintenance and/or preservation of areas suitable for fishing cats. Finally, we conclude that the official presence of park officers in communities beyond the protected area would be beneficial, as would the implementation of public outreach programming to mitigate negative attitudes toward fishing cats, and provide recommendations on strategies for coexisting with them. (C) 2021 The Author(s). Published by Elsevier B.V. CC_BY_NC_ND_4.0
C1 [Phosri, Kitipat; Ngoprasert, Dusit] King Mongkuts Univ Technol Thonburi, Sch Bioresources & Technol, Conservat Ecol Program, Bangkok 10150, Thailand.
   [Tantipisanuh, Naruemon; Chutipong, Wanlop; Ngoprasert, Dusit] King Mongkuts Univ Technol Thonburi, Pilot Plant Dev & Training Inst, Conservat Ecol Program, Bangkok 10150, Thailand.
   [Gore, Meredith L.] Univ Maryland, Dept Geog Sci, College Pk, MD 20742 USA.
   [Phosri, Kitipat; Giordano, Anthony J.; Ngoprasert, Dusit] SPECIES Soc Preservat Endangered Carnivores & Int, POB 7403, Ventura, CA 93006 USA.
RP Ngoprasert, D (corresponding author), King Mongkuts Univ Technol Thonburi, Pilot Plant Dev & Training Inst, Conservat Ecol Program, Bangkok 10150, Thailand.
EM ndusit@gmail.com
FU King Mongkut's University of Technology Thonburi; ASAHI Glass
   Foundation, Japan; Society for the Preservation of Endangered Carnivores
   and their International Ecological Study
FX We appreciate financial supports from the King Mongkut's University of
   Technology Thonburi (KM2017) , the ASAHI Glass Foundation, Japan (2018)
   , and Society for the Preservation of Endangered Carnivores and their
   International Ecological Study (S.P.E.C.I.E.S.) . We would like to thank
   the Thai Department of National Parks, Wildlife and Plant Conservation,
   KSRY director R. Asawakultharin and all park staff for supporting our
   work. We thank M. Grainger for the useful comments and M. Namkhan for
   helping on GIS analysis. We especially thank our field assistants (J.
   Tananantayot, R. Angkaew, S. Khamngam, N. Sangpan and E. Sidum) for
   field work, and appreciate local villagers who participated in
   questionnaire survey.
CR Akaike H., 1998, P 2 INT S INF THEOR, P199, DOI [10.1007/978-1-4612-1694-0_15, 10.1007/978-1-4612-1694-0]
   Ali A, 2018, ETHOL ECOL EVOL, V30, P399, DOI 10.1080/03949370.2017.1423113
   Anderson D.R, 2002, TECHNOMETRICS
   [Anonymous], ROYAL THAI GOVT GAZE, V136, P104
   [Anonymous], ROYAL THAI GOVT GAZE, V136, P21
   Appel, 2016, P 1 INT FISH CAT CON, P48
   Augustine B, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2627
   Barbier EB, 2004, LAND ECON, V80, P389, DOI 10.2307/3654728
   Barrientos R., 2017, RAILWAY ECOLOGY, P43
   Barrientos R, 2019, EUR J WILDLIFE RES, V65, DOI 10.1007/s10344-018-1248-0
   Bauer H, 2017, ORYX, V51, P106, DOI 10.1017/S003060531500068X
   Braczkowski Alexander, 2013, Cat News, V58, P13
   Braczkowski AR, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151033
   Brodie JF, 2013, BIOL CONSERV, V163, P58, DOI 10.1016/j.biocon.2013.01.003
   Chapron G, 2014, SCIENCE, V346, P1517, DOI 10.1126/science.1257553
   Cherdymova EI, 2018, EKOLOJI, V27, P541
   Chowdhury Sayam U., 2015, Cat News, V62, P4
   Chutipong Wanlop, 2019, Journal of Threatened Taxa, V11, P13459, DOI 10.11609/jott.4557.11.4.13459-13469
   Cutter P, 2015, THESIS U MINNESOTA
   DAS KS, 2017, CAT, V66, P25
   Daszak P, 2000, SCIENCE, V287, P443, DOI 10.1126/science.287.5452.443
   Davis EO, 2019, BIOL CONSERV, V235, P119, DOI 10.1016/j.biocon.2019.04.003
   Department of National Parks Wildlife and Plant Conservation, 2015, KHAO SAM ROI YOT NAT
   Dickman AJ, 2011, P NATL ACAD SCI USA, V108, P13937, DOI 10.1073/pnas.1012972108
   Duckworth J.W, 2016, P 1 INT FISH CAT CON
   Duckworth J.W., 2016, P 1 INT FISH CAT CON, P25
   Efford M, 2019, SECRDESIGN SAMPLING
   Efford M.G, SECR SPATIALLY EXPLI
   Espinosa S, 2012, J ENVIRON EDUC, V43, P55, DOI 10.1080/00958964.2011.579642
   Fukuda S, 2017, WORLD C TRANSP RES W
   Gore ML, 2013, CONSERV LETT, V6, P430, DOI 10.1111/conl.12032
   Greenspan E, 2020, HUM DIMENS WILDL, V25, P301, DOI 10.1080/10871209.2020.1728789
   Havmoller RW, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0209541
   Huitric M, 2002, ECOL ECON, V40, P441, DOI 10.1016/S0921-8009(02)00011-3
   Jakes AF, 2018, BIOL CONSERV, V227, P310, DOI 10.1016/j.biocon.2018.09.026
   Jones S, 2019, CONSERV BIOL, V33, P895, DOI 10.1111/cobi.13275
   Kahler JS, 2013, CONSERV BIOL, V27, P177, DOI 10.1111/j.1523-1739.2012.01960.x
   KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
   LeClerq AT, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.131
   Lin Naing, 2019, Journal of Threatened Taxa, V11, P13910, DOI 10.11609/jott.4795.11.7.13910-13914
   Lucas P.S., 2017, RAILW ECOL, P81, DOI [DOI 10.1007/978-3-319-57496-7_6, 10.1007/978-3-319-57496-7_6]
   Machado Renata F, 2017, Ecol. austral, V27, P232, DOI 10.25260/EA.17.27.2.0.416
   Mishra C, 2003, CONSERV BIOL, V17, P1512, DOI 10.1111/j.1523-1739.2003.00092.x
   Mohamed A, 2021, ORYX, V55, P56, DOI 10.1017/S0030605318001503
   Mukherjee S, 2010, IUCN RED LIST THREAT
   Mukherjee S, 2016, IUCN RED LIST THREAT
   Nair S, 2012, HABITAT USE ABUNDANC
   Nuno A, 2015, BIOL CONSERV, V189, P5, DOI 10.1016/j.biocon.2014.09.047
   Official Statistics Registration Systems, 2010, PRACH KHIR POP
   OTP, 2015, STRAT IMPR TRANSP IN
   Pathumratanathan S, 2015, THESIS GRADUATE SCH
   POLLOCK KH, 1990, WILDLIFE MONOGR, P1
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Ramsar, 2015, THAIL KHAO SAM ROI Y
   Schlexer Fredrick V., 2008, P263
   Setianto A., 2013, J APPL GEOL, V5, P21, DOI [10.22146/jag.7204, DOI 10.22146/JAG.7204]
   Shirley EA, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0207973
   Sidorovich A.A., 2020, ZOODIVERSITY, V54, P211
   Sollmann R, 2011, BIOL CONSERV, V144, P1017, DOI 10.1016/j.biocon.2010.12.011
   Thai Meteorological Department, 2013, MONTHL ANN RAINF SEL
   Thaung R, 2018, ORYX, V52, P636, DOI 10.1017/S0030605317001491
   Ullah Z, 2020, GLOB ECOL CONSERV, V24, DOI 10.1016/j.gecco.2020.e01351
   Young JK, 2018, ANIM CONSERV, V21, P285, DOI 10.1111/acv.12438
   Young JC, 2018, METHODS ECOL EVOL, V9, P10, DOI 10.1111/2041-210X.12828
NR 64
TC 0
Z9 0
U1 4
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
EI 2351-9894
J9 GLOB ECOL CONSERV
JI Glob. Ecol. Conserv.
PD JUN
PY 2021
VL 27
AR e01615
DI 10.1016/j.gecco.2021.e01615
PG 16
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA SU9NB
UT WOS:000663455600004
OA gold
DA 2022-02-10
ER

PT C
AU Wang, HN
   Su, H
   Chen, P
   Hou, R
   Zhang, ZH
   Xie, WY
AF Wang, Hongnian
   Su, Han
   Chen, Peng
   Hou, Rong
   Zhang, Zhihe
   Xie, Weiyi
GP IEEE
TI Learning Deep Features for Giant Panda Gender Classification using Face
   Images
SO 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS
   (ICCVW)
SE IEEE International Conference on Computer Vision Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF International Conference on Computer Vision (ICCV)
CY OCT 27-NOV 02, 2019
CL Seoul, SOUTH KOREA
SP IEEE, IEEE Comp Soc, CVF
ID FOOTPRINTS
AB Giant panda (panda) has lived on earth for at least eight million years and is known as the living fossil. It is also a vulnerable species which requires urgent protection. It is essential to conduct population survey collecting information of their population, density, age structure, and gender ratio so as to design protection schemes and measure their effectiveness. However, it is challenging to accurately and timely obtain gender ratio of pandas because their pelage lacks distinguishable gender patterns and panda is sparsely distributed population in large habitats. All current approaches rely heavily on manual collection of samples in the wild, which are time consuming, costly, or even dangerous. With the widely deployed camera traps, if the gender of pandas can be determined from images, it is possible to monitor panda gender ratio in different regions in real-time. However, no such study was done. In this paper, a deep learning method is developed to study the distinctiveness of panda face for gender classification, in which the largest panda image dataset with 6,549 panda face images collected from 100 male and 121 female pandas is established. The experimental results show that panda faces contain some gender information, although they look very similar to human vision.
C1 [Wang, Hongnian; Su, Han; Xie, Weiyi] Sichuan Normal Univ, Chengdu, Sichuan, Peoples R China.
   [Chen, Peng; Hou, Rong; Zhang, Zhihe] Chengdu Res Base Giant Panda Breeding, Chengdu, Sichuan, Peoples R China.
RP Wang, HN (corresponding author), Sichuan Normal Univ, Chengdu, Sichuan, Peoples R China.
EM hongnianwang@gmail.com; jkxy_sh@scinu.edu.cn; capricorncp@163.com;
   405536517@qq.com; zzh@panda.org.cn; xwylove@gmail.com
OI Wang, Hongnian/0000-0002-7543-3957
FU Chengdu Research Base of Giant Panda Breeding [CPB2018-02, CPB2018-01];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61403266, 61403196, 31300306]; Chinese
   overseas returnees science and technology activities project funding of
   Ministry of Human Resources and Social Security; Sichuan Science and
   Technology Program [2018JY0096]
FX This research is supported by Chengdu Research Base of Giant Panda
   Breeding (NO. CPB2018-02, CPB2018-01), the National Natural Science
   Foundation of China (61403266, 61403196, and 31300306), Chinese overseas
   returnees science and technology activities project funding of Ministry
   of Human Resources and Social Security, and the Sichuan Science and
   Technology Program (2018JY0096).
CR Alibhai S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172065
   [Anonymous], 2015, TECHNICAL REPORT
   Castrillon-Santana M, 2016, PATTERN RECOGN LETT, V82, P181, DOI 10.1016/j.patrec.2015.09.014
   Dehghan A., 2017, ARXIV170204280
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Gu J, 2014, WILDLIFE SOC B, V38, P495, DOI 10.1002/wsb.432
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He Q., 2019, ARXIV190803391
   IOFFE S, 2015, ARXIV 1502 03167, V1502, DOI DOI 10.1007/S13398-014-0173-7.2
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   Kumar S, 2017, IET BIOMETRICS, V6, P139, DOI 10.1049/iet-bmt.2016.0017
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li BBV, 2018, BIOL CONSERV, V218, P83, DOI 10.1016/j.biocon.2017.11.029
   Li S., 2019, ARXIV190605586
   Liu YZ, 2018, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2018.00093
   Matkowski W. M., 2019, ABS190511163 ARXIV
   Moorhouse TP, 2005, J APPL ECOL, V42, P91, DOI 10.1111/j.1365-2664.2005.00998.x
   Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
   Polzounov A., 2016, ARXIV160405605
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Su H, 2014, IEEE T INF FOREN SEC, V9, P666, DOI 10.1109/TIFS.2014.2306591
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villa A. Gomez, 2016, ECOLOGICAL INFORM, V41
   Wang HN, 2019, IEEE INT CONF MOB DA, P304, DOI 10.1109/MDM.2019.00-44
   Zhan XJ, 2007, MOL ECOL, V16, P3792, DOI 10.1111/j.1365-294X.2007.03450.x
   Zhan XJ, 2009, URSUS, V20, P56
NR 28
TC 5
Z9 5
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2473-9936
BN 978-1-7281-5023-9
J9 IEEE INT CONF COMP V
PY 2019
BP 279
EP 285
DI 10.1109/ICCVW.2019.00037
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BP4UH
UT WOS:000554591600031
DA 2022-02-10
ER

PT J
AU Wang, YW
   Allen, ML
   Wilmers, CC
AF Wang, Yiwei
   Allen, Maximilian L.
   Wilmers, Christopher C.
TI Mesopredator spatial and temporal responses to large predators and human
   development in the Santa Cruz Mountains of California
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Mesopredator; Puma concolor; Occupancy; Habitat fragmentation;
   Mesopredator release; Trophic cascade; Canis latrans; Recreation;
   Competition
ID INTRAGUILD PREDATION; EXURBAN DEVELOPMENT; MODEL SELECTION; COYOTES;
   HABITAT; CARNIVORES; PATTERNS; RELEASE; URBANIZATION; COMPETITION
AB Human-driven declines of apex predators can trigger widespread impacts throughout ecological communities. Reduced apex predator occupancy or activity can release mesopredators from intraguild competition, with unknown repercussions on the ecological community. As exurban development continues to expand worldwide, it is important to document how mesopredators are impacted by the combined influences of apex predators and humans. We used motion-detecting camera traps to examine spatial and temporal patterns of meso- and apex predator occupancy and activity in a fragmented landscape in California. We hypothesized that both spatial and temporal partitioning among the carnivore guild would be affected by varied levels of human influence. We found that higher residential development reduced puma occupancy but was not related to the occupancy of mesopredators. Bobcats, grey foxes, and Virginia opossums were detected more often at sites occupied by pumas, whereas coyotes and raccoons were detected less often. The detection probabilities of smaller mesopredators were related to coyotes, a dominant mesopredator, but the magnitude and direction of these correlations differed depending upon puma occupancy. We also found that species altered their activities temporally in locations with higher human use, with pumas, bobcats and coyotes reducing diurnal activities and increasing nocturnal ones. These activity shifts were reflected in reduced temporal partitioning between intraguild competitors, with unknown effects on species interactions and repercussions to the prey community. Our results suggest that human development and activity alters predator community structure through both direct and indirect pathways. Therefore effective carnivore conservation requires an understanding of how mesopredators respond to varying levels of apex predator and anthropogenic influences. (C) 2015 The Authors. Published by Elsevier Ltd.
C1 [Wang, Yiwei; Allen, Maximilian L.; Wilmers, Christopher C.] Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Dept Environm Studies, Santa Cruz, CA 95064 USA.
RP Wang, YW (corresponding author), Univ Calif Santa Cruz, Ctr Integrated Spatial Res, Dept Environm Studies, 1156 High St, Santa Cruz, CA 95064 USA.
EM yiweiwang@dataone.unm.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU American Mammalogy Association; Sigma Xi [G2009101017]; NSFNational
   Science Foundation (NSF) [0963022]; Gordon and Betty Moore
   FoundationGordon and Betty Moore Foundation; Department of Environmental
   Studies at University of California Santa Cruz
FX We thank the numerous public and private landowners for giving us access
   to their land and for their willingness to support science and
   conservation. Many undergraduate interns contributed to this project by
   maintaining cameras in the field and sorting camera photographs. In
   particular, we thank Lee Hibbeler, who helped coordinate many of these
   activities. We thank Barry Nickel and Aaron Cole for help obtaining GIS
   data, Yasaman Shakeri and Paul Houghtaling for providing field
   assistance, and Jim Estes for insightful comments on the manuscript.
   Funding was provided by the American Mammalogy Association, Sigma Xi
   Grant G2009101017, NSF Grant #0963022, the Gordon and Betty Moore
   Foundation, and the Department of Environmental Studies at University of
   California Santa Cruz.
CR Allen ML, 2015, CALIF FISH GAME, V101, P51
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Anderson DR, 2002, J WILDLIFE MANAGE, V66, P912, DOI 10.2307/3803155
   Arnold TW, 2010, J WILDLIFE MANAGE, V74, P1175, DOI 10.2193/2009-367
   Bateman PW, 2012, J ZOOL, V287, P1, DOI 10.1111/j.1469-7998.2011.00887.x
   Berger KM, 2008, ECOL APPL, V18, P599, DOI 10.1890/07-0308.1
   Bidlack A.L., 2007, ENV SCI POLICY MANAG, P201
   Brook LA, 2012, J APPL ECOL, V49, P1278, DOI 10.1111/j.1365-2664.2012.02207.x
   Carter NH, 2012, P NATL ACAD SCI USA, V109, P15360, DOI 10.1073/pnas.1210490109
   Crooks KR, 1999, NATURE, V400, P563, DOI 10.1038/23028
   Crooks KR, 2002, CONSERV BIOL, V16, P488, DOI 10.1046/j.1523-1739.2002.00386.x
   Cypher B.L, 2010, URBAN CARNIVORES ECO
   Elmhagen B, 2010, J ANIM ECOL, V79, P785, DOI 10.1111/j.1365-2656.2010.01678.x
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Fedriani JM, 2000, OECOLOGIA, V125, P258, DOI 10.1007/s004420000448
   Gehrt SD, 2003, WILDLIFE SOC B, V31, P836
   Gehrt SD, 2007, BEHAV ECOL, V18, P204, DOI 10.1093/beheco/arl075
   Gehrt SD, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075718
   George SL, 2006, BIOL CONSERV, V133, P107, DOI 10.1016/j.biocon.2006.05.024
   Ginger SM, 2003, J MAMMAL, V84, P1279, DOI 10.1644/103
   Goad EH, 2014, BIOL CONSERV, V176, P172, DOI 10.1016/j.biocon.2014.05.016
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Gosselink TE, 2003, J WILDLIFE MANAGE, V67, P90, DOI 10.2307/3803065
   Hansen AJ, 2005, ECOL APPL, V15, P1893, DOI 10.1890/05-5221
   Hass CC, 2009, J ZOOL, V278, P174, DOI 10.1111/j.1469-7998.2009.00565.x
   Hayward MW, 2009, S AFR J WILDL RES, V39, P109, DOI 10.3957/056.039.0207
   Johnson CN, 2007, P R SOC B, V274, P341, DOI 10.1098/rspb.2006.3711
   Johnson JB, 2004, TRENDS ECOL EVOL, V19, P101, DOI 10.1016/j.tree.2003.10.013
   Lennartz S., 2008, FINAL REPORT LAND CO
   Levi T, 2012, ECOLOGY, V93, P921, DOI 10.1890/11-0165.1
   Linkie M, 2011, J ZOOL, V284, P224, DOI 10.1111/j.1469-7998.2011.00801.x
   Logan K. A., 2001, DESERT PUMA EVOLUTIO
   Lucherini M, 2009, J MAMMAL, V90, P1404, DOI 10.1644/09-MAMM-A-002R.1
   MacKenzie DI, 2004, J ANIM ECOL, V73, P546, DOI 10.1111/j.0021-8790.2004.00828.x
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
   McKinney ML, 2006, BIOL CONSERV, V127, P247, DOI 10.1016/j.biocon.2005.09.005
   Meredith M., 2014, OVERLAP ESTIMATES CO
   Monterroso P, 2013, ETHOLOGY, V119, P1044, DOI 10.1111/eth.12156
   Noss RF, 1996, CONSERV BIOL, V10, P949, DOI 10.1046/j.1523-1739.1996.10040949.x
   Ordenana MA, 2010, J MAMMAL, V91, P1322, DOI 10.1644/09-MAMM-A-312.1
   Pace ML, 1999, TRENDS ECOL EVOL, V14, P483, DOI 10.1016/S0169-5347(99)01723-1
   Palomares F, 1999, AM NAT, V153, P492, DOI 10.1086/303189
   POLIS GA, 1992, TRENDS ECOL EVOL, V7, P151, DOI 10.1016/0169-5347(92)90208-S
   Prange S, 2004, CAN J ZOOL, V82, P1804, DOI 10.1139/Z04-179
   R Development Core Team, 2013, R LANG ENV STAT COMP
   Recio M.R., CURRENT ZOO IN PRESS
   Reed SE, 2008, CONSERV LETT, V1, P146, DOI 10.1111/j.1755-263X.2008.00019.x
   Richmond OMW, 2010, ECOL APPL, V20, P2036, DOI 10.1890/09-0470.1
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Riley SPD, 2006, J WILDLIFE MANAGE, V70, P1425, DOI 10.2193/0022-541X(2006)70[1425:SEOBAG]2.0.CO;2
   Riley SPD, 2003, CONSERV BIOL, V17, P566, DOI 10.1046/j.1523-1739.2003.01458.x
   Rissman AR, 2008, ECOL SOC, V13
   Ritchie EG, 2009, ECOL LETT, V12, P982, DOI 10.1111/j.1461-0248.2009.01347.x
   Ruth T.K., 2009, COUGARS ECOLOGY CONS, P163
   Salek M., 2014, MAMMAL REV
   Schuette P, 2013, BIOL CONSERV, V158, P301, DOI 10.1016/j.biocon.2012.08.008
   Selva N, 2005, CAN J ZOOL, V83, P1590, DOI 10.1139/Z05-158
   Smith JA, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2014.2711
   Wang YW, 2012, WILDLIFE RES, V39, P611, DOI 10.1071/WR11210
   Wilmers CC, 2005, PLOS BIOL, V3, P571, DOI 10.1371/journal.pbio.0030092
   Wilmers CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060590
   Wilson RR, 2010, OECOLOGIA, V164, P921, DOI 10.1007/s00442-010-1797-8
   Zaradic PA, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007367
NR 63
TC 117
Z9 122
U1 5
U2 235
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD OCT
PY 2015
VL 190
BP 23
EP 33
DI 10.1016/j.biocon.2015.05.007
PG 11
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA CO2FS
UT WOS:000358972200004
OA hybrid
DA 2022-02-10
ER

PT J
AU Robinson, NJ
   Johnsen, S
   Brooks, A
   Frey, L
   Judkins, H
   Vecchione, M
   Widder, E
AF Robinson, Nathan J.
   Johnsen, Sonke
   Brooks, Annabelle
   Frey, Lee
   Judkins, Heather
   Vecchione, Michael
   Widder, Edith
TI Studying the swift, smart, and shy: Unobtrusive camera-platforms for
   observing large deep-sea squid
SO DEEP-SEA RESEARCH PART I-OCEANOGRAPHIC RESEARCH PAPERS
LA English
DT Article
DE Architeuthis; Promachoteuthis; Pholidoteuthis; Cephalopod; Gulf of
   Mexico; Pelagic; Twilight zone; Medusa; Eye-in-the-sea
ID LIVE GIANT-SQUID; ARCHITEUTHIS-DUX; CEPHALOPODS; ATLANTIC; OEGOPSIDA;
   KNOWLEDGE; BEHAVIOR; RECORDS; OCEAN; RED
AB The legend of the "kraken" has captivated humans for millennia, yet our knowledge of the large deep-sea cephalopods that inspired this myth remains limited. Conventional methods for exploring the deep sea, including the use of nets, manned submersibles, and remotely operated vehicles (ROVs), are primarily suited for studying slow-moving or sessile organisms, and baited camera-traps tend to attract scavengers rather than predators. To address these issues, unobtrusive deep-sea camera platforms were developed that used low-light cameras, red illuminators, and bioluminescence-mimicking lures. Here, we report on several opportunistic deployments of these devices in the Wider Caribbean Region where we recorded several encounters with large deep-sea squids, including the giant squid Architeuthis dux Steenstrup 1857, Pholidoteuthis adami Voss 1956, and two large squid that may be Promachoteuthis sp. (possibly P. sloani Young et al. 2006). These species were recorded between depths of 557 and 950 m. We estimate the Mantle Lengths (ML) of Promachoteuthis were -1.0 m, the ML of the Pholidoteuthis was -0.5 m, and the ML of the Archtiteuthis was -1.7 m. These encounters suggest that unobtrusive camera platforms with luminescent lures are effective tools for attracting and studying large deep-sea squids.
C1 [Robinson, Nathan J.; Brooks, Annabelle] Cape Eleuthera Inst, POB EL-26029, Rock Sound, Eleuthera, Bahamas.
   [Johnsen, Sonke] Duke Univ, Dept Biol, Durham, NC 27708 USA.
   [Frey, Lee] Arctic Rays, Groton, MA 01450 USA.
   [Judkins, Heather] Univ South Florida St Petersburg, Dept Integrat Biol, St Petersburg, FL 33701 USA.
   [Vecchione, Michael] NOAA, NMFS Natl Systemat Lab, Natl Museum Nat Hist, Washington, DC 20013 USA.
   [Widder, Edith] Ocean Res & Conservat Assoc, Ft Pierce, FL 34949 USA.
   [Robinson, Nathan J.] Fdn Oceanog Ciudad Artes & Ciencias, Valencia, Spain.
RP Robinson, NJ (corresponding author), Cape Eleuthera Inst, POB EL-26029, Rock Sound, Eleuthera, Bahamas.; Robinson, NJ (corresponding author), Fdn Oceanog Ciudad Artes & Ciencias, Valencia, Spain.
EM nathanjackrobinson@gmail.com
RI Robinson, Nathan/AAE-4717-2019
OI Robinson, Nathan/0000-0001-7384-3576
FU National Science FoundationNational Science Foundation (NSF)
   [OCE-1008145]; Monterey Bay Aquarium Research Institute; NOAA's Office
   of Ocean ExplorationNational Oceanic Atmospheric Admin (NOAA) - USA;
   Office of Naval ResearchOffice of Naval Research; National Oceanographic
   and Atmospheric Administration's Office of Exploration and Research
   [NA04OAR4600057, NA05OAR4601059, NA17OAR0110208]; Japan Broadcasting
   Corp. (NHK); Harbor Branch Oceanographic Institution; Discovery Channel;
   Cape Eleuthera Institute
FX Funding for Medusa was provided by National Science Foundation (Grant
   #OCE-1008145). Development of Eye-In-The-Sea was funded in part by the
   Monterey Bay Aquarium Research Institute and Harbor Branch Oceanographic
   Institution and grants from NOAA's Office of Ocean Exploration and the
   Office of Naval Research. The National Oceanographic and Atmospheric
   Administration's Office of Exploration and Research funded the 2004,
   2005, and 2019 expeditions in the Gulf of Mexico (Grants #`s
   NA04OAR4600057, NA05OAR4601059, and NA17OAR0110208 respectively). The
   Japan Broadcasting Corp. (NHK) and the Discovery Channel funded the 2012
   expedition off Japan. The Cape Eleuthera Institute funded the
   deployments of the Medusa in 2013 in the Exuma Sound. We thank the
   captain and crew of the R/V Point Sur as well as Alexander Davis, Dante
   Fenolio, Megan McCall, Heather Bracken-Grissom, Lorian Schweikert,
   Ruchao Qian, Tamara Frank, Tracey Sutton for assistance in the field
   during the 2019 expedition in the Gulf of Mexico. Additional field
   assistance was also provided by the interns and students at the Cape
   Eleuthera Island School for the Medusa deployments in the Exuma Sound.
   Edward Brooks provided essential support and served as academic advisor
   for the deep-sea research program at the Cape Eleuthera Institute.
CR Amante C., 2009, ETOPO1 1 ARC MINUTE, DOI 10.7289/V5C8276M
   Andre M, 2011, FRONT ECOL ENVIRON, V9, P489, DOI 10.1890/100124
   Bolstad KS, 2004, NEW ZEAL J ZOOL, V31, P15, DOI 10.1080/03014223.2004.9518354
   Bustamante P, 2008, MAR ENVIRON RES, V66, P278, DOI 10.1016/j.marenvres.2008.04.003
   Cherel Y, 2009, BIOL LETTERS, V5, P364, DOI 10.1098/rsbl.2009.0024
   Coro G, 2015, ECOL MODEL, V305, P29, DOI 10.1016/j.ecolmodel.2015.03.011
   Diete RL, 2015, AUST J ZOOL, V63, P376, DOI 10.1071/ZO15050
   Ellis Richard., 1998, SEARCH GIANT SQUID
   FRANK TM, 1988, BIOL BULL, V175, P261, DOI 10.2307/1541567
   Guerra A, 2004, J MAR BIOL ASSOC UK, V84, P427, DOI 10.1017/S0025315404009397h
   GUERRA A, 2009, CALAMAR GIGANTE
   Guerra A, 2018, ECOLOGY, V99, P755, DOI 10.1002/ecy.2073
   Guerra A, 2011, BIOL CONSERV, V144, P1989, DOI 10.1016/j.biocon.2011.04.021
   Hanlon R., 2018, OCTOPUS SQUID CUTTLE
   Herring P., 2002, BIOL DEEP OCEAN
   Hoving HJT, 2012, BIOL BULL-US, V223, P263, DOI 10.1086/BBLv223n3p263
   Hoving HJT, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.1463
   Hoving HJT, 2014, ADV MAR BIOL, V67, P235, DOI 10.1016/B978-0-12-800287-2.00003-2
   Hoving HJT, 2004, J ZOOL, V264, P153, DOI 10.1017/S0952836904005710
   Jamieson AJ, 2020, MAR BIOL, V167, DOI 10.1007/s00227-020-03701-1
   Judkins H, 2017, MAR BIODIVERS, V47, P647, DOI 10.1007/s12526-016-0597-8
   Judkins H, 2015, J NAT HIST, V49, P1267, DOI 10.1080/00222933.2013.802045
   Judkins H, 2009, P BIOL SOC WASH, V122, P162, DOI 10.2988/08-30.1
   Kaartvedt S, 2012, MAR ECOL PROG SER, V456, P1, DOI 10.3354/meps09785
   Kubodera T, 2005, P ROY SOC B-BIOL SCI, V272, P2583, DOI 10.1098/rspb.2005.3158
   Kubodera T, 2007, P R SOC B, V274, P1029, DOI 10.1098/rspb.2006.0236
   Kubodera T, 2018, MAR BIODIVERS, V48, P1391, DOI 10.1007/s12526-016-0618-7
   Leite Luciana, 2016, Marine Biodiversity Records, V9, P26, DOI 10.1186/s41200-016-0028-3
   Levin LA, 2015, SCIENCE, V350, P766, DOI 10.1126/science.aad0126
   Lindsay DJ, 2020, DIVERSITY-BASEL, V12, DOI 10.3390/d12120449
   Mooney TA, 2010, J EXP BIOL, V213, P3748, DOI 10.1242/jeb.048348
   Nixon M., 2003, BRAINS LIVES CEPHALO
   NORMAN M, 2000, CEPHALOPODS WORLD GU
   Osterhage D, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0241066
   Paxton CGM, 2016, J ZOOL, V300, P82, DOI 10.1111/jzo.12347
   Raymond EH, 2007, MAR ECOL PROG SER, V350, P291, DOI 10.3354/meps07196
   Remeslo A, 2019, DEEP-SEA RES PT I, V147, P121, DOI 10.1016/j.dsr.2019.04.008
   Roper C.F.E., 2010, FAO SPECIES CATALOGU, V2, P370
   ROPER CFE, 1982, SCI AM, V246, P96, DOI 10.1038/scientificamerican0482-96
   Roper CFE, 2015, AM MALACOL BULL, V33, P78, DOI 10.4003/006.033.0116
   Roper CFE, 2013, AM MALACOL BULL, V31, P109, DOI 10.4003/006.031.0104
   Salgado TG, 2003, NEXUS NETW J, V5, P22, DOI DOI 10.1007/S00004-002-0003-7
   SEIDOU M, 1990, J COMP PHYSIOL A, V166, P769
   Vecchione M, 2001, AM FISH S S, V25, P153
   Vecchione M, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00403
   Vecchione M, 2010, MAR BIOL RES, V6, P25, DOI 10.1080/17451000902810751
   Webb TJ, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010223
   Widder EA, 2005, DEEP-SEA RES PT I, V52, P2077, DOI 10.1016/j.dsr.2005.06.007
   Widder E, 2013, SEA TECHNOL, V54, P49
   Widder EA, 2007, OCEANOGRAPHY, V20, P46, DOI 10.5670/oceanog.2007.04
   Young RE, 2006, P BIOL SOC WASH, V119, P287, DOI 10.2988/0006-324X(2006)119[287:PSANSO]2.0.CO;2
NR 51
TC 0
Z9 0
U1 1
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0967-0637
EI 1879-0119
J9 DEEP-SEA RES PT I
JI Deep-Sea Res. Part I-Oceanogr. Res. Pap.
PD JUN
PY 2021
VL 172
AR 103538
DI 10.1016/j.dsr.2021.103538
PG 8
WC Oceanography
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Oceanography
GA SM4IX
UT WOS:000657572400001
OA hybrid
DA 2022-02-10
ER

PT J
AU Kitzes, J
   Blake, R
   Bombaci, S
   Chapman, M
   Duran, SM
   Huang, T
   Joseph, MB
   Lapp, S
   Marconi, S
   Oestreich, WK
   Rhinehart, TA
   Schweiger, AK
   Song, YL
   Surasinghe, T
   Yang, D
   Yule, K
AF Kitzes, Justin
   Blake, Rachael
   Bombaci, Sara
   Chapman, Melissa
   Duran, Sandra M.
   Huang, Tao
   Joseph, Maxwell B.
   Lapp, Samuel
   Marconi, Sergio
   Oestreich, William K.
   Rhinehart, Tessa A.
   Schweiger, Anna K.
   Song, Yiluan
   Surasinghe, Thilina
   Yang, Di
   Yule, Kelsey
TI Expanding NEON biodiversity surveys with new instrumentation and machine
   learning approaches
SO ECOSPHERE
LA English
DT Article
DE biogeography; deep learning; macroecology; monitoring; neural network;
   sensor; Special Feature; Harnessing the NEON Data Revolution; species
ID SPECIES IDENTIFICATION; RANDOM FOREST; CAMERA; CLASSIFICATION;
   DIVERSITY; LIDAR; RECOGNITION; VEGETATION; SOUNDSCAPE; AUSTRALIA
AB A core goal of the National Ecological Observatory Network (NEON) is to measure changes in biodiversity across the 30-yr horizon of the network. In contrast to NEON's extensive use of automated instruments to collect environmental data, NEON's biodiversity surveys are almost entirely conducted using traditional human-centric field methods. We believe that the combination of instrumentation for remote data collection and machine learning models to process such data represents an important opportunity for NEON to expand the scope, scale, and usability of its biodiversity data collection while potentially reducing long-term costs. In this manuscript, we first review the current status of instrument-based biodiversity surveys within the NEON project and previous research at the intersection of biodiversity, instrumentation, and machine learning at NEON sites. We then survey methods that have been developed at other locations but could potentially be employed at NEON sites in future. Finally, we expand on these ideas in five case studies that we believe suggest particularly fruitful future paths for automated biodiversity measurement at NEON sites: acoustic recorders for sound-producing taxa, camera traps for medium and large mammals, hydroacoustic and remote imagery for aquatic diversity, expanded remote and ground-based measurements for plant biodiversity, and laboratory-based imaging for physical specimens and samples in the NEON biorepository. Through its data science-literate staff and user community, NEON has a unique role to play in supporting the growth of such automated biodiversity survey methods, as well as demonstrating their ability to help answer key ecological questions that cannot be answered at the more limited spatiotemporal scales of human-driven surveys.
C1 [Kitzes, Justin; Lapp, Samuel; Rhinehart, Tessa A.] Univ Pittsburgh, Dept Biol Sci, Pittsburgh, PA 15260 USA.
   [Blake, Rachael] Natl Socioenvironm Synth Ctr, Annapolis, MD USA.
   [Bombaci, Sara] Colorado State Univ, Dept Fish Wildlife & Conservat Biol, Ft Collins, CO 80523 USA.
   [Chapman, Melissa] Univ Calif Berkeley, Dept Environm Sci Policy & Management, Berkeley, CA 94720 USA.
   [Duran, Sandra M.] Univ Arizona, Dept Ecol & Evolutionary Biol, Tucson, AZ USA.
   [Huang, Tao] Boise State Univ, Human Environm Syst, Boise, ID 83725 USA.
   [Joseph, Maxwell B.] Univ Colorado, Cooperat Inst Res Environm Sci CIRES, Earth Lab, Boulder, CO 80309 USA.
   [Marconi, Sergio] Univ Florida, Dept Wildlife Ecol & Conservat, Gainesville, FL USA.
   [Oestreich, William K.] Stanford Univ, Hopkins Marine Stn, Stanford, CA 94305 USA.
   [Schweiger, Anna K.] Univ Zurich, Dept Geog, Zurich, Switzerland.
   [Song, Yiluan] Univ Calif Santa Cruz, Environm Studies Dept, Santa Cruz, CA 95064 USA.
   [Surasinghe, Thilina] Bridgewater State Univ, Dept Biol Sci, Bridgewater, MA USA.
   [Yang, Di] Univ Wyoming, Wyoming Geog Informat Sci Ctr WyGISC, Laramie, WY 82071 USA.
   [Yule, Kelsey] Arizona State Univ, Natl Ecol Observ Network Biorepository, Tempe, AZ USA.
RP Kitzes, J (corresponding author), Univ Pittsburgh, Dept Biol Sci, Pittsburgh, PA 15260 USA.
EM justin.kitzes@pitt.edu
OI Marconi, Sergio/0000-0002-8096-754X; Oestreich,
   William/0000-0002-0137-5053; Blake, Rachael/0000-0003-0847-9100; Duran,
   Sandra M/0000-0003-2044-8139; Schweiger, Anna
   Katharina/0000-0002-5567-4200; Joseph, Maxwell/0000-0002-7745-9990
FU National Science FoundationNational Science Foundation (NSF) [1935507,
   1926542]; Department of Biological Sciences at the University of
   Pittsburgh; Mascaro Center for Sustainable Development at the University
   of Pittsburgh; Gordon and Betty Moore Foundation's Data-Driven Discovery
   Initiative [GBMF4563]; NSF Dimension of Biodiversity program grant
   [DEB-1442280]; University of Florida Informatics Institute (UFII)
   Graduate Fellowship; University of Zurich's University Research Priority
   Programme on Global Change and Biodiversity
FX We thank Lauren Chronister for assistance in preparing this manuscript
   as well as Daniel Gruner, Elizabeth LaRue, and Natalie Robinson for
   ideas and suggestions that improved earlier drafts. This material is
   based upon work supported by the National Science Foundation under Grant
   No. 1935507 and was also financially supported by the Department of
   Biological Sciences and the Mascaro Center for Sustainable Development
   at the University of Pittsburgh. This work was also supported by the
   Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative
   through grant GBMF4563 to E.P. White and by the National Science
   Foundation through grant 1926542 to E.P. White, S.A. Bohlman, A. Zare,
   D.Z. Wang, and A. Singh; by the NSF Dimension of Biodiversity program
   grant (DEB-1442280) and the University of Florida Informatics Institute
   (UFII) Graduate Fellowship to Sergio Marconi. Anna Schweiger was
   supported by the University of Zurich's University Research Priority
   Programme on Global Change and Biodiversity. APC charges for this
   article were fully paid by the University Library System, University of
   Pittsburgh.
CR Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005
   Adams MD, 2010, ACTA CHIROPTEROL, V12, P231, DOI 10.3161/150811010X504725
   Aguirre-Gutierrez J, 2021, REMOTE SENS ENVIRON, V252, DOI 10.1016/j.rse.2020.112122
   Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
   Alexander C, 2014, REMOTE SENS ENVIRON, V147, P156, DOI 10.1016/j.rse.2014.02.013
   Almeida J, 2014, ECOL INFORM, V23, P49, DOI 10.1016/j.ecoinf.2013.06.011
   Anderson CB, 2018, PEERJ, V6, DOI 10.7717/peerj.5666
   Asner GP, 2017, SCIENCE, V355, P385, DOI 10.1126/science.aaj1987
   Baishali B., 2019, THESIS U ARIZONA
   Barrett B, 2016, REMOTE SENS ECOL CON, V2, P212, DOI 10.1002/rse2.32
   Bedoya C, 2014, ECOL INFORM, V24, P200, DOI 10.1016/j.ecoinf.2014.08.009
   Beery S., 2019, ARXIV190706772
   Bermant PC, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-48909-4
   Blair J, 2020, ECOL EVOL, V10, P13143, DOI 10.1002/ece3.6905
   Blumstein DT, 2011, J APPL ECOL, V48, P758, DOI 10.1111/j.1365-2664.2011.01993.x
   Bonnet S, 2015, REMOTE SENS-BASEL, V7, P11267, DOI 10.3390/rs70911267
   Bortone SA, 2000, MAR SCI SER, P127
   Brumfield KD, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0228899
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Buetti-Dinh Antoine, 2019, Biotechnology Reports, V22, pe00321, DOI 10.1016/j.btre.2019.e00321
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Cabezas J, 2016, IEEE GEOSCI REMOTE S, V13, P646, DOI 10.1109/LGRS.2016.2532743
   Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z
   Chadwick K.D., 2020, Site-level Foliar C, N, delta 13C data from samples collected during field survey associated with NEON AOP survey, East River, CO 2018, DOI 10.15485/1631278
   Chen L.-C., 2014, COMPUTER SCI
   Cheng KC, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219570
   Chollet F., 2018, DEEP LEARNING R MANN
   Colgan MS, 2012, REMOTE SENS-BASEL, V4, P3462, DOI 10.3390/rs4113462
   Corcoran E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-39917-5
   Dalponte M, 2019, PEERJ, V6, DOI 10.7717/peerj.6227
   Darras K, 2019, ECOL APPL, V29, DOI 10.1002/eap.1954
   Delancey ER, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0218165
   Diaz S, 2001, TRENDS ECOL EVOL, V16, P646, DOI 10.1016/S0169-5347(01)02283-2
   Diaz S, 2006, PLOS BIOL, V4, P1300, DOI 10.1371/journal.pbio.0040277
   Du XX, 2019, IEEE T GEOSCI REMOTE, V57, P2741, DOI 10.1109/TGRS.2018.2876687
   Duhart C., 2019, DEEP LEARNING WILDLI
   Duran SM, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw8114
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013
   Favret C, 2016, SYST ENTOMOL, V41, P133, DOI 10.1111/syen.12146
   Ferretti R, 2017, OCEANS-IEEE
   Forrester T, 2016, BIODIVERS DATA J, V4, DOI 10.3897/BDJ.4.e10197
   Forrester Tavis, 2013, P 98 ESA ANN CONV 20
   Franklin SE, 2018, INT J REMOTE SENS, V39, P5236, DOI 10.1080/01431161.2017.1363442
   Fricker GA, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11192326
   Frommolt KH, 2014, ECOL INFORM, V21, P4, DOI 10.1016/j.ecoinf.2013.12.009
   Gage SH, 2014, ECOL INFORM, V21, P100, DOI 10.1016/j.ecoinf.2013.11.004
   Geronimo RC, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101604
   Gomez WE, 2018, ECOL INFORM, V45, P16, DOI 10.1016/j.ecoinf.2018.03.001
   Graves S., 2018, PEERJ PREPRINTS, DOI [10.7287/peerj.preprints.27182v1, DOI 10.7287/PEERJ.PREPRINTS.27182V1]
   Guirado E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50795-9
   Hess LL, 2003, HYDROBIOLOGIA, V500, P65, DOI 10.1023/A:1024665017985
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   Hooper DU, 2005, ECOL MONOGR, V75, P3, DOI 10.1890/04-0922
   Huesca M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091100
   Husson E, 2014, APPL VEG SCI, V17, P567, DOI 10.1111/avsc.12072
   Jalali MA, 2015, FISH RES, V169, P26, DOI 10.1016/j.fishres.2015.04.009
   Jiao C, 2018, ISPRS J PHOTOGRAMM, V146, P235, DOI 10.1016/j.isprsjprs.2018.08.012
   Jones FM, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.124
   Jones MO, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2430
   Jones TR, 2009, P NATL ACAD SCI USA, V106, P1826, DOI 10.1073/pnas.0808843106
   Kahl S., 2019, OVERVIEW BIRDCLEF 20
   Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236
   Kamoske AG, 2019, FOREST ECOL MANAG, V433, P364, DOI 10.1016/j.foreco.2018.11.017
   Kampe TU, 2010, J APPL REMOTE SENS, V4, DOI 10.1117/1.3361375
   Kays R., 2014, P N AM CONSERVATION, P80
   Kerkech M, 2018, COMPUT ELECTRON AGR, V155, P237, DOI 10.1016/j.compag.2018.10.006
   Kitzes J, 2019, ENVIRON CONSERV, V46, P247, DOI 10.1017/S0376892919000146
   Klemas V, 2012, BALTICA, V25, P99, DOI 10.5200/baltica.2012.25.10
   Korneliussen RJ, 2009, ICES J MAR SCI, V66, P1111, DOI 10.1093/icesjms/fsp119
   Kosmala M, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0209649
   Kosmala M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090726
   Kotta J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0063946
   Krause K.S., 2015, REMOTE SENSING VEGET
   Laliberte E, 2020, ECOL LETT, V23, P370, DOI 10.1111/ele.13429
   Lapp S, 2021, CONSERV BIOL, V35, P1659, DOI 10.1111/cobi.13718
   LaRue EA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091407
   LeBien J, 2020, ECOL INFORM, V59, DOI 10.1016/j.ecoinf.2020.101113
   Libbrecht MW, 2015, NAT REV GENET, V16, P321, DOI 10.1038/nrg3920
   Locke CM, 2019, WILDLIFE SOC B, V43, P4, DOI 10.1002/wsb.943
   Lopatin J, 2016, REMOTE SENS ENVIRON, V173, P200, DOI 10.1016/j.rse.2015.11.029
   Lorieul T, 2019, APPL PLANT SCI, V7, DOI 10.1002/aps3.1233
   Lostanlen V, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0214168
   Lyons MB, 2020, REMOTE SENS ECOL CON, V6, P557, DOI 10.1002/rse2.157
   Ma XL, 2019, REMOTE SENS ENVIRON, V233, DOI 10.1016/j.rse.2019.111368
   Magle SB, 2019, FRONT ECOL ENVIRON, V17, P232, DOI 10.1002/fee.2030
   Marconi S., 2019, BIORXIV556472
   Marconi S, 2019, PEERJ, V7, DOI 10.7717/peerj.5843
   Marvin DC, 2016, GLOB ECOL CONSERV, V7, P262, DOI 10.1016/j.gecco.2016.07.002
   Mayo M, 2007, KNOWL-BASED SYST, V20, P195, DOI 10.1016/j.knosys.2006.11.012
   McCann E, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.190
   McMahon CA, 2019, PEERJ, V7, DOI 10.7717/peerj.5837
   McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
   Meerdink SK, 2019, REMOTE SENS ENVIRON, V232, DOI 10.1016/j.rse.2019.111308
   Meineke E.K., 2019, BIORXIV790899
   Meng L, 2018, IEEE ACCESS, V6, P17880, DOI 10.1109/ACCESS.2018.2820326
   Mishra D, 2006, PHOTOGRAMM ENG REM S, V72, P1037, DOI 10.14358/PERS.72.9.1037
   Mo J., 2017, AI 2017 ADV ARTIFICI, V10400, P301, DOI [10.1007/978-3-319-63004-5_24, DOI 10.1007/978-3-319-63004-5_24]
   Moniruzzaman M, 2017, LECT NOTES COMPUT SC, V10617, P150, DOI 10.1007/978-3-319-70353-4_13
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Narayanan BN, 2019, PROC SPIE, V11139, DOI 10.1117/12.2524681
   National Ecological Observatory Network, 2019, NEON STRAT ENG PLAN
   National Ecological Observatory Network, PAP PUBL
   National Ecological Observatory Network, 2020, DAT PROD DP1 00033 0
   Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070
   Nia MS, 2015, J APPL REMOTE SENS, V9, DOI 10.1117/1.JRS.9.095990
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Obrist Martin K., 2010, Abc Taxa, V8, P68
   Park JY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131534
   Paz-Kagan T, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080953
   Pijanowski BC, 2011, LANDSCAPE ECOL, V26, P1213, DOI 10.1007/s10980-011-9600-8
   Pijanowski BC, 2011, BIOSCIENCE, V61, P203, DOI 10.1525/bio.2011.61.3.6
   Pizarro O, 2008, OCEANS-IEEE, P1863
   Plesoianu AI, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152426
   Pouliot DA, 2002, REMOTE SENS ENVIRON, V82, P322, DOI 10.1016/S0034-4257(02)00050-0
   Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
   Qiu ZF, 2018, OPT EXPRESS, V26, P26810, DOI 10.1364/OE.26.026810
   Raitoharju J, 2018, IMAGE VISION COMPUT, V78, P73, DOI 10.1016/j.imavis.2018.06.005
   Rehush N, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111735
   Rhinehart TA, 2020, ECOL EVOL, V10, P6794, DOI 10.1002/ece3.6216
   Richardson AD, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.28
   Ruppe L, 2015, P NATL ACAD SCI USA, V112, P6092, DOI 10.1073/pnas.1424667112
   Salman A, 2019, ECOL INFORM, V51, P44, DOI 10.1016/j.ecoinf.2019.02.011
   Santos MJ, 2009, INVAS PLANT SCI MANA, V2, P216, DOI 10.1614/IPSM-08-115.1
   Schneider FD, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01530-3
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Schuettpelz E, 2017, BIODIVERS DATA J, V5, DOI 10.3897/BDJ.5.e21139
   Schweiger AK, 2018, NAT ECOL EVOL, V2, P976, DOI 10.1038/s41559-018-0551-1
   Sebastia-Frasquet MT, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242926
   Seeland M, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-018-2474-x
   Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
   Shiferaw H, 2019, ECOL EVOL, V9, P2562, DOI 10.1002/ece3.4919
   Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
   Soueidan H., 2015, ARXIV PREPRINT ARXIV
   Stowell D, 2020, REMOTE SENS ECOL CON, V6, P217, DOI 10.1002/rse2.174
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Sueur J, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0004065
   Sumsion GR, 2019, PEERJ, V7, DOI 10.7717/peerj.6101
   Sung M, 2017, OCEANS-IEEE
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
   Tonolla D, 2010, HYDROL PROCESS, V24, P3146, DOI 10.1002/hyp.7730
   Tornow J.S., 2019, DEAR COLLEAGUE LETT
   Ulloa JS, 2018, ECOL INDIC, V90, P346, DOI 10.1016/j.ecolind.2018.03.026
   Uranga J, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0171382
   Vatnehol S, 2018, ICES J MAR SCI, V75, P1803, DOI 10.1093/icesjms/fsy029
   Verrelst J, 2019, SURV GEOPHYS, V40, P589, DOI 10.1007/s10712-018-9478-y
   Vihervaara P, 2017, GLOB ECOL CONSERV, V10, P43, DOI 10.1016/j.gecco.2017.01.007
   Wang ZH, 2020, NEW PHYTOL, V228, P494, DOI 10.1111/nph.16711
   Weinstein B.G., 2020, BIORXIV2020090828783, DOI [10.1101/2020.09.08.287839, DOI 10.1101/2020.09.08.287839]
   Weinstein BG, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111309
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Wolff LM, 2014, OCEANS-IEEE
   Wu CF, 2018, J FORESTRY RES, V29, P151, DOI 10.1007/s11676-017-0404-9
   Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zhang D, 2016, BIOSYST ENG, V145, P65, DOI 10.1016/j.biosystemseng.2016.02.013
   Zhou T, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10121949
   Zhou T, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010039
   Zlinszky A, 2016, INT ARCH PHOTOGRAMM, V41, P1293, DOI 10.5194/isprsarchives-XLI-B8-1293-2016
   Zou S, 2019, PEERJ, V7, DOI 10.7717/peerj.6405
NR 160
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2150-8925
J9 ECOSPHERE
JI Ecosphere
PD NOV
PY 2021
VL 12
IS 11
AR e03795
DI 10.1002/ecs2.3795
PG 20
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA XE1GA
UT WOS:000723142700022
OA gold
DA 2022-02-10
ER

PT C
AU Ishikawa, R
   Oishi, T
   Ikeuchi, K
AF Ishikawa, Ryoichi
   Oishi, Takeshi
   Ikeuchi, Katsushi
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI LiDAR and Camera Calibration using Motions Estimated by Sensor Fusion
   Odometry
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
LA English
DT Proceedings Paper
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
SP IEEE Robot & Automat Soc, IEEE Ind Elect Soc, Robot Soc Japan, Soc Instrument & Control Engineers, New Technol Fdn, IEEE, Adept MobileRobots, Willow Garage, Aldebaran Robot, Natl Instruments, Reflexxes GmbH, Schunk Intec S L U, Univ Carlos III Madrid, BOSCH, JD COM, Pal Robot, KUKA, Santander, Squirrel AI Learning, Baidu, Generat Robots, KINOVA Robot, Ouster, Univ Pablo Olavide Sevilla, Rapyuta Robot, SICK, TOYOTA, UP, Amazon, ARGO, Built Robot, Disney Res, Easy Mile, Hitachi, Robot, Khalifa Univ, Magazino, MathWorks, New Dexterity, Schunk, nuTonomy, PILZ, Prophesee, Rootnik, Saga Robot, Shadow, Soft Bank Robot, Anyverse, GalTech, Generat Robot, IEEE CAA Journal Automatica Sinica, Sci Robot, AAAS, TERAS
ID RECONSTRUCTION; LASER
AB This paper proposes a targetless and automatic camera-LiDAR calibration method. Our approach extends the hand-eye calibration framework to 2D-3D calibration. The scaled camera motions are accurately calculated using a sensorfusion odometry method. We also clarify the suitable motions for our calibration method.
   Whereas other calibrations require the LiDAR reflectance data and an initial extrinsic parameter, the proposed method requires only the three-dimensional point cloud and the camera image. The effectiveness of the method is demonstrated in experiments using several sensor configurations in indoor and outdoor scenes. Our method achieved higher accuracy than comparable state-of-the-art methods.
C1 [Ishikawa, Ryoichi; Oishi, Takeshi] Univ Tokyo, Inst Ind Sci, Tokyo, Japan.
   [Ikeuchi, Katsushi] Microsoft, Syracuse, NY USA.
RP Ishikawa, R (corresponding author), Univ Tokyo, Inst Ind Sci, Tokyo, Japan.
EM ishikawa@cvl.iis.utokyo.ac.jp; oishi@cvl.iis.utokyo.ac.jp;
   katsuike@microsoft.com
FU social corporate program (Base Technologies for Future Robots) - NIDEC
   corporation; JSPS KAKENHI GrantMinistry of Education, Culture, Sports,
   Science and Technology, Japan (MEXT)Japan Society for the Promotion of
   ScienceGrants-in-Aid for Scientific Research (KAKENHI) [JP16747698,
   JP17923471]; JSPSMinistry of Education, Culture, Sports, Science and
   Technology, Japan (MEXT)Japan Society for the Promotion of Science
   [16J09277]
FX This work was partially supported by the social corporate program (Base
   Technologies for Future Robots) sponsored by NIDEC corporation. This
   work was also supported by JSPS KAKENHI Grant Number JP16747698,
   JP17923471, and JSPS Research Fellow Grant No. 16J09277.
CR Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Banno A, 2010, COMPUT VIS IMAGE UND, V114, P491, DOI 10.1016/j.cviu.2009.12.005
   Bok Y, 2016, ROBOT AUTON SYST, V78, P17, DOI 10.1016/j.robot.2015.12.007
   Bok Y, 2011, INT J COMPUT VISION, V94, P36, DOI 10.1007/s11263-010-0397-8
   Cui TT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010070
   Fassi I, 2005, J ROBOTIC SYST, V22, P497, DOI 10.1002/rob.20082
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Heng L, 2013, IEEE INT C INT ROBOT, P1793, DOI 10.1109/IROS.2013.6696592
   Hol JD, 2010, INT J ROBOT RES, V29, P231, DOI 10.1177/0278364909356812
   Irie Kiyoshi, 2016, 2016 IEEE International Conference on Automation Science and Engineering (CASE), P1340, DOI 10.1109/COASE.2016.7743564
   Ishikawa R, 2016, INT CONF 3D VISION, P620, DOI 10.1109/3DV.2016.70
   Kelly J, 2011, INT J ROBOT RES, V30, P56, DOI 10.1177/0278364910382802
   Kneip L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2969, DOI 10.1109/CVPR.2011.5995464
   Levinson J, 2013, ROBOTICS SCI SYSTEMS, V2
   Lucas B. D., 1981, P 7 INT JOINT C ART, V2, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Nister D, 2006, J FIELD ROBOT, V23, P3, DOI 10.1002/rob.20103
   Oishi T, 2005, FIFTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P476, DOI 10.1109/3DIM.2005.41
   Pagani A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P375, DOI 10.1109/ICCVW.2011.6130266
   Pandey G, 2015, J FIELD ROBOT, V32, P696, DOI 10.1002/rob.21542
   Rodriguez F Sergio A, 2008, 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2008), P214, DOI 10.1109/MFI.2008.4648067
   SHIU YC, 1989, IEEE T ROBOTIC AUTOM, V5, P16, DOI 10.1109/70.88014
   Taylor Z., 2012, P AUSTR C ROB AUT DE, P3
   Taylor Z, 2016, IEEE T ROBOT, V32, P1215, DOI 10.1109/TRO.2016.2596771
   Taylor Z, 2015, J FIELD ROBOT, V32, P675, DOI 10.1002/rob.21523
   VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930
   Zhang J, 2017, AUTON ROBOT, V41, P31, DOI [10.1007/s10514-015-9525-1, 10.1109/MWSYM.2015.7167049]
   Zhang Q., 2004, IEEE RSJ INT C INT R, P2301, DOI [10.1109/IROS.2004.1389752, DOI 10.1109/IROS.2004.1389752]
   Zheng  B., 2015, INT C 3D VIS
NR 28
TC 21
Z9 22
U1 3
U2 18
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2153-0858
BN 978-1-5386-8094-0
J9 IEEE INT C INT ROBOT
PY 2018
BP 7342
EP 7349
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BM0LT
UT WOS:000458872706097
DA 2022-02-10
ER

PT J
AU Chen, X
   Zhao, J
   Chen, YH
   Zhou, W
   Hughes, AC
AF Chen, Xing
   Zhao, Jun
   Chen, Yan-hua
   Zhou, Wei
   Hughes, Alice C.
TI Automatic standardized processing and identification of tropical bat
   calls using deep learning approaches
SO BIOLOGICAL CONSERVATION
LA English
DT Article
DE Bats; Bioacoustics; Automated monitoring; Algorithms; Deep learning;
   Neural network; Automatic processing; Biodiversity metrics; Machine
   learning; Calls; Echolocation; Monitoring protocol
ID ECHOLOCATION CALLS; SIGNALS; CHIROPTERA; DIVERGENCE; OCCUPANCY;
   DIVERSITY; FREQUENCY; PATTERNS; SOUND; TOOL
AB Consistent and comparable metrics to automatically monitor biodiversity across the landscape remain a gold-standard for biodiversity research, yet such approaches have frequently been limited to a very small selection of species for which visual approaches (e.g., camera traps) make continuous monitoring possible. Acoustic-based methods have been widely applied in the monitoring of bats and some other taxa across extended spatial scales, but are have yet to be applied to diverse tropical communities.
   In this study, we developed a software program "Waveman" and prepared a reference library using over 880 audio-files from 36 Asian bat species. The software incorporated a novel network "BatNet" and a re-checking strategy (ReChk) to maximize accuracy. In Waveman, BatNet outperforms three other published networks: CNNFULL, VggNet and ResNet_v2, with over 90% overall accuracy and 0.94 AUC on the ROC plot. The classification accuracy rates for all 36 species are at least 86% when analysed in combination. Moreover, our library preparation and ReChk greatly improved the sensitivity and reduced the false positive rate, when tested with 15 species for which more detailed and situationally diverse records were available. Finally, BatNet was successfully used to identify Hipposideros larvatus and Rhinolophus siamensis from three different environments. We hope this pipeline is useful tool to process bioacoustic data accurately, effectively and automatically, therefore allowing for greater standardization and comparability for researchers to understand bat activities across space and time and therefore provide a consistent tool for monitoring biodiversity for management and conservation.
C1 [Chen, Xing; Chen, Yan-hua; Hughes, Alice C.] Chinese Acad Sci, Ctr Integrat Conservat, Xishuangbanna Trop Bot Garden, Menglun 666303, Peoples R China.
   [Zhao, Jun; Zhou, Wei] Yunnan Univ, Software Sch, Kunming 650500, Yunnan, Peoples R China.
RP Hughes, AC (corresponding author), Chinese Acad Sci, Ctr Integrat Conservat, Xishuangbanna Trop Bot Garden, Menglun 666303, Peoples R China.; Zhou, W (corresponding author), Yunnan Univ, Software Sch, Kunming 650500, Yunnan, Peoples R China.
EM zwei@ynu.edu.cn; ACHughes@xtbg.cas.cn
OI hughes, Alice/0000-0002-4899-3158
FU Chinese National Natural Science FoundationNational Natural Science
   Foundation of China (NSFC) [U1602265]; Strategic Priority Research
   Program of the Chinese Academy of SciencesChinese Academy of Sciences
   [XDA20050202]; High-End Foreign Experts Program of Yunnan Province
   [Y9YN021B01]; CAS 135 program [2017XTBG-T03]
FX Supported by Chinese National Natural Science Foundation (Grant #:
   U1602265, Mapping Karst Biodiversity in Yunnan). Supported by the
   Strategic Priority Research Program of the Chinese Academy of Sciences
   (Grant No. XDA20050202). Supported by the High-End Foreign Experts
   Program of Yunnan Province (Grant #: Y9YN021B01, Yunnan Bioacoustic
   monitoring program). Supported by the CAS 135 program (No.
   2017XTBG-T03).
CR ALTES RA, 1970, J ACOUST SOC AM, V48, P1014, DOI 10.1121/1.1912222
   Astaras C, 2017, FRONT ECOL ENVIRON, V15, P233, DOI 10.1002/fee.1495
   Baker E, 2015, DATABASE-OXFORD, DOI 10.1093/database/bav054
   Barratt EM, 1997, NATURE, V387, P138, DOI 10.1038/387138b0
   Benson DA, 2015, NUCLEIC ACIDS RES, V43, pD30, DOI 10.1093/nar/gku1216
   Boonman A, 2005, J COMP PHYSIOL A, V191, P13, DOI 10.1007/s00359-004-0566-8
   Cardinale BJ, 2018, BIOL CONSERV, V219, P175, DOI 10.1016/j.biocon.2017.12.021
   Christin S., 2018, APPL DEEP LEARNING E
   Clement MJ, 2014, J APPL ECOL, V51, P1460, DOI 10.1111/1365-2664.12303
   Gager Y, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150780
   Gasc A, 2013, ECOL INDIC, V25, P279, DOI 10.1016/j.ecolind.2012.10.009
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   Hughes AC, 2012, GLOBAL CHANGE BIOL, V18, P1854, DOI 10.1111/j.1365-2486.2012.02641.x
   Hughes AC, 2011, ACTA CHIROPTEROL, V13, P447, DOI 10.3161/150811011X624938
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   IOFFE S, 2015, ARXIV 1502 03167, V1502, DOI DOI 10.1007/S13398-014-0173-7.2
   Jacobs DS, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187769
   Jiang TL, 2010, ETHOLOGY, V116, P691, DOI 10.1111/j.1439-0310.2010.01785.x
   Kembel SW, 2010, BIOINFORMATICS, V26, P1463, DOI 10.1093/bioinformatics/btq166
   Kingston T, 2004, NATURE, V429, P654, DOI 10.1038/nature02487
   Kiskin I, 2020, NEURAL COMPUT APPL, V32, P915, DOI 10.1007/s00521-018-3626-7
   Mac Aodha O, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005995
   Mao XG, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0056786
   Marques A, 2014, BASIC APPL ECOL, V15, P633, DOI 10.1016/j.baae.2014.09.004
   Meagher JP, 2018, STATISTICAL DATA SCIENCE, P111
   Meyer CFJ, 2010, BIOL CONSERV, V143, P2797, DOI 10.1016/j.biocon.2010.07.029
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Parsons S, 2000, J EXP BIOL, V203, P2641
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pennell MW, 2014, BIOINFORMATICS, V30, P2216, DOI 10.1093/bioinformatics/btu181
   Proenca V, 2017, BIOL CONSERV, V213, P256, DOI 10.1016/j.biocon.2016.07.014
   R Core Team, 2019, R LANGUAGE ENV STAT
   Rich LN, 2017, GLOBAL ECOL BIOGEOGR, V26, P918, DOI 10.1111/geb.12600
   Russo D, 2002, J ZOOL, V258, P91, DOI 10.1017/S0952836902001231
   Russo D, 2018, CAN J ZOOL, V96, P63, DOI 10.1139/cjz-2017-0089
   Russo D, 2016, ECOL INDIC, V66, P598, DOI 10.1016/j.ecolind.2016.02.036
   Rydell J, 2017, ECOL INDIC, V78, P416, DOI 10.1016/j.ecolind.2017.03.023
   Silberman N., 2017, TF SLIM LIGHTWEIGHT
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Stamatakis A, 2014, BIOINFORMATICS, V30, P1312, DOI 10.1093/bioinformatics/btu033
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Thabah A, 2006, BIOL J LINN SOC, V88, P119, DOI 10.1111/j.1095-8312.2006.00602.x
   Trolle M, 2003, J MAMMAL, V84, P607, DOI 10.1644/1545-1542(2003)084<0607:EOODIT>2.0.CO;2
   Walters Charlotte L., 2013, P479
   Wilkins MR, 2013, TRENDS ECOL EVOL, V28, P156, DOI 10.1016/j.tree.2012.10.002
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   ZINGG P.E., 2019, REV SUISSE ZOOL, P263
NR 47
TC 6
Z9 7
U1 9
U2 13
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0006-3207
EI 1873-2917
J9 BIOL CONSERV
JI Biol. Conserv.
PD JAN
PY 2020
VL 241
AR 108269
DI 10.1016/j.biocon.2019.108269
PG 10
WC Biodiversity Conservation; Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KT0IV
UT WOS:000518695100048
DA 2022-02-10
ER

PT J
AU Bjerge, K
   Mann, HMR
   Hoye, TT
AF Bjerge, Kim
   Mann, Hjalte M. R.
   Hoye, Toke Thomas
TI Real-time insect tracking and monitoring with computer vision and deep
   learning
SO REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA English
DT Article; Early Access
DE Computer vision; deep learning; insects; pollinators; real-time;
   tracking
ID POLLINATORS
AB Insects are declining in abundance and diversity, but their population trends remain uncertain as insects are difficult to monitor. Manual methods require substantial time investment in trapping and subsequent species identification. Camera trapping can alleviate some of the manual fieldwork, but the large quantities of image data are challenging to analyse. By embedding the image analyses into the recording process using computer vision techniques, it is possible to focus efforts on the most ecologically relevant image data. Here, we present an intelligent camera system, capable of detecting, tracking, and identifying individual insects in situ. We constructed the system from commercial off-the-shelf components and used deep learning open source software to perform species detection and classification. We present the Insect Classification and Tracking algorithm (ICT) that performs real-time classification and tracking at 0.33 frames per second. The system can upload summary data on the identity and movement track of insects to a server via the internet on a daily basis. We tested our system during the summer 2020 and detected 2994 insect tracks across 98 days. We achieved an average precision of 89% for correctly classified insect tracks of eight different species. This result was based on 504 manually verified tracks observed in videos during 10 days with varying insect activities. Using the track data, we could estimate the mean residence time for individual flower visiting insects within the field of view of the camera, and we were able to show a substantial variation in residence time among insect taxa. For honeybees, which were most abundant, residence time also varied through the season in relation to the plant species in bloom. Our proposed automated system showed promising results in non-destructive and real-time monitoring of insects and provides novel information about phenology, abundance, foraging behaviour, and movement ecology of flower visiting insects.
C1 [Bjerge, Kim] Aarhus Univ, Dept Elect & Comp Engn, Finlandsgade 22, DK-8200 Aarhus N, Denmark.
   [Mann, Hjalte M. R.; Hoye, Toke Thomas] Aarhus Univ, Dept Ecosci, Grenavej 14, DK-8410 Ronde, Denmark.
   [Mann, Hjalte M. R.; Hoye, Toke Thomas] Aarhus Univ, Arctic Res Ctr, Grenavej 14, DK-8410 Ronde, Denmark.
RP Bjerge, K (corresponding author), Aarhus Univ, Dept Elect & Comp Engn, Finlandsgade 22, DK-8200 Aarhus N, Denmark.
EM kbe@ece.au.dk
OI Bjerge, Kim/0000-0001-6742-9504
FU European Union's Horizon 2020 Research and Innovation programme [773554]
FX The work was supported by the European Union's Horizon 2020 Research and
   Innovation programme, under Grant Agreement no. 773554 (EcoStack).
CR Arje J, 2020, METHODS ECOL EVOL, V11, P922, DOI 10.1111/2041-210X.13428
   Barlow SE, 2017, CURR BIOL, V27, P2552, DOI 10.1016/j.cub.2017.07.012
   Bjerge K, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020343
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Collett RA, 2017, ECOL EVOL, V7, P7527, DOI 10.1002/ece3.3275
   Didham RK, 2020, INSECT CONSERV DIVER, V13, P103, DOI 10.1111/icad.12408
   Eliopoulos P, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7090161
   Epsky ND., 2008, ENCY ENTOMOLOGY, P3887
   Gilpin AM, 2017, ECOL ENTOMOL, V42, P383, DOI 10.1111/een.12394
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hansen OLP, 2020, ECOL EVOL, V10, P737, DOI 10.1002/ece3.5921
   Hoye T.T., 2020, DCE NATL CTR MILJO O, V371, P18
   Hoye TT, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2002545117
   Huang Y, 2017, P 2017 C EMP METH NA, P1803, DOI [10.18653/v1/d17-1191, DOI 10.18653/V1/D17-1191]
   Ju MR, 2019, IEEE ACCESS, V7, P85771, DOI 10.1109/ACCESS.2019.2924960
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Logitech, 2020, C920 HD PRO WEBC
   Logitech, 2021, BRIO ULTR HD PRO WEB
   Lortie Christopher J., 2011, Journal of Pollination Ecology, V6, P125
   MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a
   Montgomery GA, 2021, FRONT ECOL EVOL, V8, DOI 10.3389/fevo.2020.579193
   Motion, 2021, MOT OP SOURC PROGR M
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Naz H, 2014, J ENV AGR SCI, V1, P5
   NVIDIA, 2021, JETS NAN DEV KIT US
   Pegoraro L, 2020, EMERG TOP LIFE SCI, V4, P87, DOI 10.1042/ETLS20190074
   Redmon J., 2018, ARXIV PREPRINT ARXIV
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Steen R, 2017, METHODS ECOL EVOL, V8, P203, DOI 10.1111/2041-210X.12654
   Wagner DL, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2023989118
   Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011
   Wignall VR, 2020, OECOLOGIA, V192, P351, DOI 10.1007/s00442-019-04576-w
   Wojcik VA, 2018, ENVIRON ENTOMOL, V47, P822, DOI [10.1093/ee/nvy077, 10.1093/e]
   Xia D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124169
   Zoller L, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78165-w
NR 36
TC 0
Z9 0
U1 9
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN, NJ 07030 USA
EI 2056-3485
J9 REMOTE SENS ECOL CON
JI Remote Sens. Ecol. Conserv.
DI 10.1002/rse2.245
EA NOV 2021
PG 14
WC Ecology; Remote Sensing
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Remote Sensing
GA XF2QC
UT WOS:000723919300001
OA gold
DA 2022-02-10
ER

PT J
AU Andersen, ML
   Bennett, DE
   Holbrook, JD
AF Andersen, Megan L.
   Bennett, Drew E.
   Holbrook, Joseph D.
TI Burrow webs: Clawing the surface of interactions with burrows excavated
   by American badgers
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE American badgers; burrow web; ecological network; ecosystem engineers;
   species interactions; subterranean habitat; Taxidea taxus
ID CAVITY-NESTING COMMUNITIES; ECOLOGICAL ROLES; TAXIDEA-TAXUS;
   CONSERVATION; ORGANISMS; LONGEVITY; DENSITIES; NETWORKS; RESOURCE;
   SUCCESS
AB Ecosystem engineers are organisms that influence their environment, which includes alterations leading to habitat provisioning for other species. Perhaps the most well-examined guild of species provisioning habitat for other species is tree cavity excavators or woodpeckers (Picidae). Many studies have examined the suite of secondary cavity users that rely on woodpeckers, and how the ecological network of secondary users, collectively referred to as the nest web, changes across communities. Despite similar habitat provisioning processes, fewer studies have assessed the suite of species associated with burrowers providing access to subterranean habitat. Here, we begin to characterize the burrow web provisioned by American badgers (Taxidea taxus) and evaluate the diversity and frequency of species interactions we detected at abandoned badger burrows in Wyoming, USA. We deployed camera traps at 23 badger burrows and identified interactions with the burrow by birds, mammals, and reptiles. Overall, we discovered 31 other species utilizing badger burrows, consisting of 12 mammals, 18 birds, and 1 reptile. Mammals, other than American badgers themselves and other fossorial species such as ground squirrels (Urocitellus sp.), frequently using burrows included mice (Peromyscus sp.), long-tailed weasel (Mustela frenata), pygmy rabbit (Brachylagus idahoensis), and desert cottontail (Sylvilagus audubonii). Of the 18 bird species detected, most accounted for <5% of overall detections, besides chipping sparrows (Spizella passerina) at 7.2%-11.5% of detections. The most common category of detection by bird species was foraging, contrary to mammals, which used the burrow frequently and were commonly observed entering and exiting the burrow. This work provides additional context on the ecological role of American badgers within their environment. More broadly, this work scratches the surface of many remaining questions to explore with the aim of advancing our understandings about burrow webs across the diversity of burrowing species and the communities in which they occur.
C1 [Andersen, Megan L.; Bennett, Drew E.; Holbrook, Joseph D.] Univ Wyoming, Haub Sch Environm & Nat Resources, Laramie, WY 82071 USA.
   [Holbrook, Joseph D.] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
RP Holbrook, JD (corresponding author), Univ Wyoming, Haub Sch Environm & Nat Resources, Laramie, WY 82071 USA.
EM Joe.Holbrook@uwyo.edu
FU Whitney MacMillan program; Haub School of Environment and Natural
   Resources; University of Wyoming
FX We sincerely thank the landowners and ranch managers for allowing access
   and aiding our research efforts. Support for this work was provided by
   the Whitney MacMillan program for Private Lands Stewardship, the Haub
   School of Environment and Natural Resources, and the University of
   Wyoming.
CR Bylo LN, 2014, RANGELAND ECOL MANAG, V67, P247, DOI 10.2111/REM-D-13-00152.1
   Cockle KL, 2019, BIODIVERS CONSERV, V28, P3371, DOI 10.1007/s10531-019-01826-4
   Cockle KL, 2019, ECOL APPL, V29, DOI 10.1002/eap.1916
   Cockle KL, 2011, FRONT ECOL ENVIRON, V9, P377, DOI 10.1890/110013
   Davidson AD, 2008, J ARID ENVIRON, V72, P2142, DOI 10.1016/j.jaridenv.2008.07.006
   Davidson AD, 2012, FRONT ECOL ENVIRON, V10, P477, DOI 10.1890/110054
   Dawson SJ, 2019, J ZOOL, V308, P149, DOI 10.1111/jzo.12663
   Desmond MJ, 1996, AM MIDL NAT, V136, P143, DOI 10.2307/2426639
   Di Blanco YE, 2020, J ZOOL, V311, P227, DOI 10.1111/jzo.12782
   Edworthy AB, 2012, ECOL APPL, V22, P1733, DOI 10.1890/11-1594.1
   Eldridge DJ, 2009, J ARID ENVIRON, V73, P66, DOI 10.1016/j.jaridenv.2008.09.004
   Eldridge DJ, 2004, J MAMMAL, V85, P1060, DOI 10.1644/BEH-105.1
   GLEASON RS, 1985, GREAT BASIN NAT, V45, P81
   Goodman SJ, 2018, SOUTHEAST NAT, V17, P531, DOI 10.1656/058.017.0310
   Grassel SM, 2015, ECOL EVOL, V5, P2762, DOI 10.1002/ece3.1561
   Haynes G, 2012, GEOMORPHOLOGY, V157, P99, DOI 10.1016/j.geomorph.2011.04.045
   Holbrook JD, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1307
   Holmes AL, 2003, WEST N AM NATURALIST, V63, P244
   Ings TC, 2009, J ANIM ECOL, V78, P253, DOI 10.1111/j.1365-2656.2008.01460.x
   Desbiez ALJ, 2013, BIOTROPICA, V45, P537, DOI 10.1111/btp.12052
   Jones CG, 1997, ECOLOGY, V78, P1946, DOI 10.1890/0012-9658(1997)078[1946:PANEOO]2.0.CO;2
   JONES CG, 1994, OIKOS, V69, P373, DOI 10.2307/3545850
   Kefi S, 2012, ECOL LETT, V15, P291, DOI 10.1111/j.1461-0248.2011.01732.x
   Kinlaw A, 2012, GEOMORPHOLOGY, V157, P108, DOI 10.1016/j.geomorph.2011.06.030
   Kucheravy CE, 2021, BASIC APPL ECOL, V51, P11, DOI 10.1016/j.baae.2021.01.012
   Kurek P, 2014, ECOL RES, V29, P1, DOI 10.1007/s11284-013-1094-1
   Lohr K, 2013, J WILDLIFE MANAGE, V77, P983, DOI 10.1002/jwmg.541
   Lorenz TJ, 2015, ECOL APPL, V25, P1016, DOI 10.1890/14-1042.1
   Martin K, 2004, CONDOR, V106, P5, DOI 10.1650/7482
   Martin K, 1999, FOREST ECOL MANAG, V115, P243, DOI 10.1016/S0378-1127(98)00403-4
   MESSICK JP, 1981, WILDLIFE MONOGR, P1
   Milling CR, 2018, PEERJ, V6, DOI 10.7717/peerj.4511
   Murphy CM, 2021, FOREST ECOL MANAG, V482, DOI 10.1016/j.foreco.2020.118809
   Natural Resource Conservation Service, 2019, ECOLOGICAL SITE R034, P30
   Natural Resource Conservation Service, 2020, EC SIT RO32XY162WY S, P5
   Olsson IAS, 2005, APPL ANIM BEHAV SCI, V93, P259, DOI 10.1016/j.applanim.2004.11.018
   Pagliai M., 2002, ADV GEOECOLOGY, V35, P69
   R Core Team, 2020, R FDN STAT COMP
   Read JL, 2008, J ARID ENVIRON, V72, P2124, DOI 10.1016/j.jaridenv.2008.06.018
   Symes SA, 2019, WILDLIFE BIOL, DOI 10.2981/wlb.00528
   United States Fish and Wildlife Service, 2008, FED REGISTER, V73, P1312
   Vierling KT, 2018, INT J BIOMETEOROL, V62, P553, DOI 10.1007/s00484-017-1464-4
   Whittington-Jones GM, 2011, AFR ZOOL, V46, P362, DOI 10.3377/004.046.0215
NR 43
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD SEP
PY 2021
VL 11
IS 17
BP 11559
EP 11568
DI 10.1002/ece3.7962
EA JUL 2021
PG 10
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA UN4RM
UT WOS:000679457800001
PM 34522324
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Stommel, C
   Hofer, H
   Grobbel, M
   East, ML
AF Stommel, Claudia
   Hofer, Heribert
   Grobbel, Mirjam
   East, Marion L.
TI Large mammals in Ruaha National Park, Tanzania, dig for water when water
   stops flowing and water bacterial load increases
SO MAMMALIAN BIOLOGY
LA English
DT Article
DE Great Ruaha River; Wildlife; Water holes; Salinity; Bacterial
   contamination
ID NAMIBIA; ELEPHANTS
AB As water is essential for life, animals have adaptations that increase their ability to survive during periods of water shortage. Accessing water by digging is one behavioural adaptation to water shortage used by some African mammals. Digging might also provide access to higher quality water below ground when surface water quality is poor. We investigated the digging of waterholes by wildlife in the Ruaha National Park (NP), in central Tanzania, during three dry seasons (June to November from 2011 to 2013). We monitored surface water availability and water quality at 10 sites along the Great Ruaha River (GRR) and eight non-GRR sites. We used camera-traps and direct observations to determine when and where digging to access water occurred. Elephant (Loxodonta africana), plains zebra (Equus quagga), warthog (Phacochoerus africanus) and yellow baboon (Papio cynocephalus) dug waterholes and a further four species drunk from these holes. Waterholes were dug later in the dry season along the GRR (October) than at other sites (July). The likelihood of digging and drinking from waterholes was lower along the GRR than at non-GRR sites and did not depend on the absence of surface water but increased when surface water stopped flowing. Digging of waterholes was also significantly more likely when the bacterial load in available surface water increased but was independent of salinity levels. Escherichia coli load, indicative of faecal contamination, significantly increased with total aerobic bacterial load. Our results suggest that digging is an adaptation to avoid the ingestion of poor quality surface water highly contaminated with faeces, and thereby possibly also potentially pathogenic microbes, in addition to providing access to water when surface water is absent. Our findings also highlight (1) the essential role of the GRR as the key water source for wildlife in the Ruaha NP during the dry season, and (2) that maintenance of water flow throughout the dry season is essential to prevent deterioration of water quality in the GRR. (C) 2015 Deutsche Gesellschaft fur Saugetierkunde. Published by Elsevier GmbH. All rights reserved.
C1 [Stommel, Claudia; Hofer, Heribert; Grobbel, Mirjam; East, Marion L.] Leibniz Inst Zoo & Wildlife Res, Alfred Kowalke Str 17, D-10315 Berlin, Germany.
   [Grobbel, Mirjam] Bundesinst Risikobewertung, Abt Biol Sicherheit, Diedersdorfer Weg 1, D-12277 Berlin, Germany.
RP Stommel, C (corresponding author), Leibniz Inst Zoo & Wildlife Res, Alfred Kowalke Str 17, D-10315 Berlin, Germany.
EM stommel@izw-berlin.de
RI Grobbel, Mirjam/AAP-2541-2021; Hofer, Heribert/AAF-7854-2021
OI Grobbel, Mirjam/0000-0002-8619-1498; Hofer, Heribert/0000-0002-2813-7442
FU Leibniz Institute for Zoo and Wildlife Research; German Academic
   Exchange Service (DAAD)Deutscher Akademischer Austausch Dienst (DAAD)
   [D/11/44168]
FX We are grateful to the Tanzanian Commission of Science and Technology,
   the Tanzania Wildlife Research Institute and Tanzania National Parks for
   permission to conduct this study and the Ruaha National Park ecologists
   Paul Banga and Godwell Elias Ole Meing'ataki for their support. We thank
   Luisa Ilse, Dagmar Thierer and Kerstin Wilhelm for their assistance.
   This work was financed by the Leibniz Institute for Zoo and Wildlife
   Research and a grant from the German Academic Exchange Service (DAAD)
   D/11/44168. Additionally we thank two anonymous reviewers for their
   useful comments.
CR BARNES RFW, 1983, AFR J ECOL, V21, P185, DOI 10.1111/j.1365-2028.1983.tb01180.x
   Bengis RG, 2002, REV SCI TECH OIE, V21, P53
   Bjornstad A., 1976, VEGETATION RUAHA NAT
   Dudley JP, 2001, AFR J ECOL, V39, P187, DOI 10.1046/j.0141-6707.2000.00297.x
   East M. L., 2010, Ungulate management in Europe: problems and practices, P319
   Epaphras A. M., 2008, Wetlands Ecology and Management, V16, P183, DOI 10.1007/s11273-007-9065-3
   Galat-Luong A, 2009, GEOGR TECH, V4, P199
   Gersberg RM, 2006, APPL ENVIRON MICROB, V72, P7438, DOI 10.1128/AEM.01024-06
   HAMILTON WJ, 1985, INT J PRIMATOL, V6, P451, DOI 10.1007/BF02735570
   Hellberg RS, 2015, CRIT REV MICROBIOL, P1
   Hilbe JM, 2011, NEGATIVE BINOMIAL RE
   Hilbe JM, 2009, CH CRC TEXT STAT SCI, P1
   Johnson LK, 2004, APPL ENVIRON MICROB, V70, P4478, DOI 10.1128/AEM.70.8.4478-4485.2004
   Keet DF, 1996, ONDERSTEPOORT J VET, V63, P239
   LINDEQUE PM, 1994, ONDERSTEPOORT J VET, V61, P71
   McGrew WC, 2007, FOLIA PRIMATOL, V78, P240, DOI 10.1159/000102319
   Mtahiko M. G. G., 2006, Wetlands Ecology and Management, V14, P489, DOI 10.1007/s11273-006-9002-x
   Poche R.M., 1974, Mammalia, V38, P567, DOI 10.1515/mamm.1974.38.4.567
   Ramey EM, 2013, PACHYDERM, P66
   Redfern JV, 2003, ECOLOGY, V84, P2092, DOI 10.1890/01-0625
   SAFFERMAN ROBERT S., 1967, ENVIRON SCI TECHNOL, V1, P429, DOI 10.1021/es60005a009
   Tieleman BI, 2003, P ROY SOC B-BIOL SCI, V270, P207, DOI 10.1098/rspb.2002.2205
   Wanke H, 2007, J ARID ENVIRON, V70, P553, DOI 10.1016/j.jaridenv.2007.01.011
   Western D, 1975, AFR J ECOL, V13, P265, DOI DOI 10.1111/J.1365-2028.1975.TB00139.X
   Willmer P, 2005, ENV PHYSL ANIMALS, V2nd
   Withers PC, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2014.0149
   Wolanski E, 1999, AFR J ECOL, V37, P419, DOI 10.1046/j.1365-2028.1999.00198.x
   Young WF, 1996, WATER RES, V30, P331, DOI 10.1016/0043-1354(95)00173-5
NR 28
TC 9
Z9 10
U1 1
U2 43
PU ELSEVIER GMBH, URBAN & FISCHER VERLAG
PI JENA
PA OFFICE JENA, P O BOX 100537, 07705 JENA, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
PD JAN
PY 2016
VL 81
IS 1
BP 21
EP 30
DI 10.1016/j.mambio.2015.08.005
PG 10
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA DC8IH
UT WOS:000369462600003
DA 2022-02-10
ER

PT C
AU Chalmers, C
   Fergus, P
   Wich, S
   Longmore, SN
AF Chalmers, C.
   Fergus, P.
   Wich, S.
   Longmore, S. N.
GP IEEE
TI Modelling Animal Biodiversity Using Acoustic Monitoring and Deep
   Learning
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
LA English
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
SP Int Neural Network Soc, IEEE Computat Intelligence Soc
DE Conservation; Audio Classification; Acoustic Monitoring; Modelling
   Biodiversity; Deep Learning
ID ENVIRONMENT
AB For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists have identified species by ear; however, it is now common to deploy audio recording technology to monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation and territorial defence. Animal sounds provide valuable information and help conservationists to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running for long durations to provide insights on habitat and the sounds made by animals and illegal activity. While this technology brings enormous benefits, the amount of data that is generated makes processing a time-consuming process for conservationists. Consequently, there is interest among conservationists to automatically process acoustic data to help speed up biodiversity assessments. Processing these large data sources and extracting relevant sounds from background noise introduces significant challenges. In this paper we outline an approach for achieving this using state of the art in machine learning to automatically extract features from time-series audio signals and modelling deep learning models to classify different bird species based on the sounds they make. The acquired bird songs are processed using mel-frequency cepstrum (MFC) to extract features which are later classified using a multilayer perceptron (MLP). Our proposed method achieved promising results with 0.74 sensitivity, 0.92 specificity and an accuracy of 0.74.
OI Wich, Serge/0000-0003-3954-5174
CR Ba Jimmy, 2013, ADV NEURAL INFORM PR, P3084
   Bardeli R, 2010, PATTERN RECOGN LETT, V31, P1524, DOI 10.1016/j.patrec.2009.09.014
   Berto BP, 2020, J PARASITOL, V106, P707, DOI 10.1645/19-148
   Briggs F, 2012, J ACOUST SOC AM, V131, P4640, DOI 10.1121/1.4707424
   Eibl M, 2017, CLEF WORKING NOTES
   Farina A., 2017, ECOACOUSTICS ECOLOGI
   Hill AP, 2019, HARDWAREX, V6, DOI 10.1016/j.ohx.2019.e00073
   Juanes F, 2018, J NAT CONSERV, V42, P7, DOI 10.1016/j.jnc.2018.01.003
   Kanai S, 2017, ADV NEUR IN, V30
   Knight EC, 2019, BIOACOUSTICS, V28, P539, DOI 10.1080/09524622.2018.1503971
   Lasseck M., 2018, C LABS EV FOR AV FRA
   Muda L., 2010, J COMPUTING, V2, P138
   Nanni L, 2020, EURASIP J AUDIO SPEE, V2020, DOI 10.1186/s13636-020-00175-3
   Nanni L, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101084
   Piczak K. J., 2015, 2015 IEEE 25 INT WOR, DOI DOI 10.1109/MLSP.2015.7324337
   Poma Y., 2020, HYBRID INTELLIGENT S, P71
   Prince P, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030553
   Sasmaz E, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P625, DOI 10.1109/UBMK.2018.8566449
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Teixeira D, 2019, CONSERV SCI PRACT, V1, DOI 10.1111/csp2.72
   Wrege PH, 2017, METHODS ECOL EVOL, V8, P1292, DOI 10.1111/2041-210X.12730
NR 21
TC 0
Z9 0
U1 1
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2161-4393
BN 978-0-7381-3366-9
J9 IEEE IJCNN
PY 2021
DI 10.1109/IJCNN52387.2021.9534195
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BS4TO
UT WOS:000722581707020
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Campos, CM
   Moreno, MC
   Cappa, FM
   Ontiveros, Y
   Cona, MI
   Torres, ML
AF Campos, Claudia M.
   Carolina Moreno, M.
   Cappa, Flavio M.
   Ontiveros, Yamila
   Cona, Monica, I
   Laura Torres, M.
TI "Weaving" Different Knowledge Systems through Studying Salience of Wild
   Animals in a Dryland Area of Argentina
SO JOURNAL OF ETHNOBIOLOGY
LA English
DT Article
DE cognitive salience index; cultural salience; ecological salience;
   occupancy; protected areas
ID NATURES CONTRIBUTIONS; COMMUNITIES
AB The current biodiversity conservation framework explores "nature-people" relationships, recognizing culture's central role. This study aimed to combine local knowledge with scientific ecological data to better understand the relationships between wild animals and local people. We worked in a village (Los Baldecitos) located in the area of influence of Ischigualasto Provincial Park (San Juan, Argentina). We conducted 20 free listing interviews and 12 semi-structured and open ones. We analyzed how the overall salience of different species (established through free listing and cognitive salience index) can be explained by ecological (measured through species occupancy models) and cultural (expressed in interviews) aspects of salience. The cognitive salience index and estimated animal occupancy showed a positive correlation, although it was not statistically significant (Spearman's Rho = 0.48, P = 0.095, N = 17). This could mean that cultural aspects (faunal uses, perception related to attitudes and to nature conservation) were relevant in explaining overall salience. Ten species had the highest and most statistically significant salience and were recorded by camera traps. Some of them share spaces with people (village, water points, corrals, and domestic animal areas), and others were less likely to share habitats where people are present. Wild species have cultural value related to uses and acceptance due to material (tangible benefits, ecological functions) and non-material (affectionate, emotional, aesthetic, presence in oral expression) values. Two carnivores elicited negative reactions because of their predatory damage to domestic animals. This study demonstrates methods to interweave local and scientific knowledge to understand peoplenature relationships in context.
C1 [Campos, Claudia M.; Carolina Moreno, M.; Cona, Monica, I; Laura Torres, M.] Univ Nacl Cuyo, CONICET, Gobierno Mendoza, IADIZA,Inst Argentino Invest Zonas Aridas, RA-5500 Mendoza, Argentina.
   [Cappa, Flavio M.; Ontiveros, Yamila] Univ Nacl San Juan, CONICET, Ctr Invest Geosfera & Biosfera, CIGEOBIO, San Juan, Argentina.
RP Campos, CM (corresponding author), Univ Nacl Cuyo, CONICET, Gobierno Mendoza, IADIZA,Inst Argentino Invest Zonas Aridas, RA-5500 Mendoza, Argentina.
EM ccampos@mendoza-conicet.gob.ar
FU Gobierno de San Juan; CONICETConsejo Nacional de Investigaciones
   Cientificas y Tecnicas (CONICET); Presidencia de la Nacion Argentina
FX S We thank the financial support of Presidencia de la Nacion Argentina
   and Gobierno de San Juan (Project Native Forest 2012-2022) and the
   doctoral fellowship from CONICET to MCM. We thank the administration and
   all the staff of Ischigualasto Provincial Park and the local people of
   Los Baldecitos for their cooperation and help. Yamila Andrada helped us
   during the interviews. Juan Jose Ciarlante assisted us in computational
   programming. Nelida Horak assisted us in drafting the English version.
CR Acebes P, 2010, REV CHIL HIST NAT, V83, P395, DOI 10.4067/S0716-078X2010000300007
   Albuquerque U.P, 2014, METHODS TECHNIQUES E, DOI DOI 10.1007/978-1-4614-8636-7_2
   Anderson, 2010, MODEL SELECTION MULT
   Brewer DD, 2002, FIELD METHOD, V14, P108, DOI DOI 10.1177/1525822X02014001007
   Campos CM, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162551
   Chaves LD, 2019, ACTA BOT BRAS, V33, P360, DOI 10.1590/0102-33062018abb0330
   Neto BCD, 2017, ENVIRON DEV SUSTAIN, V19, P1795, DOI 10.1007/s10668-016-9827-2
   del Solar RG, 1997, MAMMALIA, V61, P617
   Diaz S, 2018, SCIENCE, V359, P270, DOI 10.1126/science.aap8826
   Diaz S, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002040
   Giaccardi M., 2015, PLAN MANEJO PARQUE N
   Gosler AG, 2017, J ETHNOBIOL, V37, P637, DOI 10.2993/0278-0771-37.4.637
   Hernandez J, 2015, J ETHNOBIOL ETHNOMED, V11, DOI 10.1186/1746-4269-11-15
   Hunn E, 1999, FOLKBIOLOGY, P47
   Jofre R.C., 2008, REALIDAD TENDENCIAS, P63
   Liamputtong P, 2017, HDB RES METHODS HLTH, P1, DOI DOI 10.1007/978-981-10-2779-6_12-1
   Lucherini Mauro, 2008, Cat News, V49, P29
   Mace GM, 2014, SCIENCE, V345, P1558, DOI 10.1126/science.1254704
   MacKenzie D. I., 2018, OCCUPANCY ESTIMATION
   Mastrangelo ME, 2019, NAT SUSTAIN, V2, P1115, DOI 10.1038/s41893-019-0412-1
   Medinaceli A, 2018, ETHNOBIOL LETT, V9, P86, DOI 10.14237/ebl.9.1.2018.1121
   Ministerio de Ambiente y Desarrollo Sustentable de la Republica Argentina and Aves Argentinas, 2017, CATEGORIZACION AVES
   Morello J, 2012, ECORREGIONES COMPLEJ
   Nagy-Reis M. B., 2017, PLOS ONE, V12, DOI [10.1371/journal.pone.0168441, DOI 10.1371/journal.pone.0168441]
   Narosky T, 2010, AVES ARGENTINA URUGU
   Zambrana NYP, 2018, NAT PLANTS, V4, P201, DOI 10.1038/s41477-018-0128-7
   Pascual U, 2017, CURR OPIN ENV SUST, V26-27, P7, DOI 10.1016/j.cosust.2016.12.006
   Quiroga VA, 2016, J NAT CONSERV, V31, P9, DOI 10.1016/j.jnc.2016.02.004
   R Core Team, 2019, R LANGUAGE ENV STAT
   Rodrigues TF, 2020, BIOL REV, V95, P1, DOI 10.1111/brv.12551
   Roubik, 2017, POT POLLEN STINGLESS, P283, DOI 10.1007/978-3-319-61839
   Rovero F., 2016, CAMERA TRAPPING WILD
   Sacristan I., 2018, Journal of Threatened Taxa, V10, P11566, DOI 10.11609/jott.4030.10.5.11566-11573
   Sampieri R., 2010, METODOLOGIA INVESTIG, V5a
   Scolaro A., 2006, REPTILES PATAGONICOS
   Secretaria de Ambiente y Desarrollo Sustentable de la Republica Argentina and Sociedad Argentina para el Estudio de los Mamiferos, 2019, CAT 2019 MAM ARG SEG
   Smith J.J., 1997, J LINGUIST ANTHROPOL, V7, P208, DOI DOI 10.1525/JLIN.1997.7.2.208
   Sutrop Urmas, 2001, FIELD METHOD, V13, P263, DOI [10.1177/1525822X0101300303, DOI 10.1177/1525822X0101300303]
   Trillo Cecilia, 2016, Ecol. austral, V26, P7
   Vaccaro, 2007, GUIA MAMIFEROS AM
   Vilela A, 2009, J ARID ENVIRON, V73, P238, DOI 10.1016/j.jaridenv.2007.10.013
   Wajner M, 2019, ETHNOBIOL CONSERV, V8, DOI 10.15451/ec2019-07-8.09-1-23
NR 42
TC 0
Z9 0
U1 1
U2 1
PU SOC ETHNOBIOLOGY
PI DENTON
PA UNIV NORTH TEXAS, DEPT GEOGRAPHY, 1155 UNION CIRCLE 305279, DENTON, TX
   76203-5017 USA
SN 0278-0771
EI 2162-4496
J9 J ETHNOBIOL
JI J. Ethnobiol.
PD JUL
PY 2021
VL 41
IS 2
BP 292
EP 306
PG 15
WC Anthropology; Biology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Anthropology; Life Sciences & Biomedicine - Other Topics
GA TF6HH
UT WOS:000670819600010
DA 2022-02-10
ER

PT J
AU Rather, TA
   Kumar, S
   Khan, JA
AF Rather, Tahir Ali
   Kumar, Sharad
   Khan, Jamal Ahmad
TI Using machine learning to predict habitat suitability of sloth bears at
   multiple spatial scales
SO ECOLOGICAL PROCESSES
LA English
DT Article
DE Bandhavgarh; Melursus ursinus; Multi-scale; Habitat selection; Random
   forest; Sloth bear; Species distribution models
ID DISTRIBUTION MODELS; MELURSUS-URSINUS; NATIONAL-PARK; HOME RANGES;
   SELECTION; CONSERVATION; LANDSCAPE
AB Background: Habitat resources occur across the range of spatial scales in the environment. The environmental resources are characterized by upper and lower limits, which define organisms' distribution in their communities. Animals respond to these resources at the optimal spatial scale. Therefore, multi-scale assessments are critical to identifying the correct spatial scale at which habitat resources are most influential in determining the species-habitat relationships. This study used a machine learning algorithm random forest (RF), to evaluate the scale-dependent habitat selection of sloth bears (Melursus ursinus) in and around Bandhavgarh Tiger Reserve, Madhya Pradesh, India.
   Results: We used 155 spatially rarified occurrences out of 248 occurrence records of sloth bears obtained from camera trap captures (n = 36) and scats located (n = 212) in the field. We calculated focal statistics for 13 habitat variables across ten spatial scales surrounding each presence-absence record of sloth bears. Large (> 5000 m) and small (1000-2000 m) spatial scales were the most dominant scales at which sloth bears perceived the habitat features. Among the habitat covariates, farmlands and degraded forests were the essential patches associated with sloth bear occurrences, followed by sal and dry deciduous forests. The final habitat suitability model was highly accurate and had a very low out-of-bag (OOB) error rate. The high accuracy rate was also obtained using alternate validation matrices.
   Conclusions: Human-dominated landscapes are characterized by expanding human populations, changing land-use patterns, and increasing habitat fragmentation. Farmland and degraded habitats constitute similar to 40% of the landform in the buffer zone of the reserve. One of the management implications may be identifying the highly suitable bear habitats in human-modified landscapes and integrating them with the existing conservation landscapes.
C1 [Rather, Tahir Ali; Kumar, Sharad; Khan, Jamal Ahmad] Aligarh Muslim Univ, Dept Wildlife Sci, Aligarh 202002, Uttar Pradesh, India.
   [Rather, Tahir Ali; Kumar, Sharad] Corbett Fdn, 81-88,Atlanta,Nariman Point, Mumbai 400021, Maharashtra, India.
RP Rather, TA (corresponding author), Aligarh Muslim Univ, Dept Wildlife Sci, Aligarh 202002, Uttar Pradesh, India.; Rather, TA (corresponding author), Corbett Fdn, 81-88,Atlanta,Nariman Point, Mumbai 400021, Maharashtra, India.
EM murtuzatahiri@gmail.com
RI Rather, Tahir Ali/AAX-7605-2021
OI Rather, Tahir Ali/0000-0003-2204-6870
FU TCF, Shri Kedar Gore; Bandhavgarh Tiger Reserve
FX We are thankful to The Corbett Foundation (TCF) for facilitating this
   study. We wish to thank the Director of the TCF, Shri Kedar Gore, for
   his support. We are grateful to the Madhya Pradesh Forest Department for
   the necessary permission to conduct this study. Our acknowledgments are
   with the administrative body of Bandhavgarh Tiger Reserve for their
   support. The first author is thankful to Mr. Shahid A. Dar for
   troubleshooting and suggestions with the analysis. The first author also
   thanks Ms. Shaizah Tajdar for her support.
CR Akhtar N, 2004, URSUS, V15, P203, DOI 10.2192/1537-6176(2004)015<0203:SBHUID>2.0.CO;2
   Akhtar N, 2007, URSUS, V18, P203, DOI 10.2192/1537-6176(2007)18[203:COSBDD]2.0.CO;2
   [Anonymous], 2017, R LANG ENV STAT COMP, DOI DOI 10.2788/95827
   Ash E, 2021, LANDSCAPE ECOL, V36, P455, DOI 10.1007/s10980-020-01105-6
   Atzeni L, 2020, ECOL EVOL, V10, P7686, DOI 10.1002/ece3.6492
   Boyce MS, 2003, ECOSCIENCE, V10, P421, DOI 10.1080/11956860.2003.11682790
   Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Breiman L., 1996, OUT OF BAG ESTIMATIO, P1
   Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45
   Chawla NV, 2003, LECT NOTES ARTIF INT, V2838, P107, DOI 10.1007/978-3-540-39804-2_12
   Chen C., 2004, USING RANDOM FOREST
   Ciarniello LM, 2007, ECOL APPL, V17, P1424, DOI 10.1890/06-1100.1
   Cushman SA, 2004, OIKOS, V105, P117, DOI 10.1111/j.0030-1299.2004.12524.x
   Cushman SA, 2018, MACHINE LEARNING ECO, DOI [10.1007/978-3-319-96978-7_9, DOI 10.1007/978-3-319-96978-7_9]
   Cushman S, 2017, LANDSCAPE ECOL, V32, P1581, DOI 10.1007/s10980-017-0520-0
   Cutler DR, 2007, ECOLOGY, V88, P2783, DOI 10.1890/07-0539.1
   Dar SA, 2021, ANIM CONSERV, V24, P659, DOI 10.1111/acv.12671
   Das S, 2014, URSUS, V25, P111, DOI 10.2192/URSUS-D-14-00008.1
   DHARAIYA N., 2016, IUCN RED LIST THREAT, V2016, DOI 10.2305/IUCN.UK.2016
   Drew CA, 2011, PREDICTIVE SPECIES AND HABITAT MODELING IN LANDSCAPE ECOLOOGY: CONCEPTS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-7390-0
   Elith J, 2006, ECOGRAPHY, V29, P129, DOI 10.1111/j.2006.0906-7590.04596.x
   Evans J.S., 2018, rfUtilities. R package version 2.1-3
   Evans JS, 2011, PREDICTIVE SPECIES AND HABITAT MODELING IN LANDSCAPE ECOLOOGY: CONCEPTS AND APPLICATIONS, P139, DOI 10.1007/978-1-4419-7390-0_8
   Fisher JT, 2011, ECOL EVOL, V1, DOI 10.1002/ece3.45
   Garshelis David L., 1999, P225
   Guisan A, 2000, ECOL MODEL, V135, P147, DOI 10.1016/S0304-3800(00)00354-9
   Hegel TM, 2010, SPATIAL COMPLEXITY, INFORMATICS, AND WILDLIFE CONSERVATION, P273, DOI 10.1007/978-4-431-87771-4_16
   Hostetler Mark, 2000, Urban Ecosystems, V4, P25, DOI 10.1023/A:1009587719462
   Johnsingh A. J. T., 2003, Journal of the Bombay Natural History Society, V100, P190
   JOHNSON DH, 1980, ECOLOGY, V61, P65, DOI 10.2307/1937156
   JOSHI AR, 1995, J WILDLIFE MANAGE, V59, P204, DOI 10.2307/3808932
   Khosravi R, 2019, LANDSCAPE ECOL, V34, P2451, DOI 10.1007/s10980-019-00900-0
   Klaassen B, 2018, ECOL EVOL, V8, P7611, DOI 10.1002/ece3.4269
   Liaw A., 2002, R NEWS, V2, P18, DOI DOI 10.1177/154405910408300516
   Manly BFJ., 1993, RESOURCE SELECTION A, DOI [10.1007/978-94-011-1558-2, DOI 10.1007/978-94-011-1558-2]
   Martin J, 2012, J APPL ECOL, V49, P621, DOI 10.1111/j.1365-2664.2012.02139.x
   Sanchez MCM, 2014, INT J GEOGR INF SCI, V28, P1531, DOI 10.1080/13658816.2013.776684
   Mayer AL, 2003, LANDSCAPE URBAN PLAN, V65, P201, DOI 10.1016/S0169-2046(03)00057-4
   Mayor SJ, 2007, ECOLOGY, V88, P1634, DOI 10.1890/06-1672.1
   Mayor SJ, 2009, ECOSCIENCE, V16, P238, DOI 10.2980/16-2-3238
   McGarigal K, 2016, LANDSCAPE ECOL, V31, P1161, DOI 10.1007/s10980-016-0374-x
   Mi CR, 2017, PEERJ, V5, DOI 10.7717/peerj.2849
   Murphy MA, 2010, ECOLOGY, V91, P252, DOI 10.1890/08-0879.1
   Puri M, 2015, DIVERS DISTRIB, V21, P1087, DOI 10.1111/ddi.12335
   Ramesh T, 2012, URSUS, V23, P78, DOI 10.2192/URSUS-D-11-00006.1
   Rather TA, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-68167-z
   Rather TA, 2021, PEERJ, V9, DOI 10.7717/peerj.10634
   Rather TA, 2020, ECOL PROCESS, V9, DOI 10.1186/s13717-020-00265-2
   Rather TA, 2020, URSUS, V31, DOI 10.2192/URSUS-D-19-00013.2
   Ratnayeke S, 2007, WILDLIFE BIOL, V13, P272, DOI 10.2981/0909-6396(2007)13[272:HRAHUO]2.0.CO;2
   Sathyakumar S., 2012, NATL BEAR CONSERVATI
   Schaefer JA, 1995, ECOGRAPHY, V18, P333, DOI 10.1111/j.1600-0587.1995.tb00136.x
   Schneider, 1994, QUANTITATIVE ECOLOGY
   Schneider DC, 1998, COM ECO SYS, P253
   Schneider DC, 2001, BIOSCIENCE, V51, P545, DOI 10.1641/0006-3568(2001)051[0545:TROTCO]2.0.CO;2
   Schneider DC, 1997, J EXP MAR BIOL ECOL, V216, P129, DOI 10.1016/S0022-0981(97)00093-2
   Shirk AJ., 2012, SCALE DEPENDENCY AM
   Shirk AJ, 2014, ECOL APPL, V24, P1434, DOI 10.1890/13-1510.1
   Wan HY, 2017, CONDOR, V119, P641, DOI 10.1650/CONDOR-17-32.1
   Wasserman TN, 2012, RMRSRP94 USDA FOR SE, P21
   WIENS JA, 1989, FUNCT ECOL, V3, P385, DOI 10.2307/2389612
   Yoganand K., 2006, Journal of the Bombay Natural History Society, V103, P172
   Yoganand K., 2005, THESIS SAURASHTRA U
NR 65
TC 0
Z9 0
U1 4
U2 7
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
EI 2192-1709
J9 ECOL PROCESS
JI Ecol. Process.
PD JUN 29
PY 2021
VL 10
IS 1
AR 48
DI 10.1186/s13717-021-00323-3
PG 12
WC Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA TD7IL
UT WOS:000669495700001
OA gold
DA 2022-02-10
ER

PT J
AU Nagy-Reis, M
   Oshima, JED
   Kanda, CZ
   Palmeira, FBL
   de Melo, FR
   Morato, RG
   Bonjorne, L
   Magioli, M
   Leuchtenberger, C
   Rohe, F
   Lemos, FG
   Martello, F
   Alves-Eigenheer, M
   da Silva, RA
   dos Santos, JS
   Priante, CF
   Bernardo, R
   Rogeri, P
   Assis, JC
   Gaspar, LP
   Tonetti, VR
   Trinca, CT
   Ribeiro, AD
   Bocchiglieri, A
   Hass, A
   Canteri, A
   Chiarello, AG
   Paglia, AP
   Pereira, AA
   de Souza, AC
   Gatica, A
   Medeiro, AZ
   Eriksson, A
   Costa, AN
   Gonzalez-Gallina, A
   Yanosky, AA
   de la Cruz, AJ
   Bertassoni, A
   Bager, A
   Bovo, AAA
   Mol, AC
   Bezerra, AMR
   Percequillo, A
   Vogliotti, A
   Lopes, AMC
   Keuroghlian, A
   Hartley, ACZ
   Devlin, AL
   de Paula, A
   Garcia-Olaechea, A
   Sanchez, A
   Aquino, ACMM
   Srbek-Araujo, AC
   Ochoa, AC
   Tomazzoni, AC
   Lacerda, ACR
   Bacellar, AED
   Campelo, AKN
   Victoria, AMH
   Paschoal, AMD
   Potrich, AP
   Gomes, APN
   Olimpio, APM
   Costa, ARC
   Jacomo, ATD
   Calaca, AM
   Jesus, AS
   Barban, AD
   Feijo, A
   Pagoto, A
   Rolim, AC
   Hermann, AP
   Souza, ASMDE
   Alonso, AC
   Monteiro, A
   Mendonca, AF
   Luza, AL
   Moura, ALB
   da Silva, ALF
   Lanna, AM
   Antunes, AP
   Nunes, AV
   Dechner, A
   Carvalho, AS
   Novaro, AJ
   Scabin, AB
   Gatti, A
   Nobre, AB
   Montanarin, A
   Deffaci, AC
   de Albuquerque, ACF
   Mangione, AM
   Pinto, AMS
   Pontes, ARM
   Bertoldi, AT
   Calouro, AM
   Fernandes, A
   Ferreira, AN
   Ferreguetti, AC
   Rosa, ALM
   Banhos, A
   Francisco, BDD
   Cezila, BA
   Beisiegel, BD
   de Thoisy, B
   Ingberman, B
   Neves, BD
   Pereira-Silva, B
   de Camargo, BB
   Andrade, BD
   Santos, BS
   Leles, B
   Campos, BATP
   Kubiak, BB
   Franca, BRD
   Saranholi, BH
   Mendes, CP
   Devids, CC
   Pianca, C
   Rodrigues, C
   Islas, CA
   de Lima, CA
   de Lima, CR
   Gestich, CC
   Tedesco, CD
   De Angelo, C
   Fonseca, C
   Hass, C
   Peres, CA
   Kasper, CB
   Durigan, CC
   Fragoso, CE
   Verona, CE
   Rocha, CFD
   Salvador, CH
   Vieira, CL
   Ruiz, CEB
   Cheida, CC
   Sartor, CC
   Espinosa, CD
   Fieker, CZ
   Braga, C
   Sanchez-Lalinde, C
   Machado, CIC
   Cronemberger, C
   Luna, CL
   Del Vechio, C
   Bernardo, CSS
   Hurtado, CM
   Lopes, CM
   da Rosa, CA
   Cinta, CC
   Costa, CG
   Zarate-Castaneda, CP
   Novaes, CL
   Jenkins, CN
   Seixas, CS
   Martin, C
   Zaniratto, CP
   Lopez-Fuerte, CF
   da Cunha, CJ
   De-Carvalho, CB
   Chavez, C
   Santos, CC
   Polli, DJ
   Buscariol, D
   Carreira, DC
   Galiano, D
   Thornton, D
   Ferraz, DD
   Lamattina, D
   Moreno, DJ
   Moreira, DO
   Farias, DA
   Barros-Battesti, DM
   Tavares, DC
   Braga, DC
   Gaspar, DA
   Friedeberg, D
   Astua, D
   Silva, DA
   Viana, DC
   Lizcano, DJ
   Varela, DM
   Loretto, D
   Grabin, DM
   Eaton, DP
   da Silva, DM
   Dias, DD
   Camara, EMVC
   Barbier, E
   Chavez-Gonzalez, E
   Rocha, EC
   Lima, ED
   Carrano, E
   Eizirik, E
   Nakano-Oliveira, E
   Rigacci, ED
   Santos, EM
   Venticinque, EM
   Alexandrino, ER
   Ribeiro, EA
   Setz, E
   Rocha, ECLD
   Carvalho, EAR
   Rechenberg, E
   Fraga, ED
   Mendonca, EN
   D'Bastiani, E
   Isasi-Catala, E
   Guijosa-Guadarrama, E
   Ramalho, EE
   Gonzalez, E
   Hasui, E
   Saito, EN
   Fischer, E
   Aguiar, EF
   Rocha, ES
   Nambo, EDM
   de la Pena-Cuellar, E
   Castro, EP
   de Freitas, EB
   Pedo, E
   Rocha, FL
   Girardi, F
   Pereira, FD
   Soares, FAM
   Roque, FD
   Diaz-Santos, FG
   Patiu, FM
   do Nascimento, FO
   Ferreira, FK
   Diaz-Santos, F
   Fantacini, FM
   Pedrosa, F
   da Silva, FP
   Velez-Garcia, F
   Gomes, FBR
   da Silva, FG
   Michalski, F
   de Azevedo, FC
   de Barros, FC
   Santos, FD
   Abra, FD
   Ramalho, FD
   Hatano, FM
   Anaguano-Yancha, F
   Goncalves, F
   Pedroni, F
   Passos, FC
   Jacinavicius, FD
   Bonfim, FCG
   Puertas, FH
   Contreras-Moreno, FM
   Tortato, FR
   Santos, FM
   Chaves, FG
   Tirelli, FP
   Boas, FEV
   Rodrigues, FHG
   Ubaid, FK
   Grotta-Neto, F
   Palomares, F
   Souza, FL
   Costa, FE
   Franca, FGR
   Pinto, FR
   Aguiar, GL
   Hofmann, GS
   Heliodoro, G
   Duarte, GT
   de Andrade, GR
   Beca, G
   Zapata-Rios, G
   Gine, GAF
   Powell, GVN
   Fernandes, GW
   Forero-Medina, G
   Melo, GL
   Santana, GG
   Ciocheti, G
   Alves, GB
   Souto, GHBD
   Villarroel, GJ
   Porfirio, GED
   Batista, GO
   Behling, GM
   Crespo, GMA
   Mourao, GD
   Rezende, GZ
   Toledo, GAD
   Herrera, HM
   Prado, HA
   Bergallo, HD
   Secco, H
   Rajao, H
   Roig, HL
   Concone, HVB
   Duarte, H
   Ermenegildo, H
   Neto, HFP
   Quigley, H
   Lemos, HM
   Cabral, H
   Fernandes-Ferreira, H
   del Castillo, HF
   Ribeiro, IK
   Coelho, IP
   Franceschi, IC
   Melo, I
   Oliveira-Bevan, I
   Mourthe, I
   Bernardi, I
   de la Torre, JA
   Marinho-FIlho, J
   Martinez, J
   Perez, JXP
   Perez-Torres, J
   Bubadue, J
   Silveira, JR
   Seibert, JB
   Oliveira, JF
   Assis, JR
   De la Maza, J
   Hinojosa, J
   Metzger, JP
   Thompson, JJ
   Svenning, JC
   Gouvea, JA
   Souza, JRD
   Pincheira-Ulbrich, J
   Nodari, JZ
   Miranda, J
   Gebin, JCZ
   Giovanelli, JGR
   Rossi, JL
   Favoretti, JPP
   Villani, JP
   Just, JPG
   Souza-Alves, JP
   Costa, JF
   Rocha, J
   Polisar, J
   Sponchiado, J
   Cherem, JJ
   Marinho, JR
   Ziegler, J
   Cordeiro, J
   Silva, JDE
   Rodriguez-Pulido, JA
   dos Santos, JCC
   dos Reis, JC
   Mantovani, JE
   Ramirez, JFM
   Sarasola, JH
   Cartes, JL
   Duarte, JMB
   Longo, JM
   Dantas, JO
   Venancio, JO
   de Matos, JR
   Pires, JSR
   Hawes, JE
   Santos, JG
   Ruiz-Esparza, J
   Lanfranco, JAM
   Rudolf, JC
   Charre-Medellin, JF
   Zanon-Martinez, JI
   Pena-Mondragon, JL
   Krauer, JMC
   Arrabal, JP
   Beduschi, J
   Ilha, J
   Mata, JC
   Bonanomi, J
   Jordao, J
   de Almeida-Rocha, JM
   Pereira-Ribeiro, J
   Zanoni, JB
   Bogoni, JA
   Pacheco, JJC
   Palma, KMC
   Strier, KB
   Castro, KGR
   Didier, K
   Schuchmann, KL
   Chavez-Congrains, K
   Burs, K
   Ferraz, KMPMB
   Juarez, KM
   Flesher, K
   Morais, KDR
   Lautenschlager, L
   Grossel, LA
   Dahmer, LC
   de Almeida, LR
   Fornitano, L
   Barbosa, LDB
   Bailey, LL
   Barreto, LN
   Villalba, LM
   Magalhaes, LM
   Cullen, L
   Marques, L
   Costa, LM
   Silveira, L
   Moreira, LS
   Sartorello, L
   Oliveira, LD
   Gomes, LD
   Aguiar, LD
   da Silva, LH
   Mendonca, LS
   Valenzuela, LA
   Benavalli, L
   Dias, LCS
   Munhoes, LP
   Catenacci, L
   Rampim, LE
   de Paula, LM
   Nascimento, LA
   da Silva, LG
   Quintilham, L
   Segura, LR
   Perillo, LN
   Rezende, LR
   Retta, LM
   Rojas, LNS
   Guimaraes, LN
   Araujo, L
   da Silva, LZ
   Querido, LCD
   Verdade, LM
   Perera-Romero, LE
   Carvalho-Leite, LJ
   Hufnagel, L
   Bernardo, LRR
   Oliveira, LF
   Santos, LGRO
   Lyra, LH
   Borges, LHM
   Severo, MM
   Benchimol, M
   Quatrocchi, MG
   Martins, MZA
   Rodrigues, M
   Penteado, MJF
   Moraes, MFD
   Oliveira, MA
   Lima, MGM
   Ponzio, MD
   Cervini, M
   da Silva, M
   Passamani, M
   Villegas, MA
   dos Santos, MA
   Yamane, MH
   Jardim, MMD
   de Oliveira, ML
   Silveira, M
   Tortato, MA
   Figueiredo, MDL
   Vieira, MV
   Sekiama, ML
   da Silva, MAA
   Nunez, MB
   Siviero, MB
   Carrizo, MC
   Barros, MC
   Barros, MAS
   do Rosario, MCF
   Mora, MCP
   Jover, MDF
   Morandi, MED
   Huerta, ME
   Fernandes, MEA
   Sinani, MEV
   Iezzi, ME
   Pereira, MJR
   Vinassa, MLG
   Lorini, ML
   Jorge, MLSP
   Morini, MS
   Guenther, M
   Landis, MB
   Vale, MM
   Xavier, MS
   Tavares, MS
   Kaizer, M
   Velilla, M
   Bergel, MM
   Hartmann, MT
   da Silva, ML
   Rivero, M
   Munerato, MS
   da Silva, MX
   Zanin, M
   Marques, MI
   Haberfeld, M
   Di Bitetti, MS
   Bowler, M
   Galliez, M
   Ortiz-Moreno, ML
   Buschiazzo, M
   Montes, MA
   Alvarez, MR
   Melo-Dias, M
   Reis, MG
   Correa, MRJ
   Tobler, MW
   Gompper, ME
   Nunez-Regueiro, M
   Vecchi, MB
   Graipel, ME
   Godoi, MN
   Moura, MO
   Konzen, MQ
   Pardo, MV
   Beltrao, MG
   Mongelli, M
   Almeida, MO
   Gilmore, MP
   Schutte, M
   Faria, MB
   Luiz, MR
   de Paula, M
   Hidalgo-Mihart, MG
   Perilli, MLL
   Freitas-Junior, MC
   da Silva, MP
   Denkiewicz, NM
   Torres, NM
   Olifiers, N
   De Lima, ND
   de Albuquerque, NM
   Canassa, NF
   Curi, NHD
   Prestes, NP
   Falconi, N
   Gurgel-Filho, NM
   Pasqualotto, N
   Caceres, NC
   Peroni, N
   de la Sancha, NU
   Zanella, N
   Monroy-Vilchis, O
   Pays, O
   Arimoro, OA
   Ribeiro, OS
   Villalva, P
   Goncalves, PR
   Santos, PM
   Brennand, P
   Rocha, P
   Akkawi, P
   Cruz, P
   Ferreira, PM
   Prist, PR
   Martin, PS
   Arroyo-Gerala, P
   Auricchio, P
   Hartmann, PA
   Antas, PDZ
   Camargo, PHSA
   Marinho, PH
   Ruffino, PHP
   Prado, PI
   Martins, PW
   Cordeiro-Estrela, P
   Luna, P
   Sarmento, P
   Peres, PHF
   Galetti, PM
   de Castilho, PV
   Renaud, PC
   Scarascia, PO
   Cobra, PDA
   Lombardi, PM
   Bessa, R
   Reyna-Hurtado, R
   de Souza, RCC
   Hoogesteijn, RJ
   Alves, RSC
   Romagna, RS
   Silva, RL
   de Oliveira, R
   Beltrao-Mendes, R
   Alencar, RD
   Coutinho, R
   da Silva, RC
   Grando, RLSCC
   Matos, RG
   Araujo, RD
   Pedroso, RF
   Duraes, RMN
   Ribeiro, RLA
   Chagas, R
   Miotto, R
   Bonikowski, RTR
   Muylaert, RL
   Pagotto, RV
   Hilario, RR
   Faria, RT
   Bassini-Silva, R
   Sampaio, R
   Sartorello, R
   Pires, RA
   Hatakeyama, R
   Bianchi, RD
   Buitenwerf, R
   Wallace, R
   Paolino, RM
   Fusco-Costa, R
   Trovati, RG
   Tomasi, RJ
   Hack, ROE
   Magalhaes, RA
   Nobrega, RAD
   Nobre, RD
   Massara, RL
   Froes, RM
   Araujo, RPD
   Perez, RRL
   Jorge, RSP
   de Paula, RC
   Martins, R
   da Cunha, RGT
   Costa, RM
   Alves, RRN
   Garcia-Anleu, R
   Almeida, RPS
   Loachamin, RDC
   Andrade, RS
   Juarez, R
   Bordallo, SUA
   Guaragni, SA
   Carrillo-Percastegui, SE
   Seber, S
   Astete, S
   Hartz, SM
   Espinosa, S
   Solas, SA
   Lima, SR
   Silvestre, SM
   Machado, SAD
   Keuroghlian-Eaton, S
   Albanesi, S
   Costa, SA
   Bazilio, S
   Mendes, SL
   Althoff, SL
   Pinheiro, SD
   Napiwoski, SJ
   Ramirez, SF
   Talamoni, SA
   Age, SG
   Pereira, TC
   Moreira, TC
   Trigo, TC
   Gondim, TMD
   Karlovic, TC
   Cavalcante, T
   Maccarini, T
   Rodrigues, TF
   Timo, TPDE
   Monterrubio, TC
   Piovezan, U
   Cavarzere, V
   Towns, V
   Onofrio, VC
   Oliveira, VB
   Araujo, VC
   Melo, VL
   Kanaan, VT
   Iwakami, V
   Vale, V
   Picinatto, V
   Alberici, V
   Bastazini, VAG
   Orsini, VS
   Braz, VD
   Bonzi, VBR
   Layme, VMG
   Gaboardi, VTR
   Rocha, VJ
   Martins, WP
   Tomas, WM
   Hannibal, W
   Dattilo, W
   Silva, WR
   Endo, W
   Berce, W
   de la Cruz, YB
   Ribeiro, YGG
   Galetti, M
   Ribeiro, MC
AF Nagy-Reis, Mariana
   Oshima, Julia Emi de Faria
   Kanda, Claudia Zukeran
   Palmeira, Francesca Belem Lopes
   de Melo, Fabiano Rodrigues
   Morato, Ronaldo Goncalves
   Bonjorne, Lilian
   Magioli, Marcelo
   Leuchtenberger, Caroline
   Rohe, Fabio
   Lemos, Frederico Gemesio
   Martello, Felipe
   Alves-Eigenheer, Milene
   da Silva, Rafaela Aparecida
   Silveira dos Santos, Juliana
   Priante, Camila Fatima
   Bernardo, Rodrigo
   Rogeri, Patricia
   Assis, Julia Camara
   Gaspar, Lucas Pacciullio
   Tonetti, Vinicius Rodrigues
   Trinca, Cristiano Trape
   Ribeiro, Adauto de Souza
   Bocchiglieri, Adriana
   Hass, Adriani
   Canteri, Adriano
   Chiarello, Adriano Garcia
   Paglia, Adriano Pereira
   Pereira, Adriele Aparecida
   de Souza, Agnis Cristiane
   Gatica, Ailin
   Medeiro, Akyllam Zoppi
   Eriksson, Alan
   Costa, Alan Nilo
   Gonzalez-Gallina, Alberto
   Yanosky, Alberto A.
   Jesus de la Cruz, Alejandro
   Bertassoni, Alessandra
   Bager, Alex
   Bovo, Alex Augusto Abreu
   Cravino Mol, Alexandra
   Bezerra, Alexandra Maria Ramos
   Percequillo, Alexandre
   Vogliotti, Alexandre
   Costa Lopes, Alexandre Martins
   Keuroghlian, Alexine
   Zuniga Hartley, Alfonso Christopher
   Devlin, Allison L.
   de Paula, Almir
   Garcia-Olaechea, Alvaro
   Sanchez, Amadeo
   Aquino, Ana Carla Medeiros Morato
   Srbek-Araujo, Ana Carolina
   Ochoa, Ana Cecilia
   Tomazzoni, Ana Cristina
   Lacerda, Ana Cristyna Reis
   Bacellar, Ana Elisa de Faria
   Campelo, Ana Kellen Nogueira
   Herrera Victoria, Ana Maria
   Paschoal, Ana Maria de Oliveira
   Potrich, Ana Paula
   Gomes, Ana Paula Nascimento
   Olimpio, Ana Priscila Medeiros
   Cunha Costa, Ana Raissa
   Jacomo, Anah Tereza de Almeida
   Calaca, Analice Maria
   Jesus, Anamelia Souza
   de Barros Barban, Ananda
   Feijo, Anderson
   Pagoto, Anderson
   Rolim, Anderson Claudino
   Hermann, Andiara Paula
   Souza, Andiara Silos Moraes de Castro e
   Chein Alonso, Andre
   Monteiro, Andre
   Mendonca, Andre Faria
   Luza, Andre Luis
   Moura, Andre Luis Botelho
   da Silva, Andre Luiz Ferreira
   Lanna, Andre Monnerat
   Antunes, Andre Pinassi
   Nunes, Andre Valle
   Dechner, Andrea
   Carvalho, Andrea Siqueira
   Novaro, Andres Jose
   Scabin, Andressa Barbara
   Gatti, Andressa
   Nobre, Andrezza Bellotto
   Montanarin, Anelise
   Deffaci, Angela Camila
   de Albuquerque, Anna Carolina Figueiredo
   Mangione, Antonio Marcelo
   Pinto, Antonio Millas Silva
   Mendes Pontes, Antonio Rossano
   Bertoldi, Ariane Teixeira
   Calouro, Armando Muniz
   Fernandes, Arthur
   Ferreira, Arystene Nicodemo
   Ferreguetti, Atilla Colombo
   Rosa, Augusto Lisboa Martins
   Banhos, Aureo
   Francisco, Beatriz da Silva de Souza
   Cezila, Beatriz Azevedo
   Beisiegel, Beatriz de Mello
   de Thoisy, Benoit
   Ingberman, Bianca
   Neves, Bianca dos Santos
   Pereira-Silva, Brenda
   Bertagni de Camargo, Bruna
   Andrade, Bruna da Silva
   Santos, Bruna Silva
   Leles, Bruno
   Torres Parahyba Campos, Bruno Augusto
   Kubiak, Bruno Busnello
   Franca, Bruno Rodrigo de Albuquerque
   Saranholi, Bruno Henrique
   Pereira Mendes, Calebe
   Cantagallo Devids, Camila
   Pianca, Camila
   Rodrigues, Camila
   Islas, Camila Alvez
   de Lima, Camilla Angelica
   de Lima, Camilo Ribeiro
   Gestich, Carla Cristina
   Tedesco, Carla Denise
   De Angelo, Carlos
   Fonseca, Carlos
   Hass, Carlos
   Peres, Carlos A.
   Kasper, Carlos Benhur
   Durigan, Carlos Cesar
   Fragoso, Carlos Eduardo
   Verona, Carlos Eduardo
   Rocha, Carlos Frederico Duarte
   Salvador, Carlos Henrique
   Vieira, Carlos Leonardo
   Ruiz, Carmen Elena Barragan
   Cheida, Carolina Carvalho
   Sartor, Caroline Charao
   Espinosa, Caroline da Costa
   Fieker, Carolline Zatta
   Braga, Caryne
   Sanchez-Lalinde, Catalina
   Machado, Cauanne Iglesias Campos
   Cronemberger, Cecilia
   Luna, Cecilia Licariao
   Del Vechio, Christine
   Bernardo, Christine Steiner S.
   Hurtado, Cindy Meliza
   Lopes, Cintia M.
   da Rosa, Clarissa Alves
   Cinta, Claudia Cristina
   Costa, Claudia Guimaraes
   Zarate-Castaneda, Claudia Paola
   Novaes, Claudio Leite
   Jenkins, Clinton N.
   Seixas, Cristiana Simao
   Martin, Cristiane
   Zaniratto, Cristiane Patricia
   Lopez-Fuerte, Cristina Fabiola
   da Cunha, Cristina Jaques
   De-Carvalho, Crizanto Brito
   Chavez, Cuauhtemoc
   Santos, Cyntia Cavalcante
   Polli, Daiana Jeronimo
   Buscariol, Daiane
   Carreira, Daiane Cristina
   Galiano, Daniel
   Thornton, Daniel
   Ferraz, Daniel da Silva
   Lamattina, Daniela
   Moreno, Daniele Janina
   Moreira, Danielle Oliveira
   Farias, Danilo Augusto
   Barros-Battesti, Darci Moraes
   Tavares, Davi Castro
   Costa Braga, David
   Gaspar, Denise Alemar
   Friedeberg, Diana
   Astua, Diego
   Silva, Diego Afonso
   Viana, Diego Carvalho
   Lizcano, Diego J.
   Varela, Diego M.
   Loretto, Diogo
   Grabin, Diogo Maia
   Eaton, Donald P.
   Machado da Silva, Douglas
   Dias, Douglas de Matos
   Camara, Edeltrudes Maria Valadares Calaca
   Barbier, Eder
   Chavez-Gonzalez, Edgar
   Rocha, Ednaldo Candido
   Lima, Edson de Souza
   Carrano, Eduardo
   Eizirik, Eduardo
   Nakano-Oliveira, Eduardo
   Rigacci, Eduardo Delgado
   Santos, Eduardo Marques
   Venticinque, Eduardo Martins
   Alexandrino, Eduardo Roberto
   Abreu Ribeiro, Edvandro
   Setz, Eleonore
   Rocha, Eliana Cesar Laranjeira Duarte
   Carvalho, Elildo Alves Ribeiro, Jr.
   Rechenberg, Elisabete
   Fraga, Elmary da Costa
   Mendonca, Eloisa Neves
   D'Bastiani, Elvira
   Isasi-Catala, Emiliana
   Guijosa-Guadarrama, Emiliano
   Ramalho, Emiliano Esterci
   Gonzalez, Enrique
   Hasui, Erica
   Saito, Erica Naomi
   Fischer, Erich
   Aguiar, Erick Francisco
   Rocha, Erick Sekiama
   Martinez Nambo, Erik Daniel
   de la Pena-Cuellar, Erika
   Castro, Erika Paula
   de Freitas, Evellyn Borges
   Pedo, Ezequiel
   Rocha, Fabiana Lopes
   Girardi, Fabiane
   Pereira, Fabiane de Aguiar
   Soares, Fabio Angelo Melo
   Roque, Fabio de Oliveira
   Diaz-Santos, Fabio Gabriel
   Patiu, Fabio Mello
   do Nascimento, Fabio Oliveira
   Keesen Ferreira, Fabiola
   Diaz-Santos, Fabricio
   Moreli Fantacini, Felipe
   Pedrosa, Felipe
   Pessoa da Silva, Felipe
   Velez-Garcia, Felipe
   Gomes, Felipe Bittioli R.
   Guedes da Silva, Fernanda
   Michalski, Fernanda
   de Azevedo, Fernanda Cavalcanti
   de Barros, Fernanda Cristina
   Santos, Fernanda da Silva
   Abra, Fernanda Delborgo
   Ramalho, Fernanda do Passo
   Hatano, Fernanda Martins
   Anaguano-Yancha, Fernando
   Goncalves, Fernando
   Pedroni, Fernando
   Passos, Fernando C.
   Jacinavicius, Fernando de Castro
   Bonfim, Fernando Cesar Goncalves
   Puertas, Fernando Henrique
   Contreras-Moreno, Fernando M.
   Tortato, Fernando Rodrigo
   Santos, Filipe Martins
   Chaves, Flavia Guimaraes
   Tirelli, Flavia Pereira
   Vilas Boas, Flavio Eduardo
   Rodrigues, Flavio Henrique Guimaraes
   Ubaid, Flavio Kulaif
   Grotta-Neto, Francisco
   Palomares, Francisco
   Souza, Franco Leandro
   Costa, Francys Emanuelle
   Franca, Frederico G. R.
   Ramirez Pinto, Fredy
   Aguiar, Gabriel Lima
   Hofmann, Gabriel Selbach
   Heliodoro, Gabriela
   Duarte, Gabriela Teixeira
   Ribeiro de Andrade, Gabrielle
   Beca, Gabrielle
   Zapata-Rios, Galo
   Gine, Gaston Andres Fernandez
   Powell, George V. N.
   Wilson Fernandes, Geraldo
   Forero-Medina, German
   Melo, Geruza L.
   Santana, Gindomar Gomes
   Ciocheti, Giordano
   Alves, Giselle Bastos
   Souto, Glauber Henrique Borges de Oliveira
   Villarroel, Glenda Jessica
   Porfirio, Grasiela Edith de Oliveira
   Batista, Graziele Oliveira
   Behling, Greici Maia
   Ayala Crespo, Guido Marcos
   Mourao, Guilherme de Miranda
   Rezende, Guilherme Zamarian
   Toledo, Gustavo Alves da Costa
   Herrera, Heitor Miraglia
   Alves Prado, Helena
   Bergallo, Helena de Godoy
   Secco, Helio
   Rajao, Henrique
   Roig, Henrique Llacer
   Concone, Henrique Villas Boas
   Duarte, Herbert
   Ermenegildo, Hiago
   Ferreira Paulino Neto, Hipolito
   Quigley, Howard
   Lemos, Hudson Macedo
   Cabral, Hugo
   Fernandes-Ferreira, Hugo
   del Castillo, Hugo Fernando
   Ribeiro, Igor Kintopp
   Coelho, Igor Pfeifer
   Franceschi, Ingridi Camboim
   Melo, Isabel
   Oliveira-Bevan, Isabella
   Mourthe, Italo
   Bernardi, Itibere
   de la Torre, J. Antonio
   Marinho-Filho, Jader
   Martinez, Jaime
   Palacios Perez, Jaime Xavier
   Perez-Torres, Jairo
   Bubadue, Jamile
   Silveira, Jana Rangel
   Seibert, Jardel Brandao
   Oliveira, Jasmim Felipe
   Assis, Jasmine Resende
   De la Maza, Javier
   Hinojosa, Javier
   Metzger, Jean Paul
   Thompson, Jeffrey James
   Svenning, Jens-Christian
   Gouvea, Jessica Abonizio
   Souza, Jesus Rodrigues Domingos
   Pincheira-Ulbrich, Jimmy
   Nodari, Joana Zorzal
   Miranda, Joao
   Zecchini Gebin, Joao Carlos
   Giovanelli, Joao Gabriel Ribeiro
   Rossi Junior, Joao Luiz
   Pandini Favoretti, Joao Paulo
   Villani, Joao Paulo
   Just, Joao Paulo Gava
   Souza-Alves, Joao Pedro
   Costa, Jociel Ferreira
   Rocha, Joedison
   Polisar, John
   Sponchiado, Jonas
   Cherem, Jorge Jose
   Marinho, Jorge Reppold
   Ziegler, Jorn
   Cordeiro, Jose
   Silva Junior, Jose DeSousa e
   Rodriguez-Pulido, Jose Ariel
   Chaves dos Santos, Jose Carlos
   dos Reis Junior, Jose Clemensou
   Mantovani, Jose Eduardo
   Moreira Ramirez, Jose Fernando
   Sarasola, Jose Hernan
   Cartes, Jose Luis
   Duarte, Jose Mauricio Barbanti
   Longo, Jose Milton
   Dantas, Jose Oliveira
   Venancio, Jose Otavio
   de Matos, Jose Roberto
   Pires, Jose Salatiel Rodrigues
   Hawes, Joseph E.
   Santos, Joyce Goncalves
   Ruiz-Esparza, Juan
   Martinez Lanfranco, Juan Andres
   Rudolf, Juan Carlos
   Charre-Medellin, Juan Felipe
   Zanon-Martinez, Juan Ignacio
   Pena-Mondragon, Juan L.
   Campos Krauer, Juan Manuel
   Arrabal, Juan Pablo
   Beduschi, Julia
   Ilha, Julia
   Mata, Julia Carolina
   Bonanomi, Juliana
   Jordao, Juliana
   de Almeida-Rocha, Juliana Monteiro
   Pereira-Ribeiro, Juliane
   Zanoni, Juliani Bruna
   Bogoni, Juliano Andre
   Chacon Pacheco, Julio Javier
   Contreras Palma, Kamila Marianne
   Strier, Karen B.
   Rodriguez Castro, Karen Giselle
   Didier, Karl
   Schuchmann, Karl L.
   Chavez-Congrains, Karla
   Burs, Kathrin
   Ferraz, Katia M. P. M. B.
   Juarez, Keila Macfadem
   Flesher, Kevin
   Morais, Kimberly Danielle Rodrigues
   Lautenschlager, Lais
   Grossel, Lais Aline
   Dahmer, Lais Camila
   de Almeida, Lana Resende
   Fornitano, Larissa
   Barbosa, Larissa de Nazare Barros
   Bailey, Larissa L.
   Barreto, Larissa Nascimento
   Villalba, Laura Magnolia
   Magalhaes, Laura Martins
   Cullen, Laury, Jr.
   Marques, Leandro
   Marques Costa, Leonardo
   Silveira, Leandro
   Moreira, Leandro Santana
   Sartorello, Leonardo
   Oliveira, Leonardo de Carvalho
   Gomes, Leonardo de Paula
   Aguiar, Leonardo dos Santos
   da Silva, Leonardo Henrique
   Mendonca, Leonardo Siqueira
   Valenzuela, Leonor Adriana
   Benavalli, Leticia
   Dias, Leticia Coutinho Sangy
   Munhoes, Leticia Prado
   Catenacci, Lilian
   Rampim, Lilian Elaine
   de Paula, Livia Maria
   Nascimento, Lorena Anne
   Goncalves da Silva, Lucas
   Quintilham, Lucas
   Ramis Segura, Lucas
   Perillo, Lucas Neves
   Rezende, Lucas Rodrigo
   Martinez Retta, Lucia
   Rojas, Lucia Nathaly Stefany
   Guimaraes, Luiza Neves
   Araujo, Luciana
   Zago da Silva, Luciana
   Querido, Luciano Carramaschi de Alagao
   Verdade, Luciano Martins
   Perera-Romero, Lucy E.
   Carvalho-Leite, Ludimila Juliele
   Hufnagel, Ludmila
   Rezende Bernardo, Luis Renato
   Oliveira, Luiz Flamarion
   Oliveira Santos, Luiz Gustavo Rodrigues
   Lyra, Luiz Henrique
   Borges, Luiz Henrique Medeiros
   Severo, Magnus Machado
   Benchimol, Maira
   Quatrocchi, Maira Giuliana
   Martins, Maisa Ziviani Alves
   Rodrigues, Manoel
   Penteado, Marcel Jose Franco
   Figueredo Duarte Moraes, Marcela
   Oliveira, Marcela Alvares
   Lima, Marcela Guimaraes Moreira
   Ponzio, Marcella do Carmo
   Cervini, Marcelo
   da Silva, Marcelo
   Passamani, Marcelo
   Villegas, Marcelo Alejandro
   dos Santos Junior, Marcelo Augusto
   Yamane, Marcelo Hideki
   Jardim, Marcia Maria de Assis
   Leite de Oliveira, Marcio
   Silveira, Marcos
   Tortato, Marcos Adriano
   Figueiredo, Marcos de Souza Lima
   Vieira, Marcus Vinicius
   Sekiama, Margareth L.
   Andrade da Silva, Maria Augusta
   Nunez, Maria Beatriz
   Siviero, Maria Brunini
   Carrizo, Maria Celina
   Barros, Maria Claudene
   Barros, Marilia A. S.
   do Rosario, Maria Cristina Ferreira
   Penuela Mora, Maria Cristina
   Fleytas Jover, Maria del Carmen
   Morandi, Maria Elisa de Freitas
   Huerta, Maria Emilia
   Fernandes, Maria Emilia Avelar
   Viscarra Sinani, Maria Estela
   Iezzi, Maria Eugenia
   Ramos Pereira, Maria Joao
   Gomez Vinassa, Maria Laura
   Lorini, Maria Lucia
   Jorge, Maria Luisa S. P.
   Morini, Maria Santina
   Guenther, Mariana
   Landis, Mariana Bueno
   Vale, Mariana M.
   Xavier, Mariana Sampaio
   Tavares, Mariana Silva
   Kaizer, Mariane
   Velilla, Marianela
   Bergel, Mariano Maudet
   Hartmann, Marilia Teresinha
   Lima da Silva, Marina
   Rivero, Marina
   Salles Munerato, Marina
   Xavier da Silva, Marina
   Zanin, Marina
   Marques, Marinez Isaac
   Haberfeld, Mario
   Di Bitetti, Mario S.
   Bowler, Mark
   Galliez, Maron
   Ortiz-Moreno, Martha Lucia
   Buschiazzo, Martin
   Montes, Martin Alejandro
   Alvarez, Martin R.
   Melo-Dias, Mateus
   Reis, Matheus Goncalves
   Correa, Matheus Rocha Jorge
   Tobler, Mathias W.
   Gompper, Matthew E.
   Nunez-Regueiro, Mauricio
   Brandao Vecchi, Mauricio
   Graipel, Mauricio Eduardo
   Godoi, Mauricio Neves
   Moura, Mauricio O.
   Konzen, Mauricio Quoos
   Pardo, Maximiliano Victor
   Beltrao, Mayara Guimaraes
   Mongelli, Melissa
   Almeida, Meyline Oliveira
   Gilmore, Michael P.
   Schutte, Michel
   Faria, Michel Barros
   Luiz, Micheli Ribeiro
   de Paula, Milton
   Hidalgo-Mihart, Mircea G.
   Perilli, Miriam Lucia Lages
   Freitas-Junior, Mozart Caetano
   da Silva, Murillo Prado
   Denkiewicz, Natalia Mariana
   Torres, Natalia Mundim
   Olifiers, Natalie
   De Lima, Natani Da Silva
   de Albuquerque, Natasha Moraes
   Canassa, Nathalia Fernandes
   de Almeida Curi, Nelson Henrique
   Prestes, Nemora Pauletti
   Falconi, Nereyda
   Gurgel-Filho, Newton Mota
   Pasqualotto, Nielson
   Caceres, Nilton C.
   Peroni, Nivaldo
   de la Sancha, Noe U.
   Zanella, Noeli
   Monroy-Vilchis, Octavio
   Pays, Olivier
   Arimoro, Omolabake Alhambra
   Ribeiro, Otavio Santi
   Villalva, Pablo
   Goncalves, Pablo Rodrigues
   Santos, Paloma Marques
   Brennand, Pamella
   Rocha, Patricio
   Akkawi, Paula
   Cruz, Paula
   Ferreira, Paula Modenesi
   Prist, Paula Ribeiro
   Martin, Paula Sanches
   Arroyo-Gerala, Paulina
   Auricchio, Paulo
   Hartmann, Paulo Afonso
   Antas, Paulo de Tarso Zuquim
   Camargo, Paulo H. S. A.
   Marinho, Paulo Henrique
   Ruffino, Paulo Henrique Peira
   Prado, Paulo Inacio
   Martins, Paulo Wesley
   Cordeiro-Estrela, Pedro
   Luna, Pedro
   Sarmento, Pedro
   Faria Peres, Pedro Henrique
   Galetti, Pedro Manoel, Jr.
   de Castilho, Pedro Volkmer
   Renaud, Pierre-Cyril
   Scarascia, Pietro Oliveira
   Cobra, Priscilla De Paula Andrade
   Lombardi, Pryscilla Moura
   Bessa, Rafael
   Reyna-Hurtado, Rafael
   de Souza, Rafael Cerqueira Castro
   Hoogesteijn, Rafael Jan
   Alves, Rafael Souza Cruz
   Romagna, Rafael Spilere
   Silva, Ramon Lima
   de Oliveira, Ramonna
   Beltrao-Mendes, Raone
   Alencar, Raony de Macedo
   Coutinho, Raphaella
   da Silva, Raquel Costa
   Caribe Grando, Raquel L. S. C.
   Matos, Rayanne Gama
   Araujo, Raylenne da Silva
   Pedroso, Rayssa Faria
   Duraes, Rayssa Mainette Nantes
   Ribeiro, Renan Lieto Alves
   Chagas, Renata
   Miotto, Renata
   Twardowsky Ramalho Bonikowski, Renata
   Muylaert, Renata Lara
   Pagotto, Renata Valls
   Hilario, Renato Richard
   Faria, Rhayssa Terra
   Bassini-Silva, Ricardo
   Sampaio, Ricardo
   Sartorello, Ricardo
   Pires, Ricardo Araujo
   Hatakeyama, Richard
   Bianchi, Rita de Cassia
   Buitenwerf, Robert
   Wallace, Robert
   Paolino, Roberta Montanheiro
   Fusco-Costa, Roberto
   Trovati, Roberto Guilherme
   Tomasi, Roberto Junior
   Espindola Hack, Robson Odeli
   Magalhaes, Rodolfo Assis
   Nobrega, Rodrigo Affonso de Albuquerque
   Nobre, Rodrigo de Almeida
   Massara, Rodrigo Lima
   Froes, Rodrigo Medina
   Araujo, Rodrigo Paulo da Cunha
   Leon Perez, Rodrigo Raul
   Jorge, Rodrigo Silva Pinto
   de Paula, Rogerio Cunha
   Martins, Rogerio
   da Cunha, Rogerio Grassetto Teixeira
   Costa, Romulo
   Alves, Romulo Romeu Nobrega
   Garcia-Anleu, Rony
   Santos Almeida, Rony Peterson
   Cueva Loachamin, Ruben Dario
   Andrade, Rubia Santana
   Juarez, Rugieri
   Bordallo, Samanta Uchoa
   Guaragni, Samara Arsego
   Carrillo-Percastegui, Samia E.
   Seber, Samile
   Astete, Samuel
   Hartz, Sandra Maria
   Espinosa, Santiago
   Alvarez Solas, Sara
   Ramos Lima, Saulo
   Silvestre, Saulo Meneses
   Machado, Savio Augusto de Souza
   Keuroghlian-Eaton, Sean
   Albanesi, Sebastian
   Costa, Sebastian Andres
   Bazilio, Sergio
   Mendes, Sergio Lucena
   Althoff, Sergio Luiz
   Pinheiro, Shery Duque
   Napiwoski, Silvio Junior
   Fernandez Ramirez, Sixto
   Talamoni, Sonia Aparecida
   Age, Stefani Gabrieli
   Pereira, Taigua Correa
   Moreira, Tainah Cruz
   Trigo, Tatiane Campos
   Gondim, Tayana Mendonca da Silva
   Karlovic, Thamiris Christina
   Cavalcante, Thiago
   Maccarini, Thiago
   Rodrigues, Thiago Ferreira
   Timo, Thiago Philipe
   Monterrubio, Tiberio Cesar
   Piovezan, Ubiratan
   Cavarzere, Vagner
   Towns, Valeria
   Onofrio, Valeria Castilho
   Oliveira, Valeska Buchemi
   Araujo, Valquiria Cabral
   Melo, Vanessa Lazaro
   Kanaan, Vanessa Tavares
   Iwakami, Victor
   Vale, Victor
   Picinatto Filho, Vilmar
   Alberici, Vinicius
   Bastazini, Vinicius A. G.
   Orsini, Vinicius Santana
   Braz, Vivian da Silva
   Rojas Bonzi, Viviana B.
   Guedes Layme, Viviane Maria
   Gaboardi, Viviane Telles Rodrigues
   Rocha, Vlamir Jose
   Martins, Waldney Pereira
   Tomas, Walfrido Moraes
   Hannibal, Wellington
   Dattilo, Wesley
   Silva, Wesley R.
   Endo, Whaldener
   Berce, William
   Bravata de la Cruz, Yaribeth
   Ribeiro, Yuri Geraldo Gomes
   Galetti, Mauro
   Ribeiro, Milton C.
TI NEOTROPICAL CARNIVORES: a data set on carnivore distribution in the
   Neotropics
SO ECOLOGY
LA English
DT Article; Data Paper
DE canidae; carnivores; conservation; data paper; felidae; mammal;
   neotropical region; occurrence; predator; species distribution
AB Mammalian carnivores are considered a key group in maintaining ecological health and can indicate potential ecological integrity in landscapes where they occur. Carnivores also hold high conservation value and their habitat requirements can guide management and conservation plans. The order Carnivora has 84 species from 8 families in the Neotropical region: Canidae; Felidae; Mephitidae; Mustelidae; Otariidae; Phocidae; Procyonidae; and Ursidae. Herein, we include published and unpublished data on native terrestrial Neotropical carnivores (Canidae; Felidae; Mephitidae; Mustelidae; Procyonidae; and Ursidae). NEOTROPICAL CARNIVORES is a publicly available data set that includes 99,605 data entries from 35,511 unique georeferenced coordinates. Detection/non-detection and quantitative data were obtained from 1818 to 2018 by researchers, governmental agencies, non-governmental organizations, and private consultants. Data were collected using several methods including camera trapping, museum collections, roadkill, line transect, and opportunistic records. Literature (peer-reviewed and grey literature) from Portuguese, Spanish and English were incorporated in this compilation. Most of the data set consists of detection data entries (n = 79,343; 79.7%) but also includes non-detection data (n = 20,262; 20.3%). Of those, 43.3% also include count data (n = 43,151). The information available in NEOTROPICAL CARNIVORES will contribute to macroecological, ecological, and conservation questions in multiple spatio-temporal perspectives. As carnivores play key roles in trophic interactions, a better understanding of their distribution and habitat requirements are essential to establish conservation management plans and safeguard the future ecological health of Neotropical ecosystems. Our data paper, combined with other large-scale data sets, has great potential to clarify species distribution and related ecological processes within the Neotropics. There are no copyright restrictions and no restriction for using data from this data paper, as long as the data paper is cited as the source of the information used. We also request that users inform us of how they intend to use the data.
EM mariana.nbreis@gmail.com
RI Prado, Paulo I/G-3353-2012; Magioli, Marcelo/L-7809-2014; Cáceres,
   Nilton C./H-6899-2012; Passamani, Marcelo/ABG-8886-2020; Monroy-Vilchis,
   Octavio/H-8942-2019; Hartz, Sandra Maria/A-8052-2012; Benchimol, Maíra
   MB/I-7664-2014; Lizcano, Diego J./E-6893-2011; Tavares, Davi
   Castro/H-7960-2015; Massara, Rodrigo Lima/Q-2223-2015; Grossel,
   Laís/AAR-5708-2021; SARANHOLI, BRUNO HENRIQUE/AAK-2395-2020; de
   Oliveira, Márcio L/F-3792-2012; Sponchiado, Jonas/E-4501-2013;
   Figueiredo, Marcos S. L./A-9720-2014; Setz, Eleonore Z F/C-1050-2012;
   Nobrega, Rodrigo Affonso Albuquerque/M-9668-2013; Viana,
   Diego/AAG-6445-2021; Oliveira, Marcela Alvares/X-7458-2019; Peroni,
   Nivaldo/AAU-4701-2020; Cronemberger, Cecilia/O-5663-2016; Chiarello,
   Adriano Garcia/G-2510-2012; Gestich, Carla Cristina/Q-2833-2017; Braga,
   Caryne/G-2626-2014; Junior, Vagner A Cavarzere/S-3435-2016; Sartor,
   Caroline Charão/D-7509-2018; PACHECO, JULIO JAVIER CHACÓN/L-2550-2013;
   Layme, Viviane/AAR-1800-2021; Peres, Carlos Augusto/N-8275-2019;
   Rigacci, Eduardo/AAC-1793-2021; CARRANO, EDUARDO/ABA-5037-2021; Alves,
   Rômulo Romeu Nóbrega/A-7026-2009; Battesti, Darci Moraes
   Barros/A-6804-2018; Tonetti, Vinicius Rodrigues/I-3018-2015; Azevedo,
   Fernanda Cavalcanti/J-1734-2014; Nunes, André/AAF-5792-2021; da Cunha,
   Rogério Grassetto Teixeira/J-1738-2012; Pasqualotto,
   Nielson/AAF-7015-2021; Percequillo, Alexandre R/C-3067-2012; Passos,
   Fernando C/H-1073-2012; DE TARSO ZUQUIM ANTAS, PAULO/AAK-9499-2021;
   CHAVEZ, CUAUHTÉMOC/C-5754-2019; Novaes, Claudio/AAH-2137-2021;
   /AAT-4881-2021; Ferreguetti, Atilla/D-1163-2019; da Silva Ferraz,
   Daniel/F-3669-2016; Silva, Rafaela/D-8593-2019; Bassini-Silva,
   Ricardo/X-3935-2019; Perillo, Lucas/ABG-5303-2020; Jenkins,
   Clinton/D-6134-2011; Paglia, Adriano P/A-7965-2012; Azevedo,
   Fernanda/AAB-4277-2019; Vale, Mariana M./I-9408-2012; Bonikowski, Renata
   Twardowsky Ramalho/AAO-8303-2021; Kaizer, Mariane C/P-2116-2017; Santos,
   Filipe Martins/M-1061-2019; da Silva, Felipe Pessoa/AAV-6536-2020;
   Fonseca, Carlos MMS/D-9744-2011; Oliveira-Santos, Luiz Gustavo
   Rodrigues/F-9562-2012; Bezerra, Alexandra Maria Ramos/H-2180-2012;
   Souza-Alves, João Pedro/Q-9316-2019; Fernandes, Geraldo/AAN-5602-2021;
   Metzger, Jean Paul/C-2514-2012; Bergallo, Helena Godoy/F-9257-2011;
   Bailey, Larissa/ABH-3513-2020; Talamoni, Sônia Aparecida/B-9422-2013;
   Sponchiado, Jonas/AAT-3039-2021; Paschoal, Ana Maria O/N-6631-2016;
   Zanin, Marina/AAN-6327-2020; Svenning, Jens-Christian/C-8977-2012; Melo,
   Geruza Leal/P-7449-2016; da Silva, Lucas Gonçalves/B-8702-2014; Soares,
   Fábio Angelo/ABD-6311-2020; Beltrão, Mayara Guimarães/K-1432-2014;
   Ortiz-Moreno, Martha Lucia/K-1481-2017; Hilário, Renato/A-7158-2013;
   Luza, André Luís/O-1134-2017; Guenther, Mariana/A-5695-2013; de C
   Jacinavicius, Fernando/F-2856-2015; Garcia-Anleu, Rony/AAA-6992-2022;
   Cobra, Priscilla/AAC-2486-2022; Rocha, Ednaldo Cândido/K-3031-2015;
   Rodrigues, Thiago F/L-4273-2018; Mendonça, André F./D-5997-2013; Rocha,
   Patrício/B-3875-2013; Cavarzere, Vagner/I-2971-2015; Camargo, Paulo H.
   S. A./C-3271-2018; Barbier, Eder/C-3515-2012; Pônzio,
   Marcella/AAW-1617-2020; Lemos, Frederico Gemesio/G-1991-2014; Peroni,
   Nivaldo/ABA-2663-2021; Costa, Alan Nilo/F-2836-2014; Ubaid,
   Flávio/ABA-8231-2021; Michalski, Fernanda/J-4691-2012; de Moura Bubadué,
   Jamile/G-5378-2015; Prado, Paulo Inácio/AAN-6512-2021; Moura, Mauricio
   O/B-1063-2013; Eizirik, Eduardo/K-8034-2012; Pereira-Ribeiro,
   Juliane/D-4371-2017; Karlovic, Thamíris C./M-5088-2017; Astúa,
   Diego/A-3583-2010; Cordeiro, Jose/D-1943-2016; Paulo da Cunha Araujo,
   Rodrigo/U-9219-2018; Zuniga Hartley, Alfonso Christopher/O-4283-2016;
   Goncalves Bonfim, Fernando Cesar/D-3356-2019; Perez-Torres,
   Jairo/F-1395-2010; Alexandrino, Eduardo Roberto/F-8261-2013; Galiano,
   Daniel/O-1671-2013; Astete, Samuel/M-8585-2018; Assis, Julia
   Camara/P-4264-2018; Fragoso, Carlos Eduardo/U-6488-2017; Palmeira,
   Francesca B. L./B-3646-2015; Bonjorne de Almeida, Lilian/P-3746-2018;
   Venticinque, Eduardo/G-8961-2015
OI Magioli, Marcelo/0000-0003-0865-102X; Cáceres, Nilton
   C./0000-0003-4904-0604; Passamani, Marcelo/0000-0002-0940-4074;
   Monroy-Vilchis, Octavio/0000-0003-3159-6014; Hartz, Sandra
   Maria/0000-0002-6536-1072; Benchimol, Maíra MB/0000-0002-1238-1619;
   Lizcano, Diego J./0000-0002-9648-0576; Tavares, Davi
   Castro/0000-0002-6811-9572; Massara, Rodrigo Lima/0000-0003-1221-2185;
   SARANHOLI, BRUNO HENRIQUE/0000-0002-8221-3557; de Oliveira, Márcio
   L/0000-0002-7705-0626; Sponchiado, Jonas/0000-0002-1267-1763;
   Figueiredo, Marcos S. L./0000-0003-0558-911X; Setz, Eleonore Z
   F/0000-0001-7638-7086; Viana, Diego/0000-0002-3302-9892; Oliveira,
   Marcela Alvares/0000-0002-4129-993X; Peroni,
   Nivaldo/0000-0002-6770-5377; Cronemberger, Cecilia/0000-0002-0704-0262;
   Chiarello, Adriano Garcia/0000-0003-1914-5480; Gestich, Carla
   Cristina/0000-0002-3906-025X; Braga, Caryne/0000-0003-2671-0206;
   PACHECO, JULIO JAVIER CHACÓN/0000-0002-7770-3615; Layme,
   Viviane/0000-0002-2490-777X; Peres, Carlos Augusto/0000-0002-1588-8765;
   Alves, Rômulo Romeu Nóbrega/0000-0001-6824-0797; Battesti, Darci Moraes
   Barros/0000-0002-8541-2252; Tonetti, Vinicius
   Rodrigues/0000-0003-2263-5608; Azevedo, Fernanda
   Cavalcanti/0000-0002-2424-6860; da Cunha, Rogério Grassetto
   Teixeira/0000-0002-2368-3540; Pasqualotto, Nielson/0000-0002-8283-4759;
   Percequillo, Alexandre R/0000-0002-7892-8912; DE TARSO ZUQUIM ANTAS,
   PAULO/0000-0002-8295-6297; CHAVEZ, CUAUHTÉMOC/0000-0003-2201-4748;
   Novaes, Claudio/0000-0002-1692-369X; Ferreguetti,
   Atilla/0000-0002-5139-8835; da Silva Ferraz, Daniel/0000-0001-7919-1433;
   Silva, Rafaela/0000-0002-0132-0124; Bassini-Silva,
   Ricardo/0000-0002-9568-4120; Perillo, Lucas/0000-0003-4291-4452;
   Jenkins, Clinton/0000-0003-2198-0637; Paglia, Adriano
   P/0000-0001-9957-5506; Azevedo, Fernanda/0000-0002-2424-6860; Vale,
   Mariana M./0000-0003-0734-4925; Kaizer, Mariane C/0000-0001-9105-9478;
   Santos, Filipe Martins/0000-0003-2032-8129; da Silva, Felipe
   Pessoa/0000-0003-1411-1249; Fonseca, Carlos MMS/0000-0001-6559-7133;
   Oliveira-Santos, Luiz Gustavo Rodrigues/0000-0001-9632-5173; Bezerra,
   Alexandra Maria Ramos/0000-0002-7972-5535; Souza-Alves, João
   Pedro/0000-0002-8517-1276; Fernandes, Geraldo/0000-0003-1559-6049;
   Metzger, Jean Paul/0000-0002-0087-5240; Bergallo, Helena
   Godoy/0000-0001-9771-965X; Talamoni, Sônia
   Aparecida/0000-0002-6411-1805; Paschoal, Ana Maria
   O/0000-0001-8560-1056; Zanin, Marina/0000-0001-7112-7381; Svenning,
   Jens-Christian/0000-0002-3415-0862; Melo, Geruza
   Leal/0000-0002-2384-3786; da Silva, Lucas Gonçalves/0000-0002-7993-9015;
   Ortiz-Moreno, Martha Lucia/0000-0003-0172-9111; Hilário,
   Renato/0000-0002-0346-0921; Luza, André Luís/0000-0003-0302-529X;
   Guenther, Mariana/0000-0002-3104-3105; de C Jacinavicius,
   Fernando/0000-0002-5503-3120; Cobra, Priscilla/0000-0002-7553-0212;
   Rocha, Ednaldo Cândido/0000-0002-2554-777X; Rodrigues, Thiago
   F/0000-0003-1972-0043; Mendonça, André F./0000-0002-8248-0639; Rocha,
   Patrício/0000-0003-1661-3779; Cavarzere, Vagner/0000-0003-0510-4557;
   Camargo, Paulo H. S. A./0000-0001-9081-4558; Barbier,
   Eder/0000-0001-5068-7048; Pônzio, Marcella/0000-0003-2901-5794; Lemos,
   Frederico Gemesio/0000-0002-3027-5713; Peroni,
   Nivaldo/0000-0002-6770-5377; Costa, Alan Nilo/0000-0002-7396-6370;
   Ubaid, Flávio/0000-0001-8604-1206; Michalski,
   Fernanda/0000-0002-8074-9964; de Moura Bubadué,
   Jamile/0000-0001-7069-996X; Prado, Paulo Inácio/0000-0002-7174-5005;
   Moura, Mauricio O/0000-0001-7948-2986; Eizirik,
   Eduardo/0000-0002-9658-0999; Pereira-Ribeiro,
   Juliane/0000-0002-0762-337X; Karlovic, Thamíris C./0000-0001-8409-2958;
   Astúa, Diego/0000-0002-9573-6437; CONTRERAS MORENO, FERNANDO
   MARCOS/0000-0002-5927-4925; Peres, Pedro Henrique/0000-0002-3158-0963;
   Barros, Marilia/0000-0002-3828-6433; Souza, Jesus/0000-0002-8384-3294;
   Cordeiro, Jose/0000-0001-5821-8764; Bowler, Mark/0000-0001-5236-3477;
   Garcia-Olaechea, Alvaro/0000-0002-2288-8923; Paulo da Cunha Araujo,
   Rodrigo/0000-0002-9250-6767; Santos, Paloma Marques/0000-0002-6932-1406;
   Alvarez Solas, Sara/0000-0002-8267-9816; Nobrega,
   Rodrigo/0000-0001-7058-5903; Zuniga Hartley, Alfonso
   Christopher/0000-0002-3302-1268; Trigo, Tatiane/0000-0003-3694-6802;
   Hurtado, Cindy/0000-0002-7958-236X; Oshima, Julia Emi de
   Faria/0000-0003-1545-768X; Galetti, Mauro/0000-0002-8187-8696; Tobler,
   Mathias/0000-0002-8587-0560; Sartor, Caroline/0000-0002-3552-0140;
   Morato, Ronaldo/0000-0002-8304-9779; Lamattina,
   Daniela/0000-0002-5926-8234; Goncalves Bonfim, Fernando
   Cesar/0000-0002-9924-830X; Perez-Torres, Jairo/0000-0001-7121-6210;
   Carramaschi de Alagao Querido, Luciano/0000-0002-7687-8413; Alexandrino,
   Eduardo Roberto/0000-0003-3088-4524; Grotta Neto,
   Francisco/0000-0002-2390-936X; Galiano, Daniel/0000-0003-1428-8634;
   Rodriguez Castro, Karen Giselle/0000-0002-0333-6402; Astete,
   Samuel/0000-0003-0774-9068; Espinosa, Santiago/0000-0002-7416-7167;
   Penuela Mora, Maria Cristina/0000-0002-9611-1359; Assis, Julia
   Camara/0000-0003-1104-7851; Fragoso, Carlos Eduardo/0000-0001-8971-2896;
   Palmeira, Francesca B. L./0000-0002-7597-1157; Reis,
   AlessanRSS/0000-0001-8486-7469; Hawes, Joseph/0000-0003-0053-2018;
   Kasper, Carlos Benhur/0000-0002-7089-6119; Campelo,
   Ana/0000-0002-4493-8434; Bonjorne de Almeida,
   Lilian/0000-0003-2485-5399; Schuchmann, Karl-L/0000-0002-3233-8917;
   Venticinque, Eduardo/0000-0002-3455-9107; Galvao Bastazini, Vinicius
   Augusto/0000-0001-5270-0621; Melo, Fabiano Rodrigues
   de/0000-0001-9958-2036; Endo, Whaldener/0000-0002-7305-4398;
   Guijosa-Guadarrama, Emiliano/0000-0003-2833-4646
FU Agencia Nacional de Promoción Científica y Tecnológica, Argentina
   (ANPCyT)ANPCyT [1908, PICT 2013 #1904, 1905, 1906, 1907] Funding Source:
   Medline; Brazilian National Council for Scientific and Technological
   Development (CNPq)Conselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPQ) [Edital 06/2008 CNPQ Jovens Pesquisadores,
   CNPq/PIBIC, CNPq/PELD/ILTER Site 5, CNPq/PCI #300670/2019-2, CNPq/DCR
   #300461/2016-0, CNPq/Casadinho/PROCAD Project UESC-UFRJ #552198/20,
   CNPq/CAPES/FAPs/PELD #88887.140649/2017-00, CNPq-PROBIO #680037/02-0,
   CNPq for N.C.C. (Research Fellow in Ecology), Brazilian Program for
   Biodiversity Research (PPBio, Bolsa produtividade CNPq P2
   #308503/2014-7, Bolsa produtividade CNPq P2 #308040/2017-1,
   503372/2014-5, 457451/2012-9, 441435/2017-3, 312292/2016-3,
   312045/2013-1, 308385/2014-4, 306700/2015-8, 306695/2015-4,
   303006/2014-5, 216938/2014-7, 168234/2014-9, 141667/2016-8,
   140689/2013-3, 140040/2016-1, 300917/2019-8] Funding Source: Medline;
   Brazilian Program for Biodiversity Research (PPBio) Atlantic Forest
   Network (CNPq)Conselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPQ)Fundacao de Apoio a Pesquisa do Distrito Federal
   (FAPDF) [#457451/2012-9] Funding Source: Medline; Coordenação de
   Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES)Coordenacao
   de Aperfeicoamento de Pessoal de Nivel Superior (CAPES) [Sv875/2017,
   #872, #88881.162169/2017-0, 88882.180491/2018-01, #88881.162169/2017-0,
   #88881.068425/2014-01, BEX 1298/15-1] Funding Source: Medline; Darwin
   Initiative (DEFRA, UK)Department for Environment, Food & Rural Affairs
   (DEFRA) Funding Source: Medline; Deutscher Akademischer Austauschdienst
   (DAAD)Deutscher Akademischer Austausch Dienst (DAAD) Funding Source:
   Medline; EMBRAPAEmpresa Brasileira de Pesquisa Agropecuaria (EMBRAPA)
   Funding Source: Medline; EMBRAPA PantanalEmpresa Brasileira de Pesquisa
   Agropecuaria (EMBRAPA) [#03.09.00.077.00.00] Funding Source: Medline;
   FAPDFFundacao de Apoio a Pesquisa do Distrito Federal (FAPDF) Funding
   Source: Medline; FAPEMAFundacao de Amparo a Pesquisa e Desenvolvimento
   Cientifico do Maranhao (FAPEMA) Funding Source: Medline; FAPEMATFundacao
   de Amparo a Pesquisa do Estado de Mato Grosso (FAPEMAT) Funding Source:
   Medline; FAPEMIG/CNPqConselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPQ)Fundacao de Amparo a Pesquisa do Estado de Minas
   Gerais (FAPEMIG) Funding Source: Medline; FAPERGSFundacao de Amparo a
   Ciencia e Tecnologia do Estado do Rio Grande do Sul (FAPERGS) Funding
   Source: Medline; FAPERJFundacao Carlos Chagas Filho de Amparo a Pesquisa
   do Estado do Rio De Janeiro (FAPERJ) Funding Source: Medline;
   FAPESBFundacao de Amparo a Pesquisa do Estado da Bahia (FAPESB)
   [#2366/2012, 1760/2013] Funding Source: Medline; Foundation for Research
   of the State of Minas Gerais (FAPEMIG)Fundacao de Amparo a Pesquisa do
   Estado de Minas Gerais (FAPEMIG) [CRA APQ 00604-17, APQ-00839-15,
   PPM-00139-14, CRA- RDP-00104-10] Funding Source: Medline;
   FundectFundacao de Apoio ao Desenvolvimento do Ensino Ciencia e
   Tecnologia do Estado de Mato Grosso do Sul (FUNDECT MS) [06/2016]
   Funding Source: Medline; NERC (Natural Environment Research Council,
   UK)UK Research & Innovation (UKRI)Natural Environment Research Council
   (NERC) Funding Source: Medline; NSFNational Science Foundation (NSF)
   [#BCS-0921013] Funding Source: Medline; São Paulo Research Foundation
   (FAPESP) #05/60016-1Fundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP) [2014/23095-0, 2019/04851-1, 2017/21816-0, 2016/11595-3,
   2015/22844-1, 2015/19439-8, 2008/03500-6, 2010/05343-5, 2006/04878-7,
   2005/00405-4, 05/60016-1, 2012/00534-2, 2011/22449-4, 2011/06782-5,
   2013/07162-6, 2015/18381-6, 2014/09300-0, 2013/24453-4, 2015/17739-4,
   2014/23132-2, 2012/14245-2, 2013/04957-8, 2013/50421-2, 2014/01986-0,
   2014/10192-7, 2014/14925-9] Funding Source: Medline; The Gordon and
   Betty Moore FoundationGordon and Betty Moore Foundation Funding Source:
   Medline; USAIDUnited States Agency for International Development (USAID)
   Funding Source: Medline; USFWSUS Fish & Wildlife Service Funding Source:
   Medline; Planta Funding Source: Medline; Votorantim Funding Source:
   Medline; Mohamed bin Zayed Species Conservation Fund [162512917,
   12055114] Funding Source: Medline; Brehm Funds for International Bird
   Conservation (Germany) Funding Source: Medline; Woodland Park Zoo
   Funding Source: Medline; Cambuhy Agrícola Ltda. Funding Source: Medline;
   Conservation Program of Species at Risk (PROCER-CONANP)
   [PROCER/CCER/RFSIPS/04/2016, PROCER/RFSIPS/10/2015,
   PROCER/RFSIPS/04/2015, PROCER/CCER/RFSIPS/14/2016] Funding Source:
   Medline; People&apos;s Trust for Endangered Species (PTES) Funding
   Source: Medline; Zoological Society of San Diego Funding Source:
   Medline; Overbrook Funding Source: Medline; Lion Tamarin of Brazil Fund
   (LTBF) Funding Source: Medline; Reserva Ecológica Michelin - Bahia -
   Brazil Funding Source: Medline; Consejo Nacional de Investigaciones
   Científicas y Técnicas, Argentina (CONICET) [Project UE IBS
   #22920160100130CO, PIP 2012-2014 #112-201101-00616] Funding Source:
   Medline; Global Ecotours &amp; Expeditions volunteers Funding Source:
   Medline; UFG Funding Source: Medline; Sidney José Damiani (Área
   Particular de Preservação Ambiental São Francisco) Funding Source:
   Medline; Fundação Estadual do Meio Ambiente e Recursos Hídricos de
   Roraima Funding Source: Medline; CHTP Funding Source: Medline; VILLUM
   Investigator project &quot;Biodiversity Dynamics in a Changing
   World&quot; funded by VILLUM FONDEN Funding Source: Medline; Panthera
   Foundation Funding Source: Medline; PAP-UDESC/FAPESC [#2017TR744]
   Funding Source: Medline; CASEST Funding Source: Medline; FCT/MEC CESAM
   [(UID/AMB/50017)] Funding Source: Medline; Funape Funding Source:
   Medline; University of Aveiro - Portugal Funding Source: Medline;
   Primate Conservation Inc. [(Project 1158)] Funding Source: Medline;
   Ministerio de Agroindustria Funding Source: Medline; Fundação para o
   Desenvolvimento Sustentável da Terra Potiguar-FUNDEP Funding Source:
   Medline; World Wildlife Fund (WWF) Funding Source: Medline; Departamento
   de Recursos Naturales - Universidad Nacional de La Pampa [PI #R018]
   Funding Source: Medline; Norte Energia Funding Source: Medline; Programa
   Áreas Protegidas da Amazônia (ARPA) Funding Source: Medline; Wildlife
   Conservation Society (WCS) Funding Source: Medline; TFCA/Funbio Funding
   Source: Medline; University of Wisconsin-Madison Funding Source:
   Medline; IFRJ Funding Source: Medline; Earthwatch Institute Funding
   Source: Medline; Iunes Habib Funding Source: Medline; Resource Award
   Funding Source: Medline; IBAMA Funding Source: Medline; The Rufford
   Foundation [11495-1, 20950-1] Funding Source: Medline; Comisión Nacional
   de Áreas Naturales Protegidas Área de Protección de Flora y Fauna
   Funding Source: Medline; Ministry of Culture and Science of North
   Rhine-Westphalia Funding Source: Medline; Fundação Grupo Boticário de
   Proteção à Natureza [0522-2012, 1037-20151, 0939-20121] Funding Source:
   Medline; CI-Brazil Funding Source: Medline; Heinrich Hertz-Foundation
   Funding Source: Medline; Consórcio Capim Branco Energia (CCBE) Funding
   Source: Medline; Wild Felid Legacy Scholarship Funding Source: Medline;
   Alexander Koenig Society (Bonn, Germany) Funding Source: Medline;
   CONACYT through the Programa PROCIENCIA with resources from the Fondo
   para la Excelencia de la Educacion e Investigacion (FEEI) Funding
   Source: Medline; Fundação de Amparo à Pesquisa do Estado do Pará
   (FAPESPA) [ICAAF #018/2016] Funding Source: Medline; Programa de
   Pesquisa em Biodiversidade-PPBio Rede BioM.A. Funding Source: Medline;
   Secretaría de Ciencia y Técnica-Facultad de Química Bioquímica y
   Farmacia-Universidad Nacional de San Luis Funding Source: Medline; Rede
   Sisbiota-ComCerrado [#107-CAP-2011] Funding Source: Medline; Politrade
   Funding Source: Medline; Júnior Santos (Instituto Felinos do Aguaí)
   Funding Source: Medline; Belmond Hotel Funding Source: Medline; Wildlife
   Research Funding Source: Medline; Région Pays de la Loire Funding
   Source: Medline; Laguna de Términos Funding Source: Medline; Zoological
   Society of London Funding Source: Medline; Carlsberg Foundation Semper
   Ardens project MegaPast2Future [(CF16-0005)] Funding Source: Medline;
   VRAC/PUC-Rio Funding Source: Medline; Pró-Reitorias de Pesquisa e
   Extensão da UEMG by the scholarship PIBIC and PAEx Funding Source:
   Medline; Agencia Nacional de Promoción Científica y Tecnológica de
   Argentina [PICT-2010-1256] Funding Source: Medline; UCAR (Unidad para el
   Cambio Rural), Ministerio de Agroindustria, Argentina through PIA 2011
   [10106, 10103, 10102, 10104, 10105] Funding Source: Medline; FAEP
   Funding Source: Medline; International Paper Co. of Brazil Funding
   Source: Medline; Chinese Academy of Sciences President&apos;s
   International Fellowship Initiative [(Grant #2018PB0040)] Funding
   Source: Medline; Primate Action Fund [(Project 1001257)] Funding Source:
   Medline; Pró-Reitorias de Pesquisa e Extensão da UEMG and FAPEMIG by the
   scholarship PIBIC Funding Source: Medline; Disney World Conservation
   Fund Funding Source: Medline; Neotropical Grassland Conservancy (NGC)
   Funding Source: Medline; FAPES (BPIG/I Biologia da Conservação) Funding
   Source: Medline; Tropical Conservation and Development Program Funding
   Source: Medline; Oswaldo Cruz Foundation Funding Source: Medline;
   ZGAP/SPS/FbP Funding Source: Medline; PROPe UNESP Funding Source:
   Medline; Superintendência da Zona Franca de Manaus-Coordenação Regional
   de Rio Branco Funding Source: Medline; Conservation Leadership Programme
   [(Project #02224115)] Funding Source: Medline; Fundação de Amparo à
   Ciência e Tecnologia do Estado de Pernambuco (FACEPE)
   [#BCT-0025-2.05/17] Funding Source: Medline; Consejo Nacional de Ciencia
   y Tecnología de México Funding Source: Medline; DIBIO/ICMBio [(Projeto
   PEM 011.034)] Funding Source: Medline; von Humboldt Foundation/CAPES
   [#88881.162169/2017-01] Funding Source: Medline; Fundação Mary Brown
   Funding Source: Medline; Biofaces Funding Source: Medline; Instituto
   Arapyaú Funding Source: Medline; Comisión Nacional de Áreas Naturales
   Protegidas Reserva de la Biosfera Pantanos de Centla Funding Source:
   Medline; Universidad Autónoma del Estado de México Funding Source:
   Medline; Fundação Monsanto Funding Source: Medline; Liz Claiborne &amp;
   Art Ortenberg Foundation Funding Source: Medline; Reitoria USP Funding
   Source: Medline; Biota - Projetos e Consultoria Ambiental Funding
   Source: Medline; IEMA [#9003/2014] Funding Source: Medline; Elguero Farm
   Funding Source: Medline; Programa de las Naciones Unidas para el
   Desarrollo (PNUD)/Comisión Nacional Forestal (CONAFOR) Funding Source:
   Medline; Parrot Wildlife Foundation Funding Source: Medline; Legado das
   Águas Funding Source: Medline; International Association for Bear
   Research and Management - IBA Funding Source: Medline; Idea Wild Funding
   Source: Medline; PROBIO/MMA Funding Source: Medline; Fibria Celulose
   S.A. Funding Source: Medline; CEMIG Funding Source: Medline; San Diego
   Zoo Global Funding Source: Medline; The Social Sciences and Humanities
   Research Council (SSHRC, Canadá) Funding Source: Medline; EW volunteers
   Funding Source: Medline; MRN - Mineração Rio do Norte Funding Source:
   Medline; Landowners that have participated in the research Funding
   Source: Medline; Concessionária Auto Raposo Tavares (CART) Funding
   Source: Medline; Ecopetrol and Fundación Marío Santo Domingo Funding
   Source: Medline
NR 0
TC 7
Z9 7
U1 7
U2 18
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0012-9658
EI 1939-9170
J9 ECOLOGY
JI Ecology
PD NOV
PY 2020
VL 101
IS 11
AR e03128
DI 10.1002/ecy.3128
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA OP9CN
UT WOS:000588387700012
PM 32862433
OA Bronze, Green Published
DA 2022-02-10
ER

PT C
AU Goorts, P
   Maesen, S
   Liu, Y
   Dumont, M
   Bekaert, P
   Lafruit, G
AF Goorts, Patrik
   Maesen, Steven
   Liu, Yunjun
   Dumont, Maarten
   Bekaert, Philippe
   Lafruit, Gauthier
GP IEEE
TI Self-calibration of Large Scale Camera Networks
SO 2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA
   APPLICATIONS (SIGMAP)
LA English
DT Proceedings Paper
CT 11th International Conference on Signal Processing and Multimedia
   Applications (SIGMAP) is part of 11th International Joint Conference on
   E-Business and Telecommunications (ICETE)
CY AUG 28-30, 2014
CL Vienna, AUSTRIA
SP Inst Syst & Technologies Informat, Control & Commun, Austrian Comp Soc, Vienna Univ Technol, ACM Special Interest Grp Design Commun, ACM Special Interest Grp Multimedia, Workflow Management Coalit, Object Management Grp, Fdn Intelligent Phys Agents
DE Calibration; Feature Matching; Multicamera Matches; Outlier Filtering
ID VIDEO
AB In this paper, we present a method to calibrate large scale camera networks for multi-camera computer vision applications in sport scenes. The calibration process determines precise camera parameters, both within each camera (focal length, principal point, etc) and inbetween the cameras (their relative position and orientation). To this end, we first extract candidate image correspondences over adjacent cameras, without using any calibration object, solely relying on existing feature matching computer vision algorithms applied on the input video streams. We then pairwise propagate these camera feature matches over all adjacent cameras using a chained, confident-based voting mechanism and a selection relying on the general displacement across the images. Experiments show that this removes a large amount of outliers before using existing calibration toolboxes dedicated to small scale camera networks, that would otherwise fail to work properly in finding the correct camera parameters over large scale camera networks. We succesfully validate our method on real soccer scenes.
C1 [Goorts, Patrik; Maesen, Steven; Liu, Yunjun; Dumont, Maarten; Bekaert, Philippe; Lafruit, Gauthier] Hasselt Univ, tUL, iMinds, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
RP Goorts, P (corresponding author), Hasselt Univ, tUL, iMinds, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
EM patrik.goorts@uhasselt.be; steven.maesen@uhasselt.be;
   yunjun.liu@uhasselt.be; maarten.dumont@uhasselt.be;
   philippe.bekaert@uhasselt.be; gauthier.lafruit@uhasselt.be
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Doshi A., 2010, J VIRTUAL REALITY BR, V7
   Farin D., 2005, 2005 IEEE International Conference on Multimedia and Expo
   Farin D, 2004, ELECT IMAGING, V2003, P80
   Goorts P, 2013, P 8 INT C COMP VIS T
   Goorts P., 2014, P 9 INT C COMP VIS T
   Grau O., 2005, P WIAMIS
   Hartley R., 2003, MULTIPLE VIEW GEOMET, V2
   Hayet JB, 2005, IEEE IMAGE PROC, P3017
   Li Qihe, 2004, P INT C COMP INT IST, P482
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ohta Y, 2007, INT J COMPUT VISION, V75, P173, DOI 10.1007/s11263-006-0030-z
   Svoboda T, 2005, PRESENCE-VIRTUAL AUG, V14, P407, DOI 10.1162/105474605774785325
   Thomas G, 2007, J REAL-TIME IMAGE PR, V2, P117, DOI 10.1007/s11554-007-0041-1
   Triggs Bill, 1999, P 1999 INT WORKSH VI, P298, DOI DOI 10.1007/3-540-44480-7_21
   Yang RG, 2003, COMPUT GRAPH FORUM, V22, P207, DOI 10.1111/1467-8659.00661
   Yu XG, 2009, COMPUT VIS IMAGE UND, V113, P643, DOI 10.1016/j.cviu.2008.01.006
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 18
TC 1
Z9 1
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-989-8565-96-9
PY 2014
BP 107
EP 116
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BI4IJ
UT WOS:000411790800019
DA 2022-02-10
ER

PT J
AU Bi, QL
   Liu, ZJ
   Wang, MH
   Lai, ML
   Xiao, LM
   Yan, YP
   Liu, XG
AF Bi, Qilin
   Liu, Zhijun
   Wang, Miaohui
   Lai, Minling
   Xiao, Leming
   Yan, Yipu
   Liu, Xiaoguang
TI An automatic camera calibration method based on checkerboard
SO TRAITEMENT DU SIGNAL
LA English
DT Article
DE computer vision; camera calibration; checkerboard; corner recognition;
   corner matching
AB The traditional camera calibration methods faces many problems, such as the need for manual operation and high-quality images as well as the heavy time consumption. To solve these problems, this paper puts forward an adaptive extraction and matching algorithm for checkerboard inner-corners for camera calibration. Firstly, the coordinates of all corner points of the checkerboard were derived by the Harris algorithm. Then, the four vertices of the checkerboard were acquired in the image coordinate system based on polygonal convexity. After that, the coordinates of the inner-corner points of the checkerboard image were obtained against the judgement rules that distinguish inner-corner points from other points on that image. On this basis, the matching relationship was established between the inner-corner points of the checkerboard image in the image coordinate system and those in the checkerboard coordinate system. Finally, the theoretical modelling, judgement rules and a mature camera calibration model were integrated for automatic camera calibration experiments. The results show that the automatic camera calibration method based on the proposed algorithm consumed 75% less time than the Matlab toolbox and controlled the error within 0.3 pixels. This research provides a real-time, robust and accurate automatic camera calibration method for engineering applications.
C1 [Bi, Qilin; Liu, Zhijun; Lai, Minling; Xiao, Leming; Yan, Yipu] Guangzhou Maritime Univ, Guangzhou 510725, Guangdong, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Natl Engn Lab Big Data Syst Comp Technol, Shenzhen 518060, Peoples R China.
   [Liu, Xiaoguang] Guangdong Inst Intelligent Mfg, Guangzhou 510070, Guangdong, Peoples R China.
RP Liu, ZJ (corresponding author), Guangzhou Maritime Univ, Guangzhou 510725, Guangdong, Peoples R China.
EM hbbql@163.com
FU National Natural Science Foundation of Guangdong ProvinceNational
   Natural Science Foundation of Guangdong Province [2016A030310309];
   Guangdong Province Science and Technology Project [2017A010102009,
   20178010118004]; Guangzhou City Science and Technology Project
   [201804010354, 201707010187]; Guangdong Applied Science and Technology
   Research and Development Special Fund Project [20168020243012];
   Innovation and Entrepreneurship Education Project in Colleges and
   Universities in Guangzhou [201709P09]; Guangdong Provincial Department
   of Transportation Science and Technology Project [2017-02-025]
FX Fund project: National Natural Science Foundation of Guangdong Province
   (2016A030310309), Guangdong Province Science and Technology Project
   (2017A010102009, 20178010118004), Guangzhou City Science and Technology
   Project (201804010354, 201707010187), Guangdong Applied Science and
   Technology Research and Development Special Fund Project
   (20168020243012), Innovation and Entrepreneurship Education Project in
   Colleges and Universities in Guangzhou (201709P09).Guangdong Provincial
   Department of Transportation Science and Technology
   Project(Technology-2017-02-025).
CR Baataoui A, 2016, ROBUST METHOD CAMERA
   Bushnevskiy A, 2016, IEEE IMAGE PROC, P1165, DOI 10.1109/ICIP.2016.7532541
   ENGELENLEE J, 2017, IEEE T PATTERN ANAL, V18, P210, DOI DOI 10.1080/21678421.2016.1245757
   Geiger A, 2012, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2012.6224570
   Hou ZJ, 2012, INFORMATION-TOKYO, V15, P4393
   [黄风山 HUANG Fengshan], 2006, [光电子·激光, Journal of Optoelectronics·Laser], V17, P705
   Jin J, 2013, J OPT SOC AM A, V30, P287, DOI 10.1364/JOSAA.30.000287
   Kruger L, 2011, PATTERN RECOGN LETT, V32, P1428, DOI 10.1016/j.patrec.2011.04.002
   Leal-Taixe L, 2012, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2012.6247901
   Li Hai, 2015, Optics and Precision Engineering, V23, P3480, DOI 10.3788/OPE.20152312.3480
   Liu Z, 2013, ACTA OPT SINICA, V33
   Shan B H, 2016, ACTA OPTICA SINICA, V36
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Stephens M, 1988, COMBINED CORNER EDGE, V15, P10, DOI DOI 10.5244/C.2.23
   Wei G.-Q., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P133, DOI 10.1109/CVPR.1991.139675
   Yang Xingfang, 2010, China Mechanical Engineering, V21, P2541
   Zhang YJ, 2014, OPT ENG, V53, DOI 10.1117/1.OE.53.11.112203
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 18
TC 1
Z9 1
U1 0
U2 7
PU PRESSES UNIV GRENOBLE
PI GRENOBLE
PA 1041 RUE DES RESIDENCES, GRENOBLE, 38040, FRANCE
SN 0765-0019
J9 TRAIT SIGNAL
JI Trait. Signal
PY 2017
VL 34
IS 3-4
BP 209
EP 226
DI 10.3166/TS.34.209-226
PG 18
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA VI3DA
UT WOS:000469311500007
DA 2022-02-10
ER

PT J
AU McCarthy, MS
   Despres-Einspenner, ML
   Farine, DR
   Samuni, L
   Angedakin, S
   Arandjelovic, M
   Boesch, C
   Dieguez, P
   Havercamp, K
   Knight, A
   Langergraber, KE
   Kuhl, HS
AF McCarthy, Maureen S.
   Despres-Einspenner, Marie-Lyne
   Farine, Damien R.
   Samuni, Liran
   Angedakin, Samuel
   Arandjelovic, Mimi
   Boesch, Christophe
   Dieguez, Paula
   Havercamp, Kristin
   Knight, Alex
   Langergraber, Kevin E.
   Kuehl, Hjalmar S.
TI Camera Traps Offer a Robust Means for Social Network Analysis in Wild
   Chimpanzees
SO FOLIA PRIMATOLOGICA
LA English
DT Meeting Abstract
C1 [McCarthy, Maureen S.; Despres-Einspenner, Marie-Lyne; Samuni, Liran; Angedakin, Samuel; Arandjelovic, Mimi; Boesch, Christophe; Dieguez, Paula; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Dept Primatol, Leipzig, Germany.
   [Farine, Damien R.] Max Planck Inst Ornithol, Dept Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Konstanz, Dept Biol, Chair Biodivers & Collect Behav, Constance, Germany.
   [Farine, Damien R.] Univ Oxford, Dept Zool, Edward Grey Inst Field Ornithol, Oxford, England.
   [Samuni, Liran] CSRS, Tai Chimpanzee Project, Abidjan, Cote Ivoire.
   [Havercamp, Kristin] Kyoto Univ, Wildlife Res Ctr, Kyoto, Japan.
   [Knight, Alex] Univ Auckland, Sch Biol Sci, Auckland, New Zealand.
   [Langergraber, Kevin E.] Arizona State Univ, Sch Human Evolut & Social Change, Tempe, AZ USA.
   [Langergraber, Kevin E.] Arizona State Univ, Inst Human Origins, Tempe, AZ USA.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res iDiv, Halle, Germany.
EM maureen_mc@eva.mpg.de
RI Farine, Damien R./Y-2454-2019
OI Farine, Damien R./0000-0003-2208-7613
NR 0
TC 0
Z9 0
U1 0
U2 2
PU KARGER
PI BASEL
PA ALLSCHWILERSTRASSE 10, CH-4009 BASEL, SWITZERLAND
SN 0015-5713
EI 1421-9980
J9 FOLIA PRIMATOL
JI Folia Primatol.
PD MAY
PY 2020
VL 91
IS 3
BP 347
EP 347
PG 1
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA LU6HU
UT WOS:000537855200173
DA 2022-02-10
ER

PT J
AU Marcon, A
   Bongi, P
   Battocchio, D
   Apollonio, M
AF Marcon, Andrea
   Bongi, Paolo
   Battocchio, Daniele
   Apollonio, Marco
TI REM: performance on a high-density fallow deer (Dama dama) population
SO MAMMAL RESEARCH
LA English
DT Article
DE Field test; Fenced area; Total count; Known density; Camera trapping
ID ESTIMATING ANIMAL DENSITY; CAMERA TRAPS; ACTIVITY PATTERNS; DISTANCE;
   DEFENSE
AB We present an application of the random encounter model (REM) to estimate population density of a closed population of fallow deer (Dama dama). REM promises to be a powerful tool for providing density estimates for species which are not individually recognisable, but it still requires thorough testing to assess its limits and performance. In our study area, effective fencing prevents animals from migrating in or out of the area thus assuring a closed population; still the application of REM presented several challenges, including a very high density (more than 600 ind/km(2)) and a constant fission/fusion of groups of the study species. We applied both stratified and unstratified analysis approach, and we estimated density over a range of daily range values. The REM approach underestimates the true density value of about 31% and 28% the CV of the estimates was 0.39 and 0.52, for stratified and unstratified approach, respectively. Still it provides evidence in support of REM as a method for providing density estimates of free-ranging unmarked individuals with a fission-fusion social structure.
C1 [Marcon, Andrea; Battocchio, Daniele; Apollonio, Marco] Univ Sassari, Dept Vet Med, I-07100 Sassari, Italy.
   [Marcon, Andrea] Via Cavin Sala 39-A, I-30036 Santa Maria Di Sala, VE, Italy.
   [Bongi, Paolo] Pzza Don S Venturini 3 Olivola, I-54010 Aulla, Massa, Italy.
RP Marcon, A (corresponding author), Univ Sassari, Dept Vet Med, I-07100 Sassari, Italy.; Marcon, A (corresponding author), Via Cavin Sala 39-A, I-30036 Santa Maria Di Sala, VE, Italy.
EM amarcon.work@gmail.com
OI Marcon, Andrea/0000-0003-3434-1279
FU Italian Navy, La Spezia North Command
FX We are thankful to the Italian Navy, La Spezia North Command, and the
   personnel of the branch office Filatteria Communication Centre,
   Massa-Carrara, Italy, for letting us perform the field work inside the
   Navy Base. We would like to thank Milena Baruffetti and Martina Marin
   for their help with data collection.
CR ALVAREZ F, 1990, J MAMMAL, V71, P692, DOI 10.2307/1381810
   APOLLONIO M, 1989, BEHAV ECOL SOCIOBIOL, V25, P89, DOI 10.1007/BF00302925
   Berzi D., 2010, P INT C WOLV PEOPL T
   Campos-Candela A, 2018, J ANIM ECOL, V87, P825, DOI 10.1111/1365-2656.12787
   Caravaggi A, 2016, REMOTE SENS ECOL CON, V2, P45, DOI 10.1002/rse2.11
   Chandler RB, 2013, ANN APPL STAT, V7, P936, DOI 10.1214/12-AOAS610
   Chauvenet ALM, 2017, ECOL MODEL, V350, P79, DOI 10.1016/j.ecolmodel.2017.02.007
   Ciuti S, 2008, BEHAV ECOL SOCIOBIOL, V62, P1747, DOI 10.1007/s00265-008-0603-7
   CLUTTONBROCK TH, 1988, BEHAV ECOL SOCIOBIOL, V23, P281, DOI 10.1007/BF00300575
   Core Team R., 2020, R LANG ENV STAT COMP
   Cusack JJ, 2015, J WILDLIFE MANAGE, V79, P1014, DOI 10.1002/jwmg.902
   Farina A, 1980, LUNIGIANA AMBIENTE S, P103
   Goswami VR, 2012, ANIM CONSERV, V15, P174, DOI 10.1111/j.1469-1795.2011.00501.x
   Hofmeester TR, 2017, REMOTE SENS ECOL CON, V3, P81, DOI 10.1002/rse2.25
   Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
   Karanth KU, 2006, ECOLOGY, V87, P2925, DOI 10.1890/0012-9658(2006)87[2925:ATPDUP]2.0.CO;2
   Karanth KU, 2004, ANIM CONSERV, V7, P285, DOI 10.1017/S1367943004001477
   Manzo E, 2012, ACTA THERIOL, V57, P165, DOI 10.1007/s13364-011-0055-8
   Marcon A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0222349
   Mayle B.A, 1999, MANY DEER FIELD GUID
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Meek PD, 2012, AUST MAMMAL, V34, P223, DOI 10.1071/AM11032
   Moeller AK, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2331
   Morellet N, 2007, J APPL ECOL, V44, P634, DOI 10.1111/j.1365-2664.2007.01307.x
   Nakashima Y, 2018, J APPL ECOL, V55, P735, DOI 10.1111/1365-2664.13059
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4_1
   Oliveira-Santos LGR, 2008, J TROP ECOL, V24, P563, DOI 10.1017/S0266467408005324
   Carbajal-Borges JP, 2014, TROP CONSERV SCI, V7, P100, DOI 10.1177/194008291400700102
   Rademaker M, 2017, J BIODIVERS ENDANGER, V5, P200, DOI [10.4172/2332-2543.1000200, DOI 10.4172/2332-2543.1000200]
   Ramsey David S.L., 2015, Journal of Wildlife Management, V79, P491
   Rovero F, 2009, J APPL ECOL, V46, P1011, DOI 10.1111/j.1365-2664.2009.01705.x
   Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
   Rowcliffe JM, 2011, METHODS ECOL EVOL, V2, P464, DOI 10.1111/j.2041-210X.2011.00094.x
   Seber G. A., 1982, ESTIMATION ANIMAL AB
   Zero VH, 2013, ORYX, V47, P410, DOI 10.1017/S0030605312000324
NR 35
TC 1
Z9 1
U1 0
U2 12
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2199-2401
EI 2199-241X
J9 MAMMAL RES
JI Mammal Res.
PD OCT
PY 2020
VL 65
IS 4
BP 835
EP 841
DI 10.1007/s13364-020-00522-x
EA JUL 2020
PG 7
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA NO3BE
UT WOS:000552760000001
OA Bronze
DA 2022-02-10
ER

PT J
AU Danaher, MW
   Ward, C
   Zettler, LW
   Covell, CV
AF Danaher, Mark W.
   Ward, Carlton, Jr.
   Zettler, Lawrence W.
   Covell, Charles V., Jr.
TI Pollinia removal and suspected pollination of the endangered ghost
   orchid, Dendrophylax lindenii (Orchidaceae) by various hawk moths
   (Lepidoptera: Sphingidae): another mystery dispelled
SO FLORIDA ENTOMOLOGIST
LA English
DT Article
DE Florida Panther National Wildlife Refuge; Fakahatchee Strand;
   conservation
AB The ghost orchid, Dendrophylax lindenii (Lindl.) Bentham ex Rolfe (Orchidaceae), is a rare, leafless epiphyte restricted to forests in southernmost Florida and western Cuba. The species' appealing floral display, high public profile, and challenging cultivation contribute to its ongoing removal from the wild by unethical collectors. To effectively conserve this and other native orchids that rely on seed for reproduction, a thorough understanding of natural pollination mechanisms is essential. Digital single lens reflex camera traps were used to survey for potential pollinators visiting D. lindenii flowers on the Florida Panther National Wildlife Refuge during the summers of 2016 to 2018. Based on suspected D. lindenii pollinia affixed to photographed moths, we provide visual evidence that D. lindenii is pollinated by at least 2 large hawk moths (Sphingidae) in southern Florida, which include the fig sphinx moth, Pachylia ficus Linnaeus, and pawpaw sphinx moth, Dolba hyloeus Drury (both Lepidoptera: Sphingidae). Species that were documented probing D. lindenii flowers, but lacked pollinia, included the giant sphinx moth (Cocytius antaeus Drury), banded sphinx moth (Eumorpha fasciatus Sulzer), and streaked sphinx moth (Protambulyx strigilis Linnaeus) (all Lepidoptera: Sphingidae). In addition to the aforementioned species of hawk moths (sphinx moths), the seagrape spanworm moth (Ametris nitocris Cramer; Lepidoptera: Geometridae), palamedes swallowtail (Papilio palamedes Drury; Lepidoptera: Papilionidae), monk skipper (Asbolis capucinus Lucas; Lepidoptera: Hesperiidae), Brazilian skipper (Calpodes ethlius Stoll; Lepidoptera: Hesperiidae), and 3 unidentifiable geometrid moths were observed visiting D. lindenii flowers within the study area. During 2017 and 2018, a total of 21 different visits by Lepidoptera were recorded, and the duration of each visit was rarely longer than 1 s. Hawk moth visits were infrequent, but did show some evidence of clustering by species. Measurements of proboscis lengths of the 2 documented pollinators from museum specimens were of sufficient length (50-100 mm) to probe D. lindenii nectar spurs, further lending support to our field observations. Larval food sources of the 2 confirmed pollinators include plant species native to southern Florida, suggesting that these moths are natural pollinators of D. lindenii. Our findings, although preliminary, provide critically needed baseline information that will augment ongoing conservation efforts in southern Florida aimed at the recovery of D. lindenii.
C1 [Danaher, Mark W.] US Fish & Wildlife Serv, Florida Panther Natl Wildlife Refuge, 12085 SR 29 South, Immokalee, FL 34142 USA.
   [Ward, Carlton, Jr.] Florida Wild, 520 East Davis Blvd, Tampa, FL 33606 USA.
   [Zettler, Lawrence W.] Illinois Coll, Dept Biol, 1101 West Coll Ave, Jacksonville, IL 62650 USA.
   [Covell, Charles V., Jr.] Univ Florida, McGuire Ctr Lepidoptera & Biodivers, Florida Museum Nat Hist, Gainesville, FL 32611 USA.
RP Danaher, MW (corresponding author), US Fish & Wildlife Serv, Florida Panther Natl Wildlife Refuge, 12085 SR 29 South, Immokalee, FL 34142 USA.
EM mark_danaher@fws.gov; carltonward@floridawild.com; lwzettle@ic.edu;
   ccovell@flmnh.ufl.edu
CR Brown P.M., 2005, WILD ORCHIDS FLORIDA
   Coile NC, 2003, CONTRIBUTION N DAKOT
   Coopman J, 2019, NATIVE PLANTS J, V19, P100
   Correll DS., 1950, NATIVE ORCHIDS N AM
   Covell Jr CV, 1984, PETERSON FIELD GUIDE
   Dressler R. L, 1981, ORCHIDS NATURAL HIST
   FDEP-Florida Department of Environmental Protection, 2014, FAK STRAND STAT PRES
   Hammer RL., 2002, EVERGLADES WILDFLOWE
   Hoang NH, 2016, ANN BOT, V119, P379
   Langdon KR., 1979, 56 FLOR DEP AGR CONS
   Lind Henirk, 1994, Svensk Botanisk Tidskrift, V88, P185
   Luer C. A., 1972, NATIVE ORCHIDS FLORI
   Mujica EB, 2018, BOT J LINN SOC, V186, P572, DOI 10.1093/botlinnean/box106
   Pailler T., 2019, 7 INT ORCH CONS C 28
   Reese RS., 2010, 20101270 US GEOL SUR
   SADLER JJ, 2011, EUR J ENVIRON SCI, V1, P137
   Sheehan T, 1979, ORCHID GENERA ILLUST
   Sonenshein RS., 2008, 2008 GREAT EV EC RES
   Stewart J., 2006, ANGRAECOID ORCHIDS S
   Stewart S. L., 2008, N AM NATIV ORCHID J, V14, P70
   Swarts ND, 2009, ANN BOT-LONDON, V104, P543, DOI 10.1093/aob/mcp025
   Tuttle J. P., 2007, HAWK MOTHS N AM
   VANDERCINGEL NA, 2007, ORCHID BIOL REV PERS, P201
   Wiegand T, 2013, BIOTROPICA, V45, P441, DOI 10.1111/btp.12025
   Zettler JA, 2012, SOUTHEAST NAT, V11, P127, DOI 10.1656/058.011.0112
   Zettler L.W., 2019, P 22 WORLD ORCH C GU, V2, P136
NR 26
TC 3
Z9 3
U1 9
U2 23
PU FLORIDA ENTOMOLOGICAL SOC
PI LUTZ
PA 16125 E LAKE BURRELL DR, LUTZ, FL 33548 USA
SN 0015-4040
EI 1938-5102
J9 FLA ENTOMOL
JI Fla. Entomol.
PD DEC
PY 2019
VL 102
IS 4
BP 671
EP 683
DI 10.1653/024.102.0401
PG 13
WC Entomology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Entomology
GA KF4ND
UT WOS:000509220000001
OA gold
DA 2022-02-10
ER

PT C
AU Goorts, P
   Maesen, S
   Liu, YJ
   Dumont, M
   Bekaert, P
   Lafruit, G
AF Goorts, Patrik
   Maesen, Steven
   Liu, Yunjun
   Dumont, Maarten
   Bekaert, Philippe
   Lafruit, Gauthier
BE Obaidat, MS
   Holzinger, A
   Filipe, J
TI Automatic Calibration of Soccer Scenes Using Feature Detection
SO E-BUSINESS AND TELECOMMUNICATIONS, ICETE 2014
SE Communications in Computer and Information Science
LA English
DT Proceedings Paper
CT 11th International Joint Conference on E-Business and Telecommunications
   (ICETE)
CY AUG 28-30, 2014
CL Vienna, AUSTRIA
DE Calibration; Soccer; Feature detection; Feature matching filtering;
   Outlier detection; Multicamera feature detection
ID CONSENSUS; VIDEO
AB In this paper, we present a method to calibrate large scale camera networks for multi-camera computer vision applications in soccer scenes. The calibration process determines camera parameters, both within each camera (focal length, principal point, etc.) and inbetween the cameras (their relative position and orientation). We first extract candidate image correspondences over adjacent cameras, without using any calibration object, relying on existing feature matching methods. We then combine these pairwise camera feature matches over all adjacent cameras using a confident-based voting mechanism and a selection relying on the general displacement across the images. Experiments show that this removes a large amount of outliers before using existing calibration toolboxes dedicated to small scale camera networks, that would otherwise fail to work properly in finding the correct camera parameters over large scale camera networks. We succesfully validate our method on real soccer scenes.
C1 [Goorts, Patrik; Maesen, Steven; Liu, Yunjun; Dumont, Maarten; Bekaert, Philippe; Lafruit, Gauthier] Hasselt Univ, tUL, iMinds, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
RP Goorts, P (corresponding author), Hasselt Univ, tUL, iMinds, Expertise Ctr Digital Media, Wetenschapspk 2, B-3590 Diepenbeek, Belgium.
EM patrik.goorts@uhasselt.be; steven.maesen@uhasselt.be;
   yunjun.liu@uhasselt.be; maarten.dumont@uhasselt.be;
   philippe.bekaert@uhasselt.be; gauthier.lafruit@uhasselt.be
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Farin D., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5307, P80, DOI 10.1117/12.526813
   Farin D., 2005, P SPIE INT SOC OPTIC, P1
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Goorts Patrik, 2013, Proceedings of the 8th International Conference on Computer Vision Theory and Applications. VISAPP 2013, P131
   Goorts P., 2014, P 9 INT C COMP VIS T
   Grau O., 2005, P 6 INT WORKSH IM AN
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hayet JB, 2005, IEEE IMAGE PROC, P3017
   Li Qihe, 2004, P INT C COMP INT IST, P482
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ohta Y, 2007, INT J COMPUT VISION, V75, P173, DOI 10.1007/s11263-006-0030-z
   Svoboda T, 2005, PRESENCE-VIRTUAL AUG, V14, P407, DOI 10.1162/105474605774785325
   Thomas G, 2007, J REAL-TIME IMAGE PR, V2, P117, DOI 10.1007/s11554-007-0041-1
   Triggs Bill, 1999, P 1999 INT WORKSH VI, P298, DOI DOI 10.1007/3-540-44480-7_21
   Yang RG, 2003, COMPUT GRAPH FORUM, V22, P207, DOI 10.1111/1467-8659.00661
   Yu XG, 2009, COMPUT VIS IMAGE UND, V113, P643, DOI 10.1016/j.cviu.2008.01.006
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 18
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1865-0929
EI 1865-0937
BN 978-3-319-25915-4; 978-3-319-25914-7
J9 COMM COM INF SC
PY 2015
VL 554
BP 418
EP 434
DI 10.1007/978-3-319-25915-4_22
PG 17
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BE2HX
UT WOS:000369316300022
DA 2022-02-10
ER

PT C
AU Wadenback, M
   Karlsson, M
   Heyden, A
   Robertsson, A
   Johansson, R
AF Wadenback, Marten
   Karlsson, Martin
   Heyden, Anders
   Robertsson, Anders
   Johansson, Rolf
BE Imai, F
   Tremeau, A
   Braz, J
TI Visual Odometry from Two Point Correspondences and Initial Automatic
   Camera Tilt Calibration
SO PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER
   VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP
   2017), VOL 6
LA English
DT Proceedings Paper
CT 12th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP)
CY FEB 27-MAR 01, 2017
CL Porto, PORTUGAL
SP Inst Syst & Technologies Informat, Control & Commun, ACM SIGGRAPH, AFIG, Eurographics
DE Visual Odometry; Tilted Camera; Trajectory Recovery
ID INTEGRATION; MOTION
AB Ego-motion estimation is an important step towards fully autonomous mobile robots. In this paper we propose the use of an initial but automatic camera tilt calibration, which transforms the subsequent motion estimation to a 2D rigid body motion problem. This transformed problem is solved '2-optimally using RANSAC and a two-point method for rigid body motion. The method is experimentally evaluated using a camera mounted onto a mobile platform. The results are compared to measurements from a highly accurate external camera positioning system which are used as gold standard. The experiments show promising results on real data.
C1 [Wadenback, Marten; Heyden, Anders] Lund Univ, Ctr Math Sci, Lund, Sweden.
   [Karlsson, Martin; Robertsson, Anders; Johansson, Rolf] Lund Univ, Dept Automat Control, Lund, Sweden.
RP Wadenback, M (corresponding author), Lund Univ, Ctr Math Sci, Lund, Sweden.
RI Johansson, Rolf/C-1845-2013
OI Johansson, Rolf/0000-0002-0786-8561; Wadenback,
   Marten/0000-0002-0675-2794
FU Swedish Foundation for Strategic Research through the SSF project
   ENGROSS
FX This work has been partly funded by the Swedish Foundation for Strategic
   Research through the SSF project ENGROSS (www.engross.lth.se). Among the
   authors are members of the LCCC Linnaeus Center and the ELLIIT
   Excellence Center at Lund University.
CR [Anonymous], 2005, P IEEE INT C ROB AUT
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Axis Communications AB, 2012, AXIS P3364 VE NETW C
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   DURRANTWHYTE HF, 1987, INT J ROBOT RES, V6, P3, DOI 10.1177/027836498700600301
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fraunhofer IPA, 2012, COMPACT DRIVE MODULE
   Gustafsson F., 2012, STAT SENSOR FUSION
   Hajjdiab H, 2004, 1ST CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P155, DOI 10.1109/CCCRV.2004.1301439
   HARRIS CG, 1988, IMAGE VISION COMPUT, V6, P87, DOI 10.1016/0262-8856(88)90003-0
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Jones ES, 2011, INT J ROBOT RES, V30, P407, DOI 10.1177/0278364910388963
   Karlsson N, 2005, IEEE INT CONF ROBOT, P24
   Liang BJ, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P205, DOI 10.1109/ROBOT.2002.1013362
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Newman P, 2005, IEEE INT CONF ROBOT, P635
   Nikon Corporation, 2011, K SER OPT CMM SOL SU
   Ortin D, 2001, ROBOTICA, V19, P331, DOI 10.1017/S0263574700003143
   Scaramuzza D, 2011, INT J COMPUT VISION, V95, P74, DOI 10.1007/s11263-011-0441-3
   Scaramuzza D, 2011, J FIELD ROBOT, V28, P792, DOI 10.1002/rob.20411
   Wadenback M, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P635
   Zienkiewicz J, 2015, J FIELD ROBOT, V32, P803, DOI 10.1002/rob.21547
NR 24
TC 4
Z9 4
U1 1
U2 1
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-227-1
PY 2017
BP 340
EP 346
DI 10.5220/0006079903400346
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BK9RU
UT WOS:000444903900035
OA hybrid, Green Submitted
DA 2022-02-10
ER

PT J
AU Kenney, ML
   Belthoff, JR
   Carling, M
   Miller, TA
   Katzner, TE
AF Kenney, Macy L.
   Belthoff, James R.
   Carling, Matthew
   Miller, Tricia A.
   Katzner, Todd E.
TI Spatial and temporal patterns in age structure of Golden Eagles
   wintering in eastern North America
SO JOURNAL OF FIELD ORNITHOLOGY
LA English
DT Article
DE age cohorts; Aquila chrysaetos; camera trap; latitudinal variation;
   wintering behavior
ID AQUILA-CHRYSAETOS; POPULATION-DYNAMICS; CAMERA-TRAP; SPACE USE; SIZE;
   PERFORMANCE; SUMMER; BIRDS; DEER
AB The behavior of wildlife varies seasonally, and that variation can have substantial demographic consequences. This is especially true for long-distance migrants where the use of landscapes varies by season and, sometimes, age cohort. We tested the hypothesis that distributional patterns of Golden Eagles (Aquila chrysaetos) wintering in eastern North America are age-structured (i.e., birds of similar ages winter together) through the analysis of 370,307 images collected by motion-sensitive trail cameras set over bait during the winters of 2012-2013 and 2013-2014. At nine sites with sufficient data for analysis, we documented 145 eagle visits in 2012-2013 and 146 in 2013-2014. We found significant between-year variation in age structure of wintering eastern Golden Eagles, driven largely by annual differences in the proportion of first-winter birds. However, although many other species show spatial structure in wintering behavior, our analysis revealed no latitudinal organization among age cohorts of wintering eastern Golden Eagles. The lack of age-related latitudinal segregation in wintering behavior does not exclude the possibility that these eagles have sex-based or other types of dominance hierarchies that could result in spatial or temporal segregation. Alternatively, other mechanisms such as food availability or habitat structure may determine the distribution and abundance of Golden Eagles in winter.
C1 [Kenney, Macy L.; Belthoff, James R.] Boise State Univ, Raptor Res Ctr, Boise, ID 83725 USA.
   [Kenney, Macy L.; Belthoff, James R.] Boise State Univ, Dept Biol Sci, Boise, ID 83725 USA.
   [Kenney, Macy L.; Carling, Matthew] Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.
   [Miller, Tricia A.] West Virginia Univ, Div Forestry & Nat Resources, Morgantown, WV 26506 USA.
   [Miller, Tricia A.] Conservat Sci Global, West Cape May, NJ 08204 USA.
   [Katzner, Todd E.] US Geol Survey, Forest & Rangeland Ecosyst Sci Ctr, Boise, ID 83706 USA.
RP Kenney, ML; Belthoff, JR (corresponding author), Boise State Univ, Raptor Res Ctr, Boise, ID 83725 USA.; Kenney, ML; Belthoff, JR (corresponding author), Boise State Univ, Dept Biol Sci, Boise, ID 83725 USA.; Kenney, ML (corresponding author), Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USA.; Katzner, TE (corresponding author), US Geol Survey, Forest & Rangeland Ecosyst Sci Ctr, Boise, ID 83706 USA.
EM macykenney5@gmail.com; jbeltho@boisestate.edu; tkatzner@usgs.gov
OI Belthoff, James/0000-0002-6051-2353; Miller, Trish/0000-0001-5152-9789
FU Virginia Department of Game and Inland Fisheries through a Federal Aid
   in Wildlife Restoration grant from USFWS, Pennsylvania SWG [T-12,
   T47-R-1]; US DoEUnited States Department of Energy (DOE) [DEEE0003538];
   Charles A. and Anne Morrow Lindbergh Foundation; National Science
   Foundation REU Site AwardNational Science Foundation (NSF) [DBI:
   1263167]; Boise State University's Raptor Research Center; Boise State
   University's Department of Biological Sciences; Boise State University's
   College of Arts and Sciences; Boise State University's Division of
   Research
FX Funding for this work was received from the Virginia Department of Game
   and Inland Fisheries through a Federal Aid in Wildlife Restoration grant
   from USFWS, Pennsylvania SWG grants T-12 and T47-R-1, US DoE grant
   DEEE0003538, Charles A. and Anne Morrow Lindbergh Foundation, and the
   authors' organizations. Jeff Cooper, Michael Lanzone, Kieran O'Malley,
   and many others assisted with numerous phases of this research. At the
   time of this research, MLK and JRB were supported by National Science
   Foundation REU Site Award (DBI: 1263167) to Boise State University, and
   by Boise State University's Raptor Research Center, Department of
   Biological Sciences, College of Arts and Sciences, and Division of
   Research. MLK also received additional logistical support from the
   University of Wyoming Museum of Vertebrates. Any use of trade, product,
   or firm names is for descriptive purposes only and does not imply
   endorsement by the U.S. Government.
CR Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
   Akesson S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086779
   Bloom Peter H., 2001, North American Bird Bander, V26, P97
   Boulanger JR, 2012, NORTHEAST NAT, V19, P159, DOI 10.1656/045.019.s612
   Calvert AM, 2009, AVIAN CONSERV ECOL, V4
   Czenze ZJ, 2017, OECOLOGIA, V183, P1, DOI 10.1007/s00442-016-3707-1
   Dearing MD, 1997, J MAMMAL, V78, P1156, DOI 10.2307/1383058
   Dennhardt AJ, 2015, BIOL CONSERV, V184, P68, DOI 10.1016/j.biocon.2015.01.003
   Duerr AE, 2015, FUNCT ECOL, V29, P779, DOI 10.1111/1365-2435.12381
   Gill F.B., 2006, ORNITHOLOGY
   Jachowski DS, 2015, WILDLIFE SOC B, V39, P553, DOI 10.1002/wsb.571
   Jimenez J, 2017, SCI REP-UK, V7, DOI 10.1038/srep41036
   Jollie Malcolm, 1947, AUK, V64, P549
   Katzner T, 2015, WILSON J ORNITHOL, V127, P102, DOI 10.1676/14-066.1
   Katzner T, 2012, AUK, V129, P168, DOI 10.1525/auk.2011.11078
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Ketterson E.D., 1983, Current Ornithology, V1, P357
   Kochert MN, 2002, BIRDS N AM
   Komar O, 2005, AUK, V122, P938, DOI 10.1642/0004-8038(2005)122[0938:EOLSSA]2.0.CO;2
   Lazaro J, 2017, CURR BIOL, V27, pR1106, DOI 10.1016/j.cub.2017.08.055
   Luo G, 2019, AVIAN RES, V10, DOI 10.1186/s40657-019-0144-y
   Marra PP, 1998, SCIENCE, V282, P1884, DOI 10.1126/science.282.5395.1884
   Mcintyre CL, 2012, IBIS, V154, P124, DOI 10.1111/j.1474-919X.2011.01181.x
   Mendez D, 2019, IBIS, V161, P867, DOI 10.1111/ibi.12681
   Mettke-Hofmann C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123775
   Miller TA, 2017, CONDOR, V119, P697, DOI 10.1650/CONDOR-16-154.1
   Miller TA, 2016, IBIS, V158, P116, DOI 10.1111/ibi.12331
   Molina David, 2018, Huitzil, V19, P22, DOI 10.28947/hrmo.2018.19.1.308
   Nadjafzadeh M, 2016, IBIS, V158, P1, DOI 10.1111/ibi.12311
   Newton I, 2008, MIGRATION ECOLOGY OF BIRDS, P1
   Norris DR, 2004, P ROY SOC B-BIOL SCI, V271, P59, DOI 10.1098/rspb.2003.2569
   Pande Satish, 2010, Journal of Threatened Taxa, V2, P1214
   Pandolfino ER, 2011, J RAPTOR RES, V45, P236, DOI 10.3356/JRR-10-66.1
   Riley SJ, 2003, ECOSCIENCE, V10, P455, DOI 10.1080/11956860.2003.11682793
   Roncal CM, 2019, J FIELD ORNITHOL, V90, P203, DOI 10.1111/jofo.12299
   Rus AI, 2017, AUK, V134, P485, DOI 10.1642/AUK-16-147.1
   Rushing CS, 2017, ECOLOGY, V98, P2837, DOI 10.1002/ecy.1967
   Saino N, 2017, J ANIM ECOL, V86, P239, DOI 10.1111/1365-2656.12625
   Stewart FEC, 2019, J WILDLIFE MANAGE, V83, P985, DOI 10.1002/jwmg.21657
   Sumasgutner P, 2016, BIRD STUDY, V63, P430, DOI 10.1080/00063657.2016.1214814
   Vukovich M., 2019, J FIELD ORNITHOL, V86, P337
   Ward DH, 2018, J WILDLIFE MANAGE, V82, P362, DOI 10.1002/jwmg.21388
   WATSON J., 2010, GOLDEN EAGLE, V2nd
   Watson JW, 2019, J WILDLIFE MANAGE, V83, P1735, DOI 10.1002/jwmg.21760
   Zar J.H., 2010, BIOSTATISTICAL ANAL, Vfifth
NR 45
TC 0
Z9 0
U1 2
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0273-8570
EI 1557-9263
J9 J FIELD ORNITHOL
JI J. Field Ornithol.
PD MAR
PY 2020
VL 91
IS 1
BP 92
EP 101
DI 10.1111/jofo.12325
EA FEB 2020
PG 10
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA KX4BP
UT WOS:000515834700001
OA Bronze
DA 2022-02-10
ER

PT J
AU Orban, B
   Kabafouako, G
   Morley, R
   Gaugris, CV
   Melville, H
   Gaugris, J
AF Orban, Ben
   Kabafouako, Gerard
   Morley, Robert
   Gaugris, Caroline
   Melville, Haemish
   Gaugris, Jerome
TI Common mammal species inventory utilizing camera trapping in the forests
   of Kouilou Departement, Republic of Congo
SO AFRICAN JOURNAL OF ECOLOGY
LA English
DT Article
DE camera; Congo; forest; mammal; survey; trapping
C1 [Orban, Ben; Kabafouako, Gerard; Morley, Robert; Gaugris, Caroline; Gaugris, Jerome] Flora Fauna & Man Ecol Serv, Tortola, British Virgin Isl.
   [Gaugris, Caroline; Gaugris, Jerome] Univ Witwatersrand, Ctr African Ecol, Sch Anim Plant & Environm Sci, Johannesburg, South Africa.
   [Melville, Haemish] UNISA Univ South Africa, Dept Environm Sci, Pretoria, South Africa.
RP Gaugris, J (corresponding author), Flora Fauna & Man Ecol Serv, Tortola, British Virgin Isl.
EM jeromegaugris@florafaunaman.com
OI Gaugris, Jerome/0000-0002-6606-060X
CR Ancrenaz M., 2012, HDB WILDLIFE MONITOR
   Barnes RFW, 1996, MAMMAL REV, V26, P67, DOI 10.1111/j.1365-2907.1996.tb00147.x
   DesWasseige C., 2014, FOR C BAS STAT FOR 2, P328
   Devers D., 2006, FOR C BAS STAT FOR 2
   Guil F, 2010, EUR J WILDLIFE RES, V56, P633, DOI 10.1007/s10344-009-0353-5
   Gullison T., 2015, GOOD PRACTICES COLLE
   HECKETSWEILER P, 1991, RESERVE CONKOUATI CO
   Hortal J, 2006, J ANIM ECOL, V75, P274, DOI 10.1111/j.1365-2656.2006.01048.x
   IUCN, 2017, IUCN RED LIST THREAT
   Kays Roland, 2011, International Journal of Research and Reviews in Wireless Sensor Networks, V1, P19
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Maisels Fiona, 2007, Primate Conservation, V22, P111, DOI 10.1896/052.022.0110
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Mohd-Azlan J, 2013, RAFFLES B ZOOL, V61, P397
   Mugerwa B, 2013, AFR J ECOL, V51, P21, DOI 10.1111/aje.12004
   O'Brien C., 2009, TERRESTRIAL SMALL MA
   Rostand A. N., 2007, ESTIMATION DENSITE L
   Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Tobler MW, 2008, ANIM CONSERV, V11, P169, DOI 10.1111/j.1469-1795.2008.00169.x
   Van Rooyen M, 2016, TROP ECOL, V57, P805
   Walters J., 2010, EVALUATING SUITABILI
NR 22
TC 4
Z9 5
U1 1
U2 6
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0141-6707
EI 1365-2028
J9 AFR J ECOL
JI Afr. J. Ecol.
PD DEC
PY 2018
VL 56
IS 4
SI SI
BP 750
EP 754
DI 10.1111/aje.12551
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA HC1PV
UT WOS:000451574200008
DA 2022-02-10
ER

PT C
AU Radig, B
   Follmann, P
AF Radig, Bernd
   Follmann, Patrick
GP CENPARMI
TI Training a classifier for automatic flash detection in million images
   from camera-traps
SO PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND
   ARTIFICIAL INTELLIGENCE (ICPRAI 2018)
LA English
DT Proceedings Paper
CT International Conference on Pattern Recognition and Artificial
   Intelligence (ICPRAI)
CY MAY 13-17, 2018
CL Montreal, CANADA
SP Concordia Univ, Ctr Pattern Recognit & Machine Intelligence
C1 [Radig, Bernd; Follmann, Patrick] Tech Univ Munich, Informat, Munich, Germany.
   [Follmann, Patrick] Tech Univ Munich, MVTec Software GmbH, Munich, Germany.
RP Radig, B (corresponding author), Tech Univ Munich, Informat, Munich, Germany.
EM radig@in.tum.de; follmann@mvtec.com
CR Barz B., 2018, IMTA WORKSH UNPUB
   Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
   Krizhevsky A., 2012, IMAGENET CLASSIFICAT, V25
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
NR 4
TC 0
Z9 0
U1 0
U2 0
PU CENPARMI
PI Montreal, QC
PA 1455 Maisonneuve Blvd, West EV 003 403, Montreal, QC, CANADA
BN 978-1-895193-04-6
PY 2018
BP 589
EP 591
PG 3
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS3YW
UT WOS:000716970300102
DA 2022-02-10
ER

PT J
AU Sarkar, D
   Chapman, CA
AF Sarkar, Dipto
   Chapman, Colin A.
TI The Smart Forest Conundrum: Contextualizing Pitfalls of Sensors and AI
   in Conservation Science for Tropical Forests
SO TROPICAL CONSERVATION SCIENCE
LA English
DT Article
DE sensors; monitoring technologies; poaching; protected area; Kibale
   National Park; drones; camera-traps; remote sensing; AI
ID CAMERA TRAPS; WAR; BIODIVERSITY; BIG; SURVEILLANCE; DRONES; DONT
AB The term 'smart forest' is not yet common, but the proliferation of sensors, algorithms, and technocentric thinking in conservation, as in most other aspects of our lives, suggests we are at the brink of this evolution. While there has been some critical discussion about the value of using smart technology in conservation, a holistic discussion about the broader technological, social, and economic interactions involved with using big data, sensors, artificial intelligence, and global corporations is largely missing. Here, we explore the pitfalls that are useful to consider as forests are gradually converted to technological sites of data production for optimized biodiversity conservation and are consequently incorporated in the digital economy. We consider who are the enablers of the technologically enhanced forests and how the gradual operationalization of smart forests will impact the traditional stakeholders of conservation. We also look at the implications of carpeting forests with sensors and the type of questions that will be encouraged. To contextualize our arguments, we provide examples from our work in Kibale National Park, Uganda which hosts the one of the longest continuously running research field station in Africa.
C1 [Sarkar, Dipto] Carleton Univ, Dept Geog & Environm Studies, Ottawa, ON K1S 5B6, Canada.
   [Chapman, Colin A.] George Washington Univ, Dept Anthropol, Ctr Adv Study Human Paleobiol, Washington, DC 20052 USA.
   [Chapman, Colin A.] Northwest Univ, Shaanxi Key Lab Anim Conservat, Xian, Peoples R China.
   [Chapman, Colin A.] Univ KwaZulu Natal, Sch Life Sci, Pietermaritzburg, South Africa.
RP Sarkar, D (corresponding author), Carleton Univ, Dept Geog & Environm Studies, Ottawa, ON K1S 5B6, Canada.
EM dipto.sarkar@carleton.ca
CR Abdelnour S., 2015, SUSTAINABLE ACCESS E, P205
   Adams WM, 2019, PROG HUM GEOG, V43, P337, DOI 10.1177/0309132517740220
   Arts K, 2015, AMBIO, V44, pS661, DOI 10.1007/s13280-015-0705-1
   Arvidsson A, 2016, THEOR CULT SOC, V33, P3, DOI 10.1177/0263276416658104
   Bakker K, 2018, GLOBAL ENVIRON CHANG, V52, P201, DOI 10.1016/j.gloenvcha.2018.07.011
   Barns S, 2017, URBAN POLICY RES, V35, P20, DOI 10.1080/08111146.2016.1235032
   Begg CM, 2005, J ZOOL, V265, P23, DOI 10.1017/S0952836904005989
   Benkler Y, 2019, NATURE, V569, P161, DOI 10.1038/d41586-019-01413-1
   Bennett EL, 2015, CONSERV BIOL, V29, P54, DOI 10.1111/cobi.12377
   Birhane A, 2020, SCRIPT ED, V17, P389
   Bonnin N, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2020017
   Bortolamiol S., CONSERV SCI PRACT
   Bradshaw CJA, 2009, FRONT ECOL ENVIRON, V7, P79, DOI 10.1890/070193
   Buscher B, 2016, NEW MEDIA SOC, V18, P726, DOI 10.1177/1461444814545841
   Buscher B, 2016, AFR AFFAIRS, V115, P1, DOI 10.1093/afraf/adv058
   Calzada I, 2020, SMART CITIES-BASEL, V3, P1145, DOI 10.3390/smartcities3040057
   Dechmann DKN, 2011, J EXP BIOL, V214, P3605, DOI 10.1242/jeb.056010
   Ditmer MA, 2015, CURR BIOL, V25, P2278, DOI 10.1016/j.cub.2015.07.024
   Dourish P, 2018, BIG DATA SOC, V5, DOI 10.1177/2053951718784083
   Duffy R, 2016, GEOFORUM, V69, P238, DOI 10.1016/j.geoforum.2015.09.014
   Duffy R, 2014, INT AFF, V90, P819, DOI 10.1111/1468-2346.12142
   Faraway JJ, 2018, STAT PROBABIL LETT, V136, P142, DOI 10.1016/j.spl.2018.02.031
   Fletcher R., 2014, NATURE INC ENV CONSE
   Fourcade M, 2017, SOCIO-ECON REV, V15, P9, DOI 10.1093/ser/mww033
   Gabrys J., 2016, PROGRAM EARTH ENV SE
   Gabrys J, 2020, BIG DATA SOC, V7, DOI 10.1177/2053951720904871
   Galaz V, 2017, TRENDS ECOL EVOL, V32, P628, DOI 10.1016/j.tree.2017.06.013
   Garstang M, 2004, J COMP PHYSIOL A, V190, P791, DOI 10.1007/s00359-004-0553-0
   Gautam H, 2019, BIOTROPICA, V51, P443, DOI 10.1111/btp.12651
   Getzin S, 2012, METHODS ECOL EVOL, V3, P397, DOI 10.1111/j.2041-210X.2011.00158.x
   Goetz Scott J, 2009, Carbon Balance Manag, V4, P2, DOI 10.1186/1750-0680-4-2
   Guo ST, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101412
   Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1215904, 10.1126/science.1244693]
   Harvey D., 2014, 17 CONTRADICTIONS EN
   Howard A, 2018, SCI ENG ETHICS, V24, P1521, DOI 10.1007/s11948-017-9975-2
   Huesemann M., 2011, TECHNO FIX WHY TECHN
   Humle T, 2014, SCIENCE, V344, P1351, DOI 10.1126/science.344.6190.1351-a
   Joly A, 2018, LECT NOTES COMPUT SC, V11018, P247, DOI 10.1007/978-3-319-98932-7_24
   Joppa LN, 2015, AMBIO, V44, pS522, DOI 10.1007/s13280-015-0702-4
   Kang C., 2020, NEW YORK TIMES
   Karagounis B., 2020, INTRO MICROSOFT AZUR
   Kays R, 2020, METHODS ECOL EVOL, V11, P700, DOI 10.1111/2041-210X.13370
   Keeping D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096598
   Kirumira D, 2019, CONSERV SOC, V17, P51, DOI 10.4103/cs.cs_17_72
   Kitchin R., 2014, DATA REVOLUTION BIG
   Kolowski JM, 2021, ECOSPHERE, V12, DOI 10.1002/ecs2.3350
   Kull CA, 2007, SOC NATUR RESOUR, V20, P723, DOI 10.1080/08941920701329702
   Leyland C., 2020, THE GUARDIAN
   Linkie M, 2013, BIOL CONSERV, V162, P107, DOI 10.1016/j.biocon.2013.03.028
   Lyon D, 2014, BIG DATA SOC, V1, DOI 10.1177/2053951714541861
   Lyubenova M., 2015, Journal of Balkan Ecology, V18, P363
   Maisels F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059469
   Marvin DC, 2016, GLOB ECOL CONSERV, V7, P262, DOI 10.1016/j.gecco.2016.07.002
   Meek P, 2016, ECOL EVOL, V6, P3216, DOI 10.1002/ece3.2111
   Meek PD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110832
   Merritt SM, 2013, HUM FACTORS, V55, P520, DOI 10.1177/0018720812465081
   Morozov E., 2013, SAVE EVERYTHING CLIC
   Morrissey L.F., 2012, DEVELOPMENT, V55, P13
   Naughton-Treves L, 2011, P NATL ACAD SCI USA, V108, P13919, DOI 10.1073/pnas.1013332108
   Neumann RP, 2004, POLIT GEOGR, V23, P813, DOI 10.1016/j.polgeo.2004.05.011
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Connell F., 2011, CAMERA TRAPS ANIMAL
   O'Neil Cathy., 2016, WEAPONS MATH DESTRUC
   Park JY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131534
   Pasquale F., 2015, BLACK BOX SOC
   Pimm SL, 2014, SCIENCE, V344, P987, DOI 10.1126/science.1246752
   Rebolo-Ifran N, 2019, ENVIRON CONSERV, V46, P205, DOI 10.1017/S0376892919000080
   Redford KH, 2013, CONSERV BIOL, V27, P437, DOI 10.1111/cobi.12071
   Rivas-Romero JA, 2015, SOUTHWEST NAT, V60, P366, DOI 10.1894/0038-4909-60.4.366
   Roderick L, 2014, CRIT SOCIOL, V40, P729, DOI 10.1177/0896920513501350
   Sadowski J, 2019, BIG DATA SOC, V6, DOI 10.1177/2053951718820549
   Sandbrook C, 2018, CONSERV SOC, V16, P493, DOI 10.4103/cs.cs_17_165
   Sandbrook C, 2015, AMBIO, V44, pS636, DOI 10.1007/s13280-015-0714-0
   Sarkar D, 2019, PROF GEOGR, V71, P422, DOI 10.1080/00330124.2018.1547976
   Sequin ES, 2003, CAN J ZOOL, V81, P2015, DOI 10.1139/Z03-204
   Shelton T, 2019, CITY, V23, P35
   Simonson WD, 2014, METHODS ECOL EVOL, V5, P719, DOI 10.1111/2041-210X.12219
   Sosnowski MC, 2019, BIOL CONSERV, V237, P392, DOI 10.1016/j.biocon.2019.07.020
   Srnicek N., 2017, PLATFORM CAPITALISM
   Succi S, 2019, PHILOS T R SOC A, V377, DOI 10.1098/rsta.2018.0145
   Tembon M., 2019, INVESTING BHUTANS FO
   Vanolo A, 2014, URBAN STUD, V51, P883, DOI 10.1177/0042098013494427
   Vas E, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2014.0754
   Wasser SK, 2015, SCIENCE, V349, P84, DOI 10.1126/science.aaa2457
   Wasser SK, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat0625
   Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
   Wiig A, 2018, ENVIRON PLAN C-POLIT, V36, P403, DOI 10.1177/2399654417743767
   Wrege PH, 2017, METHODS ECOL EVOL, V8, P1292, DOI 10.1111/2041-210X.12730
   Xiao Y., 2016, TRAFFIC BULL, V17
   Xu FF, 2020, J SUSTAIN TOUR, V28, P144, DOI 10.1080/09669582.2019.1631318
   Yoccoz NG, 2001, TRENDS ECOL EVOL, V16, P446, DOI 10.1016/S0169-5347(01)02205-4
   ZIEGLER TE, 1995, HORM BEHAV, V29, P407, DOI 10.1006/hbeh.1995.1028
   Zuboff S, 2015, J INF TECHNOL-UK, V30, P75, DOI 10.1057/jit.2015.5
NR 94
TC 3
Z9 3
U1 5
U2 6
PU SAGE PUBLICATIONS INC
PI THOUSAND OAKS
PA 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN 1940-0829
J9 TROP CONSERV SCI
JI Trop. Conserv. Sci.
PD APR
PY 2021
VL 14
AR 19400829211014740
DI 10.1177/19400829211014740
PG 11
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA SJ2QF
UT WOS:000655374400001
OA gold
DA 2022-02-10
ER

PT C
AU Ring, L
   Utami, D
   Olafsson, S
   Bickmore, T
AF Ring, Lazlo
   Utami, Dina
   Olafsson, Stefan
   Bickmore, Timothy
BE Traum, D
   Swartout, W
   Khooshabeh, P
   Kopp, S
   Scherer, S
   Leuski, A
TI Increasing Engagement with Virtual Agents Using Automatic Camera Motion
SO INTELLIGENT VIRTUAL AGENTS, IVA 2016
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 16th International Conference on Intelligent Virtual Agents (IVA)
CY SEP 20-23, 2016
CL Los Angeles, CA
SP Alelo, Springer, Univ So Calif, Inst Creat Technologies
DE Relational agent; Cinematography; Natural language understanding
ID EXPRESSION; HUMANS
AB We describe a series of algorithms which automatically control camera position in a virtual environment while a user is engaged in a simulated face-to-face dialog with a single virtual agent. The common objective of the algorithms is to increase user engagement with the interaction. In our work, we describe three different automated camera control systems that: (1) control the camera's position based on topic changes in dialog; (2) use sentiment analysis to control the camera-to-agent distance; and (3) adjust the camera's depth-of-field based on "important" segments of the dialog. Evaluation studies of each method are described. We find that changing camera position based on topic shifts results in significant increases in a self-reported measure of engagement, while the other methods seem to actually decrease user engagement. Interpretations and ramifications of the results are discussed.
C1 [Ring, Lazlo; Utami, Dina; Olafsson, Stefan; Bickmore, Timothy] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
RP Ring, L; Utami, D; Olafsson, S; Bickmore, T (corresponding author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
EM lring@ccs.neu.edu; dinau@ccs.neu.edu; stefanolafs@ccs.neu.edu;
   bickmore@ccs.neu.edu
CR Arijan D., 1976, GRAMMAR FILM LANGUAG
   Battaglino C., 2015, INT8 WORKSH SANT CRU
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   Bickmore T, 2010, APPL ARTIF INTELL, V24, P648, DOI 10.1080/08839514.2010.492259
   Bickmore T, 2009, LECT NOTES ARTIF INT, V5773, P6
   CALAHAN S, 1996, STORYTELLING LIGHTIN
   Canini L, 2011, INT SYMP IMAGE SIG, P253
   Cassell J, 2001, 39TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P106
   Cassell J, 2001, COMP GRAPH, P477
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Collins, 1988, COGNITIVE STRUCTURE
   de Melo C, 2007, LECT NOTES COMPUT SC, V4738, P546
   DeVault D., 2015, AAAI 2015 SPRING S T
   Hymes Del., 1972, DIRECTIONS SOCIOLING
   Lehmann J., 2012, P 20 INT C US MOD AD, P164, DOI [10.1007/978-3-642-31454-4_14, DOI 10.1007/978-3-642-31454-4_14]
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   MARCUS BH, 1993, J SPORT MED PHYS FIT, V33, P83
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Rui Y., 2003, P SIGCHI C HUM FACT, P457
   Schiffrin Deborah, 1987, DISCOURSE MARKERS
   SMITH J, 2000, GRANDCHAIR CONVERSAT
   Tannen D., 1993, FRAMING DISCOURSE
NR 22
TC 5
Z9 5
U1 1
U2 6
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-47665-0; 978-3-319-47664-3
J9 LECT NOTES ARTIF INT
PY 2016
VL 10011
BP 29
EP 39
DI 10.1007/978-3-319-47665-0_3
PG 11
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BG6GQ
UT WOS:000390287300003
DA 2022-02-10
ER

PT J
AU Lee, SJ
   Lee, JW
   Lee, W
   Jang, C
AF Lee, Sang Jun
   Lee, Jae-Woo
   Lee, Wonju
   Jang, Cheolhun
TI Constrained Multiple Planar Reconstruction for Automatic Camera
   Calibration of Intelligent Vehicles
SO SENSORS
LA English
DT Article
DE computer vision; intelligent vehicles; extrinsic camera calibration;
   structure from motion; convex optimization
ID EGO-MOTION
AB In intelligent vehicles, extrinsic camera calibration is preferable to be conducted on a regular basis to deal with unpredictable mechanical changes or variations on weight load distribution. Specifically, high-precision extrinsic parameters between the camera coordinate and the world coordinate are essential to implement high-level functions in intelligent vehicles such as distance estimation and lane departure warning. However, conventional calibration methods, which solve a Perspective-n-Point problem, require laborious work to measure the positions of 3D points in the world coordinate. To reduce this inconvenience, this paper proposes an automatic camera calibration method based on 3D reconstruction. The main contribution of this paper is a novel reconstruction method to recover 3D points on planes perpendicular to the ground. The proposed method jointly optimizes reprojection errors of image features projected from multiple planar surfaces, and finally, it significantly reduces errors in camera extrinsic parameters. Experiments were conducted in synthetic simulation and real calibration environments to demonstrate the effectiveness of the proposed method.
C1 [Lee, Sang Jun] Jeonbuk Natl Univ, Div Elect Engn, 567 Baekje Daero, Jeonju Si, Jeollabuk Do, South Korea.
   [Lee, Jae-Woo; Lee, Wonju; Jang, Cheolhun] Samsung Adv Inst Technol SAIT, 130 Samsung Ro, Suwon 16678, Gyeonggi Do, South Korea.
RP Lee, JW (corresponding author), Samsung Adv Inst Technol SAIT, 130 Samsung Ro, Suwon 16678, Gyeonggi Do, South Korea.
EM sj.lee@jbnu.ac.kr; magic0ad@gmail.com; wonjulee@kaist.ac.kr;
   c_h.jang@samsung.com
OI Lee, Sang Jun/0000-0002-9312-6299
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2021R1G1A1009792]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIT) (No.
   2021R1G1A1009792). This paper was supported by research funds for newly
   appointed professors of Jeonbuk National University in 2020.
CR Antunes M, 2017, PROC CVPR IEEE, P6691, DOI 10.1109/CVPR.2017.708
   Bazargani H, 2015, IEEE INSTRU MEAS MAG, V18, P20, DOI 10.1109/MIM.2015.7335834
   Bustos AP, 2019, IEEE INT CONF ROBOT, P2385, DOI 10.1109/ICRA.2019.8793749
   Chum O, 2005, COMPUT VIS IMAGE UND, V97, P86, DOI 10.1016/j.cviu.2004.03.004
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599
   Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   HORAUD R, 1989, COMPUT VISION GRAPH, V47, P33, DOI 10.1016/0734-189X(89)90052-2
   Itu R, 2017, INT C INTELL COMP CO, P273, DOI 10.1109/ICCP.2017.8117016
   Jinwoo Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P541, DOI 10.1007/978-3-030-58610-2_32
   Kanatani K, 2016, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-48493-8
   Kanatani Kenichi, 2011, IPSJ Transactions on Computer Vision and Applications, V3, P67, DOI 10.2197/ipsjtcva.3.67
   Kanatani K., 2008, 19 BRIT MACH VIS C B, P173, DOI DOI 10.5244/C.22.18
   Kolupaev A.V., 2018, P 2018 IEEE E W DES, P1, DOI [10.1109/EWDTS.2018.8524815, DOI 10.1109/EWDTS.2018.8524815]
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li Y, 2019, IEEE INT CONF ROBOT, P5439, DOI 10.1109/ICRA.2019.8793706
   Lindstrom P, 2010, PROC CVPR IEEE, P1554, DOI 10.1109/CVPR.2010.5539785
   Lucas B.D., 1981, P IJCAI, P121
   Miksch M, 2010, IEEE INT VEH SYM, P832, DOI 10.1109/IVS.2010.5548048
   Mouragnon E., 2006, P IEEE COMP SOC C CO, V1, P363
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Song SY, 2014, PROC CVPR IEEE, P1566, DOI 10.1109/CVPR.2014.203
   Wang XB, 2009, IEEE SYS MAN CYBERN, P1770, DOI 10.1109/ICSMC.2009.5346611
   Yamaguchi K, 2006, INT C PATT RECOG, P610
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao Ji, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3030161
   Zhou ZH, 2012, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2012.6247837
NR 33
TC 0
Z9 0
U1 1
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JUL
PY 2021
VL 21
IS 14
AR 4643
DI 10.3390/s21144643
PG 13
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA TO6AX
UT WOS:000676993000001
PM 34300383
OA Green Published, gold
DA 2022-02-10
ER

PT C
AU Daftry, S
   Hoppe, C
   Bischof, H
AF Daftry, Shreyansh
   Hoppe, Christof
   Bischof, Horst
GP IEEE
TI Building with Drones: Accurate 3D Facade Reconstruction using MAVs
SO 2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)
SE IEEE International Conference on Robotics and Automation ICRA
LA English
DT Proceedings Paper
CT IEEE International Conference on Robotics and Automation (ICRA)
CY MAY 26-30, 2015
CL Seattle, WA
SP IEEE
AB Automatic reconstruction of 3D models from images using multi-view Structure-from-Motion methods has been one of the most fruitful outcomes of computer vision. These advances combined with the growing popularity of Micro Aerial Vehicles as an autonomous imaging platform, have made 3D vision tools ubiquitous for large number of Architecture, Engineering and Construction applications among audiences, mostly unskilled in computer vision. However, to obtain high-resolution and accurate reconstructions from a large-scale object using SfM, there are many critical constraints on the quality of image data, which often become sources of inaccuracy as the current 3D reconstruction pipelines do not facilitate the users to determine the fidelity of input data during the image acquisition. In this paper, we present and advocate a closed-loop interactive approach that performs incremental reconstruction in real-time and gives users an online feedback about the quality parameters like Ground Sampling Distance (GSD), image redundancy, etc on a surface mesh. We also propose a novel multi-scale camera network design to prevent scene drift caused by incremental map building, and release the first multi-scale image sequence dataset as a benchmark. Further, we evaluate our system on real outdoor scenes, and show that our interactive pipeline combined with a multi-scale camera network approach provides compelling accuracy in multi-view reconstruction tasks when compared against the state-of-the-art methods.
C1 [Daftry, Shreyansh] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
EM daftry@cmu.edu; hoppe@icg.tugraz.at; bischof@icg.tugraz.at
RI Daftry, Shreyansh/AAI-1710-2019
OI Bischof, Horst/0000-0002-9096-6671
CR Agarwal S., 2010, EUR C COMP VIS ECCV
   Bischof, 2014, ISPRS ANN PHOTOGRAMM, V3, P135, DOI [DOI 10.5194/ISPRSANNALS-II-3-135-2014, 10.5194/isprsannals-II-3-135-2014]
   Bouguet J. Y., CAMERA CALIBRATION T
   Bradski G., 2005, INTEL TECHNOL J, V9, P119
   Daftry S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.19
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   Hoppe C, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.94
   Hoppe C, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.70
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Irschara A., 2009, IEEE C COMP VIS PATT
   Irschara A, 2007, IEEE I CONF COMP VIS, P2996
   Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128
   Labatut P, 2007, IEEE I CONF COMP VIS, P504
   Nister D., 2006, IEEE C COMP VIS PATT
   Pan Q., 2009, BRIT MACH VIS C BMVC
   Seitz S.M., 2006, IEEE C COMP VIS PATT
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Strecha C., 2008, IEEE C COMP VIS PATT
   Yu S., 2012, JOINT 3DIM 3DPVT C 3
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 22
TC 36
Z9 36
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 1050-4729
EI 2577-087X
BN 978-1-4799-6923-4
J9 IEEE INT CONF ROBOT
PY 2015
BP 3487
EP 3494
PG 8
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BE3MR
UT WOS:000370974903071
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Reisslein, M
   Rinner, B
   Roy-Chowdhury, A
AF Reisslein, Martin
   Rinner, Bernhard
   Roy-Chowdhury, Amit
TI Smart Camera Networks
SO COMPUTER
LA English
DT Article
AB By bringing together advances in computer vision, embedded computing, image sensors, and networks, researchers are working toward more automated analysis of large-scale camera networks' data.
C1 [Reisslein, Martin] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85287 USA.
   [Rinner, Bernhard] Alpen Adria Univ Klagenfurt, Klagenfurt, Austria.
   [Roy-Chowdhury, Amit] Univ Calif Riverside, Riverside, CA 92521 USA.
RP Reisslein, M (corresponding author), Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85287 USA.
EM reisslein@asu.edu; bernhard.rinner@aau.at; amitrc@ee.ucr.edu
RI Reisslein, Martin/B-3278-2014
OI Reisslein, Martin/0000-0003-1606-233X; Roy-Chowdhury,
   Amit/0000-0001-6690-9725
FU European CommissionEuropean CommissionEuropean Commission Joint Research
   Centre [257906]; Carinthian Economic Promotion Fund
   [KWF-3520/23312/35521]; US National Science FoundationNational Science
   Foundation (NSF) [IIS-0712253]; US Office of Naval ResearchOffice of
   Naval Research [N000140910666]
FX Bernhard Rinner's research was supported in part by the European
   Commission under grant 257906 and the Carinthian Economic Promotion Fund
   under grant KWF-3520/23312/35521. Amit Roy-Chowdhury's research has been
   supported in part by the US National Science Foundation under grant
   IIS-0712253 and the US Office of Naval Research under grant
   N000140910666.
CR Aghajan H, 2009, MULTI-CAMERA NETWORKS: PRINCIPLES AND APPLICATIONS, P1
   Chellappa R, 2010, IEEE T IMAGE PROCESS, V19, P2513, DOI 10.1109/TIP.2010.2063650
   Henricksen K., 2006, P ACM INT WORKSH MID, P60
   Micheloni C, 2010, IEEE SIGNAL PROC MAG, V27, P78, DOI 10.1109/MSP.2010.937333
   Mohamed N, 2011, SERV ORIENTED COMPUT, V5, P71, DOI 10.1007/s11761-011-0083-x
   Regazzoni CS, 2010, IEEE SIGNAL PROC MAG, V27, P16, DOI 10.1109/MSP.2010.937451
   Rinner B, 2008, P IEEE, V96, P1565, DOI 10.1109/JPROC.2008.928742
   Roy-Chowdhury A. K., 2012, SYNTHESIS LECT COMPU
   Seema A, 2011, IEEE COMMUN SURV TUT, V13, P462, DOI 10.1109/SURV.2011.102910.00098
   Terzopoulos, 2011, DISTRIBUTED VIDEO SE
NR 10
TC 20
Z9 20
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0018-9162
EI 1558-0814
J9 COMPUTER
JI Computer
PD MAY
PY 2014
VL 47
IS 5
BP 23
EP 25
PG 3
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AI8CH
UT WOS:000337132300019
DA 2022-02-10
ER

PT J
AU Tan, L
   Wang, YN
   Yu, HS
   Zhu, J
AF Tan, Lei
   Wang, Yaonan
   Yu, Hongshan
   Zhu, Jiang
TI Automatic Camera Calibration Using Active Displays of a Virtual Pattern
SO SENSORS
LA English
DT Article
DE camera calibration; 2D pattern; active display; lens distortion;
   closed-form solution; maximum likelihood estimation
AB Camera calibration plays a critical role in 3D computer vision tasks. The most commonly used calibration method utilizes a planar checkerboard and can be done nearly fully automatically. However, it requires the user to move either the camera or the checkerboard during the capture step. This manual operation is time consuming and makes the calibration results unstable. In order to solve the above problems caused by manual operation, this paper presents a full-automatic camera calibration method using a virtual pattern instead of a physical one. The virtual pattern is actively transformed and displayed on a screen so that the control points of the pattern can be uniformly observed in the camera view. The proposed method estimates the camera parameters from point correspondences between 2D image points and the virtual pattern. The camera and the screen are fixed during the whole process; therefore, the proposed method does not require any manual operations. Performance of the proposed method is evaluated through experiments on both synthetic and real data. Experimental results show that the proposed method can achieve stable results and its accuracy is comparable to the standard method by Zhang.
C1 [Tan, Lei; Wang, Yaonan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
   [Wang, Yaonan; Yu, Hongshan] Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
   [Zhu, Jiang] Xiangtan Univ, Coll Informat Engn, Xiangtan 411105, Peoples R China.
RP Tan, L (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.; Yu, HS (corresponding author), Hunan Univ, Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
EM leit@hnu.edu.cn; wangyaonan@hnu.edu.cn; yuhongshan@hnu.edu.cn;
   jiang126@126.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61573134, 61573135]; National Key Technology
   Support ProgramNational Key Technology R&D Program [2015BAF11B01];
   National Key Scientific Instrument and Equipment Development Project of
   China [2013YQ140517]; Key Research and Development Project of Science
   and Technology Plan of Hunan Province [2015GK3008]; Key Project of
   Science and Technology Plan of Guangdong Province [2013B011301014]
FX This work has been supported by National Natural Science Foundation of
   China (Grant No. 61573134, 61573135), National Key Technology Support
   Program (Grant No. 2015BAF11B01), National Key Scientific Instrument and
   Equipment Development Project of China (Grant No. 2013YQ140517), Key
   Research and Development Project of Science and Technology Plan of Hunan
   Province(Grant No. 2015GK3008), Key Project of Science and Technology
   Plan of Guangdong Province(Grant No. 2013B011301014).
CR Agrawal M., 2003, P 2003 9 IEEE INT C, P1
   Atcheson B., 2010, P VIS MOD VIS WORKSH, V10, P41
   Bergamasco F, 2014, INT C PATT RECOG, P2137, DOI 10.1109/ICPR.2014.372
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chen Q, 2004, LECT NOTES COMPUT SC, V3023, P521
   Donne S, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111858
   Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164
   Li B, 2013, IEEE INT C INT ROBOT, P1301, DOI 10.1109/IROS.2013.6696517
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Moreno D, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P464, DOI 10.1109/3DIMPVT.2012.77
   Orghidan R, 2012, FED CONF COMPUT SCI, P123
   Oyamada Y., 2012, P IEEE ISMAR 2012 WO
   Pilet J., 2006, P 5 IEEE ACM INT S M, P69, DOI 10.1109/ISMAR.2006.297796
   Raposo C, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P342, DOI 10.1109/3DV.2013.52
   Rufli M, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3121, DOI 10.1109/IROS.2008.4650703
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Vezhnevets V., OPENCV CALIBRATION O
   Wong KYK, 2011, IEEE T IMAGE PROCESS, V20, P305, DOI 10.1109/TIP.2010.2063035
   ZHANG Z, 1998, MSRTR9871
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 21
TC 17
Z9 17
U1 1
U2 12
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD APR
PY 2017
VL 17
IS 4
AR 685
DI 10.3390/s17040685
PG 13
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA EU1YU
UT WOS:000400822900026
PM 28346366
OA gold, Green Published, Green Submitted
DA 2022-02-10
ER

PT J
AU Wysong, ML
   Iacona, GD
   Valentine, LE
   Morris, K
   Ritchie, EG
AF Wysong, Michael L.
   Iacona, Gwenllian D.
   Valentine, Leonie E.
   Morris, Keith
   Ritchie, Euan G.
TI On the right track: placement of camera traps on roads improves
   detection of predators and shows non-target impacts of feral cat baiting
SO WILDLIFE RESEARCH
LA English
DT Article
DE apex predator; audio lure; dingo (Canis dingo); macropodid;
   mesopredator; occupancy; poison baiting
ID INVASIVE PREDATORS; SAMPLING DESIGN; CAPTURE RATES; FELIS-CATUS;
   CANIS-LUPUS; OCCUPANCY; DINGO; AUSTRALIA; KANGAROOS; ABUNDANCE
AB WR19175_toc.jpg
C1 [Wysong, Michael L.; Valentine, Leonie E.] Univ Western Australia, Sch Biol Sci, 35 Stirling Highway, Crawley, WA 6009, Australia.
   [Iacona, Gwenllian D.] Univ Queensland, Australian Res Council, Sch Biol Sci, Ctr Excellence Environm Decis, St Lucia, Qld 4072, Australia.
   [Morris, Keith] Bentley Delivery Ctr, Dept Biodivers Conservat & Attract, Biodivers & Conservat Sci, Locked Bag 104, Bentley, WA 6983, Australia.
   [Ritchie, Euan G.] Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, 221 Burwood Highway, Burwood, Vic 3125, Australia.
RP Wysong, ML (corresponding author), Univ Western Australia, Sch Biol Sci, 35 Stirling Highway, Crawley, WA 6009, Australia.
EM mlwysong@gmail.com
RI Valentine, Leonie Ellen/G-9963-2012; Wysong, Michael/H-7992-2013;
   Ritchie, Euan/N-1088-2014
OI Valentine, Leonie Ellen/0000-0003-1479-0755; Iacona,
   Gwenllian/0000-0002-7408-3895; Ritchie, Euan/0000-0003-4410-8868
CR Algar D., 2010, Journal of the Royal Society of Western Australia, V93, P133
   Allen L, 1996, WILDLIFE RES, V23, P197, DOI 10.1071/WR9960197
   Anderson K. A., 2002, MODEL SELECTION MULT
   Ballard G, 2020, WILDLIFE RES, V47, P99, DOI 10.1071/WR18188
   Balme GA, 2009, J WILDLIFE MANAGE, V73, P433, DOI 10.2193/2007-368
   Beard J., 1976, VEGETATION MURCHISON
   Bjornstad O. N., 2009, NCF SPATIAL NONPARAM
   Brook L. A, 2013, THESIS
   Brook LA, 2012, J APPL ECOL, V49, P1278, DOI 10.1111/j.1365-2664.2012.02207.x
   Buckmaster T, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0107788
   Burrows ND, 2003, J ARID ENVIRON, V55, P691, DOI 10.1016/S0140-1963(02)00317-8
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   CAUGHLEY G, 1980, AUST WILDLIFE RES, V7, P1, DOI 10.1071/WR9800001
   Christensen Per E. S., 2013, Ecological Management & Restoration, V14, P47, DOI 10.1111/emr.12025
   Colman NJ, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2013.3094
   Comer S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23495-z
   Creel S, 2008, TRENDS ECOL EVOL, V23, P194, DOI 10.1016/j.tree.2007.12.004
   Cusack JJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126373
   Denes FV, 2015, METHODS ECOL EVOL, V6, P543, DOI 10.1111/2041-210X.12333
   Doherty TS, 2019, CONSERV LETT, V12, DOI 10.1111/conl.12633
   Doherty TS, 2019, MAMMAL REV, V49, P31, DOI 10.1111/mam.12139
   Doherty TS, 2017, MAMMAL REV, V47, P83, DOI 10.1111/mam.12080
   Doherty TS, 2017, CONSERV LETT, V10, P15, DOI 10.1111/conl.12251
   Doherty TS, 2016, P NATL ACAD SCI USA, V113, P11261, DOI 10.1073/pnas.1602480113
   Doherty TS, 2015, BIOL CONSERV, V190, P60, DOI 10.1016/j.biocon.2015.05.013
   Doherty TS, 2015, ECOL MANAG RESTOR, V16, P124, DOI 10.1111/emr.12158
   Doherty TS, 2015, J BIOGEOGR, V42, P964, DOI 10.1111/jbi.12469
   Dubey JP, 2008, J EUKARYOT MICROBIOL, V55, P467, DOI 10.1111/j.1550-7408.2008.00345.x
   Executive Steering Committee for Australian Vegetation Information, 2003, AUSTR VEG ATTR MAN N
   Fancourt BA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0119303
   Fiske I, 2010, UNMARKED MODELS DATA
   Fleming P., 2001, MANAGING IMPACTS DIN
   Fleming PJS, 2006, AUST J EXP AGR, V46, P753, DOI 10.1071/EA06009
   Gause GF., 2019, STRUGGLE EXISTENCE C
   Geary WL, 2019, J APPL ECOL, V56, P1992, DOI 10.1111/1365-2664.13427
   Greenville AC, 2014, OECOLOGIA, V175, P1349, DOI 10.1007/s00442-014-2977-8
   Gu WD, 2004, BIOL CONSERV, V116, P195, DOI 10.1016/S0006-3207(03)00190-3
   Hamilton N.E.I.L, 2013, CONSERV SCI W AUST, V8, P367
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   Hayward MW, 2015, J APPL ECOL, V52, P286, DOI 10.1111/1365-2664.12408
   Heiniger J, 2018, WILDLIFE RES, V45, P518, DOI 10.1071/WR17171
   Hines D.I, 2017, OCCUPANCY ESTIMATION
   Johnston M., 2011, Occasional Papers of the IUCN Species Survival Commission, V42, P182
   Kellner KF, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111436
   Kery M, 2004, BASIC APPL ECOL, V5, P65, DOI 10.1078/1439-1791-00194
   Koch K, 2015, BMC EVOL BIOL, V15, DOI 10.1186/s12862-015-0542-7
   Larrucea ES, 2007, J WILDLIFE MANAGE, V71, P1682, DOI 10.2193/2006-407
   Legge S, 2017, BIOL CONSERV, V206, P293, DOI 10.1016/j.biocon.2016.11.032
   Leo V, 2019, OIKOS, V128, P630, DOI 10.1111/oik.05546
   Letnic M, 2013, OIKOS, V122, P761, DOI 10.1111/j.1600-0706.2012.20425.x
   Letnic M, 2012, BIOL REV, V87, P390, DOI 10.1111/j.1469-185X.2011.00203.x
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2005, ECOLOGY, V86, P1101, DOI 10.1890/04-1060
   MacKenzie DI, 2004, J AGR BIOL ENVIR ST, V9, P300, DOI 10.1198/108571104X3361
   Medina FM, 2011, GLOBAL CHANGE BIOL, V17, P3503, DOI 10.1111/j.1365-2486.2011.02464.x
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
   Meek PD, 2013, AUST MAMMAL, V35, P123, DOI 10.1071/AM12014
   Milakovic B, 2011, J MAMMAL, V92, P568, DOI 10.1644/10-MAMM-A-040.1
   Moseby KE, 2011, BIOL CONSERV, V144, P2863, DOI 10.1016/j.biocon.2011.08.003
   Moseby KE, 2011, WILDLIFE RES, V38, P338, DOI 10.1071/WR10235
   Moseby Katherine E., 2004, Ecological Management & Restoration, V5, P228, DOI 10.1111/j.1442-8903.2004.209-8.x
   Nimmo DG, 2015, J APPL ECOL, V52, P281, DOI 10.1111/1365-2664.12369
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   Pierpaoli M, 2003, MOL ECOL, V12, P2585, DOI 10.1046/j.1365-294X.2003.01939.x
   Pike JR, 1999, WILDLIFE SOC B, V27, P4
   Pople AR, 2000, WILDLIFE RES, V27, P269, DOI 10.1071/WR99030
   Raiter KG, 2018, BIOL CONSERV, V228, P281, DOI 10.1016/j.biocon.2018.10.011
   Read JL, 2015, WILDLIFE RES, V42, P1, DOI 10.1071/WR14193
   Ripple WJ, 2014, SCIENCE, V343, P151, DOI 10.1126/science.1241484
   Ritchie EG, 2012, TRENDS ECOL EVOL, V27, P265, DOI 10.1016/j.tree.2012.01.001
   Ritchie EG, 2009, ECOL LETT, V12, P982, DOI 10.1111/j.1461-0248.2009.01347.x
   Rocha DG, 2016, J ZOOL, V300, P205, DOI 10.1111/jzo.12372
   Schmitz OJ, 2000, AM NAT, V155, P141, DOI 10.1086/303311
   Schuette P, 2013, BIOL CONSERV, V158, P301, DOI 10.1016/j.biocon.2012.08.008
   SHORT J, 1983, AUST WILDLIFE RES, V10, P435
   Sollmann R, 2012, MAMM BIOL, V77, P41, DOI 10.1016/j.mambio.2011.06.011
   Spong G, 2002, BEHAV ECOL SOCIOBIOL, V52, P303, DOI 10.1007/s00265-002-0515-x
   Srbek-Araujo AC, 2013, BIOTA NEOTROP, V13, P51, DOI 10.1590/S1676-06032013000200005
   Stephens D.W., 1986, pi
   Stokeld D, 2015, WILDLIFE RES, V42, P642, DOI 10.1071/WR15083
   Swann DE, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P3
   THOMSON PC, 1992, WILDLIFE RES, V19, P519, DOI 10.1071/WR9920519
   Tille P. J, 2006, SOIL LANDSCAPES W AU
   TOBLER M. W., 2007, CAMERA BASE VERSION
   Torretta E, 2016, ACTA ETHOL, V19, P123, DOI 10.1007/s10211-015-0231-y
   Towerton AL, 2011, WILDLIFE RES, V38, P208, DOI 10.1071/WR10213
   Wallach AD, 2010, ECOL LETT, V13, P1008, DOI 10.1111/j.1461-0248.2010.01492.x
   Wallach AD, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006861
   Wang YW, 2015, BIOL CONSERV, V190, P23, DOI 10.1016/j.biocon.2015.05.007
   Wang YW, 2012, WILDLIFE RES, V39, P611, DOI 10.1071/WR11210
   Woinarski J. C., 2014, ACTION PLAN AUSTR MA
   Woinarski J.C., 2019, CATS AUSTR COMPANION
   Wysong ML, 2020, MOV ECOL, V8, DOI 10.1186/s40462-020-00203-z
   Wysong ML, 2019, J MAMMAL, V100, P410, DOI 10.1093/jmammal/gyz040
   Zuur Alain F., 2009, P1
NR 96
TC 8
Z9 8
U1 2
U2 8
PU CSIRO PUBLISHING
PI CLAYTON
PA UNIPARK, BLDG 1, LEVEL 1, 195 WELLINGTON RD, LOCKED BAG 10, CLAYTON, VIC
   3168, AUSTRALIA
SN 1035-3712
EI 1448-5494
J9 WILDLIFE RES
JI Wildl. Res.
PY 2020
VL 47
IS 8
BP 557
EP 569
DI 10.1071/WR19175
PG 13
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA OE6NL
UT WOS:000580645200005
DA 2022-02-10
ER

PT C
AU Li, Y
   Cai, KJ
AF Li, Yong
   Cai, Kejie
GP IEEE
TI Intelligent Video Surveillance System in factory Based on TLD Algorithm
SO 2017 CHINESE AUTOMATION CONGRESS (CAC)
SE Chinese Automation Congress
LA English
DT Proceedings Paper
CT Chinese Automation Congress (CAC)
CY OCT 20-22, 2017
CL Jinan, PEOPLES R CHINA
SP IEEE, CAA, IEEE Syst Man & Cybernet Soc
DE RaspberryPi; TLD; Image Processing; stereoscopic vision; AGV
AB There are many key device nodes in factory, in order to ensure these devices working in a safe and stable way, we need to monitor the status of this devices in real-time. The system based on the video surveillance and computer vision methods, to monitor real-time status of equipment and give some feedback to central control room automatically. The web camera site obtains the video information and send it to the monitoring terminal. According to the properties and operational modes of the monitoring targets in a different way, the function of web camera sites can be personalized customization. This paper focuses on the detection and tracking algorithm of moving object in intelligent video surveillance. A real-time TLD motion detection and tracking algorithm is adopted. It is proved that the intelligent video monitoring system has good real-time and high accuracy.
C1 [Li, Yong] Chongqing Univ Posts & Telecommun, Minist Educ, Key Lab Ind Internet Things & Intelligent Instrum, Chongqing, Peoples R China.
   [Cai, Kejie] Chongqing Univ Posts & Telecommun, Ind IoT Collaborat Innovat Ctr, Chongqing Educ Commiss, Chongqing, Peoples R China.
RP Li, Y (corresponding author), Chongqing Univ Posts & Telecommun, Minist Educ, Key Lab Ind Internet Things & Intelligent Instrum, Chongqing, Peoples R China.
EM 023liyong@163.com; Jake.cai@outlook.com
FU Chongqing S cience and Technology CommissionNatural Science Foundation
   Project of CQ CSTC [cstc2016jcyja0586]; Chongqing Innovation Project on
   Common Key Technology of Internet of Things Industry
   [cstc2015zdcy-ztzx70008]
FX Thanks to the support from the project by the Chongqing S cience and
   Technology Commission (Grant No. cstc2016jcyja0586), Chongqing
   Innovation Project on Common Key Technology of Internet of Things
   Industry (Grant No. cstc2015zdcy-ztzx70008). The authors also express
   many than ks for the comments from the reviewers that improved the
   paper.
CR Butler D., INT C AC SPEECH SIGN, P349
   Cheng M F, 2006, C COMP GRAPH INT TEC, P367
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Hengstler S., 2006, ACM SENSYS WORKSH DI
   Javed O, 2002, LECT NOTES COMPUT SC, V2353, P343
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Teixeria T., 2006, ACM SENSYS WORKSH DI
   Wang Y, 2012, INTELLIGENT INFORM H, P178
   ZHANG K, 2012, ECCV, P864
NR 11
TC 1
Z9 1
U1 3
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2688-092X
EI 2688-0938
BN 978-1-5386-3524-7
J9 CHIN AUTOM CONGR
PY 2017
BP 5993
EP 5997
PG 5
WC Automation & Control Systems; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Engineering
GA BJ7XU
UT WOS:000427816106013
DA 2022-02-10
ER

PT J
AU Weinstein, BG
AF Weinstein, Ben G.
TI MotionMeerkat: integrating motion video detection and ecological
   monitoring
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE camera traps; computer vision; hummingbirds; plant-animal interactions
AB Human observation is expensive and limits the breadth of data collection. For this reason, remotely placed video cameras are increasingly used to monitor animals. One drawback of field-based video recordings is extensive review time. Computer vision can mitigate this cost and enhance data collection by extracting biological information from images with minimal time investment. MotionMeerkat is a new standalone program that identifies motion events from a video stream. After running a video, the user reviews a folder of candidate motion frames for the target organism. This tool reduces the time needed to review videos and accommodates a variety of inputs. I tested MotionMeerkat using hummingbird-plant videos recorded in a tropical montane forest. To validate the optimal parameter set for finding motion events, I counted hummingbirds observed from direct video review compared to events found in images returned from MotionMeerkat. To show the generality of the approach, MotionMeerkat was tested on a set of terrestrial and underwater videos. To assess the performance of the background subtraction for further image analysis, I hand counted the number of frames with target organisms and compared them to the MotionMeerkat output. MotionMeerkat was highly successful in finding motion events and often reduced the number of frames needed to capture hummingbird visitation by over 90%. Both background approaches effectively found a variety of organisms in ecological videos. I provide general recommendations for parameter settings and extending this approach in the future.
C1 SUNY Stony Brook, Dept Ecol & Evolut, Stony Brook, NY 11794 USA.
RP Weinstein, BG (corresponding author), SUNY Stony Brook, Dept Ecol & Evolut, Stony Brook, NY 11794 USA.
EM bweinste@life.bio.sunysb.edu
OI Weinstein, Ben/0000-0002-2176-7935
FU National Geographic GrantNational Geographic Society [9382-13]; NZ
   government through Ministry for Primary Industries and Land Information
   New Zealand Ocean Survey
FX I would thank Rebecca Justicia and the families of Santa Lucia for their
   support. Field work was assisted by K. Lohman, H. Beck, A. Shankar and
   N. Munoz. Camera review was assisted by L. Ditmar, and the manuscript
   was improved with help from C. Graham and H. Lynch. This project was
   supported by a National Geographic Grant #9382-13. Thanks to A. Rees, R.
   O'Driscoll and N. Moy for sharing videos. The O'Driscoll fisheries video
   data were collected by the New Zealand National Institute of Water and
   Atmospheric Research (NIWA) with funding from the NZ government through
   Ministry for Primary Industries and Land Information New Zealand Ocean
   Survey 2020. This study was greatly improved by comments from two
   anonymous reviewers. The author declares no conflict of interest in this
   work.
CR Maglianesi MA, 2014, ECOLOGY, V95, P3325, DOI 10.1890/13-2261.1
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Boom BJ, 2014, ECOL INFORM, V23, P83, DOI 10.1016/j.ecoinf.2013.10.006
   Bouwmans T, 2014, MACH VISION APPL, V25, P1101, DOI 10.1007/s00138-013-0578-x
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chunmei Qing, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3577, DOI 10.1109/ICIP.2011.6116491
   Crall J. P., 2013, 2013 IEEE WORKSH APP
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   Kalal Z., 2011, PAMI, P1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Mallet D, 2014, FISH RES, V154, P44, DOI 10.1016/j.fishres.2014.01.019
   O'Driscoll RL, 2012, ICES J MAR SCI, V69, P648, DOI 10.1093/icesjms/fss010
   Pennekamp F, 2013, METHODS ECOL EVOL, V4, P483, DOI 10.1111/2041-210X.12036
   Ribic CA, 2012, STUD AVIAN BIOL, P1, DOI 10.1525/california/9780520273139.001.0001
   Rutz C, 2013, METHODS ECOL EVOL, V4, P114, DOI 10.1111/2041-210x.12003
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 19
TC 52
Z9 52
U1 4
U2 50
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD MAR
PY 2015
VL 6
IS 3
BP 357
EP 362
DI 10.1111/2041-210X.12320
PG 6
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA CE2AF
UT WOS:000351613900014
OA Bronze
DA 2022-02-10
ER

PT J
AU Kitzes, J
   Schricker, L
AF Kitzes, Justin
   Schricker, Lauren
TI The Necessity, Promise and Challenge of Automated Biodiversity Surveys
   Comment
SO ENVIRONMENTAL CONSERVATION
LA English
DT Editorial Material
DE acoustic; machine learning; camera trap; citizen science
AB We are in the midst of a transformation in the way that biodiversity is observed on the planet. The approach of direct human observation, combining efforts of both professional and citizen scientists, has recently generated unprecedented amounts of data on species distributions and populations. Within just a few years, however, we believe that these data will be swamped by indirect biodiversity observations that are generated by autonomous sensors and machine learning classification models. In this commentary, we discuss three important elements of this shift towards indirect, technology-driven observations. First, we note that the biodiversity data sets available today cover a very small fraction of all places and times that could potentially be observed, which suggests the necessity of developing new approaches that can gather such data at even larger scales, with lower costs. Second, we highlight existing tools and efforts that are already available today to demonstrate the promise of automated methods to radically increase biodiversity data collection. Finally, we discuss one specific outstanding challenge in automated biodiversity survey methods, which is how to extract useful knowledge from observations that are uncertain in nature. Throughout, we focus on one particular type of biodiversity data - point occurrence records - that are frequently produced by citizen science projects, museum records and systematic biodiversity surveys. As indirect observation methods increase the spatiotemporal scope of these point occurrence records, ecologists and conservation biologists will be better able to predict shifting species distributions, track changes to populations over time and understand the drivers of biodiversity occurrence.
C1 [Kitzes, Justin; Schricker, Lauren] Univ Pittsburgh, Dept Biol Sci, Fifth & Ruskin Ave, Pittsburgh, PA 15260 USA.
RP Kitzes, J (corresponding author), Univ Pittsburgh, Dept Biol Sci, Fifth & Ruskin Ave, Pittsburgh, PA 15260 USA.
EM justin.kitzes@pitt.edu
FU Department of Biological Sciences; Mascaro Center for Sustainable
   Innovation at the University of Pittsburgh; Microsoft; National
   GeographicNational Geographic Society [NGS-55651T-18]
FX This work was supported by the Department of Biological Sciences and the
   Mascaro Center for Sustainable Innovation at the University of
   Pittsburgh, as well as Microsoft and National Geographic under grant
   NGS-55651T-18.
CR Bravo CJC, 2017, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.113
   Buxton RT, 2018, GLOB ECOL CONSERV, V16, DOI 10.1016/j.gecco.2018.e00493
   Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
   iNaturalist, 2019, INATURALIST COMP VIS
   LifeCLEF, 2019, BIRDCLEF 2018 IMAGEC
   Marconi S, 2019, PEERJ, V7, DOI 10.7717/peerj.5843
   Microsoft, 2019, AI EARTH APIS APPL S
   Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
   Towsey M, 2014, ECOL INFORM, V21, P1, DOI 10.1016/j.ecoinf.2014.02.002
   USFWS, 2019, USFWS IND BAT SUMM S
   Zuur Alain F., 2009, P1
NR 15
TC 6
Z9 6
U1 1
U2 12
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 0376-8929
EI 1469-4387
J9 ENVIRON CONSERV
JI Environ. Conserv.
PD DEC
PY 2019
VL 46
IS 4
BP 247
EP 250
DI 10.1017/S0376892919000146
PG 4
WC Biodiversity Conservation; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA KI5OM
UT WOS:000511399300001
DA 2022-02-10
ER

PT C
AU Zhang, RZ
   Zhu, SY
   Fang, T
   Quan, L
AF Zhang, Runze
   Zhu, Siyu
   Fang, Tian
   Quan, Long
GP IEEE
TI Distributed Very Large Scale Bundle Adjustment by Global Camera
   Consensus
SO 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)
SE IEEE International Conference on Computer Vision
LA English
DT Proceedings Paper
CT 16th IEEE International Conference on Computer Vision (ICCV)
CY OCT 22-29, 2017
CL Venice, ITALY
SP IEEE, IEEE Comp Soc
ID PROXIMAL POINT ALGORITHM
AB The increasing scale of Structure-from-Motion is fundamentally limited by the conventional optimization framework for the all-in-one global bundle adjustment. In this paper, we propose a distributed approach to coping with this global bundle adjustment for very large scale Structure-from-Motion computation. First, we derive the distributed formulation from the classical optimization algorithm ADMM, Alternating Direction Method of Multipliers, based on the global camera consensus. Then, we analyze the conditions under which the convergence of this distributed optimization would be guaranteed. In particular, we adopt over-relaxation and self-adaption schemes to improve the convergence rate. After that, we propose to split the large scale camera-point visibility graph in order to reduce the communication overheads of the distributed computing. The experiments on both public large scale SfM data-sets and our very large scale aerial photo sets demonstrate that the proposed distributed method clearly outperforms the state-of-the-art method in efficiency and accuracy.
C1 [Zhang, Runze; Zhu, Siyu; Fang, Tian; Quan, Long] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
RP Zhang, RZ (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM rzhangaj@cse.ust.hk; szhu@cse.ust.hk; fangtian@altizure.com;
   quan@cse.ust.hk
RI Zhang, Runze/N-3486-2015; Zhang, Runze/N-7568-2019
OI Zhang, Runze/0000-0001-9698-0178; Zhang, Runze/0000-0001-9698-0178
FU Hong Kong RGCHong Kong Research Grants Council [16208614, T22-603/15N];
   Hong Kong ITC [P-SKL12EG02]; China 973 programNational Basic Research
   Program of China [2012CB316300]
FX This work is supported by Hong Kong RGC 16208614, T22-603/15N, Hong Kong
   ITC P-SKL12EG02, and China 973 program, 2012CB316300.
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9_3
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED, V23
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Carlone Luca, 2014, P BRIT MACH VIS C
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552
   Chum Ondrej, 2015, C COMP VIS PATT REC
   Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   ECKSTEIN J, 1994, J OPTIMIZ THEORY APP, V80, P39, DOI 10.1007/BF02196592
   Eckstein J., 1998, INFORMS Journal on Computing, V10, P218, DOI 10.1287/ijoc.10.2.218
   Eriksson A, 2016, PROC CVPR IEEE, P1754, DOI 10.1109/CVPR.2016.194
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   FUKUSHIMA M, 1981, INT J SYST SCI, V12, P989, DOI 10.1080/00207728108963798
   He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434
   Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949
   Kaplan A, 1998, J GLOBAL OPTIM, V13, P389, DOI 10.1023/A:1008321423879
   Klingner B, 2013, IEEE I CONF COMP VIS, P953, DOI 10.1109/ICCV.2013.122
   Kushal A, 2012, PROC CVPR IEEE, P1442, DOI 10.1109/CVPR.2012.6247832
   Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44
   Ni K, 2007, IEEE I CONF COMP VIS, P2009
   Ni K, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P144, DOI 10.1109/3DIMPVT.2012.47
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shen TW, 2016, LECT NOTES COMPUT SC, V9907, P139, DOI 10.1007/978-3-319-46487-9_9
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Snavely N, 2008, PROC CVPR IEEE, P2617
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Sra S., 2012, ADV NEURAL INFORM PR, P539
   Triggs B., 1999, LECT NOTES COMPUTER, P298, DOI DOI 10.1007/3-540-44480-7_21
   Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5
   Yong-Dian Jian, 2012, Outdoor and Large-Scale Real-World Scene Analysis. 15th International Workshop on Theoretical Foundations of Computer Vision. Revised Selected Papers, P131, DOI 10.1007/978-3-642-34091-8_6
   Zhang RZ, 2015, IEEE I CONF COMP VIS, P2084, DOI 10.1109/ICCV.2015.241
   Zhu SY, 2014, PROC CVPR IEEE, P3938, DOI 10.1109/CVPR.2014.503
NR 34
TC 24
Z9 25
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1550-5499
BN 978-1-5386-1032-9
J9 IEEE I CONF COMP VIS
PY 2017
BP 29
EP 38
DI 10.1109/ICCV.2017.13
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4TW
UT WOS:000425498400004
DA 2022-02-10
ER

PT J
AU Chen, JH
   Little, JJ
AF Chen, Jianhui
   Little, James J.
TI Where should cameras look at soccer games: Improving smoothness using
   the overlapped hidden Markov model
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Camera planning; Camera calibration; Hidden Markov model; Soccer games
ID TRACKING
AB Automatic camera planning for sports has been a long term goal in computer vision and machine learning. In this paper, we study camera planning for soccer games using pan, tilt and zoom (PTZ) cameras. Two important problems have been addressed. First, we propose the Overlapped Hidden Markov Model (OHMM) method which effectively optimizes the camera trajectory in overlapped local windows. The OHMM method significantly improves the smoothness of the camera planning by optimizing the camera trajectory in the temporal space, resulting in much more natural camera movements present in real broadcasts. We also propose CalibMe which is a highly automatic camera calibration method for soccer games. CalibMe enables users to collect large amounts of training data for learning algorithms. The precision of CalibMe is evaluated on a motion blur affected sequence and outperforms several strong existing methods. The performance of the OHMM method is extensively evaluated on both synthetic and real data. It outperforms the state-of-the-art algorithms in terms of smoothness without sacrificing accuracy. (C) 2016 Elsevier Inc. All rights reserved.
C1 [Chen, Jianhui; Little, James J.] Univ British Columbia, Dept Comp Sci, 2366 Main Mall, Vancouver, BC, Canada.
RP Chen, JH (corresponding author), Univ British Columbia, Dept Comp Sci, 2366 Main Mall, Vancouver, BC, Canada.
EM jhchen14@cs.ubc.ca; little@cs.ubc.ca
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)CGIAR; Disney
   Research
FX This work was funded partially by the Natural Sciences and Engineering
   Research Council of Canada and a Research Grant from Disney Research.
CR Ariki Y, 2006, IEEE INT SYM MULTIM, P851
   Bishop C.M.., 2006, PATTERN RECOGN
   Bloit J., 2008, IEEE INT C AC SPEECH
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Bu J, 2011, IEEE INT CON MULTI
   Carr P., 2012, EUR C COMP VIS
   Carr P., 2012, IEEE WORKSH APPL COM
   Chen J., 2015, IEEE WINT C APPL COM
   Chen J., 2014, WORKSH AAAI C ART IN
   Chen J, 2016, IEEE IPCCC
   Dantone M., 2012, IEEE C COMP VIS PATT
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Donoser M., 2006, IEEE C COMP VIS PATT
   Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620
   Fanello S., 2014, IEEE C COMP VIS PATT
   Farin D., 2003, Proceedings of the SPIE - The International Society for Optical Engineering, V5307, P80, DOI 10.1117/12.526813
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963
   Ghanem B., 2012, IEEE INT C AC SPEECH
   Gupta A., 2011, CAN C COMP ROB VIS
   Hess R., 2007, COMPUTER VISION PATT
   Hilton A, 2011, IEEE T BROADCAST, V57, P462, DOI 10.1109/TBC.2011.2131870
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Homayounfar N., 2016, ARXIV160402715
   Irani R, 2015, IEEE COMPUT SOC CONF
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Liu SM, 2014, IEEE CONF COMPU INTE
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu WL, 2013, IEEE T PATTERN ANAL, V35, P1704, DOI 10.1109/TPAMI.2012.242
   Lu Y., 2015, IEEE INT C COMP VIS
   Maksai A., 2016, IEEE C COMP VIS PATT
   Narasimhan M., 2006, INT C MACH LEARN
   Okuma K., 2004, AS C COMP VIS
   Patraucean V., 2012, EUR C COMP VIS
   Pettersen S.A., 2014, ACM MULT SYST C
   Puwein J., 2012, IEEE WORKSH APPL COM
   Puwein J., 2011, IEEE WORKSH APPL COM
   Ramanathan V., 2016, COMPUTER VISION PATT
   Rosten E., 2005, INT C COMP VIS
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047
   Schaffalitzky F., 2002, EUR C COMP VIS
   Shah R., 2015, IEEE WINT C APPL COM
   Sutskever I., 2014, ADV NEURAL INFORM PR
   Thomas G, 2007, J REAL-TIME IMAGE PR, V2, P117, DOI 10.1007/s11554-007-0041-1
   Tomasi Carlo, 1991, TECH REP
   Vir Singh D., 2010, AUTOMATIC VIRTUAL CA
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang XC, 2014, COMPUT VIS IMAGE UND, V119, P102, DOI 10.1016/j.cviu.2013.11.010
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
NR 51
TC 9
Z9 10
U1 1
U2 10
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD JUN
PY 2017
VL 159
SI SI
BP 59
EP 73
DI 10.1016/j.cviu.2016.10.017
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA EZ0UR
UT WOS:000404422100005
DA 2022-02-10
ER

PT C
AU Xu, ZD
   Sinha, S
   Harshil, SS
   Ramachandran, U
AF Xu, Zhuangdi
   Sinha, Sayan
   Harshil, Shah S.
   Ramachandran, Umakishore
GP ACM
TI Space-Time Vehicle Tracking at the Edge of the Network
SO PROCEEDINGS OF THE 2019 WORKSHOP ON HOT TOPICS IN VIDEO ANALYTICS AND
   INTELLIGENT EDGES (HOTEDGEVIDEO '19)
LA English
DT Proceedings Paper
CT ACM Workshop on Hot Topics in Video Analytics and Intelligent Edges
   (HotEdgeVideo)
CY OCT 21, 2019
CL Los Cabos, MEXICO
SP Assoc Comp Machinery, ACM SIGMOBILE
DE Large-scale camera system; Cross-camera vehicle tracking; Edge computing
AB While a large number of cameras have been deployed on campus, governments and restricted areas to protect the safety of residents and their properties, manually searching for the track of a suspicious human or vehicle is painful and prone to errors. An intelligent camera surveillance system is crucial to make the usage of the large-scale camera system efficient and smooth. In this paper, we present our Space-Time Vehicle Tracking system (STVT) which targets to track all vehicles over time and store the result as trajectories of vehicles in a graph database. We are then able to respond to queries such as Where did this suspicious vehicle come from? and Where is it headed to? directly from the stored trajectories. STVT makes use of computer vision techniques including Yolo v2, Dlib's correlation tracker, and adaptive histogram to transform each video stream into a stream of vehicle detection events. A publish-subscribe-based messaging system is built above the camera networks to enable communicating vehicle detection events between cameras. A graph database is used to store the vehicle detections and connect them into trajectories of vehicles. We have a built a prototype of STVT and are currently assessing its effectiveness with a small subset of cameras. Specifically, we process video streams from 7 Georgia Tech's on-campus street cameras with STVT and identify new challenges and future works in a real-world deployment.
C1 [Xu, Zhuangdi; Harshil, Shah S.; Ramachandran, Umakishore] Georgia Tech, Atlanta, GA 30332 USA.
   [Sinha, Sayan] Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.
RP Xu, ZD (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.
EM xzdandy@gatech.edu; sayan.sinha@iitkgp.ac.in;
   harshilshah4251@gatech.edu; rama@gatech.edu
CR Danelljan M., 2014, BRIT MACH VIS C
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hsieh K, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P269
   JanusGraph Authors, 2019, JANUSGRAPH DISTR OP
   Kafka Authors, 2019, KAFK DISTR STREAM PL
   Kang D, 2017, PROC VLDB ENDOW, V10, P1586, DOI 10.14778/3137628.3137664
   Liu X., 2016, SCI PROGRAM, V2016, P1, DOI DOI 10.1080/15384101.2016.1249545
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Shu CF, 2005, AVSS 2005: Advanced Video and Signal Based Surveillance, Proceedings, P318
   Sterckval Sam, 2019, NVIDIA JETSON NANO Q
   Tang Z, 2018, IEEE COMPUT SOC CONF, P108, DOI 10.1109/CVPRW.2018.00022
   Xu M., 2019, ARXIV190412342
   Xu ZD, 2018, DEBS'18: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL CONFERENCE ON DISTRIBUTED AND EVENT-BASED SYSTEMS, P124, DOI 10.1145/3210284.3210291
NR 13
TC 2
Z9 2
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-6928-2
PY 2019
BP 15
EP 20
DI 10.1145/3349614.3356025
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BO9XI
UT WOS:000532790000003
DA 2022-02-10
ER

PT J
AU Yang, XC
   Li, H
   Huang, T
   Zhai, XM
   Wang, FL
   Wang, C
AF Yang, Xincong
   Li, Heng
   Huang, Ting
   Zhai, Ximei
   Wang, Fenglai
   Wang, Chen
TI Computer-Aided Optimization of Surveillance Cameras Placement on
   Construction Sites
SO COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING
LA English
DT Article
ID DIRECTIONAL SENSOR NETWORKS; CRACK DETECTION; DAMAGE DETECTION; NSGA-II;
   COVERAGE; TIME; TRACKING; MODEL; IDENTIFICATION; ALGORITHM
AB Surveillance system is becoming an indispensable system on construction sites with the fast development of computer vision techniques, thus an optimal placement of surveillance cameras is essential for the successful performance of this system. However, to develop effective models and solutions for large-scale camera placement still remain as opening challenges. Therefore, this study investigated two fundamental placement problems and proposed a multiobjective placement problem, where the maximum-coverage problem is to monitor the construction layout as much as possible with a limited budget; the minimum-cost problem is to minimize the cost given a layout required to be fully covered; and the multiobjective problem is to identify the Pareto fronts of cost and coverage ratio of the system. To solve these problems, the objective space and search space were discretized, and the deterministic and heuristic approaches were revised and developed to provide effective solutions. Finally, experiments in a practical project in Hong Kong were conducted to verify the sufficiency of the developed algorithms and findings revealed potential implementations in many scenarios.
C1 [Yang, Xincong; Wang, Fenglai] Harbin Inst Technol, Sch Civil Engn, Harbin, Heilongjiang, Peoples R China.
   [Li, Heng; Huang, Ting; Zhai, Ximei] Hong Kong Polytech Univ, Dept Bldg & Real Estate, Kowloon, Hong Kong, Peoples R China.
   [Wang, Chen] Huaqiao Univ, Coll Civil Engn, Intelligence & Automat Construct Fujian Prov High, Xiamen 361021, Peoples R China.
RP Wang, C (corresponding author), Huaqiao Univ, Coll Civil Engn, Intelligence & Automat Construct Fujian Prov High, Xiamen 361021, Peoples R China.
EM wch@hqu.edu.cn
RI Wang, Chen/F-1586-2010; Li, Heng/B-2821-2015
OI Wang, Chen/0000-0001-7892-3575; Li, Heng/0000-0002-3187-9041; Yang,
   Xincong/0000-0002-7292-9324
CR Ai J, 2006, J COMB OPTIM, V11, P21, DOI 10.1007/s10878-006-5975-x
   Albahri A. H., 2016, P INT S AUT ROB CONS, P1
   Altahir AA, 2017, IEEE SENSOR LETT, V1, DOI 10.1109/LSENS.2017.2758371
   Azar ER, 2012, AUTOMAT CONSTR, V24, P194, DOI 10.1016/j.autcon.2012.03.003
   Bodor R, 2007, J INTELL ROBOT SYST, V50, P257, DOI 10.1007/s10846-007-9164-7
   Brilakis I, 2011, ADV ENG INFORM, V25, P713, DOI 10.1016/j.aei.2011.01.003
   Bugler M, 2017, COMPUT-AIDED CIV INF, V32, P107, DOI 10.1111/mice.12235
   Cha YJ, 2018, COMPUT-AIDED CIV INF, V33, P731, DOI 10.1111/mice.12334
   Cha YJ, 2017, COMPUT-AIDED CIV INF, V32, P361, DOI 10.1111/mice.12263
   Chakrabarty K., 2009, IEEE T COMPUT, V51, P1448
   Chen FC, 2017, COMPUT-AIDED CIV INF, V32, P271, DOI 10.1111/mice.12256
   Chen XF, 2016, COMPUT-AIDED CIV INF, V31, P229, DOI 10.1111/mice.12163
   Cheng T, 2013, AUTOMAT CONSTR, V34, P3, DOI 10.1016/j.autcon.2012.10.017
   Cheng T, 2013, AUTOMAT CONSTR, V29, P24, DOI 10.1016/j.autcon.2012.08.003
   Chi S, 2012, J CONSTR ENG M, V138, P341, DOI 10.1061/(ASCE)CO.1943-7862.0000438
   Chi S, 2011, COMPUT-AIDED CIV INF, V26, P368, DOI 10.1111/j.1467-8667.2010.00690.x
   Chi S, 2009, COMPUT-AIDED CIV INF, V24, P199, DOI 10.1111/j.1467-8667.2008.00580.x
   Chiu-Kuo Liang, 2011, Proceedings of the 2011 IEEE Asia-Pacific Services Computing Conference (APSCC), P377, DOI 10.1109/APSCC.2011.41
   Contreras M, 2017, COMPUT ELECTRON AGR, V135, P208, DOI 10.1016/j.compag.2017.02.005
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Debaque B, 2009, FUSION: 2009 12TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4, P1730
   Dumitrescu A, 2012, COMP GEOM-THEOR APPL, V45, P326, DOI 10.1016/j.comgeo.2012.02.001
   Erdem UM, 2006, COMPUT VIS IMAGE UND, V103, P156, DOI 10.1016/j.cviu.2006.06.005
   Fang Q, 2018, AUTOMAT CONSTR, V85, P1, DOI 10.1016/j.autcon.2017.09.018
   Fleishman S, 2000, COMPUT GRAPH FORUM, V19, P101, DOI 10.1111/1467-8659.00447
   Golparvar-Fard M, 2013, ADV ENG INFORM, V27, P652, DOI 10.1016/j.aei.2013.09.001
   Gong J, 2011, ADV ENG INFORM, V25, P771, DOI 10.1016/j.aei.2011.06.002
   Gong J, 2010, J COMPUT CIVIL ENG, V24, P252, DOI 10.1061/(ASCE)CP.1943-5487.0000027
   Gonzalez-Banos H., 2001, P 17 ANN S COMP GEOM, P232, DOI [10.1145/378583.378674, DOI 10.1145/378583.378674]
   Gualdi G, 2011, EURASIP J IMAGE VIDE, DOI 10.1155/2011/684819
   Hamledari H, 2017, AUTOMAT CONSTR, V74, P78, DOI 10.1016/j.autcon.2016.11.009
   Han S, 2013, AUTOMAT CONSTR, V35, P131, DOI 10.1016/j.autcon.2013.05.001
   Han S, 2013, J COMPUT CIVIL ENG, V27, P635, DOI 10.1061/(ASCE)CP.1943-5487.0000279
   Hu ZZ, 2009, COMPUT-AIDED CIV INF, V24, P385, DOI 10.1111/j.1467-8667.2009.00598.x
   Huang HP, 2011, ADV ENG INFORM, V25, P4, DOI 10.1016/j.aei.2010.05.002
   Jensen MT, 2003, IEEE T EVOLUT COMPUT, V7, P503, DOI 10.1109/TEVC.2003.817234
   Kong XX, 2018, COMPUT-AIDED CIV INF, V33, P783, DOI 10.1111/mice.12353
   Kumar P. P., 1993, CCCG. Proceedings of the Fifth Canadian Conference on Computational Geometry, P91
   LEE DT, 1986, IEEE T INFORM THEORY, V32, P276, DOI 10.1109/TIT.1986.1057165
   Li H, 2017, AUTOMAT CONSTR, V81, P328, DOI 10.1016/j.autcon.2017.04.007
   Li XY, 2003, IEEE T COMPUT, V52, P753, DOI 10.1109/TC.2003.1204831
   Ma HD, 2005, LECT NOTES COMPUT SC, V3794, P721
   Ma HD, 2009, IEEE INFOCOM SER, P2791, DOI 10.1109/INFCOM.2009.5062233
   Mak CL, 2006, COMPUT-AIDED CIV INF, V21, P120, DOI 10.1111/j.1467-8667.2005.00418.x
   Malinovskiy Y, 2009, COMPUT-AIDED CIV INF, V24, P157, DOI 10.1111/j.1467-8667.2008.00578.x
   Mirchandani PB, 2010, COMPUT-AIDED CIV INF, V25, P89, DOI 10.1111/j.1467-8667.2009.00623.x
   Morsly Y, 2012, IEEE SENS J, V12, P1402, DOI 10.1109/JSEN.2011.2170833
   Murray AT, 2007, COMPUT ENVIRON URBAN, V31, P133, DOI 10.1016/j.compenvurbsys.2006.06.002
   Oh BK, 2017, COMPUT-AIDED CIV INF, V32, P34, DOI 10.1111/mice.12229
   Olague G, 2002, PATTERN RECOGN, V35, P927, DOI 10.1016/S0031-3203(01)00076-0
   Park HS, 2007, COMPUT-AIDED CIV INF, V22, P19, DOI 10.1111/j.1467-8667.2006.00466.x
   Park K, 2018, COMPUT-AIDED CIV INF, V33, P51, DOI 10.1111/mice.12312
   Park MW, 2012, J COMPUT CIVIL ENG, V26, P541, DOI 10.1061/(ASCE)CP.1943-5487.0000168
   Park MW, 2012, AUTOMAT CONSTR, V28, P15, DOI 10.1016/j.autcon.2012.06.001
   Rabie T, 2005, COMPUT-AIDED CIV INF, V20, P231, DOI 10.1111/j.1467-8667.2005.00390
   Ray SJ, 2012, ADV ENG INFORM, V26, P439, DOI 10.1016/j.aei.2012.02.011
   Salem A, 2011, PR ELECTROMAGN RES S, P771
   Seo J, 2015, ADV ENG INFORM, V29, P239, DOI 10.1016/j.aei.2015.02.001
   Sharif MM, 2017, COMPUT-AIDED CIV INF, V32, P893, DOI 10.1111/mice.12306
   Slijepcevic S, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-10, CONFERENCE RECORD, P472, DOI 10.1109/ICC.2001.936985
   Sun C, 2003, COMPUT-AIDED CIV INF, V18, P161, DOI 10.1111/1467-8667.00307
   Tao D, 2006, LECT NOTES COMPUT SC, V4325, P256
   Teizer J, 2009, ADV ENG INFORM, V23, P452, DOI 10.1016/j.aei.2009.06.011
   Urrutia J, 2000, HANDBOOK OF COMPUTATIONAL GEOMETRY, P973, DOI 10.1016/B978-044482537-7/50023-1
   van der Vlies AE, 2009, PSYCHOL MED, V39, P1907, DOI 10.1017/S0033291709005492
   Yabuta K, 2008, IEEE INT SYMP CIRC S, P2114
   Yang J, 2010, ADV ENG INFORM, V24, P428, DOI 10.1016/j.aei.2010.06.008
   Yang XC, 2017, ADV ENG INFORM, V34, P36, DOI 10.1016/j.aei.2017.09.003
   Yang XC, 2017, FRONT ENG MANAG, V4, P84, DOI 10.15302/J-FEM-2017004
   Yeum CM, 2015, COMPUT-AIDED CIV INF, V30, P759, DOI 10.1111/mice.12141
   Zhang A, 2017, COMPUT-AIDED CIV INF, V32, P805, DOI 10.1111/mice.12297
   Zhang H., 2005, WIREL AD HOC SENS NE, V1, P89, DOI [DOI 10.1201/9780203323687.CH28, DOI 10.1155/2012/720734]
   Zhu ZH, 2016, COMPUT-AIDED CIV INF, V31, P936, DOI 10.1111/mice.12230
NR 73
TC 13
Z9 13
U1 3
U2 31
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1093-9687
EI 1467-8667
J9 COMPUT-AIDED CIV INF
JI Comput.-Aided Civil Infrastruct. Eng.
PD DEC
PY 2018
VL 33
IS 12
BP 1110
EP 1126
DI 10.1111/mice.12385
PG 17
WC Computer Science, Interdisciplinary Applications; Construction &
   Building Technology; Engineering, Civil; Transportation Science &
   Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Construction & Building Technology; Engineering;
   Transportation
GA HA1AW
UT WOS:000449944800006
DA 2022-02-10
ER

PT C
AU Lewis, JS
   Dhar, NK
   Elizondo, LA
   Dat, R
AF Lewis, Jay S.
   Dhar, Nibir K.
   Elizondo, Lee A.
   Dat, Ravi
BE LeVan, PD
   Sood, AK
   Wijewarnasuriya, P
   DSouza, AI
TI Advanced EO/IR Technologies at DARPA/MTO
SO INFRARED SENSORS, DEVICES, AND APPLICATIONS V
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Infrared Sensors, Devices, and Applications V
CY AUG 12-13, 2015
CL San Diego, CA
SP SPIE
DE Imaging; Infrared; Focal Plane Array; Gigapixel; Fused; Multiband; Joule
   Thomson Cooling
ID FABRICATION; DETECTOR
AB In this paper, we review selected imaging and related technology development programs in the Defense Advanced Research Projects Agency (DARPA) Microsystems Technologies Office (MTO). An overview is presented for the evolution of Joule-Thomson (J-T) micro-cryogenic cooler (MCC) technology. The initial design of a system on a chip method is shown for these micro-coolers to be used in conjunction with high operating temperature mid-wave infrared (MWIR) and long-wave infrared (LWIR) focal plane arrays. For the reflective visible band, results are shown for a gigapixel monocentric multi-scale camera design to solve the scaling issues for high pixel count and wide field of view. Lastly, we discuss two different approaches to multiband imaging and the potential advantages of this technology for the enhanced detection, recognition, and identification of targets.
C1 [Lewis, Jay S.] Def Adv Res Project Agcy, Microsyst Technol Off, Arlington, VA 22203 USA.
   [Dhar, Nibir K.] US Army Night Vis & Elect Sensors Directorate, Ft Belvoir, VA 22060 USA.
   [Elizondo, Lee A.; Dat, Ravi] Booz Allen Hamilton, Arlington, VA 22203 USA.
RP Lewis, JS (corresponding author), Def Adv Res Project Agcy, Microsyst Technol Off, 675 North Randolph St, Arlington, VA 22203 USA.
CR ASHLEY T, 1985, ELECTRON LETT, V21, P451, DOI 10.1049/el:19850321
   Bangs J. W., 2010, M MIL SENS S MSS ORL
   Bradley P., 2009, CRYOCOOLERS, V15, P425
   Brady DJ, 2012, NATURE, V486, P386, DOI 10.1038/nature11150
   Brady DJ, 2009, OPT EXPRESS, V17, P10659, DOI 10.1364/OE.17.010659
   Brady David J., 2013, P SOC PHOTO-OPT INS, V8657
   Dhar NK, 2012, PROC SPIE, V8353, DOI 10.1117/12.923682
   Dhar Nibir K., 2013, P SOC PHOTO-OPT INS, V8868
   Gautam N., 2013, INFRARED PHYS TECHNO
   Gautam N, 2012, APPL PHYS LETT, V101, DOI 10.1063/1.4733660
   Knowles P., 2011, P SOC PHOTO-OPT INS, V8185
   Lewis R, 2013, CRYOGENICS, V54, P37, DOI 10.1016/j.cryogenics.2012.12.004
   Lewis R, 2013, SENSOR ACTUAT A-PHYS, V190, P84, DOI 10.1016/j.sna.2012.11.008
   Li Chuan, 2010, P SOC PHOTO-OPT INS, V7660
   Maimon S, 2006, APPL PHYS LETT, V89, DOI 10.1063/1.2360235
   Marks D., 2010, OSA TOP M IM SYST
   Marks DL, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.8.083202
   Marks DL, 2011, OPT ENG, V50, DOI 10.1117/1.3554389
   Melkonian Leon, 2010, P SOC PHOTO-OPT INS, V7660
   Schaake H. F., 2010, P SOC PHOTO-OPT INS, V7608
   Sharifi H, 2013, PROC SPIE, V8704, DOI 10.1117/12.2015083
   Smith KD, 2012, PROC SPIE, V8353, DOI 10.1117/12.921480
   Son HS, 2011, OPT EXPRESS, V19, P16132, DOI 10.1364/OE.19.016132
   Wang Y. D., 2012, MEMS 2012
   Wang YD, 2013, J MICROELECTROMECH S, V22, P244, DOI 10.1109/JMEMS.2012.2227461
NR 25
TC 1
Z9 1
U1 0
U2 9
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-62841-775-3
J9 PROC SPIE
PY 2015
VL 9609
AR 960902
DI 10.1117/12.2192887
PG 10
WC Instruments & Instrumentation; Remote Sensing; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Instruments & Instrumentation; Remote Sensing; Optics
GA BE0OW
UT WOS:000366502300001
DA 2022-02-10
ER

PT J
AU Li, JW
   Ma, AJ
   Yuen, PC
AF Li, Jiawei
   Ma, Andy J.
   Yuen, Pong C.
TI Semi-supervised Region Metric Learning for Person Re-identification
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Person re-identification; Semi-supervised learning; Imbalanced unlabeled
   data
AB In large-scale camera networks, label information for person re-identification is usually not available under a large amount of cameras due to expensive human labor efforts. Semi-supervised learning could be employed to train a discriminative classifier by using unlabeled data and unmatched image pairs (negatives) generated from non-overlapping camera views, but existing methods suffer from the problem of imbalanced unlabeled data. In this context, this paper proposes a novel semi-supervised region metric learning method to improve person re-identification performance under imbalanced unlabeled data. Firstly, instead of seeking for matched image pairs (positives) from the unlabeled data, we propose to estimate positive neighbors by label propagation with cross person score distribution alignment. Secondly, multiple positive regions are generated using sets of positive neighbors to learn a discriminative region-to-point metric. Experimental results demonstrate that the superiority of the proposed method over existing unsupervised, semi-supervised and person re-identification methods.
C1 [Li, Jiawei; Ma, Andy J.; Yuen, Pong C.] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Ma, Andy J.] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
RP Yuen, PC (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM jwli@comp.hkbu.edu.hk; majh8@mail.sysu.edu.cn; pcyuen@comp.hkbu.edu.hk
RI Jinhua, Andy/Y-9408-2019
FU Hong Kong RGC General Research Fund [HKBU 212313, HKBU 12202514]; SYSU
   Research Fund [67000-18821116]
FX This work is partially supported by Hong Kong RGC General Research Fund
   HKBU 212313, HKBU 12202514 and SYSU Research Fund 67000-18821116. The
   authors would like to thank the editor and reviewers for their helpful
   comments which improve the quality of this paper.
CR Ahmed E., 2015, IEEE C COMP VIS PATT
   Bootkrajang Jakramate, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P143, DOI 10.1007/978-3-642-33460-3_15
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen Y.-T., 2017, IEEE T POWER ELECTR, V99, P1
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Cheng D., 2016, IEEE C COMP VIS PATT
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Figueira D, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P111, DOI 10.1109/AVSS.2013.6636625
   Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Gheissari N., 2006, P 2006 IEEE COMP SOC, P1528, DOI DOI 10.1109/CVPR.2006.223
   Gray Douglas, 2007, 10 IEEE INT WORKSH P
   Gu SQ, 2017, IEEE INT SYMP ELEC
   Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56
   Jing XY, 2015, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2015.7298669
   Kodirov E., 2016, EUR C COMP VIS
   Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kumar KCA, 2013, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2013.250
   Kviatkovsky I, 2013, IEEE T PATTERN ANAL, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Lan XY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2481325
   Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156
   Lee W. S., 2003, P 20 INT C MACH LEAR, P448, DOI DOI 10.1016/J.TCS.2005.09.007
   Li FQ, 2014, NEW REV HYPERMEDIA M, V20, P5, DOI 10.1080/13614568.2013.846416
   Li S., 2011, P 22 INT JOINT C ART, P1
   Li W., 2012, P AS C COMP VIS
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li X., 2003, IJCAI, P587
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin J, 2017, IEEE PAC RIM CONF CO
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Liu X, 2014, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2014.454
   Liu Z., 2017, THE IEEE INTERNATION
   Ma A. J., 2014, P AS C COMP VIS
   Ma AJ, 2015, IEEE T IMAGE PROCESS, V24, P1599, DOI 10.1109/TIP.2015.2395715
   Ma B., 2012, COMPUTER VISION ECCV
   Mahmood A, 2014, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2014.23
   Matsukawa T., 2016, IEEE C COMP VIS PATT
   Peng P., 2016, IEEE C COMP VIS PATT
   Prosser B., 2010, P BMVC, DOI DOI 10.5244/C.24.21
   Roth PM, 2014, ADV COMPUT VIS PATT, P247, DOI 10.1007/978-1-4471-6296-4_12
   Sun YS, 2017, IEEE ICC
   Xie J., 2011, ACT LEARN EXP DES WO, P85
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550
   Ye M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1239, DOI 10.1145/2733373.2806326
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu H. X, 2017, IEEE INT C COMP VIS
   Zhao R., 2013, IEEE C COMP VIS PATT
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng WS, 2009, BRIT MACH VIS C
   Zhu PF, 2013, IEEE I CONF COMP VIS, P2664, DOI 10.1109/ICCV.2013.331
NR 55
TC 21
Z9 21
U1 1
U2 15
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD AUG
PY 2018
VL 126
IS 8
BP 855
EP 874
DI 10.1007/s11263-018-1075-5
PG 20
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GL5ZB
UT WOS:000437253500004
DA 2022-02-10
ER

PT J
AU Marmol, A
   Banach, A
   Peynot, T
AF Marmol, Andres
   Banach, Artur
   Peynot, Thierry
TI Dense-ArthroSLAM: Dense Intra-Articular 3-D Reconstruction With Robust
   Localization Prior for Arthroscopy
SO IEEE ROBOTICS AND AUTOMATION LETTERS
LA English
DT Article
DE Medical robots and systems; computer vision for medical robotics; SLAM
ID MINIMALLY INVASIVE SURGERY; SLAM
AB Arthroscopy is a minimally invasive surgery that imposes great physical and mental challenges to surgeons. Extensive experience is required to safely navigate camera and instruments in narrow spaces of human joints. Robust camera localization as well as a detailed reconstruction of the anatomy can benefit surgeons and would be essential for future robotic assistants. Our existing simultaneous localization and mapping (SLAM) system provides a robust, at-scale camera localization and a sparse map. However, a denser map is required to be of clinical relevance. In this latter, we propose a new system that combines the robust localizer with a keyframe selection strategy and a batch multiview stereo (MVS) for three-dimensional reconstruction. Tissues are reconstructed at scale, accurately and densely even under challenging arthroscopic conditions. The consistency of our system is verified in tests with synthetic noise and several keyframing strategies. Nine experiments were performed in phantom and three cadavers including various imaging conditions, camera settings, and scope motions. Our system reconstructed surfaces of more than 12 cm(2) with a root mean square error of no more than 0.5 mm. In comparison, monocular state-of-the-art SLAMfeature-based (ORBSLAM) and direct (LSDSLAM) methods commonly failed to track more than 20% of any camera motion and, in the few successful cases, yielded much larger estimation errors.
C1 [Marmol, Andres; Banach, Artur; Peynot, Thierry] Australian Ctr Robot Vis, Brisbane, Qld 4000, Australia.
   [Marmol, Andres; Banach, Artur; Peynot, Thierry] Queensland Univ Technol, Brisbane, Qld 4000, Australia.
RP Marmol, A (corresponding author), Australian Ctr Robot Vis, Brisbane, Qld 4000, Australia.
EM andres.marmolvelez@qut.edu.au; artur.banach@qut.edu.au;
   t.peynot@qut.edu.au
OI Marmol, Andres/0000-0002-8925-6269; Banach, Artur/0000-0003-2622-3301
FU Australian Research Council Centre of Excellence for Robotic
   VisionAustralian Research Council [CE140100016]
FX This work was supported in part by the Australian Research Council
   Centre of Excellence for Robotic Vision under Project CE140100016.
CR Bouguet J-Y, 2002, CAMERA CALIBRATION T
   Chen L, 2018, COMPUT METH PROG BIO, V158, P135, DOI 10.1016/j.cmpb.2018.02.006
   Chen L, 2017, HEALTHC TECHNOL LETT, V4, P163, DOI 10.1049/htl.2017.0068
   DACRE JE, 1991, BRIT J RHEUMATOL, V30, P426
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Fox AJS, 2009, SPORTS HEALTH, V1, P461, DOI 10.1177/1941738109350438
   Hohe J, 2002, MAGN RESON MED, V47, P554, DOI 10.1002/mrm.10097
   Kazhdan M.M., 2006, ACM T GRAPHIC, V32, P61
   Lin BX, 2016, INT J MED ROBOT COMP, V12, P158, DOI 10.1002/rcs.1661
   Mahmoud N., 2017, COMPUTER ASSISTED RO
   Mahmoud N, 2019, IEEE T MED IMAGING, V38, P79, DOI 10.1109/TMI.2018.2856109
   Maier-Hein L, 2014, IEEE T MED IMAGING, V33, P1913, DOI 10.1109/TMI.2014.2325607
   Marmol A, 2018, IEEE INT C INT ROBOT, P3882, DOI 10.1109/IROS.2018.8593501
   Marmol A, 2017, IEEE ROBOT AUTOM LET, V2, P2135, DOI 10.1109/LRA.2017.2714150
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Song JW, 2018, IEEE ROBOT AUTOM LET, V3, P155, DOI 10.1109/LRA.2017.2735487
   Szekely, 2006, BILDVERARBEITUNG MED, P419, DOI DOI 10.1007/3-540-32137-3_85
   Turan M., 2017, ARXIVORGABS170506196
   Zheng EL, 2014, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2014.196
NR 21
TC 10
Z9 10
U1 3
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2377-3766
J9 IEEE ROBOT AUTOM LET
JI IEEE Robot. Autom. Lett.
PD APR
PY 2019
VL 4
IS 2
BP 918
EP 925
DI 10.1109/LRA.2019.2892199
PG 8
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Robotics
GA HM5JM
UT WOS:000459512000001
DA 2022-02-10
ER

PT J
AU Mikhelson, IV
   Lee, PG
   Sahakian, AV
   Wu, Y
   Katsaggelos, AK
AF Mikhelson, Ilya V.
   Lee, Philip G.
   Sahakian, Alan V.
   Wu, Ying
   Katsaggelos, Aggelos K.
TI Automatic, fast, online calibration between depth and color cameras
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Calibration; Depth cameras; Color cameras; Automatic; Online; Fast;
   Point cloud; Point correspondence
AB Automatic camera calibration has remained a hard topic in computer vision since its inception due to its reliance on the image correspondence problem. This problem becomes even more pronounced when calibrating a depth image with a color image due to a lack of simple correspondences between the two modalities. In this work, we develop a completely automatic, very fast, online algorithm that demonstrates how a consumer-grade depth camera can be calibrated with a color camera with minimal user interaction. (C) 2013 Elsevier Inc. All rights reserved.
C1 [Mikhelson, Ilya V.; Lee, Philip G.; Sahakian, Alan V.; Wu, Ying; Katsaggelos, Aggelos K.] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
   [Sahakian, Alan V.] Northwestern Univ, Dept Biomed Engn, Evanston, IL 60208 USA.
RP Mikhelson, IV (corresponding author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
EM i-mikhelson@u.northwestern.edu
RI Katsaggelos, Aggelos K/B-7233-2009; Wu, Ying/B-7283-2009; Sahakian, Alan
   V/B-7268-2009
OI Mikhelson, Ilya/0000-0001-5111-791X; Sahakian, Alan/0000-0003-3090-0328
CR [Anonymous], 2012, MATLAB VERS 8 0 0 R2
   Bouguet J. Y., 2004, CAMERA CALIBRATION T
   Bradski G, 2000, OPENCV LIB
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fuchs S., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587828
   Herrera CD, 2012, IEEE T PATTERN ANAL, V34, P2058, DOI 10.1109/TPAMI.2012.125
   KASSIR A., 2010, AUSTR C ROB AUT
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   Mikhelson I., 2013, CALIBRT TOOLBOX MATL
   Mikhelson IV, 2012, IEEE T INF TECHNOL B, V16, P927, DOI 10.1109/TITB.2012.2204760
   Mikhelson IV, 2011, IEEE T BIO-MED ENG, V58, P1671, DOI 10.1109/TBME.2011.2111371
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Shim H., 2011, VISUAL COMPUT, P1
   Silva M., 2012, WORKSH COL DEPTH CAM
   Smisek J., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1154, DOI 10.1109/ICCVW.2011.6130380
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stephens M, 1988, COMBINED CORNER EDGE, V15, P10, DOI DOI 10.5244/C.2.23
   Wang ZS, 2007, APPL MATH COMPUT, V185, P894, DOI 10.1016/j.amc.2006.05.210
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Zhang XG, 2011, PROCEEDINGS OF THE 2011 INTERNATIONAL CONFERENCE ON ENGINEERING AND RISK MANAGEMENT, P1
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 23
TC 14
Z9 15
U1 0
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JAN
PY 2014
VL 25
IS 1
SI SI
BP 218
EP 226
DI 10.1016/j.jvcir.2013.03.010
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 297MP
UT WOS:000330259900019
DA 2022-02-10
ER

PT C
AU Gnatyuk, V
   Zavalishin, S
   Petrova, X
   Odinokikh, G
   Fartukov, A
   Danilevich, A
   Eremeev, V
   Yoo, J
   Lee, K
   Lee, H
   Shin, D
   Solomatin, I
AF Gnatyuk, Vitaly
   Zavalishin, Sergey
   Petrova, Xenya
   Odinokikh, Gleb
   Fartukov, Alexey
   Danilevich, Alexey
   Eremeev, Vladimir
   Yoo, Juwoan
   Lee, Kwanghyun
   Lee, Heejun
   Shin, Daekyu
   Solomatin, Ivan
GP IEEE
TI Fast Automatic Exposure Adjustment Method for Iris Recognition System
SO PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON ELECTRONICS,
   COMPUTERS AND ARTIFICIAL INTELLIGENCE (ECAI-2019)
SE International Conference on Electronics Computers and Artificial
   Intelligence
LA English
DT Proceedings Paper
CT 11th International Conference on Electronics, Computers and Artificial
   Intelligence (ECAI)
CY JUN 27-29, 2019
CL Pitesti, ROMANIA
SP IEEE, IEEE Romania Sect, IEEE Ind Applicat Soc, Univ Pitesti, Fac Elect, Commun & Comp, Guvernul Romaniei, Ministerul Educatiei Cercetarii Stiintifice
DE iris recognition; auto exposure; computer vision
AB In this paper, we propose a novel algorithm for automatic camera parameter adjustment, which is exploited for getting the correct image exposure required for iris recognition. We use two-step processing, where the first step adjusts the camera parameters on the basis of a single shot, and the second step applies precise iterative adjustment. In order to get the correct iris exposure, we use a weighted mask, which is constructed offline using a set of face images. In contrast to the existing algorithms, our method does not need to be calibrated for a particular camera sensor. We show that the proposed method significantly decreases false rejection rate caused by incorrect image exposure and reduces recognition time.
C1 [Gnatyuk, Vitaly; Zavalishin, Sergey; Petrova, Xenya; Odinokikh, Gleb; Fartukov, Alexey; Danilevich, Alexey; Eremeev, Vladimir; Solomatin, Ivan] Samsung R&D Inst Russia, Bio Recognit Lab, Moscow, Russia.
   [Yoo, Juwoan; Lee, Kwanghyun; Lee, Heejun] Samsung Elect, Multimedia R&D Grp, Suwon, South Korea.
   [Shin, Daekyu] Samsung Elect, Visual SW R&D Grp, Suwon, South Korea.
RP Gnatyuk, V (corresponding author), Samsung R&D Inst Russia, Bio Recognit Lab, Moscow, Russia.
EM v.gnatyuk@samsung.com; s.zavalishin@samsung.com;
   xenya.petrova@samsung.com; g.odinokikh@samsung.com;
   a.fartukov@samsung.com; a.danilevich@samsung.com; v.eremeev@samsung.com;
   juwoan.yoo@samsung.com; kwangh86.lee@samsung.com;
   heejun_lee@samsung.com; daekyu.shin@samsung.com; i.solomatin@samsung.com
CR Battiato S, 2009, IMAGE PROCESS SER, P323
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Dunstone T., 2009, BIOMETRICS SYSTEM DA, DOI [10.1007/978-0-387-77627-9, DOI 10.1007/978-0-387-77627-9]
   Ilstrup D, 2010, LECT NOTES COMPUT SC, V6311, P200, DOI 10.1007/978-3-642-15549-9_15
   Liang JY, 2007, ASICON 2007: 2007 7TH INTERNATIONAL CONFERENCE ON ASIC, VOLS 1 AND 2, PROCEEDINGS, P725, DOI 10.1109/ICASIC.2007.4415733
   Messina G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P549
   Nourani-Vatani N., 2007, AUSTR C ROB AUT 2007
   Odinokikh G. A., 2018, Pattern Recognition and Image Analysis, V28, P516, DOI 10.1134/S105466181803015X
   Shim I, 2019, IEEE T CIRC SYST VID, V29, P1569, DOI 10.1109/TCSVT.2018.2846292
   Su YH, 2015, I SYMP CONSUM ELECTR, P13, DOI 10.1109/ICCE.2015.7066300
   Yang H, 2019, IEEE T VIS COMPUT GR, V25, P2953, DOI 10.1109/TVCG.2018.2865555
NR 11
TC 0
Z9 0
U1 1
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2378-7147
BN 978-1-7281-1624-2
J9 INT C ELECT COMPUT
PY 2019
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BP9NX
UT WOS:000569985400106
DA 2022-02-10
ER

PT C
AU Ma, RB
   Wang, R
   Pizer, S
   Rosenman, J
   McGill, SK
   Frahm, JM
AF Ma, Ruibin
   Wang, Rui
   Pizer, Stephen
   Rosenman, Julian
   McGill, Sarah K.
   Frahm, Jan-Michael
BE Shen, D
   Liu, T
   Peters, TM
   Staib, LH
   Essert, C
   Zhou, S
   Yap, PT
   Khan, A
TI Real-Time 3D Reconstruction of Colonoscopic Surfaces for Determining
   Missing Regions
SO MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI
   2019, PT V
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 10th International Workshop on Machine Learning in Medical Imaging
   (MLMI) / 22nd International Conference on Medical Image Computing and
   Computer-Assisted Intervention (MICCAI)
CY OCT 13-17, 2019
CL Shenzhen, PEOPLES R CHINA
DE Colonoscopy; SLAM; Reconstruction; RNN
AB Colonoscopy is the most widely used medical technique to screen the human large intestine (colon) for cancer precursors. However, frequently parts of the surface are not visualized, and it is hard for the endoscopist to realize that from the video. Non-visualization derives from lack of orientations of the endoscope to the full circumference of parts of the colon, occlusion from colon structures, and intervening materials inside the colon. Our solution is real-time dense 3D reconstruction of colon chunks with display of the missing regions. We accomplish this by a novel deep-learning-driven dense SLAM (simultaneous localization and mapping) system that can produce a camera trajectory and a dense reconstructed surface for colon chunks (small lengths of colon). Traditional SLAM systems work poorly for the low-textured colonoscopy frames and are subject to severe scale/camera drift. In our method a recurrent neural network (RNN) is used to predict scale-consistent depth maps and camera poses of successive frames. These outputs are incorporated into a standard SLAM pipeline with local windowed optimization. The depth maps are finally fused into a global surface using the optimized camera poses. To the best of our knowledge, we are the first to reconstruct dense colon surface from video in real time and to display missing surface.
C1 [Ma, Ruibin; Wang, Rui; Pizer, Stephen; Rosenman, Julian; McGill, Sarah K.; Frahm, Jan-Michael] Univ N Carolina, Chapel Hill, NC 27515 USA.
RP Wang, R (corresponding author), Univ N Carolina, Chapel Hill, NC 27515 USA.
EM wrlife@cs.unc.edu
OI McGill, Sarah/0000-0002-4006-2703
CR Armin MA, 2016, INT J COMPUT ASS RAD, V11, P1599, DOI 10.1007/s11548-016-1462-8
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Hong DH, 2014, COMPUT MED IMAG GRAP, V38, P22, DOI 10.1016/j.compmedimag.2013.10.005
   Hong W., 2007, P SPIE
   Jemal A, 2010, CANCER EPIDEM BIOMAR, V19, P1893, DOI 10.1158/1055-9965.EPI-10-0437
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Qingyu Zhao, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P439, DOI 10.1007/978-3-319-46720-7_51
   Schonberger J. L., 2016, C COMP VIS PATT REC
   Schops T., 2018, CORR
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   van Rijn JC, 2006, AM J GASTROENTEROL, V101, P343, DOI 10.1111/j.1572-0241.2006.00390.x
   Wang R., 2019, P IEEE C COMP VIS PA P IEEE C COMP VIS PA
   Yang N, 2018, LECT NOTES COMPUT SC, V11212, P835, DOI 10.1007/978-3-030-01237-3_50
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhou T., 2017, CVPR
NR 16
TC 17
Z9 17
U1 0
U2 4
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-32254-0; 978-3-030-32253-3
J9 LECT NOTES COMPUT SC
PY 2019
VL 11768
BP 573
EP 582
DI 10.1007/978-3-030-32254-0_64
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Engineering, Biomedical; Neuroimaging; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Neurosciences & Neurology; Imaging
   Science & Photographic Technology; Radiology, Nuclear Medicine & Medical
   Imaging
GA BP3RM
UT WOS:000548735200064
DA 2022-02-10
ER

PT J
AU Weinstein, BG
AF Weinstein, Ben G.
TI A computer vision for animal ecology
SO JOURNAL OF ANIMAL ECOLOGY
LA English
DT Review
DE automation; camera traps; ecological monitoring; images; unmanned aerial
   vehicles
ID SATELLITE IMAGERY; AERIAL IMAGES; IDENTIFICATION; CLASSIFICATION;
   RECOGNITION; APPEARANCE; CAMOUFLAGE; ABUNDANCE; BEHAVIOR; SYSTEM
AB 1. A central goal of animal ecology is to observe species in the natural world. The cost and challenge of data collection often limit the breadth and scope of ecological study. Ecologists often use image capture to bolster data collection in time and space. However, the ability to process these images remains a bottleneck.
   2. Computer vision can greatly increase the efficiency, repeatability and accuracy of image review. Computer vision uses image features, such as colour, shape and texture to infer image content.
   3.I provide a brief primer on ecological computer vision to outline its goals, tools and applications to animal ecology.
   4.I reviewed 187 existing applications of computer vision and divided articles into ecological description, counting and identity tasks.
   5. I discuss recommendations for enhancing the collaboration between ecologists and computer scientists and highlight areas for future growth of automated image analysis.
C1 [Weinstein, Ben G.] Oregon State Univ, Marine Mammal Inst, Dept Fisheries & Wildlife, Newport, OR 97365 USA.
RP Weinstein, BG (corresponding author), Oregon State Univ, Marine Mammal Inst, Dept Fisheries & Wildlife, Newport, OR 97365 USA.
EM weinsteb@oregonstate.edu
OI Weinstein, Ben/0000-0002-2176-7935
CR Abramoff M.D., 2004, BIOPHOTONICS INT, V11, P36, DOI DOI 10.1117/1.3589100
   Adams Jeffrey D., 2006, Aquatic Mammals, V32, P374, DOI 10.1578/AM.32.3.2006.374
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Arzoumanian Z, 2005, J APPL ECOL, V42, P999, DOI 10.1111/j.1365-2664.2005.01117.x
   Atanbori J, 2016, PATTERN RECOGN LETT, V81, P53, DOI 10.1016/j.patrec.2015.08.015
   Barber-Meyer SM, 2007, POLAR BIOL, V30, P1565, DOI 10.1007/s00300-007-0317-8
   Barnard S, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0158748
   Bartomeus I, 2016, FUNCT ECOL, V30, P1894, DOI 10.1111/1365-2435.12666
   Beekmans Bas W. P. M., 2005, Aquatic Mammals, V31, P243, DOI 10.1578/AM.31.2.2005.243
   Beijbom O, 2016, SCI REP-UK, V6, DOI 10.1038/srep23166
   Belongie S, 2016, PATTERN RECOGN LETT, V72, P15, DOI 10.1016/j.patrec.2015.11.023
   Berg TL, 2010, P IEEE, V98, P1434, DOI 10.1109/JPROC.2009.2032355
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Blanc K., 2014, P 3 ACM INT WORKSH M, P1, DOI [10.1145/2661821.2661827, DOI 10.1145/2661821.2661827]
   Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
   Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906
   Bradski G, 2000, DR DOBBS J, V25, P120
   Branson S., 2010, LECT NOTES COMPUTER, V6314
   Branson S., 2014, BRIT MACH VIS C BMVC
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
   Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
   Cross Matthew D., 2014, Herpetological Review, V45, P584
   Crutsinger GM, 2016, J UNMANNED VEH SYST, V4, P161, DOI 10.1139/juvs-2016-0008
   Dala-Corte RB, 2016, NEOTROP ICHTHYOL, V14, DOI 10.1590/1982-0224-20150074
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   Descamps S, 2011, BIRD STUDY, V58, P302, DOI 10.1080/00063657.2011.588195
   Desell T, 2013, P IEEE INT C E-SCI, P107, DOI 10.1109/eScience.2013.50
   Duyck J, 2015, PATTERN RECOGN, V48, P1059, DOI 10.1016/j.patcog.2014.07.017
   Farnsworth EJ, 2013, BIOSCIENCE, V63, P891, DOI 10.1525/bio.2013.63.11.8
   Favret C, 2016, SYST ENTOMOL, V41, P133, DOI 10.1111/syen.12146
   Feng LN, 2016, PATTERN RECOGN, V51, P225, DOI 10.1016/j.patcog.2015.09.012
   Fretwell PT, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088655
   Gilmetdinova A., 2016, INT J BILING EDUC BI, P1, DOI [10.1080/13670050.2016.1231772, DOI 10.1080/13670050.2016.1231772]
   Giraldo-Zuluaga J.-H., 2017, ARXIV170108180
   Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
   Groom G, 2011, INT J REMOTE SENS, V32, P4611, DOI 10.1080/01431161.2010.489068
   Haggag H, 2016, IEEE SYS MAN CYBERN, P855, DOI 10.1109/SMC.2016.7844347
   Hernandez-Serna A, 2014, PEERJ, V2, DOI 10.7717/peerj.563
   Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
   Howland J. C., 2012, OCEANS 2012 MTS IEEE, P1, DOI [10. 1109/OCEANS. 2012. 6404859, DOI 10.1109/0CEANS.2012.6404859]
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006
   Jones AM, 2008, CORAL REEFS, V27, P521, DOI 10.1007/s00338-008-0354-y
   Kosmala M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8090726
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kvilekval K, 2010, BIOINFORMATICS, V26, P544, DOI 10.1093/bioinformatics/btp699
   LaRue MA, 2017, CONSERV BIOL, V31, P213, DOI 10.1111/cobi.12809
   LaRue MA, 2015, WILDLIFE SOC B, V39, P772, DOI 10.1002/wsb.596
   Lavy A, 2015, METHODS ECOL EVOL, V6, P521, DOI 10.1111/2041-210X.12331
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levy K, 2014, BIOL OPEN, V3, P1245, DOI 10.1242/bio.20149175
   Liu CC, 2015, ECOL INFORM, V30, P170, DOI 10.1016/j.ecoinf.2015.10.008
   Lynch HJ, 2012, POLAR BIOL, V35, P963, DOI 10.1007/s00300-011-1138-3
   Lytle DA, 2010, J N AM BENTHOL SOC, V29, P867, DOI 10.1899/09-080.1
   Marburg A., 2016, OCEANS 2016 MTS IEEE, P1
   Matuska S, 2014, AASRI PROC, V9, P25, DOI 10.1016/j.aasri.2014.09.006
   McDowall P, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0166773
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Naumann MS, 2009, CORAL REEFS, V28, P109, DOI 10.1007/s00338-008-0459-3
   Olsen AM, 2015, METHODS ECOL EVOL, V6, P351, DOI 10.1111/2041-210X.12326
   Pennekamp F, 2013, METHODS ECOL EVOL, V4, P483, DOI 10.1111/2041-210X.12036
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Reda K, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-48
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Robie AA, 2017, J EXP BIOL, V220, P25, DOI 10.1242/jeb.142281
   Seymour AC, 2017, SCI REP-UK, V7, DOI 10.1038/srep45127
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Steen R, 2014, MAR FRESHWATER RES, V65, P1094, DOI 10.1071/MF13139
   Stoddard MC, 2016, SCI REP-UK, V6, DOI 10.1038/srep32059
   Stoddard MC, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5117
   Sun X, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P471, DOI 10.1109/CISP-BMEI.2016.7852757
   Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Tankus A, 2009, PHILOS T R SOC B, V364, P529, DOI 10.1098/rstb.2008.0211
   Torney CJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156342
   Town C, 2013, ECOL EVOL, V3, P1902, DOI 10.1002/ece3.587
   Troscianko J, 2017, BMC EVOL BIOL, V17, DOI 10.1186/s12862-016-0854-2
   Van Andel AC, 2015, AM J PRIMATOL, V77, P1122, DOI 10.1002/ajp.22446
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Van Horn Grant, 2017, ARXIV170706642
   Villon S., 2016, LECT NOTES COMPUTER, V10016
   Weinstein B. G., 2017, DRYAD DIGITAL REPOSI, DOI [10. 5061/dryad. b700h, DOI 10.5061/DRYAD.B700H]
   Weinstein BG, 2017, ECOL LETT, V20, P326, DOI 10.1111/ele.12730
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Witharana C, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8050375
   Yang CC, 2016, BIOL J LINN SOC, V117, P422, DOI 10.1111/bij.12690
   Yang Zhixian, 2014, ScientificWorldJournal, V2014, P140863, DOI 10.1155/2014/140863
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
   Zhang H., 2012, P 21 INT C WORLD WID, P749, DOI [DOI 10.1145/2187836.2187938, 10.1145/2187836.2187938]
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 96
TC 111
Z9 113
U1 18
U2 128
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0021-8790
EI 1365-2656
J9 J ANIM ECOL
JI J. Anim. Ecol.
PD MAY
PY 2018
VL 87
IS 3
BP 533
EP 545
DI 10.1111/1365-2656.12780
PG 13
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA GC8QE
UT WOS:000430059900003
PM 29111567
OA Bronze
DA 2022-02-10
ER

PT J
AU Zhang, RZ
   Zhu, SY
   Shen, TW
   Zhou, L
   Luo, ZX
   Fang, T
   Quan, L
AF Zhang, Runze
   Zhu, Siyu
   Shen, Tianwei
   Zhou, Lei
   Luo, Zixin
   Fang, Tian
   Quan, Long
TI Distributed Very Large Scale Bundle Adjustment by Global Camera
   Consensus
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Cameras; Bundle adjustment; Optimization; Convex functions; Convergence;
   Merging; Bundle adjustment; structure-from-motion; 3D reconstruction;
   distributed computing
ID ALTERNATING DIRECTION METHOD; PROXIMAL POINT ALGORITHM; NONCONVEX;
   CONVERGENCE
AB The increasing scale of Structure-from-Motion is fundamentally limited by the conventional optimization framework for the all-in-one global bundle adjustment. In this paper, we propose a distributed approach to coping with this global bundle adjustment for very large scale Structure-from-Motion computation. First, we derive the distributed formulation from the classical optimization algorithm ADMM, Alternating Direction Method of Multipliers, based on the global camera consensus. Then, we analyze the conditions under which the convergence of this distributed optimization would be guaranteed. In particular, we adopt over-relaxation and self-adaption schemes to improve the convergence rate. After that, we propose to split the large scale camera-point visibility graph in order to reduce the communication overheads of the distributed computing. The experiments on both public large scale SfM data-sets and our very large scale aerial photo sets demonstrate that the proposed distributed method clearly outperforms the state-of-the-art method in efficiency and accuracy.
C1 [Zhang, Runze; Shen, Tianwei; Zhou, Lei; Luo, Zixin; Quan, Long] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Zhu, Siyu] Alibaba AI Labs, Hangzhou, Zhejiang, Peoples R China.
   [Fang, Tian] Shenzhen Zhuke Innovat Technol, Shenzhen, Peoples R China.
RP Zhang, RZ (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
EM rzhangaj@cse.ust.hk; siting.zsy@alibaba-inc.com; tshenaa@cse.ust.hk;
   lzhouai@cse.ust.hk; zluoag@cse.ust.hk; fangtian@altizure.com;
   quan@cse.ust.hk
RI Shen, Tianwei/AAF-7731-2019; Zhou, Lei/H-4799-2016; Zhang,
   Runze/N-3486-2015
OI Shen, Tianwei/0000-0002-3290-2258; Zhou, Lei/0000-0003-4988-5084; Zhang,
   Runze/0000-0001-9698-0178
FU Hong Kong RGCHong Kong Research Grants Council [16208614, T22-603/15N];
   Hong Kong ITC [PSKL12EG02]; China 973 programNational Basic Research
   Program of China [2012CB316300]
FX This work is supported by Hong Kong RGC 16208614, T22-603/15N, Hong Kong
   ITC PSKL12EG02, and China 973 program, 2012CB316300.
CR Agarwal S., CERES SOLVER
   Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9_3
   Bauschke H. H., 2017, CONVEX ANAL MONOTONE, V2011
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED, V23
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Carlone L., 2014, P BRIT MACH VIS C, DOI [10.5244/C.28.3, DOI 10.5244/C.28.3]
   Cui ZP, 2015, IEEE I CONF COMP VIS, P864, DOI 10.1109/ICCV.2015.105
   Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   ECKSTEIN J, 1994, J OPTIMIZ THEORY APP, V80, P39, DOI 10.1007/BF02196592
   Eckstein J., 1998, INFORMS Journal on Computing, V10, P218, DOI 10.1287/ijoc.10.2.218
   Eriksson A, 2016, PROC CVPR IEEE, P1754, DOI 10.1109/CVPR.2016.194
   Fang T, 2010, LECT NOTES COMPUT SC, V6312, P1, DOI 10.1007/978-3-642-15552-9_1
   Frahm JM, 2010, LECT NOTES COMPUT SC, V6314, P368, DOI 10.1007/978-3-642-15561-1_27
   FUKUSHIMA M, 1981, INT J SYST SCI, V12, P989, DOI 10.1080/00207728108963798
   Gallego G, 2015, J MATH IMAGING VIS, V51, P378, DOI 10.1007/s10851-014-0528-x
   He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434
   Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949
   Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Jiang NJ, 2013, IEEE I CONF COMP VIS, P481, DOI 10.1109/ICCV.2013.66
   Kaplan A, 1998, J GLOBAL OPTIM, V13, P389, DOI 10.1023/A:1008321423879
   Klingner B, 2013, IEEE I CONF COMP VIS, P953, DOI 10.1109/ICCV.2013.122
   Kushal A, 2012, PROC CVPR IEEE, P1442, DOI 10.1109/CVPR.2012.6247832
   Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44
   Li GY, 2015, SIAM J OPTIMIZ, V25, P2434, DOI 10.1137/140998135
   Lu Q, 2011, PR ELECTROMAGN RES S, P576
   Ni K, 2007, IEEE I CONF COMP VIS, P2009
   Ni K, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P144, DOI 10.1109/3DIMPVT.2012.47
   Olsson C, 2009, PROC CVPR IEEE, P1216, DOI 10.1109/CVPRW.2009.5206864
   SCHONBERGER JL, 2016, PROC CVPR IEEE, P4104, DOI DOI 10.1109/CVPR.2016.445
   Schonberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148
   Shen TW, 2016, LECT NOTES COMPUT SC, V9907, P139, DOI 10.1007/978-3-319-46487-9_9
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Snavely N., 2008, P IEEE C COMP VIS PA, P1
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Sra Suvrit, 2012, ADV NEURAL INFORM PR, P530
   Triggs B., 1999, LECT NOTES COMPUTER, P298, DOI DOI 10.1007/3-540-44480-7_21
   Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5
   Yang L, 2017, SIAM J IMAGING SCI, V10, P74, DOI 10.1137/15M1027528
   Yong-Dian Jian, 2012, Outdoor and Large-Scale Real-World Scene Analysis. 15th International Workshop on Theoretical Foundations of Computer Vision. Revised Selected Papers, P131, DOI 10.1007/978-3-642-34091-8_6
   Zhang RZ, 2015, IEEE I CONF COMP VIS, P2084, DOI 10.1109/ICCV.2015.241
   Zhu S., 2017, CORR
   Zhu SY, 2014, PROC CVPR IEEE, P3938, DOI 10.1109/CVPR.2014.503
NR 47
TC 1
Z9 2
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD FEB
PY 2020
VL 42
IS 2
BP 291
EP 303
DI 10.1109/TPAMI.2018.2840719
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA KE2KB
UT WOS:000508386100004
PM 29993533
DA 2022-02-10
ER

PT J
AU Sochor, J
   Juranek, R
   Herout, A
AF Sochor, Jakub
   Juranek, Roman
   Herout, Adam
TI Traffic surveillance camera calibration by 3D model bounding box
   alignment for accurate vehicle speed measurement
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Speed measurement; Camera calibration; Fully automatic; Traffic
   surveillance; Bounding box alignment; Vanishing point detection
AB In this paper, we focus on fully automatic traffic surveillance camera calibration, which we use for speed measurement of passing vehicles. We improve over a recent state-of-the-art camera calibration method for traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference method. The method is based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from arbitrary viewpoints, since it has no constraints on camera placement. We evaluate our method on the recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration method by detection of two vanishing points reduces error by 50% (mean distance ratio error reduced from 0.18 to 0.09) compared to the previous state-of-the-art method. We also show that our scene scale inference method is more precise, outperforming both state-of-the-art automatic calibration method for speed measurement (error reduction by 86 % - 7.98 km/h to 1.10 km/h) and manual calibration (error reduction by 19 % - 1.35 km/h to 1.10 km/h). We also present qualitative results of the proposed automatic camera calibration method on video sequences obtained from real surveillance cameras in various places, and under different lighting conditions (night, dawn, day). (C) 2017 Elsevier Inc. All rights reserved.
C1 [Sochor, Jakub; Juranek, Roman; Herout, Adam] Brno Univ Technol, Fac Informat Technol, Ctr Excellence IT4Innovat, Bozetechova 2, Brno 61266, Czech Republic.
RP Sochor, J (corresponding author), Brno Univ Technol, Fac Informat Technol, Ctr Excellence IT4Innovat, Bozetechova 2, Brno 61266, Czech Republic.
EM isochor@fit.vutbr.cz; ijuranek@fit.vutbr.cz; herout@fit.vutbr.cz
RI Herout, Adam/B-5651-2014
FU Ministry of Education, Youth and Sports of the Czech Republic from the
   National Programme of Sustainability (NPU II); project IT4lnnovations
   excellence in science [LQ1602]
FX This work was supported by The Ministry of Education, Youth and Sports
   of the Czech Republic from the National Programme of Sustainability (NPU
   II); project IT4lnnovations excellence in science - LQ1602.
CR Cathey FW, 2005, 2005 IEEE Intelligent Vehicles Symposium Proceedings, P777
   Chaperon T, 2011, COMPUT VIS IMAGE UND, V115, P576, DOI 10.1016/j.cviu.2010.11.018
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dailey D. J., 2000, IEEE Transactions on Intelligent Transportation Systems, V1, P98, DOI 10.1109/6979.880967
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Do V.H., P 2015 12 INT C EL E, P1, DOI [10.1109/ECTICon.2015.7207027, DOI 10.1109/ECTICON.2015.7207027]
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dubska M., 2014, AUTOMATIC CAMERA CAL
   Dubska M, 2015, IEEE T INTELL TRANSP, V16, P1162, DOI 10.1109/TITS.2014.2352854
   Dubska M, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.90
   Fang J., 2016, IEEE INTELLIGENT TRA, P1
   Fung GSK, 2003, OPT ENG, V42, P2967, DOI 10.1117/1.1606458
   Gao Yang, 2016, IEEE C COMP VIS PATT
   Girshick R., 2014, COMPUTER VISION PATT
   Grammatikopoulos L., 2005, P INT S MOD TECHN ED, P332
   He X. C., 2007, IEEE WORKSH APPL COM
   He XC, 2007, OPT ENG, V46, DOI 10.1117/1.2714991
   Hsiao E., 2014, CAR MAKE MODEL RECOG
   Juranek R, 2015, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2015.274
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Krause J., 2015, IEEE C COMP VIS PATT
   Krause J., 2013, 3DRR13 ICCV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan JH, 2014, OPTIK, V125, P289, DOI 10.1016/j.ijleo.2013.06.036
   Li CM, 2009, PROC CVPR IEEE, P218, DOI 10.1109/CVPRW.2009.5206553
   Lin T.-Y., 2015, INT C COMP VIS ICCV
   Lin Y.-L., 2014, JOINTLY OPTIMIZING 3
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Long J., 2015, IEEE C COMP VIS PATT
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luvizon Diogo C., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6563, DOI 10.1109/ICASSP.2014.6854869
   Luvizon D. C., 2016, IEEE T INTELL TRANSP, VPP, P1
   Maduro C, 2008, IEEE IMAGE PROC, P777, DOI 10.1109/ICIP.2008.4711870
   Nurhadiyatna A, 2013, INT C ADV COMP SCI I, P451, DOI 10.1109/ICACSIS.2013.6761617
   Prokaj J., 2009, 3 D MODEL BASED VEHI
   Ren Shaoqing, 2015, ADV NEURAL INFORM PR
   Schoepflin TN, 2003, IEEE T INTELL TRANSP, V4, P90, DOI 10.1109/TITS.2003.821213
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Simon M., 2015, INT C COMP VIS ICCV
   Sina I, 2013, INT C ADV COMP SCI I, P149, DOI 10.1109/ICACSIS.2013.6761567
   Sochor J., 2016, INTELL TRAN IN PRESS
   Sochor J., 2016, IEEE C COMP VIS PATT
   Tomasi C., 1991, INT J COMPUTER VISIO
   Yang J., 2016, IEEE C COMP VIS PATT
   You XH, 2016, NEUROCOMPUTING, V204, P222, DOI 10.1016/j.neucom.2015.09.132
   Yu XG, 2009, COMPUT VIS IMAGE UND, V113, P643, DOI 10.1016/j.cviu.2008.01.006
   Zhang ZX, 2013, IEEE T CIRC SYST VID, V23, P518, DOI 10.1109/TCSVT.2012.2210670
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zheng Y, 2014, IEEE T INTELL TRANSP, V15, P831, DOI 10.1109/TITS.2013.2288353
   Zhou K, 2016, DESTECH TRANS COMP
NR 50
TC 28
Z9 30
U1 0
U2 14
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD AUG
PY 2017
VL 161
BP 87
EP 98
DI 10.1016/j.cviu.2017.05.015
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA FG8WN
UT WOS:000410718600009
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Inagaki, A
   Ward, MP
AF Allen, Maximilian L.
   Inagaki, Akino
   Ward, Michael P.
TI CANNIBALISM IN RAPTORS: A REVIEW
SO JOURNAL OF RAPTOR RESEARCH
LA English
DT Review
DE camera trap; cannibalism; competition; died filicide; infanticide;
   scavenging; siblicide
ID PARENTAL INFANTICIDE; BROOD REDUCTION; TYTO-ALBA; OWLS; SIBLICIDE;
   KESTRELS
AB Feeding strategies, including cannibalism (in which an individual eats a member of the same species), are an important aspect of predator ecology. Cannibalism comprises five forms in raptors: siblicide, filicide, non-parental infanticide, conspecific strife, and conspecific scavenging. Cannibalism by raptors has been documented opportunistically for over a century, but it is unknown how frequent or widespread the behavior is. We performed the first systematic literature review and meta-analyses of the studies documenting filicide, non-parental infanticide, conspecific strife, and conspecitic scavenging by raptors. We found 29 reports of these types of cannibalism; we did not review reports of siblicide due to the high frequency of the behavior, making it nearly ubiquitous among raptors. Filicide had nearly twice as many reports (n = 11, 37.9%) as any other type of cannibalism. Most reports were direct observations (n= 23, 79.3%), and nearly half the reports came from North America (n=14, 48.3%) and approximately a third from Europe (n= 10, 34.5%). The 29 reports involved 25 raptor species front four families, with those from Accipitridae most common (n= 19, 65.5%). Cannibalism in raptors varies but most involves nestlings, which are easier to kill than adults, possibly because brood reduction can help the stronger young survive. Documented reports of cannibalism are increasing, possibly due to recent technological advancements that have increased our ability to document cannibalism and other ecological processes. Nevertheless, we encourage future reports of cannibalism from under-represented locations and for taxa that are less well studied.
C1 [Allen, Maximilian L.; Ward, Michael P.] Univ Illinois, Illinois Nat Hist Survey, 1816 South Oak St, Champaign, IL 61820 USA.
   [Inagaki, Akino] Tokyo Univ Agr & Technol, Grad Sch Apiculture, 3-5-8 Saiwai Cho, Fuchu, Tokyo 1838509, Japan.
   [Ward, Michael P.] Univ Illinois, Dept Nat Resources & Environm Sci, 1102 South Goodwin, Urbana, IL 61801 USA.
RP Allen, ML (corresponding author), Univ Illinois, Illinois Nat Hist Survey, 1816 South Oak St, Champaign, IL 61820 USA.
EM maxallen@illinois.edu
FU University of Illinois; Illinois Natural History Survey
FX We thank the Illinois Natural History Survey and the University of
   Illinois for their support. We thank D. E. Varland and an anonymous
   reviewer for their thoughtful comments on previous versions that greatly
   improved this report.
CR Allen ML, 2019, J RAPTOR RES, V53, P410, DOI 10.3356/0892-1016-53.4.410
   Anderson A, 2015, J RAPTOR RES, V49, P498, DOI 10.3356/rapt-49-04-498-500.1
   Arroyo BE, 1997, J RAPTOR RES, V31, P390
   BECHARD MJ, 1983, WILSON BULL, V95, P233
   BORTOLOTTI GR, 1991, CAN J ZOOL, V69, P1447, DOI 10.1139/z91-205
   Camina A., 2003, VULTURE NEWS, V47, P25
   Caro J, 2014, J RAPTOR RES, V48, P292, DOI 10.3356/JRR-13-87.1
   Clements J.F., 2018, EBIRD CLEMENTS CHECK
   CLEVENGER GA, 1974, AUK, V91, P639
   Coffin L. V. B., 1906, BIRD LORE, V8, P68
   de Lecea FM, 2011, ARDEA, V99, P240, DOI 10.5253/078.099.0216
   FISHER B M, 1975, Canadian Field-Naturalist, V89, P71
   FORBES LS, 1991, BEHAV ECOL SOCIOBIOL, V29, P189, DOI 10.1007/BF00166400
   Franke A, 2013, ARCTIC, V66, P226
   Hadjikyriakou TG, 2016, J RAPTOR RES, V50, P220, DOI 10.3356/0892-1016-50.2.220
   Hollingsworth Julie, 2017, Australian Field Ornithology, V34, P129
   HOLTHUIJZEN A M A, 1987, Journal of Raptor Research, V21, P32
   Inagaki A, 2020, ECOL EVOL, V10, P1223, DOI 10.1002/ece3.5976
   INGRAM COLLINGWOOD, 1959, AUK, V76, P218
   INGRAM COLLINGWOOD, 1962, AUK, V79, P715
   JONES A M, 1990, Journal of Raptor Research, V24, P28
   Kang Seung Gu, 2018, [KOREAN JOURNAL OF ENVIRONMENT AND ECOLOG, 한국환경생태학회지], V32, P256, DOI 10.13047/KJEE.2018.32.3.256
   Kornan M, 2011, J RAPTOR RES, V45, P95
   Krofel Miha, 2011, Acrocephalus, V32, P45, DOI 10.2478/v10100-011-0003-3
   LENTON GM, 1984, IBIS, V126, P551, DOI 10.1111/j.1474-919X.1984.tb02080.x
   Lewis SB, 2017, J RAPTOR RES, V51, P476, DOI 10.3356/JRR-17-22.1
   LYONS D, 1982, ARDEA, V70, P217
   Margalida A, 2004, IBIS, V146, P386, DOI 10.1111/j.1474-919X.2004.00261.x
   Markham AC, 2007, J RAPTOR RES, V41, P41, DOI 10.3356/0892-1016(2007)41[41:DOIACI]2.0.CO;2
   MILLARD JB, 1978, WILSON BULL, V90, P449
   Miller SJ, 2015, J RAPTOR RES, V49, P152, DOI 10.3356/0892-1016-49.2.152
   Mori Devvratsinh, 2017, Indian Birds, V13, P111
   NEGRO JJ, 1992, J RAPTOR RES, V26, P225
   Newton I, 2010, POPULATION ECOLOGY R
   Newton I., 1998, POPULATION LIMITATIO
   PILZ WR, 1978, AUK, V95, P584
   Rana Gargi, 2003, Journal of the Bombay Natural History Society, V100, P116
   Redondo T, 2019, ECOL EVOL, V9, P9185, DOI 10.1002/ece3.5466
   Robinson T. S., 1954, Wilson Bulletin, V66, P72
   Sebastian-Gonzalez E, 2019, GLOBAL CHANGE BIOL, V25, P3005, DOI 10.1111/gcb.14708
   SHEFFIELD SR, 1994, J RAPTOR RES, V28, P119
   Steen R, 2016, J RAPTOR RES, V50, P217, DOI 10.3356/0892-1016-50.2.217
   STEFFEN JF, 1977, AUK, V94, P593
   Temple Dick, 2008, British Birds, V101, P687
   TRENBERTH KE, 1983, B AM METEOROL SOC, V64, P1276, DOI 10.1175/1520-0477(1983)064<1276:WATS>2.0.CO;2
   Webster A, 1999, EMU, V99, P80, DOI 10.1071/MU99009D
   Wilson EE, 2011, TRENDS ECOL EVOL, V26, P129, DOI 10.1016/j.tree.2010.12.011
   Woodford JE, 2008, J RAPTOR RES, V42, P79, DOI 10.3356/JRR-07-44.1
NR 48
TC 2
Z9 4
U1 3
U2 7
PU RAPTOR RESEARCH FOUNDATION INC
PI HASTINGS
PA 14377 117TH STREET SOUTH, HASTINGS, MN 55033 USA
SN 0892-1016
EI 2162-4569
J9 J RAPTOR RES
JI J. Raptor Res.
PD DEC
PY 2020
VL 54
IS 4
BP 424
EP 430
DI 10.3356/0892-1016-54.4.424
PG 7
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA PN9FZ
UT WOS:000604780200008
DA 2022-02-10
ER

PT J
AU Manni, F
   Elmi-Terander, A
   Burstrom, G
   Persson, O
   Edstrom, E
   Holthuizen, R
   Shan, C
   Zinger, S
   van der Sommen, F
   de With, PHN
AF Manni, Francesca
   Elmi-Terander, Adrian
   Burstrom, Gustav
   Persson, Oscar
   Edstrom, Erik
   Holthuizen, Ronald
   Shan, Caifeng
   Zinger, Svitlana
   van der Sommen, Fons
   de With, Peter H. N.
TI Towards Optical Imaging for Spine Tracking without Markers in Navigated
   Spine Surgery
SO SENSORS
LA English
DT Article
DE optical sensing; spinal surgery; image processing; image analysis for
   markerless tracking; patient tracking; image-guided surgery
ID PEDICLE SCREW PLACEMENT; FREE-HAND; AUGMENTED REALITY; SCOLIOSIS
   SURGERY; ARM NAVIGATION; ACCURACY; SYSTEM; LUMBAR; INSTRUMENTATION;
   FEASIBILITY
AB Surgical navigation systems are increasingly used for complex spine procedures to avoid neurovascular injuries and minimize the risk for reoperations. Accurate patient tracking is one of the prerequisites for optimal motion compensation and navigation. Most current optical tracking systems use dynamic reference frames (DRFs) attached to the spine, for patient movement tracking. However, the spine itself is subject to intrinsic movements which can impact the accuracy of the navigation system. In this study, we aimed to detect the actual patient spine features in different image views captured by optical cameras, in an augmented reality surgical navigation (ARSN) system. Using optical images from open spinal surgery cases, acquired by two gray-scale cameras, spinal landmarks were identified and matched in different camera views. A computer vision framework was created for preprocessing of the spine images, detecting and matching local invariant image regions. We compared four feature detection algorithms, Speeded Up Robust Feature (SURF), Maximal Stable Extremal Region (MSER), Features from Accelerated Segment Test (FAST), and Oriented FAST and Rotated BRIEF (ORB) to elucidate the best approach. The framework was validated in 23 patients and the 3D triangulation error of the matched features was <0.5mm. Thus, the findings indicate that spine feature detection can be used for accurate tracking in navigated surgery.
C1 [Manni, Francesca; Zinger, Svitlana; van der Sommen, Fons; de With, Peter H. N.] Eindhoven Univ Technol, Dept Elect Engn, NL-5600 MB Eindhoven, Netherlands.
   [Elmi-Terander, Adrian; Burstrom, Gustav; Persson, Oscar; Edstrom, Erik] Karolinska Inst, Dept Clin Neurosci, SE-17146 Stockholm, Sweden.
   [Elmi-Terander, Adrian; Burstrom, Gustav; Persson, Oscar; Edstrom, Erik] Karolinska Univ Hosp, Dept Neurosurg, SE-17146 Stockholm, Sweden.
   [Holthuizen, Ronald] Philips Healthcare, NL-5684 PC Best, Netherlands.
   [Shan, Caifeng] Philips Res, High Tech Campus 36, NL-5656 AE Eindhoven, Netherlands.
RP Manni, F (corresponding author), Eindhoven Univ Technol, Dept Elect Engn, NL-5600 MB Eindhoven, Netherlands.
EM f.manni@tue.nl; adrian.elmi-terander@sll.se; gustav.burstrom@ki.se;
   oscar.persson.1@ki.se; erik.edstrom@sll.se;
   ronald.holthuizen@philips.com; Caifeng.Shan@gmail.com; s.zinger@tue.nl;
   fvdsommen@tue.nl; P.H.N.de.With@tue.nl
RI Shan, Caifeng/W-6178-2019; Edström, Erik/AAH-6539-2020; Elmi-Terander,
   Adrian/AAC-7997-2019
OI Shan, Caifeng/0000-0002-2131-1671; Elmi-Terander,
   Adrian/0000-0002-3776-6136; Manni, Francesca/0000-0003-0470-2299;
   Edstrom, Erik/0000-0002-8781-1169; van der Sommen,
   Fons/0000-0002-3593-2356; Holthuizen, Ronaldus/0000-0002-1704-9789
FU H2020-ECSEL Joint Undertaking [692470]
FX The research activity leading to the results of this paper was funded by
   the H2020-ECSEL Joint Undertaking under Grant Agreement No. 692470
   (ASTONISH Project).
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Burstrom G, 2019, J NEUROSURG-SPINE, V31, P147, DOI 10.3171/2018.12.SPINE181397
   Burstrom G., 2020, SPINE PHILA PA 1976
   Burstrom G, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64462-x
   Burstrom G, 2019, SPINE, V44, P1097, DOI 10.1097/BRS.0000000000003006
   Calonder M., 2010, BRIEF BINARY ROBUST
   Du JP, 2018, WORLD NEUROSURG, V109, P24, DOI 10.1016/j.wneu.2017.07.154
   Edstrom E, 2020, OPER NEUROSURG, V18, P496, DOI 10.1093/ons/opz236
   Elmi-Terander A, 2019, SPINE, V44, P517, DOI 10.1097/BRS.0000000000002876
   Elmi-Terander A, 2018, SPINE, V43, P1018, DOI 10.1097/BRS.0000000000002502
   Elmi-Terander A, 2016, SPINE, V41, pE1303, DOI 10.1097/BRS.0000000000001830
   Gelalis ID, 2012, EUR SPINE J, V21, P247, DOI 10.1007/s00586-011-2011-3
   Gibby JT, 2019, INT J COMPUT ASS RAD, V14, P525, DOI 10.1007/s11548-018-1814-7
   Glossop ND, 1996, SPINE, V21, P2026, DOI 10.1097/00007632-199609010-00021
   Gumprecht HK, 1999, NEUROSURGERY, V44, P97, DOI 10.1097/00006123-199901000-00056
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hecht N, 2016, EUR SPINE J, V25, P716, DOI 10.1007/s00586-015-3814-4
   Helm PA, 2015, IEEE T MED IMAGING, V34, P1738, DOI 10.1109/TMI.2015.2391200
   Hott JS, 2004, NEUROSURGERY, V54, P1131, DOI 10.1227/01.NEU.0000119755.71141.13
   Houten JK, 2012, NEUROSURGERY, V70, P990, DOI 10.1227/NEU.0b013e318237a829
   Hubner P, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041021
   Jin MR, 2017, INT ORTHOP, V41, P773, DOI 10.1007/s00264-016-3353-6
   Kim YJ, 2004, SPINE, V29, P333, DOI 10.1097/01.BRS.0000109983.12113.9B
   Kosmopoulos V, 2007, SPINE, V32, pE111, DOI 10.1097/01.brs.0000254048.79024.8b
   Manni F., 2018, P 2018 25 IEEE INT C
   Manni F, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10124078
   Manni F, 2019, IEEE ENG MED BIO, P3909, DOI 10.1109/EMBC.2019.8856304
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nemec SF, 2007, EUR J RADIOL, V62, P192, DOI 10.1016/j.ejrad.2006.11.029
   Oertel MF, 2011, J NEUROSURG-SPINE, V14, P532, DOI 10.3171/2010.10.SPINE091032
   Parker SL, 2011, NEUROSURGERY, V68, P170, DOI 10.1227/NEU.0b013e3181fdfaf4
   Rajasekaran S, 2007, SPINE, V32, pE56, DOI 10.1097/01.brs.0000252094.64857.ab
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E., 2011, P 2011 INT C COMP VI
   Seitel A, 2016, INT J COMPUT ASS RAD, V11, P107, DOI 10.1007/s11548-015-1156-7
   Suenaga H, 2015, BMC MED IMAGING, V15, DOI 10.1186/s12880-015-0089-5
   Thomale Ulrich-W, 2005, Comput Aided Surg, V10, P151
   Tian NF, 2011, EUR SPINE J, V20, P846, DOI 10.1007/s00586-010-1577-5
   Uehara M, 2017, SPINE J, V17, P499, DOI 10.1016/j.spinee.2016.10.019
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   Van de Kelft E, 2012, SPINE, V37, pE1580, DOI 10.1097/BRS.0b013e318271b1fa
   Viau M, 2002, J SPINAL DISORD TECH, V15, P450, DOI 10.1097/00024720-200212000-00002
   Zhang W, 2017, EUR SPINE J, V26, P1756, DOI 10.1007/s00586-016-4930-5
   Zuiderveld K., 2014, GRAPHICS GEMS, V1994, P474
NR 45
TC 6
Z9 6
U1 4
U2 7
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JUL
PY 2020
VL 20
IS 13
AR 3641
DI 10.3390/s20133641
PG 17
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA MQ8KV
UT WOS:000553142700001
PM 32610555
OA gold, Green Published
DA 2022-02-10
ER

PT C
AU Brouwers, GMYE
   Zwemer, MH
   Wijnhoven, RGJ
   de With, PHN
AF Brouwers, Guido M. Y. E.
   Zwemer, Matthijs H.
   Wijnhoven, Rob G. J.
   de With, Peter H. N.
BE Hua, G
   Jegou, H
TI Automatic Calibration of Stationary Surveillance Cameras in the Wild
SO COMPUTER VISION - ECCV 2016 WORKSHOPS, PT II
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 14th European Conference on Computer Vision (ECCV)
CY OCT 08-16, 2016
CL Amsterdam, NETHERLANDS
DE Automatic camera calibration; Vanishing points
ID SELF-CALIBRATION; AUTOCALIBRATION; VIDEO
AB We present a fully automatic camera calibration algorithm for monocular stationary surveillance cameras. We exploit only information from pedestrians tracks and generate a full camera calibration matrix based on vanishing-point geometry. This paper presents the first combination of several existing components of calibration systems from literature. The algorithm introduces novel pre- and post-processing stages that improve estimation of the horizon line and the vertical vanishing point. The scale factor is determined using an average body height, enabling extraction of metric information without manual measurement in the scene. Instead of evaluating performance on a limited number of camera configurations (video seq.) as in literature, we have performed extensive simulations of the calibration algorithm for a large range of camera configurations. Simulations reveal that metric information can be extracted with an average error of 1.95% and the derived focal length is more accurate than the reported systems in literature. Calibration experiments with real-world surveillance datasets in which no restrictions are made on pedestrian movement and position, show that the performance is comparable (max. error 3.7 %) to the simulations, thereby confirming feasibility of the system.
C1 [Brouwers, Guido M. Y. E.; Zwemer, Matthijs H.; Wijnhoven, Rob G. J.] ViNot BV, Eindhoven, Netherlands.
   [Zwemer, Matthijs H.; de With, Peter H. N.] Eindhoven Univ Technol, Eindhoven, Netherlands.
RP Brouwers, GMYE (corresponding author), ViNot BV, Eindhoven, Netherlands.
EM guido.brouwers@vinotion.nl; m.zwemer@tue.nl; rob.wijnhoven@vinotion.nl;
   p.h.n.de.With@tue.nl
OI Zwemer, Matthijs/0000-0003-0835-202X
CR Berclaz J., 2011, IEEE T PATTERN ANAL
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dubska M., 2014, P BRIT MACH VIS C
   Faugeras O. D., 1986, Proceedings CVPR '86: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.86CH2290-5), P15
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Hartley R. I., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P471
   Huang S, 2016, IEEE IPCCC
   Krahnstoever N, 2005, IEEE I CONF COMP VIS, P1858
   Kusakunniran W, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P250, DOI 10.1109/DICTA.2009.49
   Liu J., 2011, BRIT MACH VIS C BMVC
   Liu JC, 2013, IEEE WORK APP COMP, P433, DOI 10.1109/WACV.2013.6475051
   Lv FJ, 2006, IEEE T PATTERN ANAL, V28, P1513, DOI 10.1109/TPAMI.2006.178
   Lv FJ, 2002, INT C PATT RECOG, P562, DOI 10.1109/ICPR.2002.1044793
   MAYBANK SJ, 1992, INT J COMPUT VISION, V8, P123, DOI 10.1007/BF00127171
   Micusik B, 2010, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2010.5539786
   MILLAR WJ, 1986, J EPIDEMIOL COMMUN H, V40, P319, DOI 10.1136/jech.40.4.319
   Orghidan R, 2012, FED CONF COMPUT SCI, P123
   Possegger Horst, 2012, P COMP VIS WINT WORK
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 19
TC 6
Z9 6
U1 0
U2 3
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-48881-3; 978-3-319-48880-6
J9 LECT NOTES COMPUT SC
PY 2016
VL 9914
BP 743
EP 759
DI 10.1007/978-3-319-48881-3_52
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BG5HT
UT WOS:000389501700052
DA 2022-02-10
ER

PT C
AU Kezebou, L
   Oludare, V
   Panetta, K
   Agaian, S
AF Kezebou, Landry
   Oludare, Victor
   Panetta, Karen
   Agaian, Sos
BE Agaian, SS
   Asari, VK
   DelMarco, SP
   Jassim, SA
TI A Deep Neural Network Framework for Detecting Wrong-Way Driving
   Incidents on Highway Roads
SO MULTIMODAL IMAGE EXPLOITATION AND LEARNING 2021
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Multimodal Image Exploitation and Learning
CY APR 12-16, 2021
CL ELECTR NETWORK
SP SPIE
DE Wrong-Way Driver Detection; Wrong-Way Driving; Object Detection; Object
   Tracking; Highway Incident Detection; Computer Vision; Centroid Tracking
AB One of the numerous drawbacks of existing systems for wrong-way driver detection (WWD) is that they require installation and maintenance of expensive sensor networks. More importantly, they fail to leverage on the growing number of traffic surveillance camera networks. Approaching wrong way driver detection from a computer vision standpoint is a rather intricate one if not well thought out. As such, recent methods which explored alternative deep learning approach for solving this problem have been shown to exhibit a high rate of false detection and consider very limited settings e.g. exit ramps. In this paper, we propose a more sophisticated computer vision framework to address the shortcomings of existing systems while also leveraging on existing preinstalled large-scale camera infrastructure to achieve real-time WWD detection with high precision. The proposed framework combines four modules working collaboratively to deliver desired results. This includes: (i) a Flow Detection Module which is initialized to determine the correct direction of flow by momentarily observing the traffic; (ii) a state-of-the-art object detection algorithm, in this case YOLOv5, for detecting all objects of interest from each frame; (iii) a sophisticated centroid-based object tracker coupled with Hungarian matching algorithm for efficiently tracking objects of interest; and (iv) a wrong way flagging module to flag vehicles moving opposite to a lane's computed flow direction as they enter and exit the camera's field of view. The Hungarian algorithm ensures that each object of interest is assigned a unique ID which not only reinforces tracking efficiency of the object tracker, but also provides traffic count capability. Tracking paths are compared against computed direction of flow to instantly detect wrong way driving. The proposed architecture achieves state-of-the-art performance with high True Positive Rate and low false detections. One of the several benefits of the proposed method is that it could potentially be integrated into the department of transport (DOT) surveillance system to significantly reduce the cognitive load pressure on traffic control agents who are overwhelmed by the large number of video feeds they are tasked to monitor in real-time. Alerts generated from this system could help mitigate such issues.
C1 [Kezebou, Landry; Oludare, Victor; Panetta, Karen] Tufts Univ, Medford, MA 02155 USA.
   [Agaian, Sos] CUNY, New York, NY 10021 USA.
RP Kezebou, L (corresponding author), Tufts Univ, Medford, MA 02155 USA.
FU US Department of Transportation, Federal Highway Administration (FHWA)
   [693JJ320C000023]
FX This work is supported by the US Department of Transportation, Federal
   Highway Administration (FHWA), grant contract: 693JJ320C000023
CR [Anonymous], 2011, WRONG WAY DRIVING DE
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Edmonds J., MAXIMUM MATCHING POL
   Farhadi A., 2018, COMPUT VISION PATTER, P1804
   Feng Q., 2021, CITYFLOW NL TRACKING
   Fernando WSP, 2007, 2007 THIRD INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION FOR SUSTAINABILITY, P106
   Haendeler S, 2014, IEEE VTS VEH TECHNOL
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Lucas B. D., 1981, P INT JOINT C ART IN, V2, P674, DOI 10.5555/1623264.1623280
   Machemehl R., 2016, DEV REALTIME FREEWAY
   Monteiro G, 2007, IEEE IMAGE PROC, P2393
   Monteiro G, 2007, LECT NOTES COMPUT SC, V4633, P1117
   REDMON J, 2016, PROC CVPR IEEE, P779, DOI DOI 10.1109/CVPR.2016.91
   Reynolds D., GAUSSIAN MIXTURE MOD
   Sherif HM, 2014, 2014 6TH INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION (SOCPAR), P59, DOI 10.1109/SOCPAR.2014.7007982
   Weisstein E. W., HUNGARIAN MAXIMUM MA
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
NR 17
TC 0
Z9 0
U1 2
U2 2
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-5106-4306-2
J9 PROC SPIE
PY 2021
VL 11734
AR 117340P
DI 10.1117/12.2586039
PG 15
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BS2PP
UT WOS:000705896500018
DA 2022-02-10
ER

PT C
AU Imaduddin, H
   Anwar, MK
   Perdana, MI
   Sulistijono, IA
   Risnumawan, A
AF Imaduddin, Hasan
   Anwar, Muhamad Khoirul
   Perdana, Muhammad Ilham
   Sulistijono, Indra Adji
   Risnumawan, Anhar
BE Ardiansyah, MF
   Permatasari, DI
   Muarifin
   Muliawati, TH
   Sari, DM
TI Indonesian Vehicle License Plate Number Detection using Deep
   Convolutional Neural Network
SO 2018 INTERNATIONAL ELECTRONICS SYMPOSIUM ON KNOWLEDGE CREATION AND
   INTELLIGENT COMPUTING (IES-KCIC)
LA English
DT Proceedings Paper
CT 20th International Electronics Symposium on Knowledge Creation and
   Intelligent Computing (IES-KCIC)
CY OCT 29-30, 2018
CL Politeknik Elektronika Negeri Surabaya, Bali, INDONESIA
SP Inst Elect & Elect Engineers, Indonesia Sect
HO Politeknik Elektronika Negeri Surabaya
DE Deep Learning; Convolutional Neural Network; Indonesian License Plate
AB In Indonesia, the license plate can be the identity of each vehicle, absolutely in every traffic, often occur traffic violations by the driver, the easiest way to identify them is by using their license plate number. Conventional plate detection is proved not solving the problem and not effective. Some problem that usually occurs when using conventional detection is unable to crack down the violator simultaneously, fraud, etc. Therefore to overcome those problems, automatic camera detection becomes an important thing. license plate detection only using camera visualization is an interesting challenge, by applying the latest artificial intelligence that is deep learning, one of them is Convolutional Neural Network (CNN) which is very good on image classification recently. The system will automatically detect the license plate number each vehicle, when the riders entering the zebra cross area, then his license plate will be detected and stored. Besides the system can also detect the plate on the vehicle that has excessive speed, by counting on the counter timer, and when there are vehicles that exceeding the speed limit it will detect the license plate.
C1 [Imaduddin, Hasan] PENS, Comp Engn Div, Kampus PENS,Jalan Raya ITS Sukolilo, Surabaya 60111, Indonesia.
   [Anwar, Muhamad Khoirul; Perdana, Muhammad Ilham; Sulistijono, Indra Adji; Risnumawan, Anhar] PENS, Mechatron Engn Div, Kampus PENS,Jalan Raya ITS Sukolilo, Surabaya 60111, Indonesia.
RP Imaduddin, H (corresponding author), PENS, Comp Engn Div, Kampus PENS,Jalan Raya ITS Sukolilo, Surabaya 60111, Indonesia.
EM hasanimaduddin@ce.student.pens.ac.id; muhkhoi@me.student.pens.ac.id;
   ilhamperdana17@me.student.pens.ac.id; indra@pens.ac.id; anhar@pens.ac.id
FU Directorate General of Research Strengthening and Development, Ministry
   of Research, Technology, and Higher Education, Republic of Indonesia by
   INSINAS 2nd PhaseMinistry of Research and Technology of the Republic of
   Indonesia (RISTEK) [13/E/KPT/2018]
FX The authors would like to thanks to Directorate General of Research
   Strengthening and Development, Ministry of Research, Technology, and
   Higher Education, Republic of Indonesia for supporting this research by
   INSINAS 2nd Phase, No. 13/E/KPT/2018, Date May 7, 2018.
CR Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A., 2012, PROC 25 INT C NEURAL, P1097, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
NR 11
TC 2
Z9 2
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-8079-7
PY 2018
BP 158
EP 163
PG 6
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BM1JG
UT WOS:000459878100026
DA 2022-02-10
ER

PT C
AU Vaquero, D
   Turk, M
AF Vaquero, Daniel
   Turk, Matthew
GP IEEE
TI Composition Context Photography
SO 2015 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
SE IEEE Winter Conference on Applications of Computer Vision
LA English
DT Proceedings Paper
CT IEEE Winter Conference on Applications of Computer Vision (WACV 2015)
CY JAN 06-09, 2015
CL Waikoloa, HI
SP IEEE, IEEE Comp Soc, IEEE Biometrics Council, Amazon
AB Cameras are becoming increasingly aware of the picture-taking context, collecting extra information around the act of photographing. This contextual information enables the computational generation of a wide range of enhanced photographic outputs, effectively expanding the imaging experience provided by consumer cameras. Computer vision and computational photography techniques can be applied to provide image composites, such as panoramas, high dynamic range images, and stroboscopic images, as well as automatically selecting individual alternative frames. Our technology can be integrated into point-and-shoot cameras, and it effectively expands the photographic possibilities for casual and amateur users, who often rely on automatic camera modes.
C1 [Vaquero, Daniel] Nokia Technol, Sunnyvale, CA 94086 USA.
   [Vaquero, Daniel; Turk, Matthew] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
RP Vaquero, D (corresponding author), Nokia Technol, Sunnyvale, CA 94086 USA.
EM daniel.vaquero@nokia.com; mturk@cs.ucsb.edu
CR Adams A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778766
   Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bourke Steven, 2011, P 16 INT C INT US IN, P13, DOI DOI 10.1145/1943403.1943408
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Cohen MF, 2006, COMPUTER, V39, P40, DOI 10.1109/MC.2006.281
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369
   Duchowski A., 2007, EYE TRACKING METHODO
   Eisemann E., 2004, ACM T GRAPHIC, V23, P673
   Fiss J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024162
   Hakansson Maria, 2006, P 4 NORD C HUM COMP, P262, DOI 10.1145/1182475.1182503
   Holleis P, 2005, 25th IEEE International Conference on Distributed Computing Systems Workshops, Proceedings, P536, DOI 10.1109/ICDCSW.2005.33
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Mihal A., ENBLEND ENFUSE 4 0
   NOMURA Y., 2007, P 18 EUR C REND TECH, P127
   Peterson B. F., 2008, UNDERSTANDING SHUTTE
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251
   Telleen J, 2007, COMPUT GRAPH FORUM, V26, P591, DOI 10.1111/j.1467-8659.2007.01082.x
   Vaquero D. A., 2012, THESIS
   Wagner D, 2010, P IEEE VIRT REAL ANN, P211, DOI 10.1109/VR.2010.5444786
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
NR 24
TC 0
Z9 1
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2472-6737
BN 978-1-4799-6683-7
J9 IEEE WINT CONF APPL
PY 2015
BP 649
EP 656
DI 10.1109/WACV.2015.92
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BF3EN
UT WOS:000380532600085
DA 2022-02-10
ER

PT C
AU Ventura, J
   Arth, C
   Reitmayr, G
   Schmalstieg, D
AF Ventura, Jonathan
   Arth, Clemens
   Reitmayr, Gerhard
   Schmalstieg, Dieter
GP IEEE
TI A Minimal Solution to the Generalized Pose-and-Scale Problem
SO 2014 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
LA English
DT Proceedings Paper
CT 27th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 23-28, 2014
CL Columbus, OH
SP Comp Vis Fdn, IEEE, IEEE Comp Soc
AB We propose a novel solution to the generalized camera pose problem which includes the internal scale of the generalized camera as an unknown parameter. This further generalization of the well-known absolute camera pose problem has applications in multi-frame loop closure. While a well-calibrated camera rig has a fixed and known scale, camera trajectories produced by monocular motion estimation necessarily lack a scale estimate. Thus, when performing loop closure in monocular visual odometry, or registering separate structure-from-motion reconstructions, we must estimate a seven degree-of-freedom similarity transform from corresponding observations.
   Existing approaches solve this problem, in specialized configurations, by aligning 3D triangulated points or individual camera pose estimates. Our approach handles general configurations of rays and points and directly estimates the full similarity transformation from the 2D-3D correspondences. Four correspondences are needed in the minimal case, which has eight possible solutions. The minimal solver can be used in a hypothesize-and-test architecture for robust transformation estimation. Our solver also produces a least-squares estimate in the overdetermined case.
   The approach is evaluated experimentally on synthetic and real datasets, and is shown to produce higher accuracy solutions to multi-frame loop closure than existing approaches.
C1 [Ventura, Jonathan; Arth, Clemens; Schmalstieg, Dieter] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
   [Reitmayr, Gerhard] Qualcomm Austria Res Ctr, Vienna, Austria.
RP Ventura, J (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
EM ventura@icg.tugraz.at; arth@icg.tugraz.at; gerhardr@qti.qualcomm.com;
   schmalstieg@tugraz.at
CR Bujnak M., 2008, IEEE C COMP VIS PATT, P1
   Bujnak M, 2012, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2012.6247840
   Chen CS, 2004, IEEE T PATTERN ANAL, V26, P848, DOI 10.1109/TPAMI.2004.34
   Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221
   Clemente L., 2007, P ROB SCI SYST ATL G
   Courchay J, 2010, LECT NOTES COMPUT SC, V6312, P85, DOI 10.1007/978-3-642-15552-9_7
   Cox D., 2007, IDEALS VARIETIES ALG
   Davison A.J, 2010, ROBOTICS SCI SYSTEMS
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Eade E., 2008, BRIT MACH VIS C BMVC
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   Klopschitz M., 2008, INT S 3D DAT PROC VI
   Kneip L., 2013, IEEE INT C ROB AUT I
   Kneip L, 2011, PROC CVPR IEEE
   KUKELOVA Z, 2007, 2007 IEEE C COMP VIS, P00001
   Kukelova Z, 2008, LECT NOTES COMPUT SC, V5304, P302, DOI 10.1007/978-3-540-88690-7_23
   Kukelova Z, 2011, LECT NOTES COMPUT SC, V6493, P216
   Lepetit V., 2009, INT J COMPUTER VISIO
   Li HD, 2006, INT C PATT RECOG, P630
   Li HD, 2006, LECT NOTES COMPUT SC, V3954, P200
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nister D., 2003, IEEE C COMP VIS PATT
   Nister D., 2004, CVPR
   Nister D, 2007, J MATH IMAGING VIS, V27, P67, DOI 10.1007/s10851-006-0450-y
   Schwartz G, 2008, PUBLIC INVESTMENT AND PUBLIC-PRIVATE PARTNERSHIPS: ADDRESSING INFRASTRUCTURE CHALLENGES AND MANAGING FISCAL RISKS, P1
   Stewenius H, 2005, PROC CVPR IEEE, P789
   Sturm J., 2012, IEEE INT C INT ROB S
   Tariq S, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P296, DOI 10.1109/ISMAR.2004.6
   Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387
   Umeyama Shinji, 1991, IEEE PAMI, V13, P376
   Ventura J., 2014, IEEE VIRTUAL REALITY
   Vetterling W.T., 2007, NUMERICAL RECIPES AR
   Williams B., 2008, IEEE RSJ INT C INT R
   Winder SAJ, 2007, PROC CVPR IEEE, P17
NR 36
TC 15
Z9 16
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1063-6919
BN 978-1-4799-5117-8
J9 PROC CVPR IEEE
PY 2014
BP 422
EP 429
DI 10.1109/CVPR.2014.61
PG 8
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BD5LI
UT WOS:000361555600054
DA 2022-02-10
ER

PT C
AU Giannakeris, P
   Kaltsa, V
   Avgerinakis, K
   Briassouli, A
   Vrochidis, S
   Kompatsiaris, L
AF Giannakeris, Panagiotis
   Kaltsa, Vagia
   Avgerinakis, Konstantinos
   Briassouli, Alexia
   Vrochidis, Stefanos
   Kompatsiaris, Loannis
GP IEEE
TI Speed Estimation and Abnormality Detection from Surveillance Cameras
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
SP IEEE Comp Soc
ID ANOMALY DETECTION; MODEL
AB Motivated by the increasing industry trends towards autonomous driving, vehicles, and transportation we focus on developing a traffic analysis framework for the automatic exploitation of a large pool of available data relative to traffic applications. We propose a cooperative detection and tracking algorithm for the retrieval of vehicle trajectories in video surveillance footage based on deep CNN features that is ultimately used for two separate traffic analysis modalities: (a) vehicle speed estimation based on a state of the art fully automatic camera calibration algorithm and (b) the detection of possibly abnormal events in the scene using robust optical flow descriptors of the detected vehicles and Fisher vector representations of spatiotemporal visual volumes. Finally we measure the performance of our proposed methods in the NVIDIA AI CITY challenge evaluation dataset.
C1 [Giannakeris, Panagiotis; Kaltsa, Vagia; Avgerinakis, Konstantinos; Vrochidis, Stefanos; Kompatsiaris, Loannis] CERTH ITI, Thessaloniki, Greece.
   [Briassouli, Alexia] Maastricht Univ, Dept Data Sci & Knowledge Engn, Maastricht, Netherlands.
RP Giannakeris, P (corresponding author), CERTH ITI, Thessaloniki, Greece.
EM giannakeris@iti.gr; vagiakal@iti.gr; koafgeri@iti.gr;
   alexia.briassouli@maastrichtuniversity.nl; stefanos@iti.gr; ikom@iti.gr
OI Briassouli, Alexia/0000-0002-0545-3215; Vrochidis,
   Stefanos/0000-0002-2505-9178
FU European CommissionEuropean CommissionEuropean Commission Joint Research
   Centre [700475, 740593]
FX This work was supported by beAWARE [1] and ROBORDER [3] projects,
   partially funded by the European Commission under grant agreement No
   700475 and No 740593.
CR Cheng KW, 2015, IEEE T IMAGE PROCESS, V24, P5288, DOI 10.1109/TIP.2015.2479561
   Dubska M., 2014, BMVC, DOI DOI 10.5244/C.28.42
   Dubska M, 2015, IEEE T INTELL TRANSP, V16, P1162, DOI 10.1109/TITS.2014.2352854
   Filipiak P, 2016, LECT NOTES COMPUT SC, V9597, P803, DOI 10.1007/978-3-319-31204-0_51
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He X. C., 2007, IEEE WORKSH APPL COM, DOI DOI 10.1109/WACV.2007.7
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hospedales T, 2012, INT J COMPUT VISION, V98, P303, DOI 10.1007/s11263-011-0510-7
   Huang JT, 2017, IEEE ICC
   Jeong H, 2014, MACH VISION APPL, V25, P1501, DOI 10.1007/s00138-014-0629-y
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869
   Lan JH, 2014, OPTIK, V125, P289, DOI 10.1016/j.ijleo.2013.06.036
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Nurhadiyatna A, 2013, INT C ADV COMP SCI I, P451, DOI 10.1109/ICACSIS.2013.6761617
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saleemi I, 2009, IEEE T PATTERN ANAL, V31, P1472, DOI 10.1109/TPAMI.2008.175
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Sochor J, 2017, COMPUT VIS IMAGE UND, V161, P87, DOI 10.1016/j.cviu.2017.05.015
   Varadarajan J, 2013, INT J COMPUT VISION, V103, P100, DOI 10.1007/s11263-012-0596-6
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Wen L., 2015, ARXIV151104136
   Yang WQ, 2013, COMPUT VIS IMAGE UND, V117, P1273, DOI 10.1016/j.cviu.2012.08.010
NR 24
TC 5
Z9 6
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 93
EP 99
DI 10.1109/CVPRW.2018.00020
PG 7
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800013
DA 2022-02-10
ER

PT C
AU Ren, YH
   Wang, Y
   Tang, Q
   Jiang, HJ
   Lu, WL
AF Ren, Yanhao
   Wang, Yi
   Tang, Qi
   Jiang, Haijun
   Lu, Wenlian
GP IEEE
TI On a videoing control system based on object detection and tracking
SO 2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
LA English
DT Proceedings Paper
CT IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 24-JAN 24, 2020-2021
CL ELECTR NETWORK
SP IEEE, RSJ
AB In this paper, we propose a camera control system towards occasionally videoing preassigned objects. Based on the technique of real-time visual detection and tracking, using the Kalman filter and re-identification (ReID), we propose continuous composition of lens, based on the atomic rules of shots, and give the trajectory planning of the camera, to generate the PID controller to the pan-tilt. By both simulation and emulation by frame-wise cropping of video clips, we illustrate the efficiency of this method. Based on this model, we design and produce an AI automatic camera for lively photography and clip videoing.
C1 [Ren, Yanhao; Lu, Wenlian] Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.
   [Ren, Yanhao; Lu, Wenlian] Fudan Univ, Shanghai Ctr Math Sci, Shanghai, Peoples R China.
   [Wang, Yi; Tang, Qi; Jiang, Haijun] Fantasy Power Shanghai Culture Commun Co Ltd, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Key Lab Math Nonlinear Sci, Minist Educ, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Shanghai Key Lab Contemporary Appl Math, Shanghai, Peoples R China.
RP Ren, YH (corresponding author), Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.; Ren, YH (corresponding author), Fudan Univ, Shanghai Ctr Math Sci, Shanghai, Peoples R China.
EM 18110840015@fudan.edu.cn
FU Shanghai Municipal Science and Technology Major Project [2018SHZDZX01]
FX This work was supported by the Shanghai Municipal Science and Technology
   Major Project under Grant 2018SHZDZX01 and ZJLab.
CR Arijon D., 1976, GRAMMAR FILM LANGUAG
   Assa J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409068
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Chen L, 2018, IEEE INT CON MULTI
   Christianson DB, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P148
   Farhadi A., 2018, COMPUT VISION PATTER, P1804
   Feng Weitao, 2019, ARXIV190106129
   Joubert N., 2016, ARXIV161001691
   Joubert N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818106
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Levinson J., 2013, ROBOT SCI SYST
   Li Bo, 2018, CVPR
   Li-Wei He, 1996, Computer Graphics Proceedings. SIGGRAPH '96, P217
   Lino C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766965
   Liu W., 2016, EUROPEAN C COMPUTER, P21, DOI DOI 10.1007/978-3-319-46448-0_2
   REDMON J, 2016, PROC CVPR IEEE, P779, DOI DOI 10.1109/CVPR.2016.91
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Rhodes C, 1997, PHYS REV E, V56, P2398, DOI 10.1103/PhysRevE.56.2398
   Dinh T, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3786, DOI 10.1109/IROS.2009.5353915
   Wang N., NIPS
   Xie F, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275078
   Yang CG, 2013, IEEE T CYBERNETICS, V43, P24, DOI 10.1109/TSMCB.2012.2198813
NR 23
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2153-0858
BN 978-1-7281-6212-6
J9 IEEE INT C INT ROBOT
PY 2020
BP 8271
EP 8278
DI 10.1109/IROS45743.2020.9341721
PG 8
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BS4YL
UT WOS:000724145802074
DA 2022-02-10
ER

PT C
AU Dhar, NK
   Elizondo, LA
   Dat, R
   Elizondo, SL
AF Dhar, Nibir K.
   Elizondo, Lee A.
   Dat, Ravi
   Elizondo, Shelly L.
BE LeVan, PD
   Sood, AK
   Wijewarnasuriya, PS
   DSouza, AI
TI Advanced Imaging Systems Programs at DARPA MTO
SO INFRARED SENSORS, DEVICES, AND APPLICATIONS III
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Infrared Sensors, Devices, and Applications III
CY AUG 26-28, 2013
CL San Diego, CA
SP SPIE
DE Imaging; Infrared; Focal Plane Array; SWIR; LWIR; Nyquist limit;
   Microbolometer
AB In this paper, we review a few selected imaging technology development programs at the Defense Advanced Research Projects Agency (DARPA) in the reflective visible to the emissive/thermal long wave infrared (LWIR) spectral bands. For the reflective visible band, results are shown for two different imagers: a gigapixel monocentric multi-scale camera design that solves the scaling issues for a high pixel count, and a wide field of view and a single photon detection camera with a large dynamic range. Also, a camera with broadband capability covering both reflective and thermal bands (0.5 mu m to 5.0 mu m) with >80% quantum efficiency is discussed. In the emissive/thermal band, data is presented for both uncooled and cryogenically cooled LWIR detectors with pixel pitches approaching the fundamental detection limits. By developing wafer scale manufacturing processes and reducing the pixel size of uncooled thermal imagers, it is shown that an affordable camera on a chip, capable of seeing through obscurants in day or night, is feasible. Also, the fabrication and initial performance of the world's first 5 mu m pixel pitch LWIR camera is discussed. Lastly, we use an initial model to evaluate the signal to noise ratio and noise equivalent differential temperature as a function of well capacity to predict the performance for this thermal imager.
C1 [Dhar, Nibir K.] Def Adv Res Project Agcy, Microsyst Technol Off, 675 North Randolph St, Arlington, VA 22203 USA.
   [Elizondo, Lee A.; Elizondo, Shelly L.] Syst Planning Corp, Arlington, VA 22201 USA.
   [Dat, Ravi] Booz Allen & Hamilton Inc, Arlington, VA 22203 USA.
RP Dhar, NK (corresponding author), Def Adv Res Project Agcy, Microsyst Technol Off, 675 North Randolph St, Arlington, VA 22203 USA.
CR Acton David, 2009, P SPIE, V7298
   ASHLEY T, 1985, ELECTRON LETT, V21, P451, DOI 10.1049/el:19850321
   Brady DJ, 2012, NATURE, V486, P386, DOI 10.1038/nature11150
   Brady DJ, 2009, OPT EXPRESS, V17, P10659, DOI 10.1364/OE.17.010659
   Brady David J., 2013, P SPIE, V8657
   DANI A, 2010, FORUM HYPER IN PRESS, P1
   Dhar NK, 2012, PROC SPIE, V8353, DOI 10.1117/12.923682
   Dhar Nibir K., 2013, SOOD ADV INFRARED DE, DOI [10.5772/51665, DOI 10.5772/51665]
   Driggers Ronald G., 2013, P SPIE, V8706
   Gautam N, 2012, APPL PHYS LETT, V101, DOI 10.1063/1.4733660
   Gautam Nutan, 2013, INFRARED PH IN PRESS
   Hudson R.D., 1969, INFRARED SYSTEM ENG
   Hyun BR, 2008, ACS NANO, V2, P2206, DOI 10.1021/nn800336b
   Klem EJD, 2012, APPL PHYS LETT, V100, DOI 10.1063/1.4707377
   Klem E, 2013, PROC SPIE, V8704, DOI 10.1117/12.2019521
   Knowles P., 2011, P SOC PHOTO-OPT INS, V8185
   Li C., 2011, P SPIE, V8012
   LI C, 2010, P SPIE, V7660
   Li Chuan, 2013, P SPIE, V8704
   Li N, 2009, APPL PHYS LETT, V94, DOI 10.1063/1.3072807
   Lohrmann D, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.6.061305
   MacDougal M, 2011, PROC SPIE, V8012, DOI 10.1117/12.887447
   MacDougal Michael, 2009, P SPIE, V7298
   Maimon S, 2006, APPL PHYS LETT, V89, DOI 10.1063/1.2360235
   Marks D., 2010, OSA TOP M IM SYST
   Marks DL, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.8.083202
   Marks DL, 2011, OPT ENG, V50, DOI 10.1117/1.3554389
   Melkonian Leon, 2010, P SPIE
   Peumans P, 2003, J APPL PHYS, V93, P3693, DOI 10.1063/1.1534621
   Rogalski A, 2009, ACTA PHYS POL A, V116, P389, DOI 10.12693/APhysPolA.116.389
   Schaake H.F., 2010, P SOC PHOTO-OPT INS, V7608
   Sharifi H., 2013, P SPIE, V8704
   Smith K. D., 2012, P SPIE, V8353
   Son HS, 2011, OPT EXPRESS, V19, P16132, DOI 10.1364/OE.19.016132
NR 34
TC 5
Z9 5
U1 0
U2 13
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-0-8194-9718-5
J9 PROC SPIE
PY 2013
VL 8868
AR 886802
DI 10.1117/12.2032203
PG 24
WC Optics; Physics, Applied
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Optics; Physics
GA BHN50
UT WOS:000325975200001
DA 2022-02-10
ER

PT J
AU Zhou, MW
   Chen, WJ
   He, TY
   Zhang, QC
   Shen, JF
AF Zhou, Mingwei
   Chen, Wenjing
   He, Tianyue
   Zhang, Qican
   Shen, Junfei
TI Scan-free end-to-end new approach for snapshot camera spectral
   sensitivity estimation
SO OPTICS LETTERS
LA English
DT Article
AB Spectral sensitivity is largely related to sensor imaging, which has drawn widespread attention in computer vision. Accurate estimation becomes increasingly urgent because manufacturers rarely disclose it. In this Letter, we present a novel, compact, inexpensive, and real-time computational system for snapshot spectral sensitivity estimation. A multi-scale camera based on the multi-scale convolutional neural network is first proposed, to the best of our knowledge, to automatically extract multiplexing features of an input image by multiscale deep learning, which is vital to solving the inverse problem in sensitivity estimation. Our network is flexible and can be designed with different convolutional kernel sizes for a given application. We build a dataset with 10,500 raw images and generate an excellent pre-trained model. Commercial cameras are adopted to test model validity; the results show that our system can achieve estimation accuracy as high as 91.35%. We provide a method for system design, propose a deep learning network, build a dataset, demonstrate training process, and present experimental results with high precision. This simple and effective method provides an accurate approach for precise estimation of spectral sensitivity and is suitable for computational applications such as pathological digital stain, virtual/augmented reality display, and high-quality image acquisition. (C) 2021 Optical Society of America
C1 [Zhou, Mingwei; Chen, Wenjing; He, Tianyue; Zhang, Qican; Shen, Junfei] Sichuan Univ, Dept Optoelect Sci & Technol, Chengdu 610065, Sichuan, Peoples R China.
RP Shen, JF (corresponding author), Sichuan Univ, Dept Optoelect Sci & Technol, Chengdu 610065, Sichuan, Peoples R China.
EM shenjunfei@scu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [62105227, 62075143]
FX National Natural Science Foundation of China (62105227, 62075143).
CR Barnard K, 2002, COLOR RES APPL, V27, P152, DOI 10.1002/col.10050
   Bian YX, 2021, APL PHOTONICS, V6, DOI 10.1063/5.0039206
   Chaji S, 2018, J OPT SOC AM A, V35, P850, DOI 10.1364/JOSAA.35.000850
   Darrodi MM, 2015, J OPT SOC AM A, V32, P381, DOI 10.1364/JOSAA.32.000381
   DiCarlo JM, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P295
   Dyas B, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P144
   Finlayson G. D., 1999, 7 IEEE INT C COMP VI, V832, P835
   Finlayson G, 2016, J OPT SOC AM A, V33, P589, DOI 10.1364/JOSAA.33.000589
   Han S, 2012, PROC CVPR IEEE, P805, DOI 10.1109/CVPR.2012.6247752
   Herzog PG, 1999, J ELECTRON IMAGING, V8, P342, DOI 10.1117/1.482704
   HUBEL PM, 1994, SECOND IS&T/SID COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P45
   Jiang J, 2013, IEEE WORK APP COMP, P168, DOI 10.1109/WACV.2013.6475015
   Kawakami R, 2013, INT J COMPUT VISION, V105, P187, DOI 10.1007/s11263-013-0632-1
   Lee MH, 2012, OPT LETT, V37, P1937, DOI 10.1364/OL.37.001937
   Lee MH, 2009, OPT LETT, V34, P2664, DOI 10.1364/OL.34.002664
   Lin MF, 2013, INFORM SYST RES, V24, P906, DOI 10.1287/isre.2013.0480
   MCCLELLAN JH, 1982, P IEEE, V70, P1029, DOI 10.1109/PROC.1982.12431
   Ogden J.M, 1983, RCA ENG, V29, P33
   Ongie Gregory, 2020, IEEE Journal on Selected Areas in Information Theory, V1, P39, DOI 10.1109/JSAIT.2020.2991563
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Sahoo SK, 2017, OPTICA, V4, P1209, DOI 10.1364/OPTICA.4.001209
   Shoji T., 1999, J ELECTRON IMAGING, V8, P332
   Sugiura H, 1999, P SOC PHOTO-OPT INS, V3650, P128, DOI 10.1117/12.342857
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Urban P., 2010, 16 WORKSH COL IM PRO, P295
   Vora P. L., 1997, HPL9754
   WANDELL BA, 1987, IEEE T PATTERN ANAL, V9, P2, DOI 10.1109/TPAMI.1987.4767868
   Xu LH, 2018, OPT EXPRESS, V26, P17335, DOI 10.1364/OE.26.017335
   Xu LH, 2018, OPT EXPRESS, V26, P11481, DOI 10.1364/OE.26.011481
   Yngve H. Jon, 1998, P SPIE, V3409, P100
   Zhao R. K. H., 2009, M IM REC UND MIRU, P7
NR 32
TC 0
Z9 0
U1 2
U2 2
PU OPTICAL SOC AMER
PI WASHINGTON
PA 2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036 USA
SN 0146-9592
EI 1539-4794
J9 OPT LETT
JI Opt. Lett.
PD DEC 1
PY 2021
VL 46
IS 23
BP 5806
EP 5809
DI 10.1364/OL.440549
PG 4
WC Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Optics
GA XD7PQ
UT WOS:000722896900007
PM 34851895
DA 2022-02-10
ER

PT J
AU Liu, KL
   Li, Q
   Qiu, GP
AF Liu, Kanglin
   Li, Qing
   Qiu, Guoping
TI PoseGAN: A pose-to-image translation framework for camera localization
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Camera localization; Generative Adversarial Networks (GANs);
   Pose-to-image translation
AB Camera localization is a fundamental requirement in robotics and computer vision. This paper introduces a pose-to-image translation framework to tackle the camera localization problem. We present PoseGANs, a conditional generative adversarial networks (cGANs) based framework for the implementation of pose-to-image translation. PoseGANs feature a number of innovations including a distance metric based conditional discriminator to conduct camera localization and a pose estimation technique for generated camera images as a stronger constraint to improve camera localization performance. Compared with learning-based regression methods such as PoseNet, PoseGANs can achieve better performance with model sizes that are 70% smaller. In addition, PoseGANs introduce the view synthesis technique to establish the correspondence between the 2D images and the scene, i.e., given a pose, PoseGANs are able to synthesize its corresponding camera images. Furthermore, we demonstrate that PoseGANs differ in principle from structure-based localization and learning-based regressions for camera localization, and show that PoseGANs exploit the geometric structures to accomplish the camera localization task, and is therefore more stable than and superior to learning-based regressions which rely on local texture features instead. In addition to camera localization and view synthesis, we also demonstrate that PoseGANs can be successfully used for other interesting applications such as moving object elimination and frame interpolation in video sequences.
C1 [Liu, Kanglin; Li, Qing; Qiu, Guoping] Shenzhen Univ, Shenzhen, Peoples R China.
   [Liu, Kanglin; Qiu, Guoping] Guangdong Key Lab Intelligent Informat Proc, Shenzhen, Peoples R China.
   [Liu, Kanglin; Qiu, Guoping] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen, Peoples R China.
   [Li, Qing; Qiu, Guoping] Univ Nottingham, Nottingham, England.
RP Qiu, GP (corresponding author), Univ Nottingham, Nottingham, England.
EM guoping.qiu@nottingham.ac.uk
FU Education Department of Guangdong Province, P.R. China [2019KZDZX1028]
FX This work is partially supported by the Education Department of
   Guangdong Province, P.R. China, under project No. 2019KZDZX1028.
CR Albl C, 2015, PROC CVPR IEEE, P2292, DOI 10.1109/CVPR.2015.7298842
   Andrew B., 2018, ARXIV180911096
   Arjovsky M., 2017, PROC INT C MACH LEAR
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Balntas V., 2018, P EUR C COMP VIS ECC, P751
   Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489
   Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277
   Cai M., 2018, BMVC, V1, P8
   Castle R, 2008, TWELFTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P15, DOI 10.1109/ISWC.2008.4911577
   Cavallari T., 2017, P IEEE C COMP VIS PA
   Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Glocker B, 2013, INT SYM MIX AUGMENT, P173, DOI 10.1109/ISMAR.2013.6671777
   Glorot X., 2011, PROC 14 INT C ARTIFI, P315, DOI DOI 10.1.1.208.6449
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Gulrajani I., 2017, ADV NEURAL INFORM PR
   Hane C, 2017, IMAGE VISION COMPUT, V68, P14, DOI 10.1016/j.imavis.2017.07.003
   IOFFE S, 2015, ARXIV 1502 03167, V1502, DOI DOI 10.1007/S13398-014-0173-7.2
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Krull A, 2017, PROC CVPR IEEE, P2566, DOI 10.1109/CVPR.2017.275
   Laskar Z, 2017, IEEE INT CONF COMP V, P920, DOI 10.1109/ICCVW.2017.113
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Melekhov I, 2017, LECT NOTES COMPUT SC, V10617, P675, DOI 10.1007/978-3-319-70353-4_57
   Melekhov I, 2017, IEEE INT CONF COMP V, P870, DOI 10.1109/ICCVW.2017.107
   Meng LL, 2017, IEEE INT C INT ROBOT, P6886, DOI 10.1109/IROS.2017.8206611
   Miyato T., 2018, INT C LEARN REPR
   Miyato T., 2018, 6 INT C LEARN REPR I
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Radford A, 2015, ARXIV151106434
   Radwan N, 2018, IEEE ROBOT AUTOM LET, V3, P4407, DOI 10.1109/LRA.2018.2869640
   Saha Soham, 2018, BMVC
   Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662
   Sattler Torsten, 2019, P IEEE C COMP VIS PA, P3302
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790
   Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596
   Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
   Zisserman A., 2013, DEEP INSIDE CONVOLUT
NR 48
TC 4
Z9 4
U1 1
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD AUG
PY 2020
VL 166
BP 308
EP 315
DI 10.1016/j.isprsjprs.2020.06.010
PG 8
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Physical Geography; Geology; Remote Sensing; Imaging Science &
   Photographic Technology
GA MO1AQ
UT WOS:000551268300024
OA Green Submitted
DA 2022-02-10
ER

PT C
AU Chawla, H
   Jukola, M
   Marzban, S
   Arani, E
   Zonooz, B
AF Chawla, Hemang
   Jukola, Matti
   Marzban, Shabbir
   Arani, Elahe
   Zonooz, Bahram
BE Farinella, GM
   Radeva, P
   Braz, J
   Bouatouch, K
TI Practical Auto-calibration for Spatial Scene-understanding from
   Crowdsourced Dashcamera Videos
SO VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS -
   VOL. 5: VISAPP
LA English
DT Proceedings Paper
CT 16th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP) / 16th
   International Conference on Computer Vision Theory and Applications
   (VISAPP)
CY FEB 08-10, 2021
CL ELECTR NETWORK
DE Vision for Robotics; Crowdsourced Videos; Auto-calibration; Depth
   Estimation; Ego-motion Estimation
AB Spatial scene-understanding, including dense depth and ego-motion estimation, is an important problem in computer vision for autonomous vehicles and advanced driver assistance systems. Thus, it is beneficial to design perception modules that can utilize crowdsourced videos collected from arbitrary vehicular onboard or dashboard cameras. However, the intrinsic parameters corresponding to such cameras are often unknown or change over time. Typical manual calibration approaches require objects such as a chessboard or additional scene-specific information. On the other hand, automatic camera calibration does not have such requirements. Yet, the automatic calibration of dashboard cameras is challenging as forward and planar navigation results in critical motion sequences with reconstruction ambiguities. Structure reconstruction of complete visualsequences that may contain tens of thousands of images is also computationally untenable. Here, we propose a system for practical monocular onboard camera auto-calibration from crowdsourced videos. We show the effectiveness of our proposed system on the KITTI raw, Oxford RobotCar, and the crowdsourced D-2-City datasets in varying conditions. Finally, we demonstrate its application for accurate monocular dense depth and ego-motion estimation on uncalibrated videos.
C1 [Chawla, Hemang; Jukola, Matti; Marzban, Shabbir; Arani, Elahe; Zonooz, Bahram] Navinfo Europe, Adv Res Lab, Eindhoven, Netherlands.
RP Chawla, H (corresponding author), Navinfo Europe, Adv Res Lab, Eindhoven, Netherlands.
CR Arani E., 2021, P IEEE CVF WINT C AP
   Chawla H., 2020, 2020 IEEE 23 INT C I
   Chawla H., 2020 IEEE RSJ INT C
   Che Z., 2019, ARXIV PREPRINT ARXIV
   Dabeer O, 2017, IEEE INT C INT ROBOT, P634, DOI 10.1109/IROS.2017.8202218
   de Agapito L., 1998, BRIT MACH VIS C BMVC, P1
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gherardi R, 2010, LECT NOTES COMPUT SC, V6311, P790, DOI 10.1007/978-3-642-15549-9_57
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kukelova Z, 2015, IEEE I CONF COMP VIS, P2309, DOI 10.1109/ICCV.2015.266
   Lopez Manuel, 2019, P IEEE C COMP VIS PA, P11817
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Meister S, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P7251
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Santana-Cedres D, 2017, COMPUT VIS IMAGE UND, V161, P1, DOI 10.1016/j.cviu.2017.05.016
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Steger C, 2012, ISPRS J PHOTOGRAMM, V74, P202, DOI 10.1016/j.isprsjprs.2012.09.012
   Tummala GK, 2019, IPSN '19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P157, DOI 10.1145/3302506.3310397
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wildenauer H, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.106
   Wu CC, 2014, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2014.11
   Ying ZQ, 2016, INT CONF ACOUST SPEE, P1921, DOI 10.1109/ICASSP.2016.7472011
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu J, 2019, IEEE I CONF COMP VIS, P3838, DOI 10.1109/ICCV.2019.00394
   Zhuang BB, 2019, IEEE INT C INT ROBOT, P3766, DOI 10.1109/IROS40897.2019.8967912
NR 29
TC 0
Z9 0
U1 1
U2 1
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-488-6
PY 2021
BP 869
EP 880
DI 10.5220/0010255808690880
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BR6JA
UT WOS:000661288200092
OA Green Submitted, hybrid
DA 2022-02-10
ER

PT C
AU John, V
   Englebienne, G
   Krose, B
AF John, Vijay
   Englebienne, Gwenn
   Krose, Ben
BE Fusiello, A
   Murino, V
   Cucchiara, R
TI Relative Camera Localisation in Non-overlapping Camera Networks Using
   Multiple Trajectories
SO COMPUTER VISION - ECCV 2012, PT III
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 12th European Conference on Computer Vision (ECCV)
CY OCT 07-13, 2012
CL Florence, ITALY
SP Google, Natl Robot Engn Ctr, Adobe, Microsoft Res, Mitsubishi Elect, Mobileye, Nvidia, Point Grey, Technicolor, Toshiba, Toyota, Datalogic, IBM Res, ST, Univ Studi Firenze, Univ Cambridge, Ente Cassa Risparmio Firenze
AB In this article we present an automatic camera calibration algorithm using multiple trajectories in a multiple camera network with non-overlapping field-of-views (FOV). Visible trajectories within a camera FOV are assumed to be measured with respect to the camera local co-ordinate system. Calibration is performed by aligning each camera local co-ordinate system with a pre-defined global co-ordinate system using three steps. Firstly, extrinsic pair-wise calibration parameters are estimated using particle swarm optimisation and Kalman filtering. The resulting pair-wise calibration estimates are used to generate an initial estimate of network calibration parameters, which are corrected to account for accumulation errors using particle swarm optimisation-based local search. Finally, a Bayesian framework with Metropolis algorithm is adopted and the posterior distribution over the network calibration parameters are estimated. We validate our algorithm using studio and synthetic datasets and compare our approach with existing state-of-the-art algorithms.
C1 [John, Vijay; Englebienne, Gwenn; Krose, Ben] Univ Amsterdam, Intelligent Autonomous Syst Grp, Amsterdam, Netherlands.
RP John, V (corresponding author), Univ Amsterdam, Intelligent Autonomous Syst Grp, Amsterdam, Netherlands.
EM v.c.k.john@uva.nl; englebienne@uva.nl; b.j.a.krose@uva.nl
OI John, Vijay/0000-0002-9553-0906
CR Anjum N, 2011, J ELECTR COMPUT ENG, V2011, DOI 10.1155/2011/604647
   BEICHL I, 2000, COMPUTING SCI ENG
   Chen G., 1990, IEEE T AEROSPACE ELE
   Ghahramani Z., 1996, CRGTR962 U TOR DEP C
   JAVED O, 2003, ICME
   John V, 2010, IMAGE VISION COMPUT, V28, P1530, DOI 10.1016/j.imavis.2010.03.008
   Kelley C., 2003, SOLVING NONLINEAR EQ
   KUMAR R, 2008, CVPR
   Micusik B., 2011, CVPR
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   RAHIMI A, 2004, CVPR
   Shi Y., 1998, IEEE INT C EV COMP P
NR 12
TC 3
Z9 3
U1 1
U2 3
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-642-33885-4; 978-3-642-33884-7
J9 LECT NOTES COMPUT SC
PY 2012
VL 7585
BP 141
EP 150
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods;
   Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BE3TK
UT WOS:000371261400015
OA Bronze
DA 2022-02-10
ER

PT J
AU Singh, S
   Shekhar, C
   Vohra, A
AF Singh, Sanjay
   Shekhar, Chandra
   Vohra, Anil
TI Real-Time FPGA-Based Object Tracker with Automatic Pan-Tilt Features for
   Smart Video Surveillance Systems
SO JOURNAL OF IMAGING
LA English
DT Article
DE real-time object tracking; VLSI architecture; FPGA implementation; video
   surveillance system; smart camera system
ID PARTICLE FILTERS; SEGMENTATION; IMPLEMENTATION
AB The design of smart video surveillance systems is an active research field among the computer vision community because of their ability to perform automatic scene analysis by selecting and tracking the objects of interest. In this paper, we present the design and implementation of an FPGA-based standalone working prototype system for real-time tracking of an object of interest in live video streams for such systems. In addition to real-time tracking of the object of interest, the implemented system is also capable of providing purposive automatic camera movement (pan-tilt) in the direction determined by movement of the tracked object. The complete system, including camera interface, DDR2 external memory interface controller, designed object tracking VLSI architecture, camera movement controller and display interface, has been implemented on the Xilinx ML510 (Virtex-5 FX130T) FPGA Board. Our proposed, designed and implemented system robustly tracks the target object present in the scene in real time for standard PAL (720 x 576) resolution color video and automatically controls camera movement in the direction determined by the movement of the tracked object.
C1 [Singh, Sanjay; Shekhar, Chandra] CSIR, Cent Elect Engn Res Inst, Pilani 333031, Rajasthan, India.
   [Vohra, Anil] Kurukshetra Univ, Elect Sci Dept, Kurukshetra 136119, Haryana, India.
RP Singh, S (corresponding author), CSIR, Cent Elect Engn Res Inst, Pilani 333031, Rajasthan, India.
EM sanjay.csirceeri@gmail.com; chandra@ceeri.ernet.in; vohra64@gmail.com
RI ; Shekhar, Chandra/O-3381-2017
OI Singh, Sanjay/0000-0002-2249-799X; Shekhar, Chandra/0000-0002-2114-9096
FU Department of Electronics & Information Technology (DeitY)/Ministry of
   Communications and Information Technology (MCIT), the Government of
   India
FX The financial support of Department of Electronics & Information
   Technology (DeitY)/Ministry of Communications and Information Technology
   (MCIT), the Government of India, is gratefully acknowledged.
CR Abd El-Halym HA, 2010, IEEE IMAGE PROC, P4497, DOI 10.1109/ICIP.2010.5653817
   Adam A., 2006, P IEEE C COMP VIS PA, P798, DOI DOI 10.1109/CVPR.2006.256
   Agrawal S, 2012, 2012 INTERNATIONAL SYMPOSIUM ON ELECTRONIC SYSTEM DESIGN (ISED 2012), P82, DOI 10.1109/ISED.2012.41
   Ahmed J., 2007, P NAT C ART INT, P1077
   Ahmed J., 2005, P INT C MACH VIS PAT, P1
   Ahmed J., 2007, INT J COMPUT INF SYS, V1, P1825
   Ahmed J, 2016, J REAL-TIME IMAGE PR, V11, P315, DOI 10.1007/s11554-012-0251-z
   Al Haj Murad, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1690, DOI 10.1109/ICPR.2010.418
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436
   Chan SC, 2013, IEEE J EM SEL TOP C, V3, P248, DOI 10.1109/JETCAS.2013.2256822
   Chen XL, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P423, DOI 10.1109/ICMI.2002.1167032
   Cho J. U., 2007, P INT C CONTR AUT SY, P1163
   Cho JU, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-5, P172
   Coifman B, 1998, TRANSPORT RES C-EMER, V6, P271, DOI 10.1016/S0968-090X(98)00019-9
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dargazany A., 2010, ADV ARTIF INTELL, V2010
   Doulamis A, 2003, IEEE T NEURAL NETWOR, V14, P616, DOI 10.1109/TNN.2003.810605
   ELEFTHERIADIS A, 1995, SIGNAL PROCESS-IMAGE, V7, P231, DOI 10.1016/0923-5965(95)00028-U
   Elkhatib LN, 2012, 2012 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT AND ADVANCED SYSTEMS (ICIAS), VOLS 1-2, P745, DOI 10.1109/ICIAS.2012.6306112
   Erdem CE, 2007, SIGNAL PROCESS-IMAGE, V22, P891, DOI 10.1016/j.image.2007.09.001
   Gevers T, 2004, IEEE T CIRC SYST VID, V14, P776, DOI 10.1109/TCSVT.2004.828347
   Gupta N, 2004, IEEE IMAGE PROC, P1041
   Gustafsson F, 2002, IEEE T SIGNAL PROCES, V50, P425, DOI 10.1109/78.978396
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Ho J, 2004, PROC CVPR IEEE, P782
   Intille SS, 1997, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.1997.609402
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Johnston T., 2005, P INT C SENS TECHN I, P66
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kang S, 2003, PROC SPIE, V5132, P103, DOI 10.1117/12.514945
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P122, DOI 10.1109/76.988659
   Kristensen F, 2008, J SIGNAL PROCESS SYS, V52, P75, DOI 10.1007/s11265-007-0100-7
   Lee SG, 2011, COMM COM INF SC, V206, P121
   Li CM, 2005, Proceedings of 2005 International Conference on Machine Learning and Cybernetics, Vols 1-9, P4957
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   McErlean M, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1 AND 2, P242, DOI 10.1109/ISSPIT.2006.270805
   Namboodiri VP, 2006, LECT NOTES COMPUT SC, V4338, P504
   Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4
   Papoutsakis KE, 2010, LECT NOTES COMPUT SC, V6453, P405
   Paschalakis S, 2004, REAL-TIME IMAGING, V10, P81, DOI 10.1016/j.rti.2004.02.004
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Peddigari V, 2007, J REAL-TIME IMAGE PR, V2, P45, DOI 10.1007/s11554-007-0036-y
   Perez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Popescu D, 2010, UNIV POLIT BUCHAR S, V72, P121
   Porikli F, 2005, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2005.188
   Porikli F, 2006, P IEEE COMP SOC C CO, DOI [DOI 10.1109/CVPR.2006.94, 10.1109/CVPR.2006.94]
   Porikli F, 2006, J REAL-TIME IMAGE PR, V1, P33, DOI 10.1007/s11554-006-0011-z
   Raju K. S., 2012, INT J COMPUT SCI ISS, V9, P43
   Raju K. S., 2013, INT J COMPUT APPL, V69, P41
   Shahzad M, 2009, ICDIP 2009: INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING, PROCEEDINGS, P220, DOI 10.1109/ICDIP.2009.81
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Su Liu, 2011, Proceedings of the 2011 Symposium on Application Accelerators in High-Performance Computing (SAAHPC 2011), P1, DOI 10.1109/SAAHPC.2011.22
   Tai JC, 2004, IMAGE VISION COMPUT, V22, P485, DOI 10.1016/j.imavis.2003.12.001
   Dinh T, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3786, DOI 10.1109/IROS.2009.5353915
   Tripathi S, 2009, ICAPR 2009: SEVENTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, PROCEEDINGS, P278, DOI 10.1109/ICAPR.2009.39
   Varcheie PDZ, 2009, 2009 IEEE INTERNATIONAL WORKSHOP ON ROBOTIC AND SENSORS ENVIRONMENTS (ROSE 2009), P98, DOI 10.1109/ROSE.2009.5355997
   Wong S, 2012, I C MECH MACH VIS PR, P156
   Wong S., 2005, P SOC PHOTO-OPT INS, V5810, P1
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Xiaofeng Lu, 2010, 2010 International Conference on Audio, Language and Image Processing (ICALIP), P1657, DOI 10.1109/ICALIP.2010.5685091
   Xu JB, 2007, DSD 2007: 10TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN ARCHITECTURES, METHODS AND TOOLS, PROCEEDINGS, P432, DOI 10.1109/DSD.2007.4341504
   Yamaoka K, 2006, IEEE INT SYMP CIRC S, P5575
   Yilmaz A, 2004, IEEE T PATTERN ANAL, V26, P1531, DOI 10.1109/TPAMI.2004.96
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yuan-Pao Hsu, 2010, Proceedings of the SICE 2010 - 49th Annual Conference of the Society of Instrument and Control Engineers of Japan, P2878
NR 70
TC 9
Z9 9
U1 1
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2313-433X
J9 J IMAGING
JI J. Imaging
PD JUN
PY 2017
VL 3
IS 2
AR 18
DI 10.3390/jimaging3020018
PG 28
WC Imaging Science & Photographic Technology
WE Emerging Sources Citation Index (ESCI)
SC Imaging Science & Photographic Technology
GA FF4KN
UT WOS:000408913500005
OA gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Chen, P
   Swarup, P
   Matkowski, WM
   Kong, AWK
   Han, S
   Zhang, ZH
   Rong, H
AF Chen, Peng
   Swarup, Pranjal
   Matkowski, Wojciech Michal
   Kong, Adams Wai Kin
   Han, Su
   Zhang, Zhihe
   Rong, Hou
TI A study on giant panda recognition based on images of a large proportion
   of captive pandas
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE giant panda; individual identification; panda face recognition;
   population estimation
ID CAMERA TRAP; DESIGN
AB As a highly endangered species, the giant panda (panda) has attracted significant attention in the past decades. Considerable efforts have been put on panda conservation and reproduction, offering the promising outcome of maintaining the population size of pandas. To evaluate the effectiveness of conservation and management strategies, recognizing individual pandas is critical. However, it remains a challenging task because the existing methods, such as traditional tracking method, discrimination method based on footprint identification, and molecular biology method, are invasive, inaccurate, expensive, or challenging to perform. The advances of imaging technologies have led to the wide applications of digital images and videos in panda conservation and management, which makes it possible for individual panda recognition in a noninvasive manner by using image-based panda face recognition method.
   In recent years, deep learning has achieved great success in the field of computer vision and pattern recognition. For panda face recognition, a fully automatic deep learning algorithm which consists of a sequence of deep neural networks (DNNs) used for panda face detection, segmentation, alignment, and identity prediction is developed in this study. To develop and evaluate the algorithm, the largest panda image dataset containing 6,441 images from 218 different pandas, which is 39.78% of captive pandas in the world, is established.
   The algorithm achieved 96.27% accuracy in panda recognition and 100% accuracy in detection.
   This study shows that panda faces can be used for panda recognition. It enables the use of the cameras installed in their habitat for monitoring their population and behavior. This noninvasive approach is much more cost-effective than the approaches used in the previous panda surveys.
C1 [Chen, Peng; Zhang, Zhihe; Rong, Hou] Chengdu Res Base Giant Panda Breeding, Chengdu, Peoples R China.
   [Chen, Peng; Zhang, Zhihe; Rong, Hou] Sichuan Key Lab Conservat Biol Endangered Wildlif, Chengdu, Sichuan, Peoples R China.
   [Chen, Peng; Zhang, Zhihe; Rong, Hou] Sichuan Acad Giant Panda, Chengdu, Peoples R China.
   [Swarup, Pranjal; Matkowski, Wojciech Michal; Kong, Adams Wai Kin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Han, Su] Sichuan Normal Univ, Coll Comp Sci, Chengdu, Peoples R China.
RP Zhang, ZH (corresponding author), Chengdu Res Base Giant Panda Breeding, Chengdu, Peoples R China.; Swarup, P (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM pswarup@ntu.edu.sg; zzh@panda.org.cn
RI Matkowski, Wojciech/AAX-5437-2020
OI Swarup, Pranjal/0000-0002-1766-7413
CR [Anonymous], 2018, PEOPLES DAILY ONLINE
   Burghardt T., 2007, INT C COMP VIS SYST
   Cheema GS, 2017, LECT NOTES ARTIF INT, V10536, P27, DOI 10.1007/978-3-319-71273-4_3
   Chen J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P911
   Chen J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P814
   Deb D., 2018, 2018 IEEE 9 INT C BI, P1, DOI DOI 10.1109/BTAS.2018.8698538
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guan TP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0159738
   Hansen ME, 2018, COMPUT IND, V98, P145, DOI 10.1016/j.compind.2018.02.016
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343
   Juan Chen, 2012, 2012 International Conference on Computational Problem-Solving (ICCP), P358, DOI 10.1109/ICCPS.2012.6384309
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kelly MJ, 2008, ANIM CONSERV, V11, P182, DOI 10.1111/j.1469-1795.2008.00179.x
   Kumar S, 2017, IET BIOMETRICS, V6, P139, DOI 10.1049/iet-bmt.2016.0017
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Matkowski WM, 2019, IEEE IMAGE PROC, P1680, DOI 10.1109/ICIP.2019.8803125
   McNeely JA, 1990, CONSERVING WORLDS BI
   Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
   Miller CR, 2005, MOL ECOL, V14, P1991, DOI 10.1111/j.1365-294X.2005.02577.x
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Pollard KA, 2010, J APPL ECOL, V47, P1103, DOI 10.1111/j.1365-2664.2010.01851.x
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Smallwood KS, 1998, OECOLOGIA, V113, P474, DOI 10.1007/s004420050400
   Solberg KH, 2006, BIOL CONSERV, V128, P158, DOI 10.1016/j.biocon.2005.09.025
   State Forestry Administration, 2015, GIANT PAND SICH 4 GI
   State Forestry Administration, 2006, 3 NAT SURV REP GIANT
   State Forestry Administration, 2015, REL 4 NAT SURV REP G
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Zhan XJ, 2006, CURR BIOL, V16, pR451, DOI 10.1016/j.cub.2006.05.042
   Zhan XJ, 2009, URSUS, V20, P56
   Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
   Zheng X, 2016, J ZOOL, V300, P247, DOI 10.1111/jzo.12377
NR 43
TC 8
Z9 8
U1 11
U2 27
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD APR
PY 2020
VL 10
IS 7
BP 3561
EP 3573
DI 10.1002/ece3.6152
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA LB1TE
UT WOS:000524417200033
PM 32274009
OA Green Published, gold
DA 2022-02-10
ER

PT C
AU Makantasis, K
   Protopapadakis, E
   Doulamis, A
   Grammatikopoulos, L
   Stentoumis, C
AF Makantasis, Konstantinos
   Protopapadakis, Eftychios
   Doulamis, Anastasios
   Grammatikopoulos, Lazaros
   Stentoumis, Christos
BE Fusiello, A
   Murino, V
   Cucchiara, R
TI Monocular Camera Fall Detection System Exploiting 3D Measures: A
   Semi-supervised Learning Approach
SO COMPUTER VISION - ECCV 2012, PT III
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 12th European Conference on Computer Vision (ECCV)
CY OCT 07-13, 2012
CL Florence, ITALY
SP Google, Natl Robot Engn Ctr, Adobe, Microsoft Res, Mitsubishi Elect, Mobileye, Nvidia, Point Grey, Technicolor, Toshiba, Toyota, Datalogic, IBM Res, ST, Univ Studi Firenze, Univ Cambridge, Ente Cassa Risparmio Firenze
DE image motion analysis; semisupervised learning; self calibration; fall
   detection
AB Falls have been reported as the leading cause of injury-related visits to emergency departments and the primary etiology of accidental deaths in elderly. The system presented in this article addresses the fall detection problem through visual cues. The proposed methodology utilize a fast, real-time background subtraction algorithm based on motion information in the scene and capable to operate properly in dynamically changing visual conditions, in order to detect the foreground object and, at the same time, it exploits 3D space's measures, through automatic camera calibration, to increase the robustness of fall detection algorithm which is based on semi-supervised learning. The above system uses a single monocular camera and is characterized by minimal computational cost and memory requirements that make it suitable for real-time large scale implementations.
C1 [Makantasis, Konstantinos; Protopapadakis, Eftychios; Doulamis, Anastasios] Tech Univ Crete, Khania 73100, Greece.
   [Grammatikopoulos, Lazaros] Technol Educ Inst Athens, Athens 12210, Greece.
   [Stentoumis, Christos] Natl Tech Univ Athens, GR-15773 Athens, Greece.
RP Makantasis, K (corresponding author), Tech Univ Crete, Khania 73100, Greece.
EM konst.makantasis@gmail.com; eft.protopapadakis@gmail.com;
   adoulam@cs.ntua.gr; lazaros.pcvg@gmail.com; cstent@mail.ntua.gr
RI Doulamis, Anastasios/AAL-5972-2021; Grammatikopoulos,
   Lazaros/AAY-5691-2021; Protopapadakis, Eftychios/AAP-1371-2021;
   Makantasis, Konstantinos/Q-4475-2018
OI Grammatikopoulos, Lazaros/0000-0002-3858-1352; Protopapadakis,
   Eftychios/0000-0003-3876-0024; Makantasis,
   Konstantinos/0000-0002-0889-2766
CR Bevilacqua A, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P126, DOI 10.1109/ICVGIP.2008.10
   Bianchi F, 2010, IEEE T NEUR SYS REH, V18, P619, DOI 10.1109/TNSRE.2010.2070807
   Debard G., 2011, CAMERA BASED FALL DE
   Diraco G, 2010, DES AUT TEST EUROPE, P1536
   Doulamis N., ACM 3 INT C PERV TEC
   Foroughi H, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P413, DOI 10.1109/ICVGIP.2008.49
   Fu ZM, 2008, IEEE INT SYMP CIRC S, P424, DOI 10.1109/ISCAS.2008.4541445
   Grammatikopoulos L, 2007, ISPRS J PHOTOGRAMM, V62, P64, DOI 10.1016/j.isprsjprs.2007.02.002
   Le T., 2009, BIOM CIRC SYST C BIO, P265
   Nyan MN, 2008, J BIOMECH, V41, P3475, DOI 10.1016/j.jbiomech.2008.08.009
   Qian HM, 2008, 2008 10TH INTERNATIONAL CONFERENCE ON CONTROL AUTOMATION ROBOTICS & VISION: ICARV 2008, VOLS 1-4, P1567, DOI 10.1109/ICARCV.2008.4795758
   Rougier C., 2011, IEEE T CSVT, P611
   Thome N, 2008, IEEE T CIRC SYST VID, V18, P1522, DOI 10.1109/TCSVT.2008.2005606
   Zigel Y, 2009, IEEE T BIO-MED ENG, V56, P2858, DOI 10.1109/TBME.2009.2030171
NR 14
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
BN 978-3-642-33885-4; 978-3-642-33884-7
J9 LECT NOTES COMPUT SC
PY 2012
VL 7585
BP 81
EP 90
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods;
   Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BE3TK
UT WOS:000371261400009
DA 2022-02-10
ER

PT J
AU Ilie, A
   Welch, G
AF Ilie, Adrian
   Welch, Greg
TI Online Control of Active Camera Networks for Computer Vision Tasks
SO ACM TRANSACTIONS ON SENSOR NETWORKS
LA English
DT Article
DE Algorithms; Experimentation; Measurement; Performance; Camera control;
   active camera networks; computer vision; surveillance; motion capture;
   3D reconstruction
ID SURVEILLANCE
AB Large networks of cameras have been increasingly employed to capture dynamic events for tasks such as surveillance and training. When using active cameras to capture events distributed throughout a large area, human control becomes impractical and unreliable. This has led to the development of automated approaches for online camera control. We introduce a new automated camera control approach that consists of a stochastic performancemetric and a constrained optimizationmethod. The metric quantifies the uncertainty in the state of multiple points on each target. It uses state-space methods with stochastic models of target dynamics and camera measurements. It can account for occlusions, accommodate requirements specific to the algorithms used to process the images, and incorporate other factors that can affect their results. The optimization explores the space of camera configurations over time under constraints associated with the cameras, the predicted target trajectories, and the image processing algorithms. The approach can be applied to conventional surveillance tasks (e. g., tracking or face recognition), as well as tasks employing more complex computer vision methods (e. g., markerless motion capture or 3D reconstruction).
C1 [Ilie, Adrian; Welch, Greg] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
   [Welch, Greg] Univ Cent Florida, Inst Simulat & Training, Orlando, FL 32816 USA.
   [Welch, Greg] Univ Cent Florida, Dept Elect Engn & Comp Sci, Orlando, FL 32816 USA.
RP Ilie, A (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
EM adyillie@cs.unc.edu
OI Welch, Gregory/0000-0002-8243-646X
FU ONROffice of Naval Research [N00014-08-C-0349]
FX This work was supported by ONR grant N00014-08-C-0349 for Behavior
   Analysis and Synthesis for Intelligent Training (BASE-IT), led by Greg
   Welch (PI) at UNC, Amela Sadagic (PI) at the Naval Post-graduate School,
   and Rakesh Kumar (PI) and Hui Cheng (Co-PI) at Sarnoff. Roy Stripling,
   Ph.D., Program Manager.
CR Allen B. D., 2007, THESIS U N CAROLINA
   Allen BD, 2005, VRST 05 P ACM S VIRT, P201, DOI DOI 10.1145/1101616.1101658
   Bagdanov A. D., 2005, P ACM INT WORKSH VID, P121
   Bakhtari A, 2006, IEEE T SYST MAN CY C, V36, P668, DOI 10.1109/TSMCC.2005.855525
   Bodor R, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P552
   Broaddus C., 2009, P SPIE MULTISENSOR M, V7345
   Casper M., 2007, EVENTSCRIPTS PYTHON
   CHANG CB, 1984, IEEE T AUTOMAT CONTR, V29, P98
   Chen X., 2002, THESIS STANFORD U
   Costello C. J., 2004, P ACM 2 INT WORKSH V, P39
   Davis J., 2002, THESIS STANFORD U
   Davison A. J., 2002, IEEE T PATTERN ANAL
   Del Bimbo A, 2006, PATTERN RECOGN LETT, V27, P1826, DOI 10.1016/j.patrec.2006.02.014
   Denzler J., 2002, Pattern Recognition. 24th DAGM Symposium. Proceedings (Lecture Notes in Computr Science Vol.2449), P17
   Denzler J., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P305
   Denzler J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P400
   Denzler J, 2002, IEEE T PATTERN ANAL, V24, P145, DOI 10.1109/34.982896
   Denzler J., 2001, OPTIMAL OBSERVATION
   Deutsch B, 2004, LECT NOTES COMPUT SC, V3175, P359
   Deutsch B., 2005, IEEE INT C IM PROC, V3, P105
   Deutsch B, 2006, LECT NOTES COMPUT SC, V4174, P536
   Grewal M., 1993, INFORM SYSTEM SCI SE
   Guan L., 2010, THESIS U N CAROLINA
   Ilie A., 2011, P INT C DISTR SMART
   Ilie A., 2008, P WORKSH MULT MULT S
   Ilie D., 2010, THESIS U N CAROLINA
   Kailath T., 2000, LINEAR ESTIMATION IN
   Kanade T., 2000, CMURITR0012
   KRAHNSTOEVER N, 2008, P WORKSH MULT MULT S
   Krahnstoever N., 2001, P IEEE WORKSH DET RE
   Kumar V., 2005, INTRO DATA MINING, V1
   Lim S., 2007, P AS C COMP VIS
   Lim S.-N., 2005, P 3 ACM INT WORKSH V, P141
   Marcenaro L, 2001, P IEEE, V89, P1419, DOI 10.1109/5.959339
   Matsuyama T, 2002, P IEEE, V90, P1136, DOI 10.1109/JPROC.2002.801442
   Mittal A., 2004, P EUR C COMP VIS
   Mittal A, 2008, INT J COMPUT VISION, V76, P31, DOI 10.1007/s11263-007-0057-9
   Naish M. D., 2001, 2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236), P2964, DOI 10.1109/ICSMC.2001.971961
   Naish MD, 2003, ROBOT CIM-INT MANUF, V19, P283, DOI 10.1016/S0736-5845(02)00085-6
   Natarajan P., 2012, P 11 INT C AUT AG MU
   Oberti F, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P415, DOI 10.1109/ICIP.2001.958516
   Olague G, 2002, PATTERN RECOGN, V35, P927, DOI 10.1016/S0031-3203(01)00076-0
   QURESHI F, 2005, P 3 ACM INT WORKSH V, V12, P131
   Qureshi F. Z., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P177
   Qureshi F. Z., 2007, IEEE C COMP VIS PATT, P1
   RAM GSV, 2006, P 4 ACM INT WORKSH V
   Remagnino P, 2004, PATTERN RECOGN, V37, P675, DOI 10.1016/j.patcog.2003.09.017
   Roy-Chowdhury AK, 2004, IEEE T IMAGE PROCESS, V13, P960, DOI 10.1109/TIP.2004.827240
   Sommerlade E., 2010, P IEEE INT C ROB AUT
   Sommerlade E., 2008, P 5 INT WORKSH ATT C
   Sommerlade E., 2008, P IEEE C COMP VIS PA
   Sommerlade E., 2008, P WORKSH MULT MULT S
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940
   Taylor GR, 2007, PROC CVPR IEEE, P3883
   Welch G., 2001, COMPUTER GRAPHICS SI
   Welch G., 2007, P WORKSH TRENDS ISS
   Wu JJ, 1998, OPT ENG, V37, P280, DOI 10.1117/1.601615
   Yous Sofiane, 2007, Journal of Multimedia, V2, P10, DOI 10.4304/jmm.2.1.10-19
   Zhang Z., 1999, P INT C COMP VIS
NR 59
TC 5
Z9 5
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1550-4859
EI 1550-4867
J9 ACM T SENSOR NETWORK
JI ACM Trans. Sens. Netw.
PD JAN
PY 2014
VL 10
IS 2
DI 10.1145/2530283
PG 40
WC Computer Science, Information Systems; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 302SL
UT WOS:000330625100008
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Mikac, KM
   Knipler, ML
   Gracanin, A
   Newbery, MS
AF Mikac, Katarina M.
   Knipler, Monica L.
   Gracanin, Ana
   Newbery, Madeline S.
TI Ground dwelling mammal response to fire: A case study from Monga
   National Park after the 2019/2020 Clyde Mountain fire
SO AUSTRAL ECOLOGY
LA English
DT Article; Early Access
DE camera trapping; fire intensity; ground dwelling mammals; spotted-tailed
   quoll; wildfire
AB Ground dwelling mammal communities are documented six months before and after the Clyde Mountain Wildfire of 2019/2020 in Monga National Park. Across eight sites before fire, approximately 12 ground dwelling mammal species were recorded. Survey effort post-fire increased to 40 sites, spanning three fire severity classes (low, moderate and extreme), revealed 16 ground dwelling mammal species. Species consist of small, medium and large native (one threatened species) and introduced mammals, though consistent with previous findings of ground dwelling mammal diversity in the area. Overall a greater number of species were found in low, compared to moderate and severe fire severity classes. Recovery and detection of mammals occurred in a shorter time period, again, in sites that experienced low, followed by moderate and extreme fire severity.
C1 [Mikac, Katarina M.; Knipler, Monica L.; Gracanin, Ana; Newbery, Madeline S.] Univ Wollongong, Sch Earth Atmospher & Life Sci, Ctr Sustainable Ecosyst Solut, Fac Sci Med & Hlth, Northfields Ave, Wollongong, NSW 2522, Australia.
RP Mikac, KM (corresponding author), Univ Wollongong, Sch Earth Atmospher & Life Sci, Ctr Sustainable Ecosyst Solut, Fac Sci Med & Hlth, Northfields Ave, Wollongong, NSW 2522, Australia.
EM kmikac@uow.edu.au
FU University of Wollongong's Centre for Sustainable Ecosystem Solutions
   and Faculty of Science, Medicine and Health- Bushfire recovery grant;
   Browning Trail Cameras; Crowdfunding via GoFundMe
FX This research was undertaken on Walbanga Country, Yuin Nation. We pay
   our respect to all Walbanga and Yuin Elders and Community past and
   present. We also thank Chris Howard, NPWS; Joslyn and Nick, Friends of
   the Forest; Val Plumwood Trust and Natasha Fiijn; and NSW Forestry
   Corporation. This work was supported by funding from University of
   Wollongong's Centre for Sustainable Ecosystem Solutions and Faculty of
   Science, Medicine and Health- Bushfire recovery grant; Crowdfunding via
   GoFundMe; and Browning Trail Cameras.
CR [Anonymous], 2020, FIRE EXTENT SEVERITY
   Arthur AD, 2012, AUSTRAL ECOL, V37, P958, DOI 10.1111/j.1442-9993.2011.02355.x
   Catling P.C., 1991, P353
   CATLING PC, 1995, WILDLIFE RES, V22, P271, DOI 10.1071/WR9950271
   Catling PC, 2001, WILDLIFE RES, V28, P555, DOI 10.1071/WR00041
   DPIE, 2020, BION VEG CLASS
   Jones SKC, 2019, AUST MAMMAL, V41, P283, DOI 10.1071/AM18028
   NSW Rural Fire Service Department of Planning Industry and Environment (DPIE), 2020, SUPP FIR MAN FIR EXT
NR 8
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1442-9985
EI 1442-9993
J9 AUSTRAL ECOL
JI Austral Ecol.
DI 10.1111/aec.13109
EA SEP 2021
PG 5
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA UM1XU
UT WOS:000693132300001
DA 2022-02-10
ER

PT J
AU Olson, LO
   Allen, ML
AF Olson, Lucas O.
   Allen, Maximilian L.
TI A Leucisitic Fisher (Pekania pennanti) and the Prevalence of Leucism in
   Wild Carnivores
SO AMERICAN MIDLAND NATURALIST
LA English
DT Article
ID ALBINISM
AB Animal coloration has adaptive roles for communication, concealment, sexual selection, and physiological function. Genetic mutations sometimes cause abnormal coloration such as leucism, in which an animal appears partially or entirely white, except for exposed soft skin tissue. Here we document a leucistic fisher (Pekania pennant:). Fisher fur normally ranges from deep brown to black, but the function of the pelt color is not understood. The literature on the occurrence of leucism includes 33 other records of leucism among carnivores. Reporting cases of rare coloration in the wild helps to understand the distribution, prevalence, and significance of abnormal colors.
C1 [Olson, Lucas O.] Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
   [Allen, Maximilian L.] Illinois Nat Hist Survey, 1816 S Oak St, Champaign, IL 61820 USA.
RP Olson, LO (corresponding author), Univ Wisconsin, Dept Forest & Wildlife Ecol, 1630 Linden Dr, Madison, WI 53706 USA.
EM loolson@wisc.edu
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU Department of Forest and Wildlife Ecology at UW-Madison; Illinois
   Natural History Survey
FX We thank Lee Ecker for sharing the leucistic fisher photo and
   information on the camera trap placement, as well as Roger Powell and an
   anonymous reviewer for their comments on earlier versions of the
   manuscript. We also thank the Department of Forest and Wildlife Ecology
   at UW-Madison for supporting this research and the Illinois Natural
   History Survey for funding.
CR Allen ML, 2016, SCI REP-UK, V6, DOI 10.1038/srep35433
   Bensch S, 2000, HEREDITAS, V133, P167, DOI 10.1111/j.1601-5223.2000.t01-1-00167.x
   Camargo I, 2014, WEST N AM NATURALIST, V74, P366, DOI 10.3398/064.074.0301
   Caro T, 2005, BIOSCIENCE, V55, P125, DOI 10.1641/0006-3568(2005)055[0125:TASOCI]2.0.CO;2
   Arriaga-Flores JC, 2016, SOUTHWEST NAT, V61, P63
   COOKE F, 1987, AVIAN GENETICS POPUL
   Ellegren H, 1997, NATURE, V389, P593, DOI 10.1038/39303
   Ferns PN, 2003, ETHOLOGY, V109, P521, DOI 10.1046/j.1439-0310.2003.00894.x
   Goad EH, 2014, BIOL CONSERV, V176, P172, DOI 10.1016/j.biocon.2014.05.016
   Harrington Fred H., 2003, P66
   Hunter JS, 2009, BEHAV ECOL, V20, P1315, DOI 10.1093/beheco/arp144
   JEHL JR, 1985, CONDOR, V87, P439, DOI 10.2307/1367236
   KAYS R, 2017, CANDID CREATURES CAM
   Kirby R, 2018, WILDLIFE BIOL, DOI 10.2981/wlb.00334
   LAWRENCE E., 1989, HENDERSONS DICT BIOL
   Lewis JC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0032726
   Moller AP, 2001, EVOLUTION, V55, P2097
   OWEN M, 1992, IBIS, V134, P22, DOI 10.1111/j.1474-919X.1992.tb07224.x
   Powell R.A., 1993, FISHER LIFE HIST ECO
   Rosel P., 2009, ENCY MARINE MAMMALS
   SAGE BRYAN L., 1962, BRIT BIRDS, V55, P201
   Talamoni S, 2017, BIOTA NEOTROP, V17, DOI [10.1590/1676-0611-BN-2017-0328, 10.1590/1676-0611-bn-2017-0328]
   Wengert GM, 2014, J WILDLIFE MANAGE, V78, P603, DOI 10.1002/jwmg.698
NR 23
TC 3
Z9 3
U1 0
U2 5
PU AMER MIDLAND NATURALIST
PI NOTRE DAME
PA UNIV NOTRE DAME, BOX 369, ROOM 295 GLSC, NOTRE DAME, IN 46556 USA
SN 0003-0031
EI 1938-4238
J9 AM MIDL NAT
JI Am. Midl. Nat.
PD JAN
PY 2019
VL 181
IS 1
BP 133
EP 138
DI 10.1674/0003-0031-181.1.133
PG 6
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA HJ5TS
UT WOS:000457245700013
DA 2022-02-10
ER

PT C
AU Pouget, A
   Ramesh, S
   Giang, M
   Chandrapalan, R
   Tanner, T
   Prussing, M
   Timofte, R
   Ignatov, A
AF Pouget, Angeline
   Ramesh, Sidharth
   Giang, Maximilian
   Chandrapalan, Ramithan
   Tanner, Toni
   Prussing, Moritz
   Timofte, Radu
   Ignatov, Andrey
GP IEEE Comp Soc
TI Fast and Accurate Camera Scene Detection on Smartphones
SO 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGITION
   WORKSHOPS (CVPRW 2021)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 19-25, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, CVF
AB AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone, though the problem of accurate scene prediction has not yet been addressed by the research community. This paper for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD) containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper are available on the project website.
C1 [Pouget, Angeline; Ramesh, Sidharth; Giang, Maximilian; Chandrapalan, Ramithan; Tanner, Toni; Prussing, Moritz; Timofte, Radu; Ignatov, Andrey] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Timofte, R (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM radu.timofte@vision.ee.ethz.ch; andrey@vision.ee.ethz.ch
CR Abadi Martin, 2016, arXiv
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Chiang C.-M., 2020, P IEEE CVF C COMP VI, P502
   Darlow L.N., 2018, ABS181003505 CORR
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dick, 2019, ARXIV190308066
   Garnett, 2018, ADV NEURAL INFORM PR, P1967
   He FJ, 2019, WOODHEAD PUBL FOOD S, P3, DOI 10.1016/B978-0-08-100890-4.00001-9
   Howard A. G, 2017, ARXIV PREPRINT ARXIV
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Ignatov A, 2018, CVPR WORKSH, P691
   Ignatov A, 2021, IEEE COMPUT SOC CONF, P2558, DOI 10.1109/CVPRW53098.2021.00289
   Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P288, DOI 10.1007/978-3-030-11021-5_19
   Ignatov A, 2019, IEEE INT CONF COMP V, P3617, DOI 10.1109/ICCVW.2019.00447
   Ignatov Andrey, 2020, P IEEE CVF C COMP VI
   Ignatov D, 2020, PATTERN RECOGN LETT, V138, P276, DOI 10.1016/j.patrec.2020.07.033
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Krizhevsky A, 2009, THESIS U TORONTO, DOI 10.1.1.222.9220
   Li YW, 2019, IEEE I CONF COMP VIS, P5622, DOI 10.1109/ICCV.2019.00572
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339D\, 10.1109/ICCV.2019.00339]
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293
   Van Gool L., 2020, INT C MACH LEARN, P7392
   Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie SN, 2019, IEEE I CONF COMP VIS, P1284, DOI 10.1109/ICCV.2019.00137
   Yang JW, 2019, PROC CVPR IEEE, P7300, DOI 10.1109/CVPR.2019.00748
   Yaohui Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13166, DOI 10.1109/CVPR42600.2020.01318
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.5555/2969033.2969197
   Yu F., 2015, CONSTRUCTION LARGE S
   Zhou B., 2014, ADV NEURAL INFORM PR, P487
   ZHOU GB, 2017, P IEEE C COMP VIS PA, P633
NR 37
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2160-7508
BN 978-1-6654-4899-4
J9 IEEE COMPUT SOC CONF
PY 2021
BP 2569
EP 2580
DI 10.1109/CVPRW53098.2021.00290
PG 12
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS2PO
UT WOS:000705890202075
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Bezen, R
   Edan, Y
   Halachmi, I
AF Bezen, Ran
   Edan, Yael
   Halachmi, Ilan
TI Computer vision system for measuring individual cow feed intake using
   RGB-D camera and deep learning algorithms
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
LA English
DT Article
DE Individual cow feed intake; RGB-D camera; Deep learning; precision
   livestock farming (PLF); Convolutional neural networks (CNN)
ID HF RFID SYSTEM; TECHNICAL-NOTE; DAIRY; BEHAVIOR; VALIDATION; EFFICIENCY;
   PATTERNS; CNN
AB This study entailed the design and implementation of a computer vision system for cow individual feed intake measurement, based on deep Convolutional Neural Networks (CNNs) models, and a low-cost RGB-D (Red, Green, Blue, Depth) camera. Individual feed intake of dairy cows is an important variable currently unavailable in commercial dairies. An RGB-D camera was positioned above the feeding area in an open cowshed. Feed intake was estimated by combining information from the RGB and depth images. Cow identification was conducted using the RGB image. Deep learning algorithms for identification and intake estimation were developed using CNN models. Data for CNN training were acquired by a specially developed automatic data acquisition system. A range of feed weights under varied configurations were collected over a period of seven days with the setup, which included an automatic scale, cameras, and a micro-controller. Test data for feed intake was acquired in an open cowshed research dairy farm, wherein the cows were fed Total Mix Ration (TMR). Images of cows eating over a period of 36 h provided the test data for cow identification. The system was able to accurately identify 93.65% of the cows. The amount of feed consumed, which ranged from 0 to 8 kg per meal, was measured with mean absolute and square errors (MAE and MSE) of 0.127 kg, and 0.034kg(2) respectively. The analysis showed that the amount and diversity of data are important for model training. Better results were achieved for the model that was trained with high-diversity data than the model trained with homogeneous data (MAE of 1.025 kg, and MSE of 2.845 kg(2) for a model trained on shadow conditions only). Additionally, the training analysis shows that the model based on RGB-D data shows better results than the model based on depth channel data without RGB (MAE of 0.241 kg, and MSE of 0.106 kg(2)). These results suggest the potential of low-cost cameras for individual feed intake measurements in advanced dairy farms.
C1 [Bezen, Ran; Halachmi, Ilan] Agr Res Org, Inst Agr Engn, Volcani Ctr, Rishon Leziyyon, Israel.
   [Bezen, Ran; Edan, Yael; Halachmi, Ilan] Ben Gurion Univ Negev, Dept Ind Engn & Management, Beer Sheva, Israel.
RP Halachmi, I (corresponding author), Agr Res Org, Volcani Ctr, PLF Lab, Rishon Leziyyon, Israel.
EM halachmi@volcani.agri.gov.il
FU "Kandel" PLF center of expertise [20-12-0029]; Rabbi W. Gunther Plaut
   Chair in Manufacturing Engineering at Ben-Gurion University of the Negev
FX This study was supported by the Israeli Chief Scientist of Agriculture
   fund, "Kandel" PLF center of expertise, project number 20-12-0029; and
   was partially supported by the Rabbi W. Gunther Plaut Chair in
   Manufacturing Engineering at Ben-Gurion University of the Negev. We
   gratefully thank all members of the Precision Livestock Farming (PLF)
   Lab. Special thanks to Dr. Victor Bloch and Harel Levit for designing
   and building the feed monitoring setup system; and the staff of the
   Volcani research cowshed for their assistance in the study.
CR Altuntas Y, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104874
   Bach A, 2004, J DAIRY SCI, V87, P4207, DOI 10.3168/jds.S0022-0302(04)73565-1
   Bloch V, 2019, J DAIRY RES, V86, P34, DOI 10.1017/S0022029918000882
   Borchersen S., 2018, U.S. Patent, Patent No. [9,861,081, 9861081]
   Buza MH, 2014, J DAIRY SCI, V97, P3073, DOI 10.3168/jds.2013-7622
   Chapinal N, 2007, J DAIRY SCI, V90, P5732, DOI 10.3168/jds.2007-0331
   Chizzotti ML, 2015, J DAIRY SCI, V98, P3438, DOI 10.3168/jds.2014-8925
   Cole MT, 2013, CASES ON ONLINE LEARNING COMMUNITIES AND BEYOND: INVESTIGATIONS AND APPLICATIONS, P1, DOI 10.4018/978-1-4666-1936-4.ch001
   Connor EE, 2019, J DAIRY SCI, V102, P6131, DOI 10.3168/jds.2018-15407
   Espejo-Garcia B, 2019, COMPUT ELECTRON AGR, V162, P106, DOI 10.1016/j.compag.2019.03.027
   Farjon G, 2020, PRECIS AGRIC, V21, P503, DOI 10.1007/s11119-019-09679-1
   Fernandes AM, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104855
   Gene-Mola J, 2019, COMPUT ELECTRON AGR, V162, P689, DOI 10.1016/j.compag.2019.05.016
   Gonzalez LA, 2008, J DAIRY SCI, V91, P1017, DOI 10.3168/jds.2007-0530
   Halachmi I, 1998, COMPUT ELECTRON AGR, V20, P131, DOI 10.1016/S0168-1699(98)00013-1
   Halachmi I, 2016, ANIMAL, V10, P1501, DOI 10.1017/S1751731115001809
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   Herd R. M., 2003, J ANIM SCI, V81, P9, DOI DOI 10.2527/2003.8113_SUPPL_1E9X
   Holtenius K., 2018, P 9 NORD FEED SCI C
   Hu GS, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104852
   Ioffe S, 2015, PR MACH LEARN RES, P448, DOI DOI 10.1109/CVPR.2016.90
   Maselyne J, 2014, COMPUT ELECTRON AGR, V108, P209, DOI 10.1016/j.compag.2014.08.006
   Maselyne J, 2014, COMPUT ELECTRON AGR, V102, P10, DOI 10.1016/j.compag.2013.12.015
   Nevavuori P, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104859
   Ren SQ, 2015, ADV NEUR IN, V28
   Alvarez JR, 2018, COMPUT ELECTRON AGR, V155, P12, DOI 10.1016/j.compag.2018.09.039
   ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352
   Shalloo L, 2004, J DAIRY SCI, V87, P1945, DOI 10.3168/jds.S0022-0302(04)73353-6
   Shelley AN, 2016, J DAIRY SCI, V99, P386, DOI 10.3168/jds.2014-8964
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Stajnko D., 2010, NEW TRENDS TECHNOLOG, P243
   Stankovski S, 2012, SCI AGR, V69, P75, DOI 10.1590/S0103-90162012000100011
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tsai H, 2018, J PHYS D APPL PHYS, V51, DOI 10.1088/1361-6463/aac8a5
   Vandehaar MJ, 1998, J DAIRY SCI, V81, P272, DOI 10.3168/jds.S0022-0302(98)75576-6
   Volden H., 2011, NORFOR NORDIC FEED E
   Wang D, 2018, COMPUT ELECTRON AGR, V154, P443, DOI 10.1016/j.compag.2018.09.030
   Wang Z, 2006, J ANIM SCI, V84, P2289, DOI 10.2527/jas.2005-715
   Yu Y, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.06.001
   Zhang QC, 2018, INFORM FUSION, V42, P146, DOI 10.1016/j.inffus.2017.10.006
NR 40
TC 21
Z9 22
U1 8
U2 40
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0168-1699
EI 1872-7107
J9 COMPUT ELECTRON AGR
JI Comput. Electron. Agric.
PD MAY
PY 2020
VL 172
AR 105345
DI 10.1016/j.compag.2020.105345
PG 11
WC Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Computer Science
GA LC4VU
UT WOS:000525324700009
DA 2022-02-10
ER

PT C
AU Gomez-Fernandez, F
   Liu, ZC
   Pardo, A
   Mejail, M
AF Gomez-Fernandez, Francisco
   Liu, Zicheng
   Pardo, Alvaro
   Mejail, Marta
BE BayroCorrochano, E
   Hancock, E
TI Automatic Camera-Screen Localization
SO PROGRESS IN PATTERN RECOGNITION IMAGE ANALYSIS, COMPUTER VISION, AND
   APPLICATIONS, CIARP 2014
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 19th Iberoamerican Congress on Pattern Recognition (CIARP)
CY NOV 01-05, 2014
CL Puerto Vallarta, MEXICO
SP CINVESTAV, Campus Guadalajara, Mexican Assoc Comp Vis, Neural Comp & Robot, Int Assoc Pattern Recognit, Cuban Assoc Pattern Recognit, Chilean Assoc Pattern Recognit, Brazilian Comp Soc, Special Interest Grp, Spanish Assoc Pattern Recognt & Image Anal, Portuguese Assoc Pattern Recognit, INTEL Educ
DE Human-Computer Interaction; Head pose estimation; Screen localization
AB Knowing the location of the TV screen with respect to a camera it is important for many applications. This work addresses this problem in a configuration where there are people looking at the TV and a RGB-D camera facing them, located near the TV screen. We propose a method to automatically estimate the screen location and camera rotation using only people's head pose obtained from a Face Tracking analysis on the RGB-D video. We validated these algorithms on a dataset with groundtruth and obtained very promising results.
C1 [Gomez-Fernandez, Francisco; Mejail, Marta] Univ Buenos Aires, RA-1053 Buenos Aires, DF, Argentina.
   [Pardo, Alvaro] Univ Catolica Uruguay, Montevideo, Uruguay.
   [Liu, Zicheng] Microsoft Res, Washington, DC USA.
RP Gomez-Fernandez, F (corresponding author), Univ Buenos Aires, RA-1053 Buenos Aires, DF, Argentina.
CR Asteriadis S., 2013, INT J COMPUT VISION, V107, P1
   Cai Q, 2010, LECT NOTES COMPUT SC, V6313, P229
   Duda RO, 2012, PATTERN CLASSIFICATI
   Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11
   Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kondori F.A., 2011, WIR COMM SIGN PROC W, P1
   Mora K. A. F., 2012, P IEEE COMP SOC C CO, P25, DOI DOI 10.1109/CVPRW.2012.6239182
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
NR 9
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-12568-8; 978-3-319-12567-1
J9 LECT NOTES COMPUT SC
PY 2014
VL 8827
BP 588
EP 595
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Mathematical & Computational Biology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematical & Computational Biology
GA BB8DA
UT WOS:000346407400072
DA 2022-02-10
ER

PT J
AU de Silva, EMK
   Kumarasinghe, P
   Indrajith, KKDAK
   Pushpakumara, TV
   Vimukthi, RDY
   de Zoysa, K
   Gunawardana, K
   de Silva, S
AF de Silva, Elgiriyage M. K.
   Kumarasinghe, Prabhash
   Indrajith, Kottahachchi K. D. A. K.
   Pushpakumara, Tennekoon, V
   Vimukthi, Ranapura D. Y.
   de Zoysa, Kasun
   Gunawardana, Kasun
   de Silva, Shermin
TI Feasibility of using convolutional neural networks for
   individual-identification of wild Asian elephants
SO MAMMALIAN BIOLOGY
LA English
DT Article; Early Access
DE Artificial Intelligence; Computer Vision; Image Classification;
   Individual-Identification; Machine Learning; Pattern Recognition
ID CAMERA TRAPS; POPULATION; PHOTOIDENTIFICATION; MAXIMUS
AB Individual identification is a basic requirement for research in behavior, ecology and conservation. Photographic records are commonly used in situations where individuals are visually distinct. However, keeping track of identities becomes challenging with increasing population sizes and corresponding datasets. There is growing interest in the potential of deep-learning methods for computer vision to assist with automating this task. Here we apply Convolutional Neural Networks, a popular architecture for Artificial Neural Networks used in image classification, to the problem of identifying individual Asian elephants through photographs. We evaluate the performance of five different types of CNN models used in facial recognition (VGG16, ResNet50, InceptionV3, Xception, and Alexnet), on datasets representing three different feature regions (the full body, face, and ears), trained with two techniques (transfer learning vs. training from scratch) for n = 56 elephants. We tested accuracy in matching the top candidate as well as top five candidates. We found that VGG16 trained with the transfer-learning technique outperformed other models on the body and face datasets with accuracies of 21.34% and 42.35%, respectively, in matching the top candidate. Nevertheless, the best performance was achieved by an Xception model trained from the scratch on the ear dataset, with an accuracy of 89.02% for matching the top candidate and 99.27% for including the correct individual among the top five. However, this impressive level of accuracy was obtained with a dataset of 3816 labeled training images of 56 elephants. There are more than 1000 wild elephants in the population under observation, requiring extensive human effort and skill to initially annotate the images used as training data. Therefore, we consider this approach impractical for monitoring large wild populations. Nevertheless this it could be very useful in record keeping and fraud prevention for large captive elephant populations, as well as monitoring animals that have been rehabilitated and released or moved for management purposes.
C1 [de Silva, Elgiriyage M. K.; Kumarasinghe, Prabhash; Indrajith, Kottahachchi K. D. A. K.; Vimukthi, Ranapura D. Y.; de Zoysa, Kasun; Gunawardana, Kasun] Univ Colombo, Sch Comp, Colombo, Sri Lanka.
   [Pushpakumara, Tennekoon, V; de Silva, Shermin] EFECT, Colombo, Sri Lanka.
   [de Silva, Shermin] Trunks & Leaves Inc, Newtonville, MA 02460 USA.
RP de Silva, S (corresponding author), EFECT, Colombo, Sri Lanka.; de Silva, S (corresponding author), Trunks & Leaves Inc, Newtonville, MA 02460 USA.
EM shermin@trunksnleaves.org
FU U.S. Fish and Wildlife Asian Elephant Conservation Grant program
FX The photographic data were collected as part of the long-term research
   of the Udawalawe Elephant Research project with permission of the
   Department of Wildlife Conservation, Sri Lanka and supported by funding
   from the U.S. Fish and Wildlife Asian Elephant Conservation Grant
   program.
CR Adams Jeffrey D., 2006, Aquatic Mammals, V32, P374, DOI 10.1578/AM.32.3.2006.374
   Alexander JS, 2016, BIOL CONSERV, V197, P27, DOI 10.1016/j.biocon.2016.02.023
   Alexander JS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134815
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Barron UG, 2009, IRISH VET J, V62, P204
   Bedetti A., 2020, PACHYDERM, V61, P15
   Bush JM, 2016, ANIM BEHAV, V118, P65, DOI 10.1016/j.anbehav.2016.04.026
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   de Silva EMK., 2017, THESIS U COLOMBO
   de Silva Shermin, 2014, Gajah, V40, P46
   de Silva S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082788
   de Silva S, 2011, BIOL CONSERV, V144, P1742, DOI 10.1016/j.biocon.2011.03.011
   Fernando P, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0050917
   Gabriele CM, 2017, ECOSPHERE, V8, DOI 10.1002/ecs2.1641
   Ge HW, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/5987906
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Jackson RM, 2006, WILDLIFE SOC B, V34, P772, DOI 10.2193/0091-7648(2006)34[772:ESLPAU]2.0.CO;2
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   Karahan S., 2016, IEEE BIOSIG, P1, DOI [10.1109/BIOSIG.2016.7736924, DOI 10.1109/BIOSIG.2016.7736924]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krogh A, 2008, NAT BIOTECHNOL, V26, P195, DOI 10.1038/nbt1386
   Kumar S, 2017, J REAL-TIME IMAGE PR, V13, P505, DOI 10.1007/s11554-016-0645-4
   Kwasnicka H, 2010, STUD COMPUT INTELL, V263, P387
   Lahiri M, 2011, P 1 ACM INT C MULT R, P1, DOI DOI 10.1145/1991996.1992002
   Langtimm CA, 2004, MAR MAMMAL SCI, V20, P438, DOI 10.1111/j.1748-7692.2004.tb01171.x
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lonsdorf EV, 2017, J NEUROSCI RES, V95, P213, DOI 10.1002/jnr.23862
   Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
   McCoy E, 2018, FRONT MAR SCI, V5, DOI 10.3389/fmars.2018.00271
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Menon V, 2019, International Zoo Yearbook, V53, P17, DOI 10.1111/izy.12247
   Mikolajczyk Agnieszka, 2018, 2018 International Interdisciplinary PhD Workshop (IIPhDW), P117, DOI 10.1109/IIPHDW.2018.8388338
   Ostner J, 2018, INTERDISC EVOL RES, V5, P97, DOI 10.1007/978-3-319-93776-2_7
   Prakash TGSL, 2020, NAT CONSERV-BULGARIA, P51, DOI 10.3897/natureconservation.42.57283
   Raj A., 2015, OPEN INT J TECHNOL I, V15, P1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Rood E, 2010, DIVERS DISTRIB, V16, P975, DOI 10.1111/j.1472-4642.2010.00704.x
   Royle JA, 2018, ECOGRAPHY, V41, P444, DOI 10.1111/ecog.03170
   Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi CM, 2020, INTEGR ZOOL, V15, P461, DOI 10.1111/1749-4877.12453
   Shinde PP, 2018, 2018 FOURTH INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND AUTOMATION (ICCUBEA)
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Thitaram Chatchote, 2020, Gajah, V52, P56
   Towner AV, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066035
   Vidya T.N.C., 2014, Gajah, V40, P3
   Wang M, 2019, ARXIV180406655CS
   Wardrope DD, 1995, VET REC, V137, P675
   Weideman HJ, 2020, IEEE WINT CONF APPL, P1265, DOI 10.1109/WACV45572.2020.9093266
   Wursig B., 1990, Reports of the International Whaling Commission Special Issue, P43
NR 54
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1616-5047
EI 1618-1476
J9 MAMM BIOL
JI Mamm. Biol.
DI 10.1007/s42991-021-00206-2
EA JAN 2022
PG 11
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA YK8VD
UT WOS:000745482700003
DA 2022-02-10
ER

PT C
AU Yin, HP
   Jiao, XG
   Luo, XK
   Yi, C
AF Yin, Hongpeng
   Jiao, Xuguo
   Luo, Xianke
   Yi, Chai
GP IEEE
TI Sift-based Camera Tamper Detection For Video Surveillance
SO 2013 25TH CHINESE CONTROL AND DECISION CONFERENCE (CCDC)
SE Chinese Control and Decision Conference
LA English
DT Proceedings Paper
CT 25th Chinese Control and Decision Conference (CCDC)
CY MAY 25-27, 2013
CL Guiyang, PEOPLES R CHINA
SP IEEE, NE Univ, IEEE Ind Elect Chapter, IEEE Harbin Sect Control Syst Soc Chapter, Guizhou Univ, IEEE Control Syst Soc, Syst Engn Soc China, Chinese Assoc Artificial Intelligence, Chinese Assoc Automat, Tech Comm Control Theory, Chinese Assoc Aeronaut, Automat Control Soc, Chinese Assoc Syst Simulat, Simulat Methods & Modeling Soc, Intelligent Control & Management Soc
DE video surveillance; camera tamper; SIFT algorithm; covered camera
   detection; moved camera detection
AB Keeping the camera long time proper functioning without tamper is the fundamentally requirement of a video surveillance system. Traditional camera tamper detection is applied by surveillance system operators. It's large human resource consuming and inefficiency. In this paper, a SIFT-based automatic camera tamper detection algorithm for video surveillance is proposed. When camera tamper occurred, the real-time frame will be large changed. Therefore, a Sift feature based decision function is employed to detect camera tamper. The threshold is carefully chosen to reduce false alarms. Several experiments are conducted to demonstrate the effectiveness and robust of the proposed method.
C1 [Yin, Hongpeng; Jiao, Xuguo; Luo, Xianke; Yi, Chai] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Yin, HP (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM yinhongpeng@gmail.com
CR Aksay A, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P558, DOI 10.1109/AVSS.2007.4425371
   Gil-Jimenez P, 2007, LECT NOTES COMPUT SC, V4528, P222
   Grabner M, 2006, LECT NOTES COMPUT SC, V3851, P918
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kim S., 2006, C COMP VIS PATT REC, P193
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Ribnick E., 2006, IEEE INT C VID SIGN
   Saglam Ali, 2006, IEEE INT C VID SIGN, P430
   Wang XH, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION AND LOGISTICS, VOLS 1-6, P843, DOI 10.1109/ICAL.2008.4636267
NR 10
TC 13
Z9 13
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1948-9439
BN 978-1-4673-5532-2; 978-1-4673-5533-9
J9 CHIN CONT DECIS CONF
PY 2013
BP 665
EP 668
PG 4
WC Automation & Control Systems
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems
GA BHY11
UT WOS:000326977300124
DA 2022-02-10
ER

PT C
AU Shabalina, K
   Sagitov, A
   Svinin, M
   Magid, E
AF Shabalina, Ksenia
   Sagitov, Artur
   Svinin, Mikhail
   Magid, Evgeni
BE Ronzhin, A
   Rigoll, G
   Meshcheryakov, R
TI Comparing Fiducial Markers Performance for a Task of a Humanoid Robot
   Self-calibration of Manipulators: A Pilot Experimental Study
SO INTERACTIVE COLLABORATIVE ROBOTICS, ICR 2018
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 3rd International Conference on Interactive Collaborative Robotics (ICR)
CY SEP 18-22, 2018
CL Leipzig, GERMANY
DE ARTag; AprilTag; CALTag; Fiducial marker systems; AR-601M; Humanoid
   robot; Experimental comparison
AB This paper presents our pilot study of experiments automation with a real robot in order to compare performance of different fiducial marker systems, which could be used in automated camera calibration process. We used Russian humanoid robot AR-601M and automated it's manipulators for performing joint rotations. This paper is an extension of our previous work on ARTag, AprilTag and CALTag marker comparison in laboratory settings with large-sized markers that had showed significant superiority of CALTag system over the competitors. This time the markers were scaled down and placed on AR-601M humanoid's palms. We automated experiments of marker rotations, analyzed the results and compared them with the previously obtained results of manual experiments with large-sized markers. The new automated pilot experiments, which were performed both in pure laboratory conditions and pseudo field environments, demonstrated significant differences with previously obtained manual experimental results: AprilTag marker system demonstrated the best performance with a success rate of 97,3% in the pseudo field environment, while ARTag was the most successful in the laboratory conditions.
C1 [Shabalina, Ksenia; Sagitov, Artur; Magid, Evgeni] Kazan Fed Univ, Lab Intelligent Robot Syst, Kazan 420008, Russia.
   [Svinin, Mikhail] Ritsumeikan Univ, Coll Informat Sci & Engn, Robot Dynam & Control Lab, Noji Higashi 1-1-1, Kusatsu 5258577, Japan.
RP Magid, E (corresponding author), Kazan Fed Univ, Lab Intelligent Robot Syst, Kazan 420008, Russia.
EM ks.shabalina@it.kfu.ru; sagitov@it.kfu.ru; svinin@fc.ritsumei.ac.jp;
   magid@it.kfu.ru
RI Magid, Evgeni/B-9697-2014; Shabalina, Ksenia/F-9962-2018
OI Magid, Evgeni/0000-0001-7316-5664; Shabalina,
   Ksenia/0000-0003-4537-9467; Svinin, Mikhail/0000-0003-2459-2250
FU Russian Foundation for Basic Research (RFBR)Russian Foundation for Basic
   Research (RFBR) [18-58-45017]
FX This work was partially supported by the Russian Foundation for Basic
   Research (RFBR) project ID 18-58-45017. Part of the work was performed
   according to the Russian Government Program of Competitive Growth of
   Kazan Federal University.
CR Atcheson B., 2010, P VIS MOD VIS WORKSH, V10, P41
   Degol J, 2017, IEEE I CONF COMP VIS, P1481, DOI 10.1109/ICCV.2017.164
   Fiala M., 2005, 2005 IEEE International Workshop on Haptic Audio Visual Environments and thier Applications (IEEE Cat. No.05EX1164C)
   Fiala M., 2004, NATL RES COUNCIL PUB, V47419, P1
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Higashino S., 2016, ACM SIGGRAPH 2016 PO, P38, DOI [10.1145/2945078.2945116, DOI 10.1145/2945078.2945116]
   Hirzer M., 2008, ICGTR0805
   Kato H., 1999, Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99), P85, DOI 10.1109/IWAR.1999.803809
   Krajnik T., 2013, 2013 16 INT C ADV RO, P1, DOI [10.1109/icar.2013.6766520, DOI 10.1109/ICAR.2013.6766520]
   Magid E, 2018, SMART INNOV SYST TEC, V74, P200, DOI 10.1007/978-3-319-59394-4_20
   Sagitov A, 2017, MATEC WEB CONF, V113, DOI 10.1051/matecconf/201711302006
   Sagitov A, 2017, 2017 INTERNATIONAL CONFERENCE ON MECHANICAL, SYSTEM AND CONTROL ENGINEERING (ICMSC), P377, DOI 10.1109/ICMSC.2017.7959505
   Uchiyama H, 2011, P IEEE VIRT REAL ANN, P35, DOI 10.1109/VR.2011.5759433
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 14
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-99582-3; 978-3-319-99581-6
J9 LECT NOTES ARTIF INT
PY 2018
VL 11097
BP 249
EP 258
DI 10.1007/978-3-319-99582-3_26
PG 10
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BS3LD
UT WOS:000712324300026
DA 2022-02-10
ER

PT C
AU Vogels, T
   van Gastel, M
   Wang, WJ
   de Haan, G
AF Vogels, Tom
   van Gastel, Mark
   Wang, Wenjin
   de Haan, Gerard
GP IEEE
TI Fully-automatic camera-based pulse-oximetry during sleep
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
SP IEEE Comp Soc
ID REMOTE-PPG
AB Current routines for the monitoring of sleep require many sensors attached to the patient during a nocturnal observational study, limiting mobility and causing stress and discomfort. Cameras have shown promise in the remote monitoring of pulse rate, respiration and oxygen saturation, which potentially allows a reduction in the number of sensors. Applying these techniques in a sleep setting is challenging, as it is unknown upfront which portion of the skin will be visible, there is no unique skin-color outside the visible range, and the pulsatility is low in infrared. We present a fully-automatic living tissue detection method to enable continuous monitoring of pulse rate and oxygen saturation during sleep. The system is validated on a dataset where various typical sleep scenarios have been simulated. Results show the proposed method to outperform the current state-of-the-art, especially for the estimation of oxygen saturation.
C1 [Vogels, Tom; van Gastel, Mark] Eindhoven Univ Technol, Eindhoven, Netherlands.
   [Wang, Wenjin; de Haan, Gerard] Philips Res, Eindhoven, Netherlands.
RP Vogels, T (corresponding author), Eindhoven Univ Technol, Eindhoven, Netherlands.
EM tommel_vogels@hotmail.com; m.j.h.v.gastel@tue.nl;
   wenjin.wang@philips.com; g.de.haan@philips.com
RI van Gastel, Mark/AAO-9675-2020
CR [Anonymous], 2011, PART REQ BAS SAF ESS
   Bobbia S., 2017, PATTERN RECOGNITION
   . C. on Sleep Disorders Research, 1993, WAK AM NAT SLEEP AL
   Chaichulee S, 2017, IEEE INT CONF AUTOMA, P266, DOI 10.1109/FG.2017.41
   de Haan G, 2014, PHYSIOL MEAS, V35, P1913, DOI 10.1088/0967-3334/35/9/1913
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Gibert G, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P449, DOI 10.1109/AVSS.2013.6636681
   Henriques J. F., 2015, PATTERN ANAL MACHINE
   Idzikowski C., SLEEP POSITION GIVES
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Lempe G., 2013, BILDVERARBEITUNG MED, P99
   Liu H, 2015, LECT NOTES COMPUT SC, V9085, P79, DOI 10.1007/978-3-319-19156-0_9
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Paruthi S., MORBIDITY MORTALITY
   Tsai WH, 1999, AM J RESP CRIT CARE, V159, P43, DOI 10.1164/ajrccm.159.1.9709017
   van Gastel M, 2018, BIOMED OPT EXPRESS, V9, P102, DOI 10.1364/BOE.9.000102
   van Gastel M, 2016, BIOMED OPT EXPRESS, V7, P4941, DOI 10.1364/BOE.7.004941
   van Gastel M, 2016, SCI REP-UK, V6, DOI 10.1038/srep38609
   van Gastel M, 2015, IEEE T BIO-MED ENG, V62, P1425, DOI 10.1109/TBME.2015.2390261
   Van Luijtelaar R., 2011, P AS C COMP VIS, P360
   Viola Paul, 2001, COMP VIS PATT REC 20, V1
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P2781, DOI 10.1109/TBME.2017.2676160
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wang WJ, 2015, IEEE T BIO-MED ENG, V62, P2629, DOI 10.1109/TBME.2015.2438321
   [No title captured]
NR 25
TC 5
Z9 5
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 1430
EP 1438
DI 10.1109/CVPRW.2018.00183
PG 9
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800176
DA 2022-02-10
ER

PT J
AU Senocak, A
   Oh, TH
   Kim, J
   Yang, MH
   Kweon, IS
AF Senocak, Arda
   Oh, Tae-Hyun
   Kim, Junsik
   Yang, Ming-Hsuan
   Kweon, In So
TI Learning to Localize Sound Sources in Visual Scenes: Analysis and
   Applications
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Visualization; Videos; Task analysis; Correlation; Deep learning;
   Network architecture; Unsupervised learning; Audio-visual learning;
   sound localization; self-supervision; multi-modal learning; cross-modal
   retrieval
AB Visual events are usually accompanied by sounds in our daily lives. However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans? To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes. In order to achieve this goal, a two-stream network structure which handles each modality with attention mechanism is developed for sound source localization. The network naturally reveals the localized response in the scene without human annotation. In addition, a new sound source dataset is developed for performance evaluation. Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases. Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception. To fix this issue, we extend our network to the supervised and semi-supervised network settings via a simple modification due to the general architecture of our two-stream network. We show that the false conclusions can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup. Furthermore, we present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we extend this proposed algorithm to a new application, sound saliency based automatic camera view panning in 360 degree videos.
C1 [Senocak, Arda; Kim, Junsik; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
   [Oh, Tae-Hyun] POSTECH, Dept Elect Engn, Pohang 37673, South Korea.
   [Yang, Ming-Hsuan] Univ Calif, Dept Elect Engn & Comp Sci, Merced, CA 95343 USA.
RP Kweon, IS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.; Oh, TH (corresponding author), POSTECH, Dept Elect Engn, Pohang 37673, South Korea.
EM arda.senocak@gmail.com; taehyun@csail.mit.edu; mibastro@gmail.com;
   mhyang@ucmerced.edu; iskweon@kaist.ac.kr
RI Yang, Ming-Hsuan/T-9533-2019; Oh, Tae-Hyun/D-7854-2016
OI Yang, Ming-Hsuan/0000-0003-4848-2304; Kim, Junsik/0000-0003-2555-5232;
   Senocak, Arda/0000-0001-9141-3270; Oh, Tae-Hyun/0000-0003-0468-1571
FU National Information Society Agency [2100-2131-305-10719]; NSF
   CAREERNational Science Foundation (NSF)NSF - Office of the Director (OD)
   [1149783]
FX A. Senocak, J. Kim, and I.S. Kweon were supported by the National
   Information Society Agency for construction of training data for
   artificial intelligence (2100-2131-305-10719). M.-H. Yang is supported
   in part by NSF CAREER (No. 1149783).
CR Abadi, 2015, TENSORFLOW LARGE SCA
   Afouras T, 2018, INTERSPEECH, P3244
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Aytar Y., 2017, ABS170600932 CORR
   Aytar Y., 2016, ADV NEURAL INFORM PR, P892, DOI DOI 10.1109/CVPR.2016.18
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Bahdanau D., 2014, ARXIV14090473, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047
   Barzelay Zohar, 2007, P IEEE C COMP VIS PA
   Bolia RS, 1999, HUM FACTORS, V41, P664, DOI 10.1518/001872099779656789
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Chou S.-H., 2017, P AAAI, P6748
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   den Oord V., 2013, NIPS, V26, P2643
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fisher III J. W., 2001, P 13 INT C NEUR INF, P742
   Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041
   Gao RH, 2018, LECT NOTES COMPUT SC, V11207, P36, DOI 10.1007/978-3-030-01219-9_3
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Harwath D, 2018, LECT NOTES COMPUT SC, V11210, P659, DOI 10.1007/978-3-030-01231-1_40
   Hershey J. R., 1999, P 12 INT C NEUR INF
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   JONES B, 1975, PERCEPT PSYCHOPHYS, V17, P241, DOI 10.3758/BF03203206
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   Kidron E, 2005, PROC CVPR IEEE, P88
   Kim C., 2018, P AS C COMP VIS, P276
   Kopf J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982405
   Majdak P, 2010, ATTEN PERCEPT PSYCHO, V72, P454, DOI 10.3758/APP.72.2.454
   Morgado P., 2018, P 32 INT C NEUR INF, P360
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Perez A. F., 2019, ARXIV190407933
   Perrott DR, 1996, HUM FACTORS, V38, P702, DOI 10.1518/001872096778827260
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   SKINNER BF, 1948, J EXP PSYCHOL, V38, P168, DOI 10.1037/h0055873
   Stein BE, 2008, NAT REV NEUROSCI, V9, P255, DOI 10.1038/nrn2331
   Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10
   Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Torresani L., 2018, ADV NEURAL INFORM PR, P7774, DOI [10.5555/3327757.3327874, DOI 10.5555/3327757.3327874]
   Van Trees H.L, 2002, OPTIMUM ARRAY PROC 4
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao H, 2018, LECT NOTES COMPUT SC, V11205, P587, DOI 10.1007/978-3-030-01246-5_35
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zunino A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P693, DOI 10.1109/ICCVW.2015.95
NR 55
TC 2
Z9 2
U1 2
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD MAY 1
PY 2021
VL 43
IS 5
BP 1605
EP 1619
DI 10.1109/TPAMI.2019.2952095
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA RJ3YD
UT WOS:000637533800009
PM 31722472
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Belghith, K
   Kabanza, F
   Bellefeuille, P
   Hartman, L
AF Belghith, Khaled
   Kabanza, Froduald
   Bellefeuille, Philipe
   Hartman, Leo
TI Automated camera planning to film robot operations
SO ARTIFICIAL INTELLIGENCE REVIEW
LA English
DT Article
DE Planning; Camera planning; Path planning; Animation generation; Robotic
   manipulations
AB Automatic 3D animation generation techniques are becoming increasingly popular in different areas related to computer graphics such as video games and animated movies. They help automate the filmmaking process even by non professionals without or with minimal intervention of animators and computer graphics programmers. Based on specified cinematographic principles and filming rules, they plan the sequence of virtual cameras that the best render a 3D scene. In this paper, we present an approach for automatic movie generation using linear temporal logic to express these filming and cinematography rules. We consider the filming of a 3D scene as a sequence of shots satisfying given filming rules, conveying constraints on the desirable configuration (position, orientation, and zoom) of virtual cameras. The selection of camera configurations at different points in time is understood as a camera plan, which is computed using a temporal-logic based planning system (TLPlan) to obtain a 3D movie. The camera planner is used within an automated planning application for generating 3D tasks demonstrations involving a teleoperated robot arm on the the International Space Station (ISS). A typical task demonstration involves moving the robot arm from one configuration to another. The main challenge is to automatically plan the configurations of virtual cameras to film the arm in a manner that conveys the best awareness of the robot trajectory to the user. The robot trajectory is generated using a path-planner. The camera planner is then invoked to find a sequence of configurations of virtual cameras to film the trajectory.
C1 [Belghith, Khaled; Kabanza, Froduald; Bellefeuille, Philipe] Univ Sherbrooke, Sherbrooke, PQ J1K 2R1, Canada.
   [Hartman, Leo] Canadian Space Agcy, John H Chapman Space Ctr, Longueuil, PQ J3Y 8Y9, Canada.
RP Belghith, K (corresponding author), Univ Sherbrooke, 2500 Boul Univ, Sherbrooke, PQ J1K 2R1, Canada.
EM khaled.belghith@usherbrooke.ca; kabanza@usherbrooke.ca;
   philipe.bellefeuille@usherbrooke.ca; leo.hartman@asc-csa.gc.ca
FU Natural Sciences and Engineering Research Council (NSERC) of
   CanadaNatural Sciences and Engineering Research Council of Canada
   (NSERC)
FX The work presented herein was supported by the Natural Sciences and
   Engineering Research Council (NSERC) of Canada.
CR Arijon D., 1976, GRAMMAR FILM LANGUAG
   Bacchus F, 2000, ARTIF INTELL, V116, P123, DOI 10.1016/S0004-3702(99)00071-5
   Bares W. H., 1998, IUI '98. 1998 International Conference on Intelligent User Interfaces, P81
   Bares WH, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P1101
   Belghith K, 2006, IEEE INT CONF ROBOT, P2372, DOI 10.1109/ROBOT.2006.1642057
   Benhamou F., 2004, ACM Transactions on Computational Logic, V5, P732, DOI 10.1145/1024922.1024927
   BLINN J, 1988, IEEE COMPUT GRAPH, V8, P76, DOI 10.1109/38.7751
   Christianson DB, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P148
   Drucker S.M., 1992, S INT 3D GRAPH NEW Y, P67
   Friedman D, 2006, EXPERT SYST APPL, V30, P694, DOI 10.1016/j.eswa.2005.07.027
   Friedman D, 2004, FR ART INT, V110, P256
   Halper N, 2001, J COMPUT GRAPH FORUM, V20
   HALPER N, 2000, P 2000 AAAI SPRING S, P92
   Jardillier F, 1988, J COMPUT GRAPH FORUM, V17, P175
   Jhala Arnav, 2005, P 20 NAT C ART INT A, V5, P307
   Kabanza F., 2008, ICAPS, P164
   Kabanza F, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1729
   Languenou E, 2002, P 8 INT C PRINC PRAC, P618
   Larsen E., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3719, DOI 10.1109/ROBOT.2000.845311
   Lucas C, 1985, DIRECTING FILM TELEV
   Nieuwenhuisen D, 2004, IEEE INT CONF ROBOT, P3870, DOI 10.1109/ROBOT.2004.1308871
   Tomlinon B., 2000, Proceedings of the Fourth International Conference on Autonomous Agents, P317, DOI 10.1145/336595.337513
NR 22
TC 1
Z9 1
U1 2
U2 9
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0269-2821
EI 1573-7462
J9 ARTIF INTELL REV
JI Artif. Intell. Rev.
PD APR
PY 2012
VL 37
IS 4
BP 313
EP 330
DI 10.1007/s10462-011-9233-y
PG 18
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 909DN
UT WOS:000301542600004
DA 2022-02-10
ER

PT C
AU Burelli, P
   Preuss, M
AF Burelli, Paolo
   Preuss, Mike
BE EsparciaAlcazar, AI
TI Automatic Camera Control: A Dynamic Multi-Objective Perspective
SO APPLICATIONS OF EVOLUTIONARY COMPUTATION
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 17th European Conference on Applications of Evolutionary Computation
   (EvpApplications)
CY APR 23-25, 2014
CL Granada, SPAIN
SP Univ Granada, Free Software Off, Granada Excellence Network Innovat Lab, Edinburgh Napier Univ, Inst Informat & Digital Innovat, World Federat Soft Comp
ID SELECTION
AB Automatically generating computer animations is a challenging and complex problem with applications in games and film production. In this paper, we investigate how to translate a shot list for a virtual scene into a series of virtual camera configurations - i.e automatically controlling the virtual camera. We approach this problem by modelling it as a dynamic multi-objective optimisation problem and show how this metaphor allows a much richer expressiveness than a classical single objective approach. Finally, we showcase the application of a multi-objective evolutionary algorithm to generate a shot for a sample game replay and we analyse the results.
C1 [Burelli, Paolo] Aalborg Univ, Dept Architecture Design & Media Technol, Copenhagen, Denmark.
   [Preuss, Mike] Univ Munster, European Res Ctr Informat Syst, D-48149 Munster, Germany.
RP Preuss, M (corresponding author), Univ Munster, European Res Ctr Informat Syst, D-48149 Munster, Germany.
EM pabu@create.aau.dk; mike.preuss@uni-muenster.de
RI Burelli, Paolo/AAM-5327-2020
OI Burelli, Paolo/0000-0003-2804-9028
CR Bares W., 2000, Proceedings ACM Multimedia 2000, P177, DOI 10.1145/354384.354463
   Beume N, 2007, EUR J OPER RES, V181, P1653, DOI 10.1016/j.ejor.2006.08.008
   Beume N, 2009, LECT NOTES COMPUT SC, V5467, P21, DOI 10.1007/978-3-642-01020-0_7
   Bourne O, 2008, CONSTRAINTS, V13, P180, DOI 10.1007/s10601-007-9026-8
   Burelli P., 2012, THESIS IT U COPENHAG
   Burelli P, 2009, AAAI C ART INT INT D
   Burelli P, 2008, LECT NOTES COMPUT SC, V5166, P130, DOI 10.1007/978-3-540-85412-8_12
   Cheong Y.G., 2008, AAAI C ART INT INT D, P167
   Christie M, 2008, COMPUT GRAPH FORUM, V27, P2197, DOI 10.1111/j.1467-8659.2008.01181.x
   Deb R., 2002, IEEE T EVOLUTIONARY, V6
   Dominguez M, 2011, AAAI C ART INT INT D
   DRUCKER SM, 1994, GRAPH INTER, P190
   Jardillier F, 1998, COMPUT GRAPH FORUM, V17, pC175
   Lowood, 2008, J MEDIA PRACTICE, V7, P25, DOI DOI 10.1386/JMPR.7.1.25/1
   Olivier P., 1999, ART INT SIM BEH
   PHILLIPS CB, 1992, P 1992 S INT 3D GRAP, P71
   Pickering J, 2002, THESIS U YORK
   PONTRIAGIN LS, 1962, MATH THEORY OPTIMAL
   SARKER R, 2002, INT SER OPER RES MAN, V48, P177
   Togelius J, 2013, GENET PROGRAM EVOL M, V14, P245, DOI 10.1007/s10710-012-9174-5
   Van Veldhuizen David A., 1998, LAT BREAK PAP GEN PR
   Ware C., 1990, Computer Graphics, V24, P175
NR 22
TC 1
Z9 1
U1 0
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-662-45523-4; 978-3-662-45522-7
J9 LECT NOTES COMPUT SC
PY 2014
VL 8602
BP 361
EP 373
DI 10.1007/978-3-662-45523-4_30
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BC7HA
UT WOS:000354874300030
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Leu, JS
   Lin, WH
   Tzeng, HJ
   Chen, CF
   Lin, MS
AF Leu, Jenq-Shiou
   Lin, Wei-Hsiang
   Tzeng, Hung-Jie
   Chen, Chi-Feng
   Lin, Mu-Sheng
TI Adaptive frame synchronization for surveillance system across a
   heterogeneous network
SO ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Surveillance system; Heterogeneous network; Frame synchronization;
   Playback liveness
AB As mobile techniques are booming, the surveillance function is extended from a stationary mode to a mobile mode. In a heterogeneous network environment, cameras and viewers are located in different networks so that frame synchronization may span across diverse network domains with different transmission capabilities. The mismatch of transmission capabilities may affect the viewing continuity and playback liveness between cameras and viewers. In the article, we propose an adaptive frame synchronization mechanism for frame capturing at cameras based on the network condition to improve the frame synchronization between two sides across a heterogeneous network. Based on a brief theoretical analysis of the asynchronization effect for video communication in a heterogeneous network environment, the proposed adaptive pause time mechanism can be an effective solution to relieve the asynchronization effect in the unmatched transmission rate situation. The evaluation results show that the proposed scheme can achieve a shorter time delay between the captured frames at the camera site and the viewer site. (C) 2012 Elsevier Ltd. All rights reserved.
C1 [Leu, Jenq-Shiou; Lin, Wei-Hsiang; Tzeng, Hung-Jie; Chen, Chi-Feng; Lin, Mu-Sheng] Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei, Taiwan.
RP Leu, JS (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei, Taiwan.
EM jsleu@mail.ntust.edu.tw; D9902101@mail.ntust.edu.tw;
   M9802138@mail.ntust.edu.tw; M9902136@mail.ntust.edu.tw;
   D9502404@mail.ntust.edu.tw
CR Cha HJ, 2006, J SYST ARCHITECT, V52, P25, DOI 10.1016/j.sysarc.2005.02.006
   Chang SH, 2007, IEEE T BROADCAST, V53, P79, DOI 10.1109/TBC.2006.887170
   Di Paola D, 2010, INT J ADV ROBOT SYST, V7, P19
   Esteve M, 2007, J NETW COMPUT APPL, V30, P479, DOI 10.1016/j.jnca.2006.06.001
   Foresti GL, 1998, IEEE T CIRC SYST VID, V8, P697, DOI 10.1109/76.728411
   Imai Yoshiro, 2008, 2008 Conference on Human System Interactions, P526, DOI 10.1109/HSI.2008.4581494
   Jammeh EA, 2009, IET COMMUN, V3, P25, DOI 10.1049/iet-com:20080047
   Javed K., 2010, J CONVERGENCE, V1, P107
   Jo GS, 2010, J INF PROCESS SYST, V6, P261, DOI 10.3745/JIPS.2010.6.2.261
   Karimi OB, 2010, COMPUT ELECTR ENG, V36, P45, DOI 10.1016/j.compeleceng.2009.04.006
   Kochnev DS, 2003, IEEE PERVAS COMPUT, V2, P90, DOI 10.1109/MPRV.2003.1203758
   Leu JS, 2011, COMPUT ELECTR ENG, V37, P1182, DOI 10.1016/j.compeleceng.2011.05.015
   Liang W.-Y., 2010, J CONVERGENCE, V1, P93
   Magharei N, 2006, MULTIMEDIA SYST, V11, P550, DOI 10.1007/s00530-006-0037-x
   Ozcelebi T, 2007, IEEE T MULTIMEDIA, V9, P826, DOI 10.1109/TMM.2007.895670
   Read K, 2003, IEEE INTERNET COMPUT, V7, P81, DOI 10.1109/MIC.2003.1167345
   Sathappan O. L., 2011, International Journal of Information Technology, Communications and Convergence, V1, P146, DOI 10.1504/IJITCC.2011.039282
   Shin WS, 2006, J INF PROCESS SYST, V2, P52
   Tonet O, 2008, IEEE T ROBOT, V24, P55, DOI 10.1109/TRO.2008.915430
   Xic B., 2010, INT J INFO TECHNOL C, V1
   Xu LZ, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 2, PROCEEDINGS, P281, DOI 10.1109/CISP.2008.235
   Yu F, 2003, IEEE T CIRC SYST VID, V13, P257, DOI 10.1109/TCSVT.2003.809829
NR 22
TC 5
Z9 5
U1 0
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0952-1976
EI 1873-6769
J9 ENG APPL ARTIF INTEL
JI Eng. Appl. Artif. Intell.
PD OCT
PY 2012
VL 25
IS 7
BP 1349
EP 1354
DI 10.1016/j.engappai.2012.02.001
PG 6
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Multidisciplinary; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems; Computer Science; Engineering
GA 020DB
UT WOS:000309787800007
DA 2022-02-10
ER

PT C
AU Ji, JJ
   Krishnan, S
   Patel, V
   Fer, D
   Goldberg, K
AF Ji, Jessica J.
   Krishnan, Sanjay
   Patel, Vatsal
   Fer, Danyal
   Goldberg, Ken
BE Reveliotis, S
   Cappelleri, D
   Dimarogonas, DV
   Dotoli, M
   Fanti, MP
   Li, J
   Lucena, V
   Seatu, C
   Xie, X
   Zhu, K
TI Learning 2D Surgical Camera Motion From Demonstrations
SO 2018 IEEE 14TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND
   ENGINEERING (CASE)
SE IEEE International Conference on Automation Science and Engineering
LA English
DT Proceedings Paper
CT 14th IEEE International Conference on Automation Science and Engineering
   (IEEE CASE)
CY AUG 20-24, 2018
CL Tech Univ Munich, Munich, GERMANY
SP IEEE
HO Tech Univ Munich
DE Surgical Robotics; Active Perception; Viewpoint Selection
AB Automating camera movement during robotassisted surgery has the potential to reduce burden on surgeons and remove the need to manually move the camera. An important sub-problem is automatic viewpoint selection, proposing camera poses that focus on important anatomical features. We use the 6 DoF Stewart Platform Research Kit (SPRK) to move the environment with a fixed endoscope, as a dual to moving the endoscope itself, to study camera motion in surgical robotics. To provide demonstrations, we link the platform's control directly to the da Vinci Research Kit (dVRK) master control system and allow control of the platform using the same pedals and tools as a clinical movable endoscope. We propose a probabilistic model that identifies image features that "dwell" close to the camera's focal point in expert demonstrations. Our experiments consider a surgical debridement scenario on silicone phantoms with inclusions of varying color and shape. We evaluate the extent to which the system correctly segments candidate debridement targets (box accuracy) and correctly ranks those targets (rank accuracy). For debridement of a single uniquely colored inclusion, the box accuracy is 80% and the rank accuracy is 100% after 100 training data points. For debridement of multiple inclusions of the same color, the box accuracy is 70.8% and the rank accuracy is 100% after 100 training data points. For debridement of inclusions of a particular shape, the box accuracy is 70.5% and the rank accuracy is 90% after 100 training data points. A demonstration video is available at: https://vimeo.com/260362958
C1 [Ji, Jessica J.; Krishnan, Sanjay; Patel, Vatsal; Goldberg, Ken] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Fer, Danyal] UC San Francisco East Bay, Oakland, CA USA.
RP Ji, JJ (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
FU NSF National Robotics Initiative Award [1734633]; Intuitive Surgical;
   National Science Foundation, via the National Robotics Initiative (NRI)
   [IIS 1637789, IIS 1637759, IIS 1637444]
FX This research was performed at the AUTOLab at UC Berkeley in affiliation
   with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent
   Secure Execution (RISE) Lab, the CITRIS "People and Robots" (CPAR)
   Initiative, by the Scalable Collaborative Human-Robot Learning (SCHooL)
   Project, NSF National Robotics Initiative Award 1734633, and in
   affiliation with UC Berkeley's Center for Automation and Learning for
   Medical Robotics (Cal-MR). The authors were supported in part by
   donations from Siemens, Google, Honda, Intel, Comcast, Cisco, Autodesk,
   Amazon Robotics, Toyota Research Institute, ABB, Samsung, Knapp,
   Loccioni, and by a major equipment grant from Intuitive Surgical and by
   generous donations from Andy Chou and Susan and Deepak Lim. The da Vinci
   Research Kit is supported by the National Science Foundation, via the
   National Robotics Initiative (NRI), as part of the collaborative
   research project "Software Framework for Research in Semi-Autonomous
   Teleoperation" between The Johns Hopkins University (IIS 1637789),
   Worcester Polytechnic Institute (IIS 1637759), and the University of
   Washington (IIS 1637444). Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the author(s)
   and do not necessarily reflect the views of the sponsors. We thank our
   colleagues who provided helpful feedback and suggestions, in particular
   Brijen Thananjeyan, Carolyn Chen, Jeff Mahler, Daniel Seita, Matthew
   Matl.
CR Ali  S., 2008, STUDIES HLTH TECHNOL, V132
   Allaf  M., 1998, SURG ENDOSCOPY, V12
   Arai  F., 2000, INT C ROB AUT, V1
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Canny J., 1987, READINGS COMPUTER VI
   Casals  A., 1996, ROB AUT ICRA 1996 IE
   Chen SY, 2011, INT J ROBOT RES, V30, P1343, DOI 10.1177/0278364911410755
   Chen  X., 2000, 22 STANF U COMP GRAP, V2
   Chitwood Jr W. R., 2001, ANN SURG, V234
   Christie M., 2008, COMPUTER GRAPHICS FO, V27, P8
   Deinzer  F., 2003, INT C COMP AN IM PAT
   Eslamian S., AUTONOMOUS CAMERA SY
   Gilbert  J., 2009, ANN ROYAL COLL SURGE, V91
   He L. W., 1996, C COMP GRAPH INT TEC
   Kavoussi L. R., 1995, J UROLOGY, V154
   Kazanzides  P., 2014, OPEN SOURCE RES KIT
   Leifman  G., 2012, C COMP VIS PATT REC
   Liang  J., 2017, C AUT SCI ENG
   Mahler  J., 2014, C AUT SCI ENG
   Motai  Y., 2008, IEEE T IND ELECT, V55
   Muhler  K., 2007, EUROVIS
   Mylonas G. P., 2006, COMPUTER AIDED SURG, V11
   Pandya A, 2014, ROBOTICS, V3, DOI 10.3390/robotics3030310
   Partin A. W., 1995, J AM COLL SURG, V181
   Patel  V., 2018, INT S MED ROB ISMR
   Ren Shaoqing, 2015, ADV NEURAL INFORM PR
   Roch P. J., 2018, SURG ENDOSCOPY, V32
   Sakane  S., 1991, ROB AUT ICRA 1991 IE
   Seita  D., 2018, INT C ROB AUT
   Tong I, 2015, IEEE INT C INT ROBOT, P2043, DOI 10.1109/IROS.2015.7353648
   Triggs  B., 1995, ROB AUT ICRA 1995 IE, V2
   Vazquez P. -P., 2001, VMV, V1
   Ware  C., 1990, SIGGRAPH COMPUTER GR, V24
   Weede  O., 2011, ROB AUT ICRA 2011 IE
   Wilson  M., 2010, SURG ENDOSCOPY, V24
NR 36
TC 7
Z9 7
U1 1
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2161-8070
BN 978-1-5386-3593-3
J9 IEEE INT CON AUTO SC
PY 2018
BP 35
EP 42
PG 8
WC Automation & Control Systems; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Engineering
GA BM1TD
UT WOS:000460536600006
DA 2022-02-10
ER

PT J
AU Nazir, S
   Kaleem, M
AF Nazir, Sajid
   Kaleem, Muhammad
TI Advances in image acquisition and processing technologies transforming
   animal ecological studies
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Animal behaviour; Big data; Computer vision; Deep learning; Image
   processing; Population monitoring; Drone; Robot
ID CAMERA TRAPS; HABITAT; CONSERVATION; BIODIVERSITY; BEHAVIOR;
   PERSPECTIVE; DISTURBANCE; NETWORKS; INSIGHTS
AB Images and videos have become pervasive in ecological research and the ease of acquiring image data and its subsequent processing can provide answers in research areas such as species recognition, animal behaviour, and population studies which are critical for animal conservation and biodiversity. Technological advances in imaging are enabling data collection from new areas such as from underwater, new modalities such as thermal and new ways of processing such as deep learning. These advances are accelerating due to ease of data collection, better storage and processing technologies with associated lowering costs. The advancements in state-of-the-art machine learning for image and video classification and analysis can directly be applied in ecology. Ecological applications are generally conducted in remote and harsh deployment environments, and therefore present formidable challenges that require appreciation of the limitations of such technologies. The ecological field is poised to make use of images acquired through drones, robotics, and satellites through machine learning for rapid advancements in critical research areas. Timely insights from such data help to understand and protect the species and environment. This paper provides a review of the advancements in image acquisition and processing technologies used in animal ecological studies. We also discuss concepts and technologies that would help foster future ecological research methodologies potentially opening new insights and quickening growth to an already rich and data-intensive field.
C1 [Nazir, Sajid] Glasgow Caledonian Univ, Sch Comp Engn & Built Environm, Glasgow, Lanark, Scotland.
   [Kaleem, Muhammad] COMSATS Univ, Dept Elect & Comp Engn, Islamabad, Pakistan.
RP Nazir, S (corresponding author), Glasgow Caledonian Univ, Sch Comp Engn & Built Environm, Glasgow, Lanark, Scotland.
EM sajid.nazir@gcu.ac.uk
CR Andersen GE, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0230216
   [Anonymous], PHIL T R SOC B, DOI [10.1098/rstb.2013.0197., DOI 10.1098/RSTB.2013.0197]
   [Anonymous], AMBIO S4, DOI [10.1007/s13280-015-0711-3., DOI 10.1007/S13280-015-0711-3]
   [Anonymous], METHODS ECOL EVOL
   Balch T, 2006, P IEEE, V94, P1445, DOI 10.1109/JPROC.2006.876969
   Beauxis-Aussalet E, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (IEEE DSAA 2015), P900
   Branson K, 2014, NAT METHODS, V11, P721, DOI 10.1038/nmeth.3004
   Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
   Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
   Crutsinger GM, 2016, J UNMANNED VEH SYST, V4, P161, DOI 10.1139/juvs-2016-0008
   Dahlen B., 2017, SUCCESSFUL AERIAL SU
   Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
   Dietterich TG, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P8
   Ditria EM, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.00429
   Evans LJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16091527
   Farley SS, 2018, BIOSCIENCE, V68, P563, DOI 10.1093/biosci/biy068
   Fretwell PT, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088655
   Glen AS, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067940
   Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
   Gremillet D., 2012, Open Journal of Ecology, V2, P49, DOI 10.4236/oje.2012.22006
   Handcock RN, 2009, SENSORS-BASEL, V9, P3586, DOI 10.3390/s90503586
   Harmsen BJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179505
   Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
   Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
   Huang ZP, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087318
   Jeantet L, 2020, ROY SOC OPEN SCI, V7, DOI 10.1098/rsos.200139
   Lopez JJ, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010010
   Joo D, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077686
   Kays R, 2019, INT J REMOTE SENS, V40, P407, DOI 10.1080/01431161.2018.1523580
   Kays R, 2015, SCIENCE, V348, DOI 10.1126/science.aaa2478
   Koniar D, 2016, COMPUT METH PROG BIO, V127, P258, DOI 10.1016/j.cmpb.2015.12.009
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kucera TE, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P9, DOI 10.1007/978-4-431-99495-4_2
   Kwok R, 2018, NATURE, V556, P137, DOI 10.1038/d41586-018-03924-9
   Lawson, 2008, INT WORKSH DISTR SEN
   LTER, 2020, LONG TERM ECOLOGICAL
   Mapes KL, 2020, DRONES-BASEL, V4, DOI 10.3390/drones4020012
   Mattern T, 2018, PEERJ, V6, DOI 10.7717/peerj.5459
   McMahon CR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092613
   Mellin C, 2012, ECOL APPL, V22, P792, DOI 10.1890/11-2105.1
   NAZIR S, 2017, PLOS, P69758
   Nazir S, 2017, INT J SATELL COMM N, V35, P201, DOI 10.1002/sat.1176
   Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   O'Brien TG, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P71, DOI 10.1007/978-4-431-99495-4_6
   Peters DPC, 2014, ECOSPHERE, V5, DOI 10.1890/ES13-00359.1
   Rafiq K, 2019, METHODS ECOL EVOL, V10, P1517, DOI 10.1111/2041-210X.13231
   Recknagel F., 2018, ECOLOGICAL INFORM DA
   Rovero F, 2013, J MAMMAL, V94, P792, DOI 10.1644/12-MAMM-A-235.1
   Rush GP, 2018, ECOL EVOL, V8, P12322, DOI 10.1002/ece3.4495
   Schmaljohann H, 2020, ECOGRAPHY, V43, P236, DOI 10.1111/ecog.04807
   Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
   Schofield G, 2017, FUNCT ECOL, V31, P2310, DOI 10.1111/1365-2435.12930
   Shan Y, 2006, ECOL MODEL, V195, P129, DOI 10.1016/j.ecolmodel.2005.11.015
   St-Louis V, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2013.0197
   Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
   Stepanian PM, 2014, METHODS ECOL EVOL, V5, P730, DOI 10.1111/2041-210X.12214
   Suraci JP, 2017, METHODS ECOL EVOL, V8, P957, DOI 10.1111/2041-210X.12711
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   TABAK MA, METHODS ECOL EVOL
   Troscianko J, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2015.0777
   Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
   van der Wal R, 2015, AMBIO, V44, pS612, DOI 10.1007/s13280-015-0711-3
   van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
   Vas E, 2015, BIOL LETTERS, V11, DOI 10.1098/rsbl.2014.0754
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Wang HQ, 2020, IEEE T NEUR NET LEAR, V31, P972, DOI 10.1109/TNNLS.2019.2912082
   Wang K, 2010, SENSORS-BASEL, V10, P9647, DOI 10.3390/s101109647
   Wich S, 2016, J UNMANNED VEH SYST, V4, P45, DOI 10.1139/juvs-2015-0015
   Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
   Wildlife Insights, 2020, WILDLIFE INSIGHTS
   Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
NR 75
TC 1
Z9 1
U1 4
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAR
PY 2021
VL 61
AR 101212
DI 10.1016/j.ecoinf.2021.101212
PG 11
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA QT6DJ
UT WOS:000626676100003
OA Green Published
DA 2022-02-10
ER

PT J
AU Harrison, D
   De Leo, FC
   Gallin, WJ
   Mir, F
   Marini, S
   Leys, SP
AF Harrison, Dominica
   De Leo, Fabio Cabrera
   Gallin, Warren J.
   Mir, Farin
   Marini, Simone
   Leys, Sally P.
TI Machine Learning Applications of Convolutional Neural Networks and Unet
   Architecture to Predict and Classify Demosponge Behavior
SO WATER
LA English
DT Article
DE convolutional neural networks (CNN); unet; machine learning; semantic
   segmentation; demosponge behavior; classification; time series; deep
   learning; image analysis
ID FOREST MAMMALS; CAMERA-TRAP; CONTRACTIONS
AB Biological data sets are increasingly becoming information-dense, making it effective to use a computer science-based analysis. We used convolution neural networks (CNN) and the specific CNN architecture Unet to study sponge behavior over time. We analyzed a large time series of hourly high-resolution still images of a marine sponge, Suberites concinnus (Demospongiae, Suberitidae) captured between 2012 and 2015 using the NEPTUNE seafloor cabled observatory, off the west coast of Vancouver Island, Canada. We applied semantic segmentation with the Unet architecture with some modifications, including adapting parts of the architecture to be more applicable to three-channel images (RGB). Some alterations that made this model successful were the use of a dice-loss coefficient, Adam optimizer and a dropout function after each convolutional layer which provided losses, accuracies and dice scores of up to 0.03, 0.98 and 0.97, respectively. The model was tested with five-fold cross-validation. This study is a first step towards analyzing trends in the behavior of a demosponge in an environment that experiences severe seasonal and inter-annual changes in climate. The end objective is to correlate changes in sponge size (activity) over seasons and years with environmental variables collected from the same observatory platform. Our work provides a roadmap for others who seek to cross the interdisciplinary boundaries between biology and computer science.
C1 [Harrison, Dominica; Gallin, Warren J.; Mir, Farin; Leys, Sally P.] Univ Alberta, Dept Biol Sci, Edmonton, AB T6H 3C4, Canada.
   [Harrison, Dominica; De Leo, Fabio Cabrera] Univ Victoria, Dept Biol, Victoria, BC V8W 2Y2, Canada.
   [De Leo, Fabio Cabrera] Univ Victoria, Ocean Networks Canada, Victoria, BC V8N IV8, Canada.
   [Marini, Simone] Natl Res Council Italy, Inst Marine Sci, Forte Santa Teresa, I-19032 La Spezia, Italy.
   [Marini, Simone] Stazione Zool Anton Dohrn SZN, I-80122 Naples, Italy.
RP Leys, SP (corresponding author), Univ Alberta, Dept Biol Sci, Edmonton, AB T6H 3C4, Canada.
EM dominica@ualberta.ca; fdeleo@uvic.ca; wgallin@ualberta.ca;
   farin@ualberta.ca; simone.marini@sp.ismar.cnr.it; sleys@ualberta.ca
RI Marini, Simone/C-3872-2012; Marini, Simone/AAA-3513-2022
OI Marini, Simone/0000-0003-0665-7815; Marini, Simone/0000-0003-0665-7815
FU Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC)
FX Natural Sciences and Engineering Research Council of Canada (NSERC)
   Discovery Grant awarded to S.P.L.
CR Aguzzi J, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-85240-3
   Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Allen RM, 2018, ANNU REV MAR SCI, V10, P19, DOI 10.1146/annurev-marine-121916-063134
   Alonso I, 2017, IEEE INT CONF COMP V, P2874, DOI 10.1109/ICCVW.2017.339
   [Anonymous], 2010, MATL COMP TOOL BOX I
   Ba J., 2015, P 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503
   Chenard C, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52648-x
   Chu JWF, 2011, MAR ECOL PROG SER, V441, P1, DOI 10.3354/meps09381
   Doya C, 2014, J MARINE SYST, V130, P69, DOI 10.1016/j.jmarsys.2013.04.003
   Elliott GRD, 2007, J EXP BIOL, V210, P3736, DOI 10.1242/jeb.003392
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   FINCH CE, 1995, Q REV BIOL, V70, P1, DOI 10.1086/418864
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Guidi L., 2020, FUTURE SCI BRIEF BIG, V6th
   Harmsen BJ, 2010, BIOTROPICA, V42, P126, DOI 10.1111/j.1744-7429.2009.00544.x
   He P., 2015, FISHERIES BYCATCH GL, DOI [10.4027/FBGICS.2015.07, DOI 10.4027/FBGICS.2015.07, 10.4027/fbgics.2015.07]
   Hughes TP, 2018, SCIENCE, V359, P80, DOI 10.1126/science.aan8048
   Ing N, 2018, PROC SPIE, V10581, DOI 10.1117/12.2293000
   Jadon S, 2020, P IEEE C COMP INT BI, DOI [10.1109/CIBCB48159.2020.9277638, DOI 10.1109/CIBCB48159.2020.9277638]
   Kahn AS, 2020, DEEP-SEA RES PT II, V173, DOI 10.1016/j.dsr2.2019.104729
   Kohavi R, 1995, AM J ORTHOD DENTOFAC, V118, P456, DOI [10.1067/mod.2000.109032, DOI 10.1067/MOD.2000.109032]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Langenkamper D, 2017, FRONT MAR SCI, V4, DOI 10.3389/fmars.2017.00083
   LAUR DR, 1986, MAR BIOL, V93, P209, DOI 10.1007/BF00508258
   Lelievre Y, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2016.2123
   Leys SP, 2019, INTEGR COMP BIOL, V59, P751, DOI 10.1093/icb/icz122
   MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a
   Malde K, 2020, ICES J MAR SCI, V77, P1274, DOI 10.1093/icesjms/fsz057
   Marini S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32089-8
   McIntosh D., 2020, ARXIV201114070
   Mcquaid C.D., 2021, OCEANOGRAPHY MARINE, V58
   Moeslund T. B., 2012, INTRO VIDEO IMAGE PR, V1st
   Nickel M, 2004, J EXP BIOL, V207, P4515, DOI 10.1242/jeb.01289
   Nylin S, 1998, ANNU REV ENTOMOL, V43, P63, DOI 10.1146/annurev.ento.43.1.63
   Pollak Daniel J, 2019, J Undergrad Neurosci Educ, V17, pT12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rovero F, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103300
   Tills O, 2018, PLOS BIOL, V16, DOI 10.1371/journal.pbio.3000074
   Torben M., 2019, PATTERN RECOGN, V1, P45
   Yao R, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3391743
   Yau T.H.Y., 2014, THESIS U ALBERTA EDM
   Zhang AB, 2017, METHODS ECOL EVOL, V8, P627, DOI 10.1111/2041-210X.12682
NR 42
TC 0
Z9 0
U1 5
U2 5
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2073-4441
J9 WATER-SUI
JI Water
PD SEP
PY 2021
VL 13
IS 18
AR 2512
DI 10.3390/w13182512
PG 17
WC Environmental Sciences; Water Resources
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Water Resources
GA UY6HL
UT WOS:000701622600001
OA gold
DA 2022-02-10
ER

PT J
AU Shao, S
   Zhou, ZX
   Deng, GJ
   Du, P
   Jian, CY
   Yu, ZR
AF Shao, Shuai
   Zhou, Zhixiang
   Deng, Guojun
   Du, Peng
   Jian, Chuanyi
   Yu, Zhongru
TI Experiment of Structural Geometric Morphology Monitoring for Bridges
   Using Holographic Visual Sensor
SO SENSORS
LA English
DT Article
DE structural geometry monitoring; computer-vision-based measurement
   technology; bridge safety; holographic visual sensor; dense full-field
   measurement; digital twins
ID SUBSTRUCTURAL DAMAGE DETECTION; DISPLACEMENT MEASUREMENT; DYNAMIC
   DISPLACEMENT; COMPUTER VISION; OPTICAL-FLOW; ARMAX MODEL; IDENTIFICATION
AB To further improve the precision and efficiency of structural health monitoring technology and the theory of large-scale structures, full-field non-contact structural geometry morphology monitoring is expected to be a breakthrough technology in structural safety state monitoring and digital twins, owing to its economic, credible, high frequency, and holographic advantages. This study validates a proposed holographic visual sensor and algorithms in a computer-vision-based full-field non-contact displacement and vibration measurement. Using an automatic camera patrol experimental device, original segmental dynamic and static video monitoring data of a model bridge under various damage/activities were collected. According to the temporal and spatial characteristics of the series data, the holographic geometric morphology tracking algorithm was introduced. Additionally, the feature points set of the structural holography geometry and the holography feature contours were established. Experimental results show that the holographic visual sensor and the proposed algorithms can extract an accurate holographic full-field displacement signal, and factually and sensitively accomplish vibration measurement, while accurately reflecting the real change in structural properties under various damage/action conditions. The proposed method can serve as a foundation for further research on digital twins for large-scale structures, structural condition assessment, and intelligent damage identification.
C1 [Shao, Shuai; Deng, Guojun; Du, Peng; Jian, Chuanyi; Yu, Zhongru] Chongqing Jiaotong Univ, Sch Civil Engn, Chongqing 400074, Peoples R China.
   [Shao, Shuai; Zhou, Zhixiang] Shenzhen Univ, Coll Civil & Transportat Engn, Shenzhen 518061, Peoples R China.
RP Zhou, ZX (corresponding author), Shenzhen Univ, Coll Civil & Transportat Engn, Shenzhen 518061, Peoples R China.
EM 622150086086@mails.cqjtu.edu.cn; zhixiangzhou@szu.edu.cn;
   guojunforsea@gmail.com; dupeng_cgjtu@163.com;
   chuanyi_jian_cqjtu@163.com; zhongru_yu_cqjtu@163.com
RI Yu, Zhongru/ABE-1572-2021
OI Yu, Zhongru/0000-0002-4104-1365; Shao, Shuai/0000-0002-9243-6666; ,
   Guojun/0000-0001-7883-2585
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [51778094]; National Science Foundation for
   Distinguished Young Scholars of ChinaNational Natural Science Foundation
   of China (NSFC)National Science Fund for Distinguished Young Scholars
   [51708068]; Science and Technology Innovation Project of Chongqing
   Jiaotong University [2019S0141]
FX This research was funded by the National Natural Science Foundation of
   China (Grant No. 51778094), the National Science Foundation for
   Distinguished Young Scholars of China (Grant No. 51608080), and the
   National Science Foundation for Distinguished Young Scholars of China
   (Grant No. 51708068), and the Science and Technology Innovation Project
   of Chongqing Jiaotong University (Grant No. 2019S0141).
CR Abe D., 2015, ACM T GRAPHIC, V34, P1
   Artese S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020338
   Bao YQ, 2019, ENGINEERING-PRC, V5, P234, DOI 10.1016/j.eng.2018.11.027
   Bao YQ, 2014, J CIV STRUCT HEALTH, V4, P77, DOI 10.1007/s13349-013-0064-1
   Bao YQ, 2015, STRUCT CONTROL HLTH, V22, P433, DOI 10.1002/stc.1681
   Cha YJ, 2017, ENG STRUCT, V132, P300, DOI 10.1016/j.engstruct.2016.11.038
   Cha YJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16071016
   Chang XL, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051360
   Chen JG, 2015, J SOUND VIB, V345, P58, DOI 10.1016/j.jsv.2015.01.024
   Chu X, 2019, SAINS MALAYS, V48, P2777, DOI 10.17576/jsm-2019-4812-19
   Chu X, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9214532
   Clough R., 2003, DYNAMICS STRUCTURES
   Dai KS, 2017, WIND ENERGY, V20, P1687, DOI 10.1002/we.2117
   Deng G.J., ADV CIV ENG
   Editorial Department of China Journal of Highway and Transport China's 2014, 2014, CHINA J HIGHWAY TRAN, V27, P1
   Feng DM, 2018, ENG STRUCT, V156, P105, DOI 10.1016/j.engstruct.2017.11.018
   Feng DM, 2017, MECH SYST SIGNAL PR, V88, P199, DOI 10.1016/j.ymssp.2016.11.021
   Feng DM, 2015, J BRIDGE ENG, V20, DOI 10.1061/(ASCE)BE.1943-5592.0000765
   Feng DM, 2015, SENSORS-BASEL, V15, P16557, DOI 10.3390/s150716557
   Ghorbani R, 2015, EXP MECH, V55, P227, DOI 10.1007/s11340-014-9906-y
   Guo J, 2016, MECH SYST SIGNAL PR, V66-67, P425, DOI 10.1016/j.ymssp.2015.06.004
   Hartley R., 2008, MULTIPLE VIEW GEOMET
   Javh J, 2017, MECH SYST SIGNAL PR, V88, P89, DOI 10.1016/j.ymssp.2016.11.009
   Jiang Teng-Jiao, 2016, Research and Exploration in Laboratory, V35, P26
   Kromanis R, 2019, INVENTIONS-BASEL, V4, DOI 10.3390/inventions4030047
   [李惠 Li Hui], 2015, [工程力学, Engineering Mechanics], V32, P1
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   [刘智 Liu Zhi], 2015, [公路交通科技, Journal of Highway and Transportation Research and Development], V32, P88
   LUHMANN T, 2015, PHOTOGRAMM ENG REMOT, V0081, P00273
   Mei L, 2019, ENG STRUCT, V191, P625, DOI 10.1016/j.engstruct.2019.04.084
   Mei L, 2016, STRUCT CONTROL HLTH, V23, P218, DOI 10.1002/stc.1766
   Mei QP, 2019, MECH SYST SIGNAL PR, V119, P523, DOI 10.1016/j.ymssp.2018.10.006
   Park JW, 2010, NDT&E INT, V43, P642, DOI 10.1016/j.ndteint.2010.06.009
   Ribeiro D, 2014, ENG STRUCT, V75, P164, DOI 10.1016/j.engstruct.2014.04.051
   Ronda JI, 2019, J MATH IMAGING VIS, V61, P252, DOI 10.1007/s10851-018-0833-x
   [邵帅 Shao Shuai], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P91
   SMITH J, 2008, US PHARM S       MAR, P4
   [孙利民 Sun Limin], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P1
   [孙倩 Sun Qian], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P83
   Wadhwa N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461966
   Wang S.R., 2013, IABSE S REP, V101, P1, DOI [10.2749/222137813808627857, DOI 10.2749/222137813808627857]
   Wang SR, 2016, J BRIDGE ENG, V21, DOI 10.1061/(ASCE)BE.1943-5592.0000956
   [王邵锐 Wang Shaorui], 2015, [计算力学学报, Chinese Journal of Computational Mechanics], V32, P627
   [王邵锐 Wang Shaorui], 2015, [土木工程学报, China Civil Engineering Journal], V48, P70
   [王邵锐 Wang Shaorui], 2014, [土木工程学报, China Civil Engineering Journal], V47, P70
   Wang Shuai, 2016, Horticultural Plant Journal, V2, P82, DOI 10.1016/j.hpj.2016.03.001
   Wang Y, 2020, J PERFORM CONSTR FAC, V34, DOI 10.1061/(ASCE)CF.1943-5509.0001366
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xu Y, 2018, STRUCT CONTROL HLTH, V25, DOI 10.1002/stc.2155
   Yang YC, 2017, MECH SYST SIGNAL PR, V85, P567, DOI 10.1016/j.ymssp.2016.08.041
   Ye X.W., 2019, CHINA J HIGHWAY TRAN, V32, P20
   YIBIN HE, 2018, INT J MOD PHYS C, V29, P169
   Zhang S.B., 2017, CHINA J HIGHW TRANSP, V30, P251
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   [周志祥 Zhou Zhixiang], 2018, [应用基础与工程科学学报, Journal of Basic Science and Engineering], V26, P1078
   [宗周红 Zong Zhouhong], 2019, [中国公路学报, China Journal of Highway and Transport], V32, P40
NR 56
TC 6
Z9 6
U1 26
U2 38
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD FEB
PY 2020
VL 20
IS 4
AR 1187
DI 10.3390/s20041187
PG 24
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA KY3CM
UT WOS:000522448600238
PM 32098079
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Sun, YX
   Liu, M
   Meng, MQH
AF Sun, Yuxiang
   Liu, Ming
   Meng, Max Q. -H.
TI Active Perception for Foreground Segmentation: An RGB-D Data-Based
   Background Modeling Method
SO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
LA English
DT Article
DE Active perception; Image color analysis; Sensors; Computer vision; Image
   segmentation; Active perception; background modeling; foreground
   segmentation; RGB-D camera
ID MOTION REMOVAL; D SLAM; KINECT
AB Foreground moving object segmentation is a fundamental problem in many computer vision applications. As a solution for foreground segmentation, background modeling has been intensively studied over past years and many effective algorithms have been developed. However, accurate foreground segmentation is still a difficult problem. Currently, most of the algorithms work solely within the color space, in which the segmentation performance is prone to be degraded by a multitude of challenges, such as illumination changes, shadows, automatic camera adjustments, and color camouflage. RGB-D cameras are active visual sensors that provide depth measurements along with color images. We present in this paper an innovative background modeling method by using both the color and depth information from an RGB-D camera. The proposed method is evaluated using a public RGB-D data set. Various experiments confirm that our method is able to achieve superior performance compared with existing well-known methods. Note to Practitioners-This paper investigates background modeling for foreground segmentation with active perception. Recent RGB-D cameras that leverage the active perception technology have advanced many computer vision algorithms. In this paper, we develop a background modeling method to achieve superior performance by using an RGB-D camera instead of a color camera. Due to the use of the active sensing technology, the proposed method is characterized by its robustness to common challenges. Our method could be used for improving existing infrastructures, such as visual surveillance systems for parking spaces. Moreover, the simple design of our method allows it to be easily deployed on various computing platforms, which facilitates many practical applications that usually require embedded computing devices. However, our method cannot run real timely at the current status. We believe that it can be further improved using parallel programming techniques to meet the real-time requirement.
C1 [Sun, Yuxiang] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Inst Robot, Hong Kong, Peoples R China.
   [Liu, Ming] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
   [Meng, Max Q. -H.] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
RP Liu, M (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.; Meng, MQH (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM eeyxsun@ust.hk; eelium@ust.hk; max.meng@cuhk.edu.hk
RI Liu, Ming/AAC-9891-2020
OI Liu, Ming/0000-0002-4500-238X; Sun, Yuxiang/0000-0002-7704-0559; Meng,
   Max Q.-H./0000-0002-5255-5898
FU Shenzhen Science and Technology Innovation Project
   [JCYJ20160428154842603, JCYJ20170413161616163]; Hong Kong Research Grant
   Council (RGC)Hong Kong Research Grants Council [11210017, 16212815,
   21202816, 14205914, 14200618]; ITC ITF Project [ITS/236/15]; National
   Natural Science Foundation of ChinaNational Natural Science Foundation
   of China (NSFC) [U1713211]
FX This work was supported by the Shenzhen Science and Technology
   Innovation Project JCYJ20160428154842603, JCYJ20170413161616163, the
   Hong Kong Research Grant Council (RGC) Project 11210017, 16212815,
   21202816, 14205914, 14200618, the ITC ITF Project ITS/236/15, the
   National Natural Science Foundation of China Project U1713211.
CR Amamra A., 2014, P INT S ELM ZAD CROA, P1, DOI DOI 10.1109/ELMAR.2014.6923325
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   BAJCSY R, 1992, CVGIP-IMAG UNDERSTAN, V56, P31, DOI 10.1016/1049-9660(92)90083-F
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bouwmans T., 2014, BACKGROUND MODELING
   Cai ZY, 2017, MULTIMED TOOLS APPL, V76, P4313, DOI 10.1007/s11042-016-3374-6
   Camplani M, 2014, J VIS COMMUN IMAGE R, V25, P122, DOI 10.1016/j.jvcir.2013.03.009
   Cheng JY, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P589, DOI 10.1109/ICAR.2017.8023671
   Davis L., 2000, COMPUTER VISION ECCV, P751, DOI DOI 10.1007/3-540-45053-X_48
   Eveland C, 1998, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.1998.698619
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fernandez-Sanchez EJ, 2013, SENSORS-BASEL, V13, P8895, DOI 10.3390/s130708895
   Godbehere AB, 2012, P AMER CONTR CONF, P4305
   Gordon G., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P459, DOI 10.1109/CVPR.1999.784721
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941
   Hofmann M., 2012, P IEEE C COMP VIS PA, P38, DOI DOI 10.1109/CVPRW.2012.6238925
   Horaud R, 2016, MACH VISION APPL, V27, P1005, DOI 10.1007/s00138-016-0784-4
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Keselman L, 2017, IEEE COMPUT SOC CONF, P1267, DOI 10.1109/CVPRW.2017.167
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Liu H., IEEE T IND ELECT
   Murgia J, 2014, LECT NOTES ARTIF INT, V8856, P380, DOI 10.1007/978-3-319-13647-9_35
   Nageli T, 2017, IEEE ROBOT AUTOM LET, V2, P1696, DOI 10.1109/LRA.2017.2665693
   Ren XF, 2013, IEEE ROBOT AUTOM MAG, V20, P49, DOI 10.1109/MRA.2013.2253409
   Roushdy M., 2006, GVIP J, V6, P17
   Shah M, 2014, MACH VISION APPL, V25, P1105, DOI 10.1007/s00138-013-0552-7
   Shao L, 2013, IEEE T CYBERNETICS, V43, P1314, DOI 10.1109/TCYB.2013.2276144
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Sun YX, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1377, DOI 10.1109/ROBIO.2015.7418963
   Sun YX, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P1617, DOI 10.1109/ICInfA.2015.7279544
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Yan TF, 2018, IEEE INT CONF ROBOT, P6766
   Zahzah, 2015, ROBUST LOW RANK SPAR
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 42
TC 18
Z9 18
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-5955
EI 1558-3783
J9 IEEE T AUTOM SCI ENG
JI IEEE Trans. Autom. Sci. Eng.
PD OCT
PY 2019
VL 16
IS 4
BP 1596
EP 1609
DI 10.1109/TASE.2019.2893414
PG 14
WC Automation & Control Systems
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems
GA JG9XE
UT WOS:000492428500011
DA 2022-02-10
ER

PT J
AU Ghanem, B
   Cao, YH
   Wonka, P
AF Ghanem, Bernard
   Cao, Yuanhao
   Wonka, Peter
TI Designing Camera Networks by Convex Quadratic Programming
SO COMPUTER GRAPHICS FORUM
LA English
DT Article; Proceedings Paper
CT 36th Annual Conference of the European-Association-for-Computer-Graphics
CY MAY 04-08, 2015
CL Zurich, SWITZERLAND
SP European Assoc Comp Graph
ID PLACEMENT; ALGORITHM
AB In this paper, we study the problem of automatic camera placement for computer graphics and computer vision applications. We extend the problem formulations of previous work by proposing a novel way to incorporate visibility constraints and camera-to-camera relationships. For example, the placement solution can be encouraged to have cameras that image the same important locations from different viewing directions, which can enable reconstruction and surveillance tasks to perform better. We show that the general camera placement problem can be formulated mathematically as a convex binary quadratic program (BQP) under linear constraints. Moreover, we propose an optimization strategy with a favorable trade-off between speed and solution quality. Our solution is almost as fast as a greedy treatment of the problem, but the quality is significantly higher, so much so that it is comparable to exact solutions that take orders of magnitude more computation time. Because it is computationally attractive, our method also allows users to explore the space of solutions for variations in input parameters. To evaluate its effectiveness, we show a range of 3D results on real-world floorplans (garage, hotel, mall, and airport).
C1 [Ghanem, Bernard; Cao, Yuanhao; Wonka, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
RP Ghanem, B (corresponding author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
RI Ghanem, Bernard/J-7605-2017
OI Ghanem, Bernard/0000-0002-5534-587X
FU King Abdullah University of Science and Technology (KAUST)King Abdullah
   University of Science & Technology
FX We thank the anonymous reviewers for their valuable comments and
   suggestions. Special thanks goes to Yoshihiro Kobayashi and Christopher
   Grasso for generating the 3D renderings. Research reported in this
   publication was supported by competitive research funding from King
   Abdullah University of Science and Technology (KAUST).
CR Amriki K, 2011, IEEE INT CON MULTI
   Bodor R, 2007, J INTELL ROBOT SYST, V50, P257, DOI 10.1007/s10846-007-9164-7
   Buchheim C, 2010, LECT NOTES COMPUT SC, V6080, P285, DOI 10.1007/978-3-642-13036-6_22
   Delbos F, 2005, J CONVEX ANAL, V12, P45
   Dunn E, 2006, PATTERN RECOGN LETT, V27, P1209, DOI 10.1016/j.patrec.2005.07.019
   Ercan AO, 2006, LECT NOTES COMPUT SC, V4026, P389
   Erdem UM, 2006, COMPUT VIS IMAGE UND, V103, P156, DOI 10.1016/j.cviu.2006.06.005
   Gonzalez-Barbosa JJ, 2009, IEEE INT CONF ROBOT, P3672
   HORSTER E., 2006, INT WORKSH VID SURV
   Jian Zhao, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1705, DOI 10.1109/ICCVW.2011.6130455
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Mavrinac A, 2014, ACM T SENSOR NETWORK, V10, DOI 10.1145/2530373
   Mavrinac A, 2013, INT J COMPUT VISION, V101, P205, DOI 10.1007/s11263-012-0587-7
   MITTAL A., 2004, EUR C COMP VIS 2004
   O'Rourke J., 1987, ART GALLERY THEOREMS
   Olsson C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383202
   RAM S., 2006, INT WORKSH VID SURV
   Sivaram GSVS, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556140
   STEINITZ A., 2012, THESIS U CALIF
   TARABANIS P., 1995, IEEE T ROBOTICS AUTO
   van der Vlies AE, 2009, PSYCHOL MED, V39, P1907, DOI 10.1017/S0033291709005492
   Yabuta K, 2008, IEEE INT SYMP CIRC S, P2114
   YAO Y., 2008, P IEEE COMP SOC C CO, P1
NR 23
TC 6
Z9 6
U1 0
U2 9
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0167-7055
EI 1467-8659
J9 COMPUT GRAPH FORUM
JI Comput. Graph. Forum
PD MAY
PY 2015
VL 34
IS 2
BP 69
EP 80
DI 10.1111/cgf.12542
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CN3LB
UT WOS:000358326600011
OA Green Submitted
DA 2022-02-10
ER

PT J
AU Garau, N
   De Natale, FGB
   Conci, N
AF Garau, Nicola
   De Natale, Francesco G. B.
   Conci, Nicola
TI Fast automatic camera network calibration through human mesh recovery
SO JOURNAL OF REAL-TIME IMAGE PROCESSING
LA English
DT Article
DE Camera calibration; Pose estimation; Human mesh recovery; 3D matching
ID SELF-CALIBRATION
AB Camera calibration is a necessary preliminary step in computer vision for the estimation of the position of objects in the 3D world. Despite the intrinsic camera parameters can be easily computed offline, extrinsic parameters need to be computed each time a camera changes its position, thus not allowing for fast and dynamic network re-configuration. In this paper we present an unsupervised and automatic framework for the estimation of the extrinsic parameters of a camera network, which leverages on optimised 3D human mesh recovery from a single image, and which does not require the use of additional markers. We show how it is possible to retrieve the real-world position of the cameras in the network together with the floor plane, exploiting regular RGB images and with a weak prior knowledge of the internal parameters. Our framework can also work with a single camera and in real-time, allowing the user to add, re-position, or remove cameras from the network in a dynamic fashion.
C1 [Garau, Nicola; De Natale, Francesco G. B.; Conci, Nicola] Univ Trento, Via Sommar 9, I-38123 Trento, TN, Italy.
RP Garau, N (corresponding author), Univ Trento, Via Sommar 9, I-38123 Trento, TN, Italy.
EM nicola.garau@unitn.it; francesco.denatale@unitn.it;
   nicola.conci@unitn.it
FU UniversitA degli Studi di Trento within the CRUI-CARE Agreement
FX Open access funding provided by UniversitA degli Studi di Trento within
   the CRUI-CARE Agreement.
CR Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   Cao Z., 2018, ARXIV181208008
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Coughlan JM, 2001, ADV NEUR IN, V13, P845
   Desai K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P250, DOI 10.1145/3204949.3204969
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Garau N., 2019, P 13 INT C DISTR SMA
   Geiger A, 2012, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2012.6224570
   Hidalgo G., 2019, ARXIV190913423
   Hold-Geoffroy Y, 2018, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR.2018.00250
   Inomata Ryo, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P325
   Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381
   Kanazawa A, 2019, IEEE C COMP VIS PATT, P5614
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kim H, 2001, IEE P-VIS IMAGE SIGN, V148, P349, DOI 10.1049/ip-vis:20010574
   Kolotouros N., 2019, ARXIV190912828
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Lucas B. D., 1981, P INT JOINT C ART IN, V2, P674, DOI 10.5555/1623264.1623280
   Miyata S, 2018, IEEE T CIRC SYST VID, V28, P2210, DOI 10.1109/TCSVT.2017.2731792
   Nister D., 2004, COMP VIS PATT REC 20, V1, pI
   Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014
   Ramakrishna V, 2014, COMPUTER VISION ECCV, V33, P47
   Seo Y, 2001, IEE P-VIS IMAGE SIGN, V148, P166, DOI 10.1049/ip-vis:20010078
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Simek K, 2013, PINHOLE CAMERA DIAGR
   Tang Z, 2019, IEEE ACCESS, V7, P10754, DOI 10.1109/ACCESS.2019.2891224
   Tang Z, 2016, INT CONF ACOUST SPEE, P1115, DOI 10.1109/ICASSP.2016.7471849
   Tome D., 2017, P IEEE C COMP VIS PA, P2500
   Vasconcelos F, 2018, IEEE T PATTERN ANAL, V40, P791, DOI 10.1109/TPAMI.2017.2699648
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Zhang GP, 2018, MEAS SCI TECHNOL, V29, DOI 10.1088/1361-6501/aab4d6
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao FD, 2018, IMAGE VISION COMPUT, V70, P46, DOI 10.1016/j.imavis.2017.12.006
NR 36
TC 2
Z9 2
U1 4
U2 4
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1861-8200
EI 1861-8219
J9 J REAL-TIME IMAGE PR
JI J. Real-Time Image Process.
PD DEC
PY 2020
VL 17
IS 6
SI SI
BP 1757
EP 1768
DI 10.1007/s11554-020-01002-w
EA SEP 2020
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA OP5TA
UT WOS:000566041200001
OA hybrid
DA 2022-02-10
ER

PT C
AU Pastarmov, Y
AF Pastarmov, Yulian
BE Rachev, B
   Smrikarov, A
TI System for Automatic Camera Calibration Robust Against Blur and Lighting
   Conditions Changes
SO COMPUTER SYSTEMS AND TECHNOLOGIES, COMPSYSTECH'16
LA English
DT Proceedings Paper
CT 17th International Conference on Computer Systems and Technologies
   (CompSysTech)
CY JUN 23-24, 2016
CL Palermo, ITALY
SP Assoc Comp Machinery, Bulgarian Chapter, Bulgarian Acad Soc Comp Syst & Informat Technologies, Bulgarian Union Automat & Informat, Univ Ruse, QUERBIE Inc, Univ Palermo, Tech Univ Varna
DE Camera Calibration; Intrinsic Camera Models; Monocular and Stereo
   Computer Vision
AB This paper presents a system for precise intrinsic and extrinsic camera calibration for monocular and stereo cameras. The presented approach is based on some well established research in the field and utilizes calibration patterns. The novelty of the presented method is the automatic detection of motion blur, that is common to the process of calibration based on moving the camera or the calibration object, especially under bad lighting conditions. The method allows for sub-pixel re-projection precision for cameras with perspective lenses and can be extended to omnidirectional cameras as well.
C1 [Pastarmov, Yulian] Google, Munich, Germany.
   [Pastarmov, Yulian] St Cyril & St Methodius Univ Veliko Tarnovo, Comp Syst & Technol Dept, Veliko Tarnovo, Bulgaria.
RP Pastarmov, Y (corresponding author), Google, Munich, Germany.; Pastarmov, Y (corresponding author), St Cyril & St Methodius Univ Veliko Tarnovo, Comp Syst & Technol Dept, Veliko Tarnovo, Bulgaria.
EM pastarmovj@google.com
CR Bao Q., 2008, BLUR METRIC IMPLEMEN
   Crete-Roffet F., 2007, P SPIE INT SOC OPTIC
   Datta A., 2009, IEEE 12 ICCV COMP VI
   Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269
   Fischler M. A., 1981, COMMUNICATIONS ACM, V24
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   MathWorks, EST GEOM PAR SINGL C
   Richardson A., 2013, P IEEE RSJ INT C INT
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Yao Y., 2006, EVALUATION SHARPNESS
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 11
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-4182-0
PY 2016
BP 167
EP 174
DI 10.1145/2983468.2983522
PG 8
WC Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BP5WA
UT WOS:000558303700022
OA Bronze
DA 2022-02-10
ER

PT J
AU Begin, PN
   Tanabe, Y
   Kumagai, M
   Culley, AI
   Paquette, M
   Sarrazin, D
   Uchida, M
   Vincent, WF
AF Begin, Paschale N.
   Tanabe, Yukiko
   Kumagai, Michio
   Culley, Alexander, I
   Paquette, Michel
   Sarrazin, Denis
   Uchida, Masaki
   Vincent, Warwick F.
TI Extreme warming and regime shift toward amplified variability in a far
   northern lake
SO LIMNOLOGY AND OCEANOGRAPHY
LA English
DT Article
ID SEA-ICE; ECOSYSTEMS; HEMISPHERE; RESPONSES; ISLAND; SNOW
AB Mean annual air temperatures in the High Arctic are rising rapidly, with extreme warming events becoming increasingly common. Little is known, however, about the consequences of such events on the ice-capped lakes that occur abundantly across this region. Here, we compared 2 years of high-frequency monitoring data in Ward Hunt Lake in the Canadian High Arctic. One of the years included a period of anomalously warm conditions that allowed us to address the question of how loss of multi-year ice cover affects the limnological properties of polar lakes. A mooring installed at the deepest point of the lake (9.7 m) recorded temperature, oxygen, chlorophylla(Chla) fluorescence, and underwater irradiance from July 2016 to July 2018, and an automated camera documented changes in ice cover. The complete loss of ice cover in summer 2016 resulted in full wind exposure and complete mixing of the water column. This mixing caused ventilation of lake water heat to the atmosphere and 4 degrees C lower water temperatures than under ice-covered conditions. There were also high values of Chlafluorescence, elevated turbidity levels and large oxygen fluctuations throughout fall and winter. During the subsequent summer, the lake retained its ice cover and the water column remained stratified, with lower Chlafluorescence and anoxic bottom waters. Extreme warming events are likely to shift polar lakes that were formerly capped by continuous thick ice to a regime of irregular ice loss and unstable limnological conditions that vary greatly from year to year.
C1 [Begin, Paschale N.; Culley, Alexander, I; Paquette, Michel; Sarrazin, Denis; Vincent, Warwick F.] CEN, Quebec City, PQ, Canada.
   [Begin, Paschale N.; Culley, Alexander, I; Paquette, Michel; Sarrazin, Denis; Vincent, Warwick F.] Takuvik Joint Int Lab, Quebec City, PQ, Canada.
   [Begin, Paschale N.; Vincent, Warwick F.] Univ Laval, Dept Biol, Quebec City, PQ, Canada.
   [Tanabe, Yukiko; Uchida, Masaki] Natl Inst Polar Res, Tachikawa, Tokyo, Japan.
   [Tanabe, Yukiko; Uchida, Masaki] Grad Univ Adv Studies, SOKENDAI, Hayama, Kanagawa, Japan.
   [Kumagai, Michio] Ritsumeikan Univ, Kyoto, Japan.
   [Culley, Alexander, I] Univ Laval, Dept Biochim Microbiol & Bioinformat, Quebec City, PQ, Canada.
   [Paquette, Michel] Queens Univ, Dept Geog & Planning, Kingston, ON, Canada.
RP Begin, PN; Vincent, WF (corresponding author), CEN, Quebec City, PQ, Canada.; Begin, PN; Vincent, WF (corresponding author), Takuvik Joint Int Lab, Quebec City, PQ, Canada.; Begin, PN; Vincent, WF (corresponding author), Univ Laval, Dept Biol, Quebec City, PQ, Canada.
EM pnbegin@gmail.com; warwick.vincent@bio.ulaval.ca
FU Ministry of Education, Culture, Sports, Science and Technology,
   JapanMinistry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT); NEIGE (Northern Ellesmere Island in the Global
   Environment) - Sentinel North (Canada First Research Excellence Fund);
   ArcticNet (Network of Centres of Excellence, Canada); Centre d'etudes
   nordiques (CEN); Fonds de Recherche du Quebec Nature et Technologies
   (FRQNT); Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC); Northern Scientific Training Program (NSTP)
FX This research is a contribution to the projects ArCS (Arctic Challenge
   for Sustainability) supported by the Ministry of Education, Culture,
   Sports, Science and Technology, Japan, and NEIGE (Northern Ellesmere
   Island in the Global Environment) supported by Sentinel North (Canada
   First Research Excellence Fund), ArcticNet (Network of Centres of
   Excellence, Canada), Centre d'etudes nordiques (CEN), Fonds de Recherche
   du Quebec Nature et Technologies (FRQNT), the Natural Sciences and
   Engineering Research Council of Canada (NSERC) and the Northern
   Scientific Training Program (NSTP), with logistic support from the Polar
   Continental Shelf Program (PCSP) and Parks Canada. This work is also a
   contribution to the International Arctic Science Committee (IASC)
   project T-MOSAiC (Terrestrial Multidisciplinary distributed
   Observatories for the Study of Arctic Connections). The authors thank
   Myriam Labbe, Jerome Comte, and Nicolas Bochaton for their help with
   fieldwork, Raoul-Marie Couture for advice on oxygen analysis, and two
   anonymous reviewers for their insightful comments and suggestions that
   improved the manuscript.
CR Babb DG, 2019, J GEOPHYS RES-OCEANS, V124, P6575, DOI 10.1029/2019JC015053
   Belzile C, 2001, CAN J FISH AQUAT SCI, V58, P2405, DOI 10.1139/cjfas-58-12-2405
   Bieniek PA, 2017, INT J CLIMATOL, V37, P208, DOI 10.1002/joc.4994
   Bonilla S, 2005, J PHYCOL, V41, P1120, DOI 10.1111/j.1529-8817.2005.00154.x
   CEN, 2020, NORDICANA, pD1, DOI [10.5885/44985SL-8F203FD3ACCD4138, DOI 10.5885/44985SL-8F203FD3ACCD4138]
   Charvet S, 2012, POLAR BIOL, V35, P733, DOI 10.1007/s00300-011-1118-7
   Cortes A, 2020, LIMNOL OCEANOGR, V65, P260, DOI 10.1002/lno.11296
   CRAIG H, 1992, SCIENCE, V255, P318, DOI 10.1126/science.11539819
   Deshpande BN, 2015, LIMNOL OCEANOGR, V60, P1656, DOI 10.1002/lno.10126
   Doran PT, 1996, LIMNOL OCEANOGR, V41, P839, DOI 10.4319/lo.1996.41.5.0839
   Du JY, 2017, CRYOSPHERE, V11, P47, DOI 10.5194/tc-11-47-2017
   Fountain AG, 2016, BIOSCIENCE, V66, P848, DOI 10.1093/biosci/biw110
   GOSSELIN M, 1985, CAN J FISH AQUAT SCI, V42, P999, DOI 10.1139/f85-125
   Hampton SE, 2017, ECOL LETT, V20, P98, DOI 10.1111/ele.12699
   Huot Y, 2010, DEVEL APPL PHYCOL, V4, P31, DOI 10.1007/978-90-481-9268-7_3
   KALFF J, 1974, J FISH RES BOARD CAN, V31, P621, DOI 10.1139/f74-094
   Kalff J., 2003, LIMNOLOGY INLAND WAT
   Kirillin G, 2012, AQUAT SCI, V74, P659, DOI 10.1007/s00027-012-0279-y
   Landy JC, 2015, GEOPHYS RES LETT, V42, P10714, DOI 10.1002/2015GL066712
   Lehnherr I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03685-z
   Ludlam SD, 1996, J PALEOLIMNOL, V16, P111
   Macias-Fauria M, 2018, BIOL LETTERS, V14, DOI 10.1098/rsbl.2017.0702
   Mackowiak CL, 1997, ACTA HORTIC, P19
   Matveev A, 2019, J GEOPHYS RES-BIOGEO, V124, P3521, DOI 10.1029/2019JG005078
   McGrath D, 2013, GEOPHYS RES LETT, V40, P2091, DOI 10.1002/grl.50456
   Meredith M., 2019, IPCC SPECIAL REPORT
   Mioduszewski JR, 2019, CRYOSPHERE, V13, P113, DOI 10.5194/tc-13-113-2019
   Mohit V, 2017, NPJ BIOFILMS MICROBI, V3, DOI 10.1038/s41522-017-0024-3
   Moore CM, 2006, LIMNOL OCEANOGR, V51, P936, DOI 10.4319/lo.2006.51.2.0936
   Mudryk LR, 2018, CRYOSPHERE, V12, P1157, DOI 10.5194/tc-12-1157-2018
   NEIGE, 2020, NORDICANA D, pD74, DOI [10.5885/45648CE-1A9AB63DFF91440B, DOI 10.5885/45648CE-1A9AB63DFF91440B]
   Obryk MK, 2019, J GEOPHYS RES-EARTH, V124, P686, DOI 10.1029/2018JF004756
   Overland J, 2019, POLAR SCI, V21, P6, DOI 10.1016/j.polar.2018.11.008
   Paquette M, 2017, ARCT SCI, V3, P334, DOI 10.1139/as-2016-0014
   Paquette M, 2015, GEOPHYS RES LETT, V42, P1433, DOI 10.1002/2014GL062960
   Paterson AM, 2008, CAN J FISH AQUAT SCI, V65, P846, DOI 10.1139/F08-022
   Pernica P, 2017, INLAND WATERS, V7, P138, DOI 10.1080/20442041.2017.1296627
   SCHINDLE.DW, 1974, J FISH RES BOARD CAN, V31, P585, DOI 10.1139/f74-092
   Schmidt NM, 2019, PLOS BIOL, V17, DOI 10.1371/journal.pbio.3000392
   Schuur EAG, 2015, NATURE, V520, P171, DOI 10.1038/nature14338
   Serreze MC, 2019, ANN NY ACAD SCI, V1436, P36, DOI 10.1111/nyas.13856
   Sharma S, 2019, NAT CLIM CHANGE, V9, P227, DOI 10.1038/s41558-018-0393-5
   Staehr PA, 2010, LIMNOL OCEANOGR-METH, V8, P628, DOI 10.4319/lom.2010.8.0628
   Tanabe Y, 2008, POLAR BIOL, V31, P199, DOI 10.1007/s00300-007-0347-2
   Vincent AC, 2008, J GEOPHYS RES-OCEANS, V113, DOI 10.1029/2007JC004360
   Vincent WF, 2020, PALGRAVE HANDBOOK OF ARCTIC POLICY AND POLITICS, P507, DOI 10.1007/978-3-030-20557-7_31
   Vincent WF, 2011, ECOSCIENCE, V18, P236, DOI 10.2980/18-3-3448
   Vopel K, 2006, LIMNOL OCEANOGR, V51, P1801, DOI 10.4319/lo.2006.51.4.1801
   WHARTON RA, 1986, LIMNOL OCEANOGR, V31, P437, DOI 10.4319/lo.1986.31.2.0437
   Wrona FJ, 2016, J GEOPHYS RES-BIOGEO, V121, P650, DOI 10.1002/2015JG003133
NR 50
TC 7
Z9 8
U1 3
U2 23
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0024-3590
EI 1939-5590
J9 LIMNOL OCEANOGR
JI Limnol. Oceanogr.
PD JAN
PY 2021
VL 66
SU 1
SI SI
BP S17
EP S29
DI 10.1002/lno.11546
EA JUL 2020
PG 13
WC Limnology; Oceanography
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Marine & Freshwater Biology; Oceanography
GA QN5FR
UT WOS:000551563700001
OA hybrid, Green Accepted
DA 2022-02-10
ER

PT C
AU Li, XH
   Wang, GY
   Liu, JG
AF Li, Xiuhua
   Wang, Guoyou
   Liu, Jianguo
BE Tian, J
   Ma, J
TI Automatic camera calibration method based on dashed lines
SO MIPPR 2013: REMOTE SENSING IMAGE PROCESSING, GEOGRAPHIC INFORMATION
   SYSTEMS, AND OTHER APPLICATIONS
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT 8th Symposium on Multispectral Image Processing and Pattern Recognition
   (MIPPR) - Remote Sensing Image Processing, Geographic Information
   Systems, and Other Applications
CY OCT 26-27, 2013
CL Wuhan, PEOPLES R CHINA
SP SPIE, Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multi Spectral Informat Proc
DE Camera Calibration; Dashed line; Endpoint; Traffic monitoring; Computer
   vision
AB We present a new method for full-automatic calibration of traffic cameras using the end points on dashed lines. Our approach uses the improved RANSAC method with the help of pixels transverse projection to detect the dashed lines and end points on them. Then combining analysis of the geometric relationship between the camera and road coordinate systems, we construct a road model to fit the end points. Finally using two-dimension calibration method we can convert pixels in image to meters along the ground truth lane. On a large number of experiments exhibiting a variety of conditions, our approach performs well, achieving less than 5% error in measuring test lengths in all cases.
C1 [Li, Xiuhua; Wang, Guoyou; Liu, Jianguo] Huazhong Univ Sci & Technol, Sch Automat, Sci & Technol Multispectral Informat Proc Lab, Wuhan 430074, Peoples R China.
RP Li, XH (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Sci & Technol Multispectral Informat Proc Lab, Wuhan 430074, Peoples R China.
EM gywang@mail.hust.edu.cn
CR [Anonymous], 2011, PANORAMA ACTUAL MED, V35, P30
   Bas EK, 1997, IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P362, DOI 10.1109/ITSC.1997.660502
   Cathey R, 2004, ITSC 2004: 7TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P865
   Dawson Douglas N., 2013, IEEE INTEL TRANSP SY, P1
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fu Y., 2012, ELECT DESIGN ENG, P49
   Fung GSK, 2003, OPT ENG, V42, P2967, DOI 10.1117/1.1606458
   He XC, 2007, OPT ENG, V46, DOI 10.1117/1.2714991
   Kanhere NK, 2008, IEEE T INTELL TRANSP, V9, P148, DOI 10.1109/TITS.2007.911357
   Kanhere NK, 2008, TRANSPORT RES REC, P30, DOI 10.3141/2086-04
   Lai AHS, 2000, IEEE T SYST MAN CY B, V30, P539, DOI 10.1109/3477.865171
   Otsu N., 1997, IEEE T SYST MAN CYB, V9, P62, DOI DOI 10.1109/TSMC.1979.4310076
   Schoepflin Todd N, 2007, 2007 IEEE Intelligent Transportation Systems Conference, P277, DOI 10.1109/ITSC.2007.4357806
   Song KT, 2006, IEEE T SYST MAN CY B, V36, P1091, DOI 10.1109/TSMCB.2006.872271
   Trajkovi'c M., 2002, P AS C COMP VIS, P1
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/jra.1987.1087109
   Wu B.F., 2007, P IEEE INT C SMC OCT, P1717
NR 17
TC 0
Z9 0
U1 0
U2 1
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-0-8194-9806-9
J9 PROC SPIE
PY 2013
VL 8921
AR 892112
DI 10.1117/12.2031362
PG 8
WC Geosciences, Multidisciplinary; Remote Sensing; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Geology; Remote Sensing; Optics
GA BID12
UT WOS:000327588700038
DA 2022-02-10
ER

PT J
AU Nian, HF
AF Nian, Hongfen
TI Civil engineering stability inspection based on computer vision and
   sensors
SO MICROPROCESSORS AND MICROSYSTEMS
LA English
DT Article
DE Monitoring applications; Computer vision; Accelerometer; Non-destructive
   evaluation; Conventional-contact displacement sensors
AB A computer that combines the purchase of vision technology and remote cameras and drones offers a promising non-contact solution for the state evaluation of civil infrastructure. This system's ultimate goal is too automatically and reliably converted to actionable information image or video data. This white paper provides an overview of computer vision technology's latest development and applies it to the state evaluation of private infrastructure. Deep learning has been applied to various computer vision; deep learning course covers most of the application. Each application has its architecture, such as the input image and labels data loss function. To explain computer vision architecture in the following figure. Review of the work can be divided into two types: application checks and application monitoring. Review inspection applications include context identifiers, local and global features, visible damage, and changes in the reference image. Monitoring applications described herein include static and dynamic strain modal analysis measurement and displacement measurement. Next, several key challenges continue to move towards civilian infrastructure automation and monitoring of visionbased. Finally, aim to address some of the ongoing challenges in our work.
C1 [Nian, Hongfen] Panzhihua Univ, Audit Off, Panzhihua 617000, Sichuan, Peoples R China.
RP Nian, HF (corresponding author), Panzhihua Univ, Audit Off, Panzhihua 617000, Sichuan, Peoples R China.
EM nhf5522391@sina.com
FU Natural Science Research Project of Sichuan Education Department
   [17ZA0231]
FX Natural Science Research Project of Sichuan Education Department
   (17ZA0231)
CR ASCE's, 2017, INFRASTRUCTURE REPOR
   Bishop C.M.., 2006, PATTERN RECOGN
   Brownjohn JMW, 2007, PHILOS T R SOC A, V365, P589, DOI 10.1098/rsta.2006.1925
   Carden EP, 2004, STRUCT HEALTH MONIT, V3, P355, DOI 10.1177/1475921704047500
   Dalal N., 2021, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   FHWA, 2004, FED REGISTER, V69, P74419
   Gallagher R. P., 2005, 201 ATC
   Glorot X., 2011, PROC 14 INT C ARTIFI, P315, DOI DOI 10.1.1.208.6449
   Li HN, 2016, J CIV STRUCT HEALTH, V6, P3, DOI 10.1007/s13349-015-0108-9
   Ou JP, 2010, STRUCT HEALTH MONIT, V9, P219, DOI 10.1177/1475921710365269
   Pingali G, 2000, INT C PATT RECOG, P152, DOI 10.1109/ICPR.2000.902885
   Roberts, 1963, THESIS
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Zhang Fang, 2020, MICROPROCESS MICROSY, P103430
   Zhu, 2020, MICROPROCESS MICROSY, V2020, P103495, DOI [10.1016/j.micpro.2020.103495, DOI 10.1016/J.MICPRO.2020.103495]
NR 18
TC 0
Z9 0
U1 9
U2 12
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0141-9331
EI 1872-9436
J9 MICROPROCESS MICROSY
JI Microprocess. Microsyst.
PD APR
PY 2021
VL 82
AR 103838
DI 10.1016/j.micpro.2021.103838
PG 6
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA RY2QO
UT WOS:000647762200010
DA 2022-02-10
ER

PT C
AU Bullinger, S
   Bodensteiner, C
   Arens, M
AF Bullinger, Sebastian
   Bodensteiner, Christoph
   Arens, Michael
BE Sousa, AA
   Havran, V
   Braz, J
   Bouatouch, K
TI A Photogrammetry-based Framework to Facilitate Image-based Modeling and
   Automatic Camera Tracking
SO GRAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS -
   VOL. 1: GRAPP
LA English
DT Proceedings Paper
CT 16th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP) / 16th
   International Conference on Computer Graphics Theory and Applications
   (GRAPP)
CY FEB 08-10, 2021
CL ELECTR NETWORK
DE Image-based Modeling; Camera Tracking; Photogrammetry; Structure from
   Motion; Multi-view Stereo; Blender
AB We propose a framework that extends Blender to exploit Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks such as sculpting or camera and motion tracking. Applying SfM allows us to determine camera motions without manually defining feature tracks or calibrating the cameras used to capture the image data. With MVS we are able to automatically compute dense scene models, which is not feasible with the built-in tools of Blender. Currently, our framework supports several state-of-the-art SfM and MVS pipelines. The modular system design enables us to integrate further approaches without additional effort. The framework is publicly available as an open source software package.
C1 [Bullinger, Sebastian; Bodensteiner, Christoph; Arens, Michael] Fraunhofer IOSB, Dept Object Recognit, Ettlingen, Germany.
RP Bullinger, S (corresponding author), Fraunhofer IOSB, Dept Object Recognit, Ettlingen, Germany.
CR AliceVision (2020a), MESHR 3D REC SOFTW
   AliceVision (2020b), MESHROOMMAYA MAYA PL
   [Anonymous], 2012, AS C COMP VIS ACCV
   Attenborrow S., 2020, BLENDER PHOTOGRAMMET
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Blender Online Community, 2020, BLEND 3D MOD REND PA
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Cernea D., 2020, OPENMVS MULT STER RE
   Cignoni P., 2008, EUR IT CHAPT C
   Fuhrmann S., 2014, P EUR WORKSH GRAPH C, P11, DOI DOI 10.2312/GCH.20141299
   Fuhrmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601163
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Girardeau-Montaut Daniel, 2020, CLOUDCOMPARE
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Hiestand R, 2020, REGARD3D FREE OPEN S
   Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Jancosek Michal, 2014, Int Sch Res Notices, V2014, P798595, DOI 10.1155/2014/798595
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Langguth Fabian, 2016, P EUR C COMP VIS ECC
   Moulon P, 2012, SCEAUX CASTLE DATASE
   Moulon P., 2013, OPENMVG OPEN MULTIPL
   Schonberger J. L, 2020, COLMAP GEN PURPOSE S
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   SideEffects, 2020, GAM DEV TOOLS HOUD
   Uhik J., 2020, AG PHOT IMP BLEND
   Ummenhofer B., 2017, INT J COMPUT VISION, P1
   Waechter M, 2014, LECT NOTES COMPUT SC, V8693, P836, DOI 10.1007/978-3-319-10602-1_54
   Woo M., 1999, OPENGL PROGRAMMING G
   Wu C., 2011, VISUALSFM VISUAL STR
   Zhou Q.-Y., 2018, ARXIV180109847
NR 32
TC 0
Z9 0
U1 0
U2 0
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-488-6
PY 2021
BP 106
EP 112
DI 10.5220/0010319801060112
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BR6JJ
UT WOS:000661346500010
OA Green Submitted, hybrid
DA 2022-02-10
ER

PT J
AU Avrin, AC
   Pekins, CE
   Sperry, JH
   Wolff, PJ
   Allen, ML
AF Avrin, Alexandra C.
   Pekins, Charles E.
   Sperry, Jinelle H.
   Wolff, Patrick J.
   Allen, Maximilian L.
TI Efficacy of attractants for detecting eastern spotted skunks: an
   experimental approach
SO WILDLIFE BIOLOGY
LA English
DT Article
DE bait; camera trap; carrion; detection; experimental framework; fatty
   acid tablet; large animal carcass; lure; sardines; Spilogale putorius
ID WINTER HABITAT ASSOCIATIONS; SPILOGALE-PUTORIUS; ACTIVITY PATTERNS; SITE
   SELECTION; CARNIVORES; CAPTURE; MOUNTAINS; ARKANSAS; RATES
AB Estimates of abundance and occupancy are essential for wildlife management, particularly for species of conservation concern such as eastern spotted skunks Spilogale putorius. Most studies of eastern spotted skunks rely on limited evidence for best monitoring practices, and while many studies use attractants to increase detections, previous studies have not tested attractants against a control of no attractant to determine their effectiveness. We tested two common attractants (sardines and fatty acid tablets) and one uncommon attractant (wild boar carcasses) against a control of no attractant to determine if any attractant increased detections of eastern spotted skunks or changed their temporal activity. Based on our model, sardines and wild boar carcasses improved detections by three and eight times that of the control, respectively. Further, for every 100 trap nights, we detected eastern spotted skunks 10.67 times with wild boar carcasses, 1.02 times with sardines, 0.53 times with fatty acid tablets and 0.44 times with no attractant. Wild boar carcasses also substantially decreased latency to detection, with skunks detected two times faster than at other attractants and almost three times faster than at the control. Eastern spotted skunks were most active in the early morning before sunrise, and their temporal activity did not vary significantly by attractant. This study is the first to use an experimental framework to test attractants for eastern spotted skunks, and our results showed that choice of attractant matters. Large animal carcasses, although rarely used, may be most effective for detecting eastern spotted skunks, while fatty acid tablets were no different than the control, and we recommend against their use in future studies. Monitoring plans should incorporate our results as increasing detections is essential to understanding the abundance, range and demographics of eastern spotted skunks.
C1 [Avrin, Alexandra C.; Sperry, Jinelle H.; Wolff, Patrick J.; Allen, Maximilian L.] Univ Illinois, Dept Nat Resources & Environm Sci, Urbana, IL 61801 USA.
   [Allen, Maximilian L.] Univ Illinois, Illinois Nat Hist Survey, Champaign, IL 61820 USA.
   [Pekins, Charles E.] Us Army Garrison, Ft Hood Nat Resources Management Branch, Ft Hood, TX USA.
   [Sperry, Jinelle H.; Wolff, Patrick J.] US Army Corps Engineers, Construct Engn Res Lab, Champaign, IL USA.
RP Avrin, AC (corresponding author), Univ Illinois, Dept Nat Resources & Environm Sci, Urbana, IL 61801 USA.
EM aavrin@illinois.edu
FU Fort Hood Natural Resources Management Branch; US Army Engineer Research
   and Development CenterUnited States Department of Defense; Illinois
   Natural History Survey
FX We thank Jess Daley for help with data collection, Michael Ward and
   Kevin Cagle for their support, and Summer LaRose for reviews that
   substantially improved our paper. We thank the Fort Hood Natural
   Resources Management Branch, US Army Engineer Research and Development
   Center and the Illinois Natural History Survey for logistical support
   and funding.
CR Allen ML, 2020, BIODIVERS CONSERV, V29, P3591, DOI 10.1007/s10531-020-02039-w
   Allen ML, 2015, AM NAT, V185, P822, DOI 10.1086/681004
   Allen Maximilian L., 2013, Canadian Field-Naturalist, V127, P64
   BAILEY TN, 1971, AM MIDL NAT, V85, P196, DOI 10.2307/2423922
   Benson IW, 2019, SOUTHEAST NAT, V18, P165, DOI 10.1656/058.018.0111
   Boulerice JT, 2017, AM MIDL NAT, V178, P17, DOI 10.1674/0003-0031-178.1.17
   Briffa M, 2007, FUNCT ECOL, V21, P627, DOI 10.1111/j.1365-2435.2006.01188.x
   Diggins CA, 2015, NORTHEAST NAT, V22, pN6, DOI 10.1656/045.022.0211
   Dytham C., 2011, CHOOSING USING STAT, V3rd ed.
   Eckrich GH, 1999, STUD AVIAN BIOL, P267
   Eng RYY, 2019, J MAMMAL, V100, P1295, DOI 10.1093/jmammal/gyz075
   Eng RYY, 2019, J WILDLIFE MANAGE, V83, P1244, DOI 10.1002/jwmg.21684
   ESRI, 2011, ARCGIS DESKT REL 10
   ESSCSG (Eastern Spotted Skunk Cooperative Study Group), 2020, E SPOTT SKUNK CONS P
   Gerber BD, 2012, POPUL ECOL, V54, P43, DOI 10.1007/s10144-011-0276-3
   Gompper M., 2016, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474.en, DOI 10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474, 10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474, DOI 10.2305/IUCN.UK.2016-1.RLTS.T41636A45211474.EN]
   Gompper ME, 2006, WILDLIFE SOC B, V34, P1142, DOI 10.2193/0091-7648(2006)34[1142:ACONTT]2.0.CO;2
   Gompper ME, 2005, ANIM CONSERV, V8, P195, DOI 10.1017/S1367943005001964
   Hackett HM, 2007, AM MIDL NAT, V158, P123, DOI 10.1674/0003-0031(2007)158[123:DROESS]2.0.CO;2
   Harris SN, 2020, J WILDLIFE MANAGE, V84, P127, DOI 10.1002/jwmg.21780
   Hayden TJ, 2000, ECOLOGY AND MANAGEMENT OF COWBIRDS AND THEIR HOSTS, P357
   Heinlein BW, 2020, WILDLIFE RES, V47, P338, DOI 10.1071/WR19117
   Higdon SD, 2020, SOUTHEAST NAT, V19, P74, DOI 10.1656/058.019.0110
   Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
   Kinlaw Al, 1995, Mammalian Species, V511, P1
   Lesmeister DB, 2013, RESTOR ECOL, V21, P267, DOI 10.1111/j.1526-100X.2012.00880.x
   Lesmeister DB, 2009, J WILDLIFE MANAGE, V73, P18, DOI 10.2193/2007-447
   Lombardi JV, 2017, NAT AREA J, V37, P506
   Meredith Mike, 2020, CRAN
   Millspaugh, 2005, WILDLIFE DEMOGRAPHY
   Perry RW, 2018, SOUTHEAST NAT, V17, P298, DOI 10.1656/058.017.0213
   Rasambainarivo F, 2017, ECOHEALTH, V14, P691, DOI 10.1007/s10393-017-1280-7
   Reed AW, 2000, AM MIDL NAT, V144, P133, DOI 10.1674/0003-0031(2000)144[0133:CSOTES]2.0.CO;2
   Ridout MS, 2009, J AGR BIOL ENVIR ST, V14, P322, DOI 10.1198/jabes.2009.08038
   Rocha DG, 2016, J ZOOL, V300, P205, DOI 10.1111/jzo.12372
   Rowcliffe, 2021, PACKAGE ACTIVITY ANI
   Russel, 2020, EMMEANS ESTIMATED MA
   Schlexer Fredrick V., 2008, P263
   Sebastian-Gonzalez E, 2020, ECOGRAPHY, V43, P1143, DOI 10.1111/ecog.05083
   Sprayberry TR, 2018, J MAMMAL, V99, P242, DOI 10.1093/jmammal/gyx168
   Sprayberry TR, 2016, SOUTHEAST NAT, V15, pN53, DOI 10.1656/058.015.0417
   Suarez-Tangil BD, 2017, EUR J WILDLIFE RES, V63, DOI 10.1007/s10344-017-1150-1
   Thorne ED, 2017, J WILDLIFE MANAGE, V81, P1042, DOI 10.1002/jwmg.21282
   Thorne ED, 2017, NORTHEAST NAT, V24, pN1, DOI 10.1656/045.024.0108
   Thornton DH, 2015, WILDLIFE RES, V42, P394, DOI 10.1071/WR15092
   Wilson SB, 2016, SOUTHEAST NAT, V15, P269, DOI 10.1656/058.015.0207
   Zielinski WJ, 1996, ECOL APPL, V6, P1254, DOI 10.2307/2269605
NR 47
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0909-6396
EI 1903-220X
J9 WILDLIFE BIOL
JI Wildlife Biol.
PY 2021
VL 2021
IS 4
DI 10.2981/wlb.00880
PG 9
WC Ecology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Zoology
GA XO1LX
UT WOS:000729955700003
OA gold
DA 2022-02-10
ER

PT C
AU Alsadik, B
AF Alsadik, Bashar
BE Halounova, L
   Schindler, K
   Limpouch, A
   Pajdla, T
   Safar, V
   Mayer, H
   Elberink, SO
   Mallet, C
   Rottensteiner, F
   Bredif, M
   Skaloud, J
   Stilla, U
TI A MODIFIED METHOD FOR IMAGE TRIANGULATION USING INCLINED ANGLES
SO XXIII ISPRS CONGRESS, COMMISSION III
SE International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences
LA English
DT Proceedings Paper
CT 23rd Congress of the
   International-Society-for-Photogrammetry-and-Remote-Sensing (ISPRS)
CY JUL 12-19, 2016
CL Prague, CZECH REPUBLIC
SP Int Soc Photogrammetry & Remote Sensing
DE image triangulation; inclined angle; spherical triangle; panorama; least
   squares
AB The ongoing technical improvements in photogrammetry, Geomatics, computer vision (CV), and robotics offer new possibilities for many applications requiring efficient acquisition of three-dimensional data. Image orientation is one of these important techniques in many applications like mapping, precise measurements, 3D modeling and navigation.
   Image orientation comprises three main techniques of resection, intersection (triangulation) and relative orientation, which are conventionally solved by collinearity equations or by using projection and fundamental matrices. However, different problems still exist in the state -of -the -art of image orientation because of the nonlinearity and the sensitivity to proper initialization and spatial distribution of the points. In this research, a modified method is presented to solve the triangulation problem using inclined angles derived from the measured image coordinates and based on spherical trigonometry rules and vector geometry. The developed procedure shows promising results compared to collinearity approach and to converge to the global minimum even when starting from far approximations. This is based on the strong geometric constraint offered by the inclined angles that are enclosed between the object points and the camera stations.
   Numerical evaluations with perspective and panoramic images are presented and compared with the conventional solution of collinearity equations. The results show the efficiency of the developed model and the convergence of the solution to global minimum even with improper starting values.
C1 [Alsadik, Bashar] CycloMedia Technol BV, Zaltbommel, Netherlands.
RP Alsadik, B (corresponding author), CycloMedia Technol BV, Zaltbommel, Netherlands.
EM BAlsadik@cyclomedia.com
RI Alsadik, Bashar/E-2052-2018
OI Alsadik, Bashar/0000-0002-9425-9325
CR Alsadik B., 2013, FILE EXCHANGE
   Gao XS, 2001, J COMPUT SCI TECHNOL, V16, P194, DOI 10.1007/BF02943199
   Ghilani C., 2006, ADJUSTMENT COMPUTATI, V4th
   Grafarend E, 1997, J GEODESY, V71, P217, DOI 10.1007/s001900050089
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   HORAUD R, 1989, COMPUT VISION GRAPH, V47, P33, DOI 10.1016/0734-189X(89)90052-2
   Lourakis M., 2004, DESIGN IMPLEMENTATIO
   Luhman T, 2014, CLOSE RANGE PHOTOGRA
   Marzan G.T., 1975, P S CLOSE RANGGE PHO, P420
   McGlone J.C., 2004, MANUAL PHOTOGRAMMETR, V5th
   Murray D.A., 1908, SPHERICAL TRIGONOMET
   Quan L, 1999, IEEE T PATTERN ANAL, V21, P774, DOI 10.1109/34.784291
   Shepherd F.A., 1982, ADV ENG SURVEYING PR
   Wolf P., 2000, ELEMENTS PHOTOGRAMME
NR 14
TC 1
Z9 1
U1 0
U2 1
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PY 2016
VL 41
IS B3
BP 453
EP 458
DI 10.5194/isprsarchives-XLI-B3-453-2016
PG 6
WC Computer Science, Interdisciplinary Applications; Geography, Physical;
   Remote Sensing; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA BG8WM
UT WOS:000392743800068
OA gold
DA 2022-02-10
ER

PT J
AU Sadgrove, EJ
   Falzon, G
   Miron, D
   Lamb, DW
AF Sadgrove, Edmund J.
   Falzon, Greg
   Miron, David
   Lamb, David W.
TI The Segmented Colour Feature Extreme Learning Machine: Applications in
   Agricultural Robotics
SO AGRONOMY-BASEL
LA English
DT Article
DE agricultural robotics; computer vision; drone; stationary camera trap;
   ensemble; extreme learning machine; feature mapping; object
   classification
ID OBJECT DETECTION; NEURAL-NETWORK; K-MEANS; LEAVES
AB This study presents the Segmented Colour Feature Extreme Learning Machine (SCF-ELM). The SCF-ELM is inspired by the Extreme Learning Machine (ELM) which is known for its rapid training and inference times. The ELM is therefore an ideal candidate for an ensemble learning algorithm. The Colour Feature Extreme Learning Machine (CF-ELM) is used in this study due to its additional ability to extract colour image features. The SCF-ELM is an ensemble learner that utilizes feature mapping via k-means clustering, a decision matrix and majority voting. It has been evaluated on a range of challenging agricultural object classification scenarios including weed, livestock and machinery detection. SCF-ELM model performance results were excellent both in terms of detection, 90 to 99% accuracy, and also inference times, around 0.01(s) per image. The SCF-ELM was able to compete or improve upon established algorithms in its class, indicating its potential for remote computing applications in agriculture.
C1 [Sadgrove, Edmund J.; Falzon, Greg] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
   [Falzon, Greg] Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA 5042, Australia.
   [Miron, David] Univ New England, Strateg Res Initiat, Armidale, NSW 2351, Australia.
   [Lamb, David W.] Univ New England, Precis Agr Res Grp, Armidale, NSW 2351, Australia.
   [Lamb, David W.] Food Agil Cooperat Res Ctr Ltd, 81 Broadway, Ultimo, NSW 2007, Australia.
RP Sadgrove, EJ (corresponding author), Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
EM esadgro2@une.edu.au; greg.falzon@flinders.edu.au; dmiron@une.edu.au;
   dave.lamb@foodagility.com
OI Falzon, Gregory/0000-0002-1989-9357; Lamb, David/0000-0002-2917-2231
FU Australian GovernmentAustralian GovernmentCGIAR; Australian Government
   Research Training Program (RTP) stipend;  [AEC12-042]
FX This research was supported in part by a Cooperative Research Centres
   Project (CRC-P) Grant from the Australian Government. E. Sadgrove was
   supported by an Australian Government Research Training Program (RTP)
   stipend.
CR Al-Tairi ZH, 2014, J INF PROCESS SYST, V10, P283, DOI 10.3745/JIPS.02.0002
   Barata JCA, 2012, BRAZ J PHYS, V42, P146, DOI 10.1007/s13538-011-0052-z
   [Anonymous], 2015, STUD ENC PAR DIG TEL
   Aslan O., 2016, ARXIV160305691
   Bishop JC, 2019, COMPUT ELECTRON AGR, V162, P531, DOI 10.1016/j.compag.2019.04.020
   Cambria E, 2013, IEEE INTELL SYST, V28, P30, DOI 10.1109/MIS.2013.140
   Chand AA, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11030530
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Daga AP, 2020, ALGORITHMS, V13, DOI 10.3390/a13020033
   Ertugrul O.F., 2014, AM J COMPUT SCI ENG, V1, P43
   Glorot X., 2010, PROC 13 INT C ARTIFI, V9, P249, DOI DOI 10.1038/S41593-021-00857-X
   Gomes GSD, 2011, NEURAL COMPUT APPL, V20, P417, DOI 10.1007/s00521-010-0407-3
   Grace BS, 2002, RANGELAND J, V24, P313, DOI 10.1071/RJ02018
   Gonzalez-Gonzalez MG, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11051002
   Harase S, 2019, MONTE CARLO METHODS, V25, P61, DOI 10.1515/mcma-2019-2029
   Hashemi A, 2022, INT J MACH LEARN CYB, V13, P49, DOI 10.1007/s13042-021-01347-z
   Hsu D., 2020, ARXIVMATHST200910670
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Jiantao Xu, 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1490
   Juntao Wang, 2011, 2011 IEEE 3rd International Conference on Communication Software and Networks (ICCSN 2011), P44, DOI 10.1109/ICCSN.2011.6014384
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kim Y, 2005, MANAGE SCI, V51, P264, DOI 10.1287/mnsc.1040.0296
   Liberti L, 2014, SIAM REV, V56, P3, DOI 10.1137/120875909
   Liu N, 2010, IEEE SIGNAL PROC LET, V17, P754, DOI 10.1109/LSP.2010.2053356
   Malinen MI, 2014, LECT NOTES COMPUT SC, V8621, P32, DOI 10.1007/978-3-662-44415-3_4
   Netlib.org, LAPACKE C INT LAPACK
   Palumbo M, 2021, AGRONOMY-BASEL, V11, DOI 10.3390/agronomy11071353
   Rahman M., 2019, PROCEEDINGS, V36, P154, DOI [10.3390/proceedings2019036154, DOI 10.3390/PROCEEDINGS2019036154]
   Rod Z.P., 2000, P IEEE C COMP VIS PA
   Sadgrove EJ, 2018, COMPUT IND, V98, P183, DOI 10.1016/j.compind.2018.03.014
   Sadgrove EJ, 2017, COMPUT ELECTRON AGR, V139, P204, DOI 10.1016/j.compag.2017.05.017
   Sheela KG, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/425740
   van Schaik, 2015, P ELM 2014, V1, P41, DOI DOI 10.1007/978-3-319-14063-6_
   Wang X, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010051
   Wang YY, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10051897
   Wood A, 2013, ANZ J SURG, V83, P206, DOI 10.1111/ans.12106
   Zhang L., 2015, THEORY ALGORITHMS AP
   Zhang MH, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13214342
   Zhou G., 2005, C GEOSC REM SENS S P, V3
NR 40
TC 0
Z9 0
U1 1
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2073-4395
J9 AGRONOMY-BASEL
JI Agronomy-Basel
PD NOV
PY 2021
VL 11
IS 11
AR 2290
DI 10.3390/agronomy11112290
PG 16
WC Agronomy; Plant Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Plant Sciences
GA XI0HM
UT WOS:000725804300001
DA 2022-02-10
ER

PT C
AU Wang, KJ
   Zheng, CY
   Mao, ZH
AF Wang, Ker-Jiun
   Zheng, Caroline Yan
   Mao, Zhi-Hong
GP IEEE
TI Human-Centered, Ergonomic Wearable Device with Computer Vision Augmented
   Intelligence for VR Multimodal Human-Smart Home Object Interaction
SO HRI '19: 2019 14TH ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT
   INTERACTION
SE ACM IEEE International Conference on Human-Robot Interaction
LA English
DT Proceedings Paper
CT 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
CY MAR 11-14, 2019
CL Daegu, SOUTH KOREA
SP Assoc Comp Machinery, IEEE, IEEE Robot & Automat Soc, ACM SIGCHI, ACM SIGAI, AAAI, Korea Tourism Org, Daegu Convent & Visitors Bur, ColorfulDaegu
DE Wearable Device; Smart Home; Telepresence Robot; Virtual Reality;
   Human-Computer Interaction; Human-Centered Design
AB In the future, Human-Robot Interaction should be enabled by a compact, human-centered and ergonomic wearable device that can merge human and machine altogether seamlessly by constantly identifying each other's intentions. In this paper, we will showcase the use of an ergonomic and lightweight wearable device that can identify human's eye/facial gestures with physiological signal measurements. Since human's intentions are usually coupled with eye movements and facial expressions, through proper design of interactions using these gestures, we can let people interact with the robots or smart home objects naturally. Combined with Computer Vision object recognition algorithms, we can allow people use very simple and straightforward communication strategies to operate telepresence robot and control smart home objects remotely, totally "Hands-Free". People can wear a VR head-mounted display and see through the robot's eyes (the remote camera attached on the robot) and interact with the smart home devices intuitively by simple facial gestures or blink of the eyes. It is tremendous beneficial for the people with motor impairment as an assistive tool. For the normal people without disabilities, they can also free their hands to do other tasks and operate the smart home devices at the same time as multimodal control strategies.
C1 [Wang, Ker-Jiun] Univ Pittsburgh, Dept Bioengn, Pittsburgh, PA 15260 USA.
   [Zheng, Caroline Yan] Royal Coll Art, Informat Experience Design & Fash, London, England.
   [Mao, Zhi-Hong] Univ Pittsburgh, Elect & Comp Engn & Bioengn, Pittsburgh, PA USA.
RP Wang, KJ (corresponding author), Univ Pittsburgh, Dept Bioengn, Pittsburgh, PA 15260 USA.
EM kew88@pitt.edu; yan.zheng@network.rca.ac.uk; zhm4@pitt.edu
CR Wang KJ, 2017, INT C REHAB ROBOT, P1073, DOI 10.1109/ICORR.2017.8009392
NR 1
TC 6
Z9 6
U1 0
U2 7
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2167-2121
EI 2167-2148
BN 978-1-5386-8555-6
J9 ACMIEEE INT CONF HUM
PY 2019
BP 767
EP 768
PG 2
WC Engineering, Electrical & Electronic; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Robotics
GA BM6QN
UT WOS:000467295400188
DA 2022-02-10
ER

PT J
AU Pandey, C
   Baghel, N
   Dutta, MK
   Srivastava, A
   Choudhary, N
AF Pandey, Chandrasen
   Baghel, Neeraj
   Dutta, Malay Kishore
   Srivastava, Ashish
   Choudhary, Nandlal
TI Machine learning approach for automatic diagnosis of Chlorosis in Vigna
   mungo leaves
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Agricultural biotechnology; Disease classification; Image processing;
   Chlorosis
AB Viral infection in crops is something that may lead to a huge loss in crop yield as there are no known recovery procedures. Also, at the onset of yellowing in a leaf, no observable changes occur in leaf structure and geometry. Therefore, the manual inspection and diagnosis of such diseases by the framers in agricultural fields are difficult on a large scale. The automatic artificial intelligence-based tool can be used for early-stage diagnosis of viral growth, where the symptoms may be available in certain parts like leaves. An automatic computer vision-based method is proposed for the identification of yellow disease, also called Chlorosis, in a prominent leguminous crop like Vigna mungo. The proposed method involves fully automatic partitioning of plant leaves, followed by feature extraction in the spatial domain and disease prediction using a support vector machine (SVM) learned upon several training samples. The method is entirely automatic and non-destructive which can predict the classification of plant health category with an accuracy rate of 95.69% with low computation complexity. This accuracy and computational complexity can be used in real-time situations for a large scale of Vigna mungo plantation using drones and remote camera.
C1 [Pandey, Chandrasen; Baghel, Neeraj; Dutta, Malay Kishore] Ctr Adv Studies, Lucknow, Uttar Pradesh, India.
   [Srivastava, Ashish; Choudhary, Nandlal] Amity Univ, Amity Inst Virol & Immunol, Noida, Uttar Pradesh, India.
RP Dutta, MK (corresponding author), Ctr Adv Studies, Lucknow, Uttar Pradesh, India.
EM developer.chandrasen@gmail.com; nbaghel777@gmail.com;
   malaykishoredutta@gmail.com; assrivastava10@amity.edu;
   nandlalc@gmail.com
RI Choudhary, Nandlal/AAV-2206-2021
OI Choudhary, Nandlal/0000-0002-4914-1643; Dutta, Malay
   Kishore/0000-0003-2462-737X; Baghel, Neeraj/0000-0002-0081-6224
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Adeel A, 2020, EXPERT SYST, DOI 10.1111/exsy.12569
   Adeel A, 2019, SUSTAIN COMPUT-INFOR, V24, DOI 10.1016/j.suscom.2019.08.002
   Ait-Sahalia Y, 2019, J AM STAT ASSOC, V114, P287, DOI 10.1080/01621459.2017.1401542
   Arivazhagan S., 2013, Agricultural Engineering International: CIGR Journal, V15, P211
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dhaygude Sanjay B., 2013, INT J ADV RES ELECT, V2, P599
   Dunham, 2006, DATA MINING INTRO AD
   Feng GC, 2003, PATTERN RECOGN, V36, P977, DOI 10.1016/S0031-3203(02)00114-0
   Galloway M.M., 1974, COMPUTER GRAPHICS IM, V75, P18555, DOI DOI 10.1016/S0146-664X(75)80008-6
   Ghaiwat Savita N., 2014, INT J RECENT ADV ENG, V2, P1
   Gohl B., 1982, ALIMENTS BETAIL SOUS
   Hussain M, 2004, PLANT PATHOL, V53, P518, DOI 10.1111/j.1365-3059.2004.01037.x
   India Brand Equity Foundation, 2017, AN IND AGR IND MARK
   Iqbal Z, 2018, COMPUT ELECTRON AGR, V153, P12, DOI 10.1016/j.compag.2018.07.032
   Jiang P, 2019, IEEE ACCESS, V7, P59069, DOI 10.1109/ACCESS.2019.2914929
   Kagan A, 1998, STAT PROBABIL LETT, V38, P329, DOI 10.1016/S0167-7152(98)00041-8
   Keerthi SS, 2001, NEURAL COMPUT, V13, P637, DOI 10.1162/089976601300014493
   Khan MA, 2020, MULTIMED TOOLS APPL, V79, P25763, DOI 10.1007/s11042-020-09244-3
   Khan MA, 2018, COMPUT ELECTRON AGR, V155, P220, DOI 10.1016/j.compag.2018.10.013
   Kumar S., 2018, SUSTAIN COMPUT INFOR, DOI [10.1016/j.suscom.2018.10.004, DOI 10.1016/J.SUSCOM.2018.10.004]
   Kumar S, 2017, BIOCATAL AGRIC BIOTE, V11, P183, DOI 10.1016/j.bcab.2017.07.004
   Lu, 2015, 2015 IEEE INT C AC S
   Lu X, 2019, IEEE T CIRC SYST VID
   Meunkaewjinda A, 2008, ECTI-CON 2008: PROCEEDINGS OF THE 2008 5TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING/ELECTRONICS, COMPUTER, TELECOMMUNICATIONS AND INFORMATION TECHNOLOGY, VOLS 1 AND 2, P513, DOI 10.1109/ECTICON.2008.4600483
   Naimuddin K, 2011, PHYTOPATHOL MEDITERR, V50, P94
   Nanni L, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083554
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Patil S. B., 2011, INT J ENG TECHNOL, V3, P297, DOI DOI 10.7763/IJET.2011.V3.241
   Powers D., 2008, EVALUATION PRECISION
   Qazi J, 2006, PLANT PATHOL, V55, P818, DOI 10.1111/j.1365-3059.2006.01475.x
   Reddy B. V. Bhaskara, 2015, Archives of Phytopathology and Plant Protection, V48, P345, DOI 10.1080/03235408.2014.888874
   RK, 2012, INT J MODERN ENG RES, V2, P3661, DOI [10.1177/0958305X16685471., DOI 10.1177/0958305X16685471]
   Sabah Bashir, 2012, IOSR J ELECT COMMUNI, V2, P31, DOI DOI 10.9790/2834-0263134
   Saran S, 2000, INDIAN J ANIM SCI, V70, P526
   Sengar N, 2018, COMPUTING, V100, P1189, DOI 10.1007/s00607-018-0638-1
   Shahid MS, 2012, AUSTRALAS PLANT DIS, V7, P85, DOI 10.1007/s13314-012-0055-9
   Sharif M, 2018, COMPUT ELECTRON AGR, V150, P220, DOI 10.1016/j.compag.2018.04.023
   Singh Alpna, 2020, 2020 International Conference on Contemporary Computing and Applications (IC3A), P187, DOI 10.1109/IC3A48958.2020.233294
   Singh Vijai, 2017, Information Processing in Agriculture, V4, P41, DOI 10.1016/j.inpa.2016.10.005
   Tan JH, 2010, INFRARED PHYS TECHN, V53, P120, DOI 10.1016/j.infrared.2009.10.006
   Zhang CL, 2017, INT J AGR BIOL ENG, V10, P74, DOI 10.3965/j.ijabe.20171002.2166
NR 42
TC 3
Z9 3
U1 1
U2 3
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD APR
PY 2021
VL 80
IS 9
BP 13407
EP 13427
DI 10.1007/s11042-020-10309-6
EA JAN 2021
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA RR7PU
UT WOS:000607776300005
DA 2022-02-10
ER

PT J
AU Francis, RJ
   Lyons, MB
   Kingsford, RT
   Brandis, KJ
AF Francis, Roxane J.
   Lyons, Mitchell B.
   Kingsford, Richard T.
   Brandis, Kate J.
TI Counting Mixed Breeding Aggregations of Animal Species Using Drones:
   Lessons from Waterbirds on Semi-Automation
SO REMOTE SENSING
LA English
DT Article
DE UAV; machine learning; colony; open source; GIS; avian; remote sensing;
   heronry
ID COLONIALLY-NESTING WATERBIRDS; MACQUARIE MARSHES; AERIAL SURVEYS; CAMERA
   TRAPS; MANAGEMENT; RIVER; POPULATIONS; ECOLOGY; FLORIDA; IMPACT
AB Using drones to count wildlife saves time and resources and allows access to difficult or dangerous areas. We collected drone imagery of breeding waterbirds at colonies in the Okavango Delta (Botswana) and Lowbidgee floodplain (Australia). We developed a semi-automated counting method, using machine learning, and compared effectiveness of freeware and payware in identifying and counting waterbird species (targets) in the Okavango Delta. We tested transferability to the Australian breeding colony. Our detection accuracy (targets), between the training and test data, was 91% for the Okavango Delta colony and 98% for the Lowbidgee floodplain colony. These estimates were within 1-5%, whether using freeware or payware for the different colonies. Our semi-automated method was 26% quicker, including development, and 500% quicker without development, than manual counting. Drone data of waterbird colonies can be collected quickly, allowing later counting with minimal disturbance. Our semi-automated methods efficiently provided accurate estimates of nesting species of waterbirds, even with complex backgrounds. This could be used to track breeding waterbird populations around the world, indicators of river and wetland health, with general applicability for monitoring other taxa.
C1 [Francis, Roxane J.; Lyons, Mitchell B.; Kingsford, Richard T.; Brandis, Kate J.] Univ New South Wales, Ctr Ecosyst Sci, Sydney, NSW 2052, Australia.
RP Francis, RJ (corresponding author), Univ New South Wales, Ctr Ecosyst Sci, Sydney, NSW 2052, Australia.
EM roxane.francis@unsw.edu.au; Mitchell.Lyons@unsw.edu.au;
   Richard.Kingsford@unsw.edu.au; Kate.Brandis@unsw.edu.au
OI Kingsford, Richard/0000-0001-6565-4134; Francis,
   Roxane/0000-0003-3172-5445; Brandis, Kate/0000-0001-6807-0142; Lyons,
   Mitchell/0000-0003-3960-3522
FU Elephants without Borders; Taronga Conservation Society; Australian
   Commonwealth Environmental Water Office; NSW Department of Primary
   Industries; University of New South Wales Sydney
FX This research received financial support from Elephants without Borders,
   Taronga Conservation Society, the Australian Commonwealth Environmental
   Water Office, the NSW Department of Primary Industries, and the
   University of New South Wales Sydney.
CR Arendt MD, 2012, MAR BIOL, V159, P101, DOI 10.1007/s00227-011-1793-5
   Arthur AD, 2012, WETLANDS, V32, P257, DOI 10.1007/s13157-011-0235-y
   Bennitt E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38610-x
   Bino G, 2014, ECOL APPL, V24, P142, DOI 10.1890/13-0202.1
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004
   Brandis KJ, 2011, ENVIRON MANAGE, V48, P489, DOI 10.1007/s00267-011-9705-5
   Brandis Kate J., 2014, Australian Field Ornithology, V31, P99
   Bregnballe T, 2014, ORNIS FENNICA, V91, P231
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brody S., UNMANNED INVESTIGATI
   Callaghan CT, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01825
   Carney KM, 1999, WATERBIRDS, V22, P68, DOI 10.2307/1521995
   Chabot D, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01205-130115
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Chambers LE, 2005, EMU, V105, P1, DOI 10.1071/MU04033
   Crutsinger GM, 2016, J UNMANNED VEH SYST, V4, P161, DOI 10.1139/juvs-2016-0008
   Descamps S, 2011, BIRD STUDY, V58, P302, DOI 10.1080/00063657.2011.588195
   Ezat MA, 2018, BIOL CONSERV, V223, P76, DOI 10.1016/j.biocon.2018.04.032
   Ferrari MA, 2013, MAR MAMMAL SCI, V29, P407, DOI 10.1111/j.1748-7692.2012.00585.x
   Frederick P, 2003, MONITORING ECOSYSTEMS, P321
   Green MC, 2008, J WILDLIFE MANAGE, V72, P697, DOI 10.2193/2006-391
   Groom G., P REM SENS PHOT SOC, DOI [10.1111/j.1477-9730.2007.00455.x, DOI 10.1111/J.1477-9730.2007.00455.X]
   Groom G, 2013, ECOL INFORM, V14, P2, DOI 10.1016/j.ecoinf.2012.12.001
   Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
   Hodgson JC, 2016, SCI REP-UK, V6, DOI 10.1038/srep22574
   Inman VL, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219652
   Kingsford RT, 2009, WILDLIFE RES, V36, P29, DOI 10.1071/WR08034
   Kingsford RT, 1999, FRESHWATER BIOL, V41, P425, DOI 10.1046/j.1365-2427.1999.00440.x
   Kingsford RT, 2005, RIVER RES APPL, V21, P187, DOI 10.1002/rra.840
   Kingsford RT, 1998, COLON WATERBIRD, V21, P159, DOI 10.2307/1521903
   Knight J, 2011, HUM ANIM STUD, V10, P1, DOI 10.1163/ej.9789004187931.i-629
   Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
   Leslie DJ, 2001, REGUL RIVER, V17, P21, DOI 10.1002/1099-1646(200101/02)17:1&lt;21::AID-RRR589&gt;3.0.CO;2-V
   Liu CC, 2015, ECOL INFORM, V30, P170, DOI 10.1016/j.ecoinf.2015.10.008
   Loots S., EVALUATION RADAR CAM
   Lopez R., 2006, EC DEV ENV SUSTAINAB
   Lowndes JSS, 2017, NAT ECOL EVOL, V1, DOI 10.1038/s41559-017-0160
   Lyons Mitchell, 2018, Australian Field Ornithology, V35, P51
   Lyons MB, 2019, METHODS ECOL EVOL, V10, P1024, DOI 10.1111/2041-210X.13194
   McEvoy JF, 2016, PEERJ, V4, DOI 10.7717/peerj.1831
   McNeill S, 2011, INT GEOSCI REMOTE SE, P4312, DOI 10.1109/IGARSS.2011.6050185
   Menkhorst P., AUSTR BIRD GUIDE CSI
   Mooii Tech, 2019, PHOT X
   Narayanan S. Prasanth, 2007, Podoces, V2, P22
   Niemczynwicz A, 2015, WATERBIRDS, V38, P282, DOI 10.1675/063.038.0308
   Ogden JC, 2014, ECOL INDIC, V44, P148, DOI 10.1016/j.ecolind.2014.03.007
   Pirotta V, 2017, FRONT MAR SCI, V4, DOI 10.3389/fmars.2017.00425
   Pix4d SA, 2019, PIX4DCAPTURE
   Pomeroy PP, 2000, J ZOOL, V250, P1
   R Core Team, R LANG ENV STAT COMP
   Rees AF, 2018, ENDANGER SPECIES RES, V35, P81, DOI 10.3354/esr00877
   Rodgers JA, 2005, WATERBIRDS, V28, P230, DOI 10.1675/1524-4695(2005)028[0230:AOASOW]2.0.CO;2
   Schofield G, 2017, FUNCT ECOL, V31, P2310, DOI 10.1111/1365-2435.12930
   Sen Gupta A, 2017, J GEOPHYS RES-SPACE, V122, P12353, DOI 10.1002/2017JA023949
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Teucher A., USING PRINCIPLES OPE
   Wakefield ED, 2017, ECOL APPL, V27, P2074, DOI 10.1002/eap.1591
   Wandler A, 2016, J CUTAN PATHOL, V43, P956, DOI 10.1111/cup.12778
   Wright MN, 2017, J STAT SOFTW, V77, P1, DOI 10.18637/jss.v077.i01
   Znidersic E, 2017, WATERBIRDS, V40, P417, DOI 10.1675/063.040.0414
NR 60
TC 13
Z9 13
U1 4
U2 6
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD APR
PY 2020
VL 12
IS 7
AR 1185
DI 10.3390/rs12071185
PG 17
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging
   Science & Photographic Technology
GA LU4EF
UT WOS:000537709600135
OA gold
DA 2022-02-10
ER

PT J
AU Conlin, MP
   Adams, PN
   Wilkinson, B
   Dusek, G
   Palmsten, ML
   Brown, JA
AF Conlin, Matthew P.
   Adams, Peter N.
   Wilkinson, Benjamin
   Dusek, Gregory
   Palmsten, Margaret L.
   Brown, Jenna A.
TI SurfRCaT: A tool for remote calibration of pre-existing coastal cameras
   to enable their use as quantitative coastal monitoring tools
SO SOFTWAREX
LA English
DT Article
DE Coastal monitoring; Surf-cameras; Camera calibration; Photogrammetry;
   Lidar
ID SHORELINE; VARIABILITY; SYSTEM
AB The Surf-camera Remote Calibration Tool (SurfRCaT) is a Python-based software application to calibrate and rectify images from pre-existing video cameras that are operating at coastal sites in the United States. The software enables remote camera calibration and subsequent image rectification by facilitating the remote-extraction of ground control points using airborne lidar observations, and guides the user through the entire process. No programming or code interaction are necessary to use the software. Calibration parameters and subsequent rectified image products derived from the software are saved locally. Users can apply SurfRCaT to any camera imagery that has stationary structures within the camera's field of view. Given current recreational camera infrastructure, SurfRCaT could increase the number of potential quantitative coastal video monitoring stations in the United States by an order of magnitude. (C) 2020 The Authors. Published by Elsevier B.V.
C1 [Conlin, Matthew P.; Adams, Peter N.] Univ Florida, Dept Geol Sci, Gainesville, FL 32611 USA.
   [Wilkinson, Benjamin] Univ Florida, Dept Geomat, Gainesville, FL 32611 USA.
   [Dusek, Gregory] NOAA, Ctr Operat Oceanog Prod & Serv, Natl Ocean Serv, Silver Spring, MD 20910 USA.
   [Palmsten, Margaret L.; Brown, Jenna A.] US Geol Survey, St Petersburg Coastal & Marine Sci Ctr, St Petersburg, FL 33701 USA.
RP Conlin, MP (corresponding author), Univ Florida, 241 Williamson Hall, Gainesville, FL 32611 USA.
EM conlinm@ufl.edu
OI Brown, Jenna/0000-0003-3137-7073; Conlin, Matthew/0000-0002-6495-436X
FU Southeast Coastal Ocean Observing Regional Association (SECOORA)
FX Funding was provided by the Southeast Coastal Ocean Observing Regional
   Association (SECOORA) through their annual student data challenge. The
   authors wish to thank Joe Long, Dave Foster, and Richard Conlin for
   valuable software testing and Justin Birchler and Kyle Kelso for
   conducting the validation camera calibration survey. The authors also
   wish to thank two anonymous reviewers whose comments greatly improved
   the quality of the software and manuscript. Any use of trade, firm, or
   product names is for discriptive purposes only and does not imply
   endorsement by the U.S. Government.
CR Adams PN, 2016, EARTH SURF PROC LAND, V41, P936, DOI 10.1002/esp.3877
   Andriolo U, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11010078
   Bracs MA, 2016, J COASTAL RES, V32, P542, DOI 10.2112/JCOASTRES-D-14-00090.1
   Brignone M, 2012, COMPUT GEOSCI-UK, V49, P53, DOI 10.1016/j.cageo.2012.06.008
   Conlin MP, 2020, ZENODO, V1, DOI [10.5281/zenodo.3946697, DOI 10.5281/ZENODO.3946697]
   Dusek G, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00353
   Here technologies, 2018, PPTK 0 1 1 DOC
   Hermann M., 2019, FMAN BUILD SYSTEM
   Holland KT, 1997, IEEE J OCEANIC ENG, V22, P81, DOI 10.1109/48.557542
   Holland KT, 1999, J GEOPHYS RES-OCEANS, V104, P13479, DOI 10.1029/1999JC900075
   Holman RA, 2007, COAST ENG, V54, P477, DOI 10.1016/j.coastaleng.2007.01.003
   Holman RA, 2006, J GEOPHYS RES-OCEANS, V111, DOI 10.1029/2005JC002965
   Lazarus ED, 2019, J GEOPHYS RES-EARTH, V124, P696, DOI 10.1029/2018JF004957
   LIPPMANN TC, 1989, J GEOPHYS RES-OCEANS, V94, P995, DOI 10.1029/JC094iC01p00995
   Mole MA, 2013, J COASTAL RES, P1433, DOI 10.2112/SI65-242.1
   Neumann B, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118571
   Nieto MA, 2010, EARTH SURF PROC LAND, V35, P1712, DOI 10.1002/esp.2025
   NOAA office for coastal management, 2020, LID DAT NOAA DIG COA
   Pianca C, 2015, J GEOPHYS RES-OCEANS, V120, P2159, DOI 10.1002/2014JC010329
   Sallenger AH, 2006, ESTUAR COAST, V29, P880, DOI 10.1007/BF02798647
   Sanchez-Garcia E, 2017, ISPRS J PHOTOGRAMM, V128, P255, DOI 10.1016/j.isprsjprs.2017.03.023
   Simarro G, 2017, J COASTAL RES, V33, P1217, DOI 10.2112/JCOASTRES-D-16-00022.1
   Taborda R, 2012, COMPUT GEOSCI-UK, V49, P248, DOI 10.1016/j.cageo.2012.07.013
   United states geological survey National oceanic and atmospheric administration and national weather service, 2020, TOT WAT LEV COAST CH
   Valentini N, 2020, J COASTAL RES, P1333, DOI 10.2112/SI95-256.1
   Vos K, 2019, COAST ENG, V150, P160, DOI 10.1016/j.coastaleng.2019.04.004
   Wolf P.R., 2014, ELEMENTS PHOTOGRAMME, V4th
   WRIGHT LD, 1984, MAR GEOL, V56, P93, DOI 10.1016/0025-3227(84)90008-2
NR 28
TC 1
Z9 1
U1 1
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 2352-7110
J9 SOFTWAREX
JI SoftwareX
PD JUL-DEC
PY 2020
VL 12
AR 100584
DI 10.1016/j.softx.2020.100584
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PH8TA
UT WOS:000600676600046
OA gold
DA 2022-02-10
ER

PT J
AU Lopez-Marcano, S
   Brown, CJ
   Sievers, M
   Connolly, RM
AF Lopez-Marcano, Sebastian
   Brown, Christopher J.
   Sievers, Michael
   Connolly, Rod M.
TI The slow rise of technology: Computer vision techniques in fish
   population connectivity
SO AQUATIC CONSERVATION-MARINE AND FRESHWATER ECOSYSTEMS
LA English
DT Article
DE artificial intelligence; behavioural ecology; deep learning; dispersal;
   environmental monitoring; machine learning; new techniques; operational
   maturity analysis; research trends; underwater video
ID MARINE BIODIVERSITY; MANAGEMENT
AB Technological advancements in data collection and analysis are producing a new generation of ecological data. Among these, computer vision (CV) has received increased attention for its robust capabilities for rapidly processing large volumes of digital imagery. In marine ecosystems, the study of fish connectivity provides fundamental information for assessing fisheries stocks, designing and implementing protected areas and understanding the impact of habitat loss. While the field of fish connectivity has benefited from technological advancements, the extent to which novel techniques, such as CV, have been utilized has not been assessed. To inform future directions and developments, this study reviewed the current use of CV in fish connectivity research, quantified how the implementation of such technology in fish connectivity research compared with other areas of marine research and described how this field could benefit from CV. The review found that the use of remote camera systems in fish connectivity research is increasing, but the implementation of automated analysis of digital imagery has been slow. Successful implementation and expansion of CV frameworks in aquaculture and coral reef ecology suggest that CV techniques could greatly benefit fish connectivity research. A case study of potential use of CV in fish connectivity research, scaling up optimal foraging models to predict marine population connectivity, highlights how beneficial it could be. The capacity for CV techniques to be adopted alongside traditional approaches, the unparalleled speed, accuracy and reliability of these approaches and the benefits of being able to study ecosystems along multiple spatial-temporal scales, all make CV a valuable tool for assessing connectivity. Ultimately, these technologies can assist data-driven decisions that directly influence the health and productivity of marine ecosystems.
C1 [Lopez-Marcano, Sebastian; Sievers, Michael; Connolly, Rod M.] Griffith Univ, Australian Rivers Inst Coast & Estuaries, Sch Environm & Sci, Gold Coast, Qld 4222, Australia.
   [Brown, Christopher J.] Griffith Univ, Australian Rivers Inst Coast & Estuaries, Sch Environm & Sci, Nathan, Qld, Australia.
RP Lopez-Marcano, S (corresponding author), Griffith Univ, Australian Rivers Inst Coast & Estuaries, Sch Environm & Sci, Gold Coast, Qld 4222, Australia.
EM sebastian.lopez-marcano@griffithuni.edu.au
RI Sievers, Michael/ABA-5513-2020; Brown, Christopher J/G-4287-2011;
   Connolly, Rod/C-4094-2008
OI Sievers, Michael/0000-0001-7162-1830; Brown, Christopher
   J/0000-0002-7271-4091; Connolly, Rod/0000-0001-6223-1291; Lopez-Marcano,
   Sebastian/0000-0002-0814-2906
CR Adams CIM, 2019, GENES-BASEL, V10, DOI 10.3390/genes10030192
   Aguzzi J, 2019, ENVIRON SCI TECHNOL, V53, P6616, DOI 10.1021/acs.est.9b00409
   Allan BM, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2163
   Althaus F, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141039
   Becker J, 2009, BUS INFORM SYST ENG+, V1, P213, DOI 10.1007/s12599-009-0044-5
   Beijbom O, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130312
   Bicknell AWJ, 2016, FRONT ECOL ENVIRON, V14, P424, DOI 10.1002/fee.1322
   Bryan-Brown DN, 2017, MAR ECOL PROG SER, V585, P243, DOI 10.3354/meps12418
   CAL A, 2013, ADV OCEANOGRAPHY LIM, V4, P150
   Costa C, 2006, AQUACULT ENG, V35, P218, DOI 10.1016/j.aquaeng.2006.02.003
   Cowen RK, 2007, OCEANOGRAPHY, V20, P14, DOI 10.5670/oceanog.2007.26
   Craig JK, 2000, CONCEPTS AND CONTROVERSIES IN TIDAL MARSH ECOLOGY, P241
   Cumming GS, 2011, LANDSCAPE ECOL, V26, P899, DOI 10.1007/s10980-011-9623-1
   Ditria EM, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.00429
   Downie RA, 2013, MAR ECOL PROG SER, V482, P217, DOI 10.3354/meps10250
   Fogarty MJ, 2007, OCEANOGRAPHY, V20, P112, DOI 10.5670/oceanog.2007.34
   Garcia-Mireles G. A., 2012, 16 INT C EV ASS SOFT
   Gonzalez-Rivero M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030489
   Langenkamper D, 2017, FRONT MAR SCI, V4, DOI 10.3389/fmars.2017.00083
   Lee J.-J., 2014, ADV COMPUTER SCI ITS, V279, P77
   Mallet D, 2014, FISH RES, V154, P44, DOI 10.1016/j.fishres.2014.01.019
   Marini S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32089-8
   Nabe-Nielsen J, 2013, OIKOS, V122, P1307, DOI 10.1111/j.1600-0706.2013.00069.x
   Olds AD., 2018, SEASCAPE ECOLOGY, P261
   Pages JF, 2014, FUNCT ECOL, V28, P1440, DOI 10.1111/1365-2435.12286
   Peterson EE, 2020, ENVIRON MODELL SOFTW, V124, DOI 10.1016/j.envsoft.2019.104557
   Pinkiewicz TH, 2011, AQUACULT ENG, V45, P20, DOI 10.1016/j.aquaeng.2011.05.002
   Pittman S.J., 2018, SEASCAPE ECOLOGY
   Saberioon M, 2017, REV AQUACULT, V9, P369, DOI 10.1111/raq.12143
   dos Santos-Neto JBS, 2019, ENTERP INF SYST-UK, V13, P719, DOI 10.1080/17517575.2019.1575986
   Snaddon J, 2013, BIOL LETTERS, V9, DOI 10.1098/rsbl.2012.1029
   Stien LH, 2007, AQUACULT ENG, V37, P115, DOI 10.1016/j.aquaeng.2007.03.002
   Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
   Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Williams ID, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00222
   Yamahara KM, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00373
NR 38
TC 4
Z9 4
U1 4
U2 17
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1052-7613
EI 1099-0755
J9 AQUAT CONSERV
JI Aquat. Conserv.-Mar. Freshw. Ecosyst.
PD JAN
PY 2021
VL 31
IS 1
BP 210
EP 217
DI 10.1002/aqc.3432
EA OCT 2020
PG 8
WC Environmental Sciences; Marine & Freshwater Biology; Water Resources
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Marine & Freshwater Biology; Water
   Resources
GA PR7XM
UT WOS:000574839300001
DA 2022-02-10
ER

PT J
AU Jakovcevic, T
   Bugaric, M
   Stipanicev, D
AF Jakovcevic, Toni
   Bugaric, Marin
   Stipanicev, Darko
TI A STEREO APPROACH TO WILDFIRE SMOKE DETECTION: THE IMPROVEMENT OF THE
   EXISTING METHODS BY ADDING A NEW DIMENSION
SO COMPUTING AND INFORMATICS
LA English
DT Article
DE Smoke detection; artificial intelligence; stereo vision; false alarms;
   wild-fires
AB In this paper, we present a novel approach to visual smoke detection based on stereo vision. General smoke detection is usually performed by analyzing the images from remote cameras using various computer vision techniques. The literature on smoke detection shows a variety of approaches, and the focus of this paper is the improvement of the general smoke detection process by introducing stereo vision. Two cameras are used to estimate the distance and size of the detected phenomena based on stereo triangulation. Using this information, the minimum size and overall dynamics of the detected regions are further examined to ensure the elimination of false alarms induced by various phenomena (such as the movement of objects located at short distances from the camera). Such false alarms could easily be detected by the proposed stereo system, allowing the increase of the sensitivity and overall performance of the detection. We analyzed the requirements of such system in terms of precision and robustness to possible error sources, especially when dealing with detection of smoke at various distances from the camera. For evaluation, three existing smoke detection methods were tested and the results were compared to their newly implemented stereo versions. The results demonstrated better overall performance, especially a decrease in false alarm rates for all tested methods.
C1 [Jakovcevic, Toni; Bugaric, Marin; Stipanicev, Darko] Univ Split, Fac Elect Engn Mech Engn & Naval Architecture, R Boskovica 32, Split 21000, Croatia.
RP Jakovcevic, T (corresponding author), Univ Split, Fac Elect Engn Mech Engn & Naval Architecture, R Boskovica 32, Split 21000, Croatia.
EM toni.jakovcevic@fesb.hr; marin.bugaric@fesb.hr; dstip@fesb.hr
RI Bugaric, Marin/D-8787-2017
OI Bugaric, Marin/0000-0003-4391-6804
CR Alejandro OB, 2011, ELECT ROBOT AUTO MEC, P126, DOI 10.1109/CERMA.2011.27
   Anton Malenichev, 2013, Pattern Recognition and Machine Intelligence. 5th International Conference, PReMI 2013. Proceedings: LNCS 8251, P445, DOI 10.1007/978-3-642-45062-4_61
   Bugaric M, 2015, ADV ELECTR COMPUT EN, V15, P55, DOI 10.4316/AECE.2015.01008
   Bugaric M, 2014, COMPUT VIS IMAGE UND, V118, P184, DOI 10.1016/j.cviu.2013.10.003
   Busch A., 2002, Proceedings of the Sixth Digital Image Computing Techniques and Applications. Dicta 2002, P341
   Cappellini V., 1989, Third International Conference on Image Processing and its Applications (Conf. Publ. No.307), P563
   Chao-Ching Ho, 2009, 2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM), P1845, DOI 10.1109/AIM.2009.5229791
   Chen J, 2013, INT J COMPUT SCI, V10, P298
   Cui Y, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 3, PROCEEDINGS, P95, DOI 10.1109/CISP.2008.397
   DongKeun Kim, 2009, 2009 WRI World Congress on Computer Science and Information Engineering, CSIE, P759, DOI 10.1109/CSIE.2009.494
   Dukuzumuremyi JP, 2014, INT J HYBRID INFORM, V7, P143, DOI DOI 10.14257/IJHIT.2014.7.3.15
   Fernandez-Berni J, 2008, WIT TRANS ECOL ENVIR, V119, P161, DOI 10.2495/FIVA080171
   Gallup D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587671
   Jakovcevic T., 2010, P 6 INT C FOR FIR RE, P1
   Jakovcevic T, 2013, MACH VISION APPL, V24, P707, DOI 10.1007/s00138-012-0481-x
   Kara B., 2003, WSEAS Transactions on Computers, V2, P920
   Ko B, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.1.017208
   Krstinic D, 2009, INF TECHNOL CONTROL, V38, P237
   Ma L., 2010, P INT C ART INT COMP, V1, P484, DOI DOI 10.1109/AICI.2010.107
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Morimitsu N., 2012, US Patent Application, Patent No. [US20120133739A1, 20120133739]
   Mrovlje J., 2008, THESIS
   Piccinini P, 2008, IEEE IMAGE PROC, P1376, DOI 10.1109/ICIP.2008.4712020
   Rafiee A, 2011, 2011 3rd International Conference on Computer Research and Development (ICCRD 2011), P262, DOI 10.1109/ICCRD.2011.5764295
   Sahabi H, 1996, COMPUT VIS IMAGE UND, V63, P447, DOI 10.1006/cviu.1996.0034
   Stipanicev D., 2012, P 3 INT C IM PROC TH
   Toreyin B. Ugur, 2005, 2005 13th European Signal Processing Conference, P1
   Vrancic D., 2006, SPIE P, V6055, P165
   Xu ZG, 2007, CIS WORKSHOPS 2007: INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY WORKSHOPS, P316, DOI 10.1109/CIS.Workshops.2007.5
   Yang J, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P301, DOI 10.1109/ICNC.2008.219
   Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/S00034-009-9130-7, 10.1007/s00034-009-9130-7]
   Yu Chunyu, 2009, Proceedings of the 2009 Second International Workshop on Computer Science and Engineering (WCSE 2009), P511, DOI 10.1109/WCSE.2009.864
   Zhao WY, 1996, PATTERN RECOGN, V29, P2115, DOI 10.1016/S0031-3203(96)00051-9
NR 33
TC 0
Z9 0
U1 2
U2 8
PU SLOVAK ACAD SCIENCES INST INFORMATICS
PI BRATISLAVA
PA DUBRAVSKA CESTA 9, 84237 BRATISLAVA, SLOVAKIA
SN 1335-9150
J9 COMPUT INFORM
JI Comput. Inform.
PY 2018
VL 37
IS 2
BP 476
EP 508
DI 10.4149/cai_2018_2_476
PG 33
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GM1KA
UT WOS:000437824300011
DA 2022-02-10
ER

PT J
AU Bugaric, M
   Jakovcevic, T
   Stipanicev, D
AF Bugaric, Marin
   Jakovcevic, Toni
   Stipanicev, Darko
TI Adaptive estimation of visual smoke detection parameters based on
   spatial data and fire risk index
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Smoke detection; Wildfire; GIS; Augmented reality; Image analysis
ID WILDFIRE SMOKE; FEATURES; FUSION
AB Standard wildfire smoke detection systems detect fires using remote cameras located at observation posts. Images from the cameras are analyzed using standard computer vision techniques, and human intervention is required only in situations in which the system raises an alarm. The number of alarms depends largely on manually set detection sensitivity parameters. One of the primary drawbacks of this approach is the false alarm rate, which impairs the usability of the system. In this paper, we present a novel approach using GIS and augmented reality to include the spatial and fire risk data of the observed scene. This information is used to improve the reliability of the existing systems through automatic parameter adjustment. For evaluation, three smoke detection methods were improved using this approach and compared to the standard versions. The results demonstrated significant improvement in different smoke detection aspects, including detection range, rate of correct detections and decrease in the false alarm rate. (C) 2013 Elsevier Inc. All rights reserved.
C1 [Bugaric, Marin; Jakovcevic, Toni; Stipanicev, Darko] Univ Split, Fac Elect Engn Mech Engn & Naval Architecture, Split 21000, Croatia.
RP Bugaric, M (corresponding author), Univ Split, Fac Elect Engn Mech Engn & Naval Architecture, Split 21000, Croatia.
EM marin.bugaric@fesb.hr; toni.jakovcevic@fesb.hr; dstip@fesb.hr
RI Jakovčević, Toni/D-8729-2017; Bugaric, Marin/D-8787-2017; Stipanicev,
   Darko/E-4759-2017
OI Jakovčević, Toni/0000-0002-5270-4099; Bugaric,
   Marin/0000-0003-4391-6804; Stipanicev, Darko/0000-0001-5932-2096
CR Alejandro OB, 2011, ELECT ROBOT AUTO MEC, P126, DOI 10.1109/CERMA.2011.27
   Appleton K., 2002, Computers, Environment and Urban Systems, V26, P141, DOI 10.1016/S0198-9715(01)00041-2
   Benazza-Benyahia A, 2012, EUR SIGNAL PR CONF, P2752
   Brooks S, 2008, COMPUT ENVIRON URBAN, V32, P278, DOI 10.1016/j.compenvurbsys.2007.11.001
   Bugaric M., 2009, PROC OF 32 INT CONFE, P270
   Busch A., 2002, Proceedings of the Sixth Digital Image Computing Techniques and Applications. Dicta 2002, P341
   Castillo M. R. M., 2009, POWERTECH JUL, P1, DOI DOI 10.1109/IPDPS.2009.5161155
   Chao-Ching Ho, 2009, 2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM), P1845, DOI 10.1109/AIM.2009.5229791
   Chen TH, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P427
   Chengcheng Gai, 2011, 2011 Fourth International Joint Conference on Computational Sciences and Optimization (CSO), P1240, DOI 10.1109/CSO.2011.140
   Cui Y, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 3, PROCEEDINGS, P95, DOI 10.1109/CISP.2008.397
   Dollner J, 2000, COMPUT GEOSCI, V26, P67, DOI 10.1016/S0098-3004(99)00038-2
   DongKeun Kim, 2009, 2009 WRI World Congress on Computer Science and Information Engineering, CSIE, P759, DOI 10.1109/CSIE.2009.494
   ERTEN E, 2002, INT J APPL EARTH OBS, V4, P1, DOI DOI 10.1016/S0303-2434(02)00006-5
   Fernandez-Berni J, 2008, WIT TRANS ECOL ENVIR, V119, P161, DOI 10.2495/FIVA080171
   Genovese A., 2011, IEEE INT C COMP INT, P1
   Gunay O, 2012, IEEE T IMAGE PROCESS, V21, P2853, DOI 10.1109/TIP.2012.2183141
   Gunay O, 2011, OPT ENG, V50, DOI 10.1117/1.3595426
   Han Ning, 2010, 2010 Second IITA International Conference on Geoscience and Remote Sensing (IITA-GRS 2010), P532, DOI 10.1109/IITA-GRS.2010.5602744
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Heikkila J, 2000, IEEE T PATTERN ANAL, V22, P1066, DOI 10.1109/34.879788
   Jakovcevic T., 2010, P 6 INT C FOR FIR RE, P1
   Jakovcevic T, 2013, MACH VISION APPL, V24, P707, DOI 10.1007/s00138-012-0481-x
   Joon Young Kwak, 2011, Proceedings of the 2011 Eighth International Conference on Computer Graphics, Imaging and Visualization (CGIV 2011), P141, DOI 10.1109/CGIV.2011.40
   Kara B., 2003, WSEAS Transactions on Computers, V2, P920
   Ko B., 2012, OPT ENG, V51
   Krstinic D, 2009, INF TECHNOL CONTROL, V38, P237
   Labati RD, 2013, IEEE T SYST MAN CY-S, V43, P1003, DOI 10.1109/TSMCA.2012.2224335
   Li M., 2001, P 9 SPAN S PATT REC, P187
   Ma L., 2010, P INT C ART INT COMP, V1, P484, DOI DOI 10.1109/AICI.2010.107
   Maruta Hidenori, 2009, 2009 IEEE International Symposium on Industrial Electronics (ISIE 2009), P1904, DOI 10.1109/ISIE.2009.5214564
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Park J, 2013, IEEE WORK APP COMP, P200, DOI 10.1109/WACV.2013.6475019
   Piccinini P, 2008, IEEE IMAGE PROC, P1376, DOI 10.1109/ICIP.2008.4712020
   Preisler HK, 2004, INT J WILDLAND FIRE, V13, P133, DOI 10.1071/WF02061
   Rafiee A, 2011, 2011 3rd International Conference on Computer Research and Development (ICCRD 2011), P262, DOI 10.1109/ICCRD.2011.5764295
   San-Miguel-Ayanz J., 2002, FOREST FIRE RES WILD, P5
   Stipanicev D., 2012, P 3 INT C IM PROC TH
   Stipanicev D., 2008, TECH REP
   Stipanicev D., 2010, P 6 INT C FOR FIR RE, P1
   Stula M, 2012, INFORM SYST FRONT, V14, P725, DOI 10.1007/s10796-011-9299-8
   Tong ZJ, 2009, STOCH ENV RES RISK A, V23, P463, DOI 10.1007/s00477-008-0233-7
   Toreyin B. Ugur, 2006, 2006 14th European Signal Processing Conference. Proceedings
   Toreyin BU, 2005, SIGNAL PROCESS-IMAGE, V20, P255, DOI 10.1016/j.image.2004.12.002
   Truong T.X., 2010, IEEE IFOST 2010 P, P437
   Van Wagner C.E., 1974, STRUCTURE CANADIAN F
   Wu DM, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P220
   Wu MY, 2012, INT C MULTIMED INFO, P281, DOI 10.1109/MINES.2012.46
   XU D., 2005, J FORESTRY RES, V16, P169, DOI DOI 10.1007/BF02856809
   Xu ZG, 2007, CIS WORKSHOPS 2007: INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY WORKSHOPS, P316, DOI 10.1109/CIS.Workshops.2007.5
   Yang J, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P301, DOI 10.1109/ICNC.2008.219
   Yang Zhao, 2012, 2012 International Conference on Machine Learning and Cybernetics (ICMLC 2012), P1474, DOI 10.1109/ICMLC.2012.6359582
   Yu Chunyu, 2009, Proceedings of the 2009 Second International Workshop on Computer Science and Engineering (WCSE 2009), P511, DOI 10.1109/WCSE.2009.864
   Yu CY, 2010, FIRE TECHNOL, V46, P651, DOI 10.1007/s10694-009-0110-z
   Zervas E, 2011, INFORM FUSION, V12, P150, DOI 10.1016/j.inffus.2009.12.006
NR 55
TC 12
Z9 13
U1 1
U2 30
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD JAN
PY 2014
VL 118
BP 184
EP 196
DI 10.1016/j.cviu.2013.10.003
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA 274FF
UT WOS:000328591500016
DA 2022-02-10
ER

PT C
AU Alsadik, B
   Remondino, F
   Menna, F
   Gerke, M
   Vosselman, G
AF Alsadik, Bashar
   Remondino, Fabio
   Menna, Fabio
   Gerke, Markus
   Vosselman, George
BE Boehm, J
   Remondino, F
   Kersten, T
   Fuse, T
   GonzalezAguilera, D
TI Robust extraction of image correspondences exploiting the image scene
   geometry and approximate camera orientation
SO 3D-ARCH 2013 - 3D VIRTUAL RECONSTRUCTION AND VISUALIZATION OF COMPLEX
   ARCHITECTURES
SE International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences
LA English
DT Proceedings Paper
CT Conference on 3D Virtual Reconstruction and Visualization of Complex
   Architectures (3D-ARCH)
CY FEB 25-26, 2013
CL Trento, ITALY
SP Int Soc Photogrammetry & Remote Sensing
DE IBM; Bundle adjustment; SIFT; 3D image; spanning tree
AB Image-based modeling techniques are an important tool for producing 3D models in a practical and cost effective manner. Accurate image-based models can be created as long as one can retrieve precise image calibration and orientation information which is nowadays performed automatically in computer vision and photogrammetry. The first step for orientation is to have sufficient correspondences across the captured images. Keypoint descriptors like SIFT or SURF are a successful approach for finding these correspondences. The extraction of precise image correspondences is crucial for the subsequent image orientation and image matching steps. Indeed there are still many challenges especially with wide-baseline image configuration. After the extraction of a sufficient and reliable set of image correspondences, a bundle adjustment is used to retrieve the image orientation parameters.
   In this paper, a brief description of our previous work on automatic camera network design is initially reported. This semi-automatic procedure results in wide-baseline high resolution images covering an object of interest, and including approximations of image orientations, a rough 3D object geometry and a matching matrix indicating for each image its matching mates. The main part of this paper will describe the subsequent image matching where the pre-knowledge on the image orientations and the pre-created rough 3D model of the study object is exploited. Ultimately the matching information retrieved during that step will be used for a precise bundle block adjustment.
   Since we defined the initial image orientation in the design of the network, we can compute the matching matrix prior to image matching of high resolution images. For each image involved in several pairs that is defined in the matching matrix, we detect the corners or keypoints and then transform them into the matching images by using the designed orientation and initial 3D model. Moreover, a window is defined for each corner and its initial correspondence in the matching images. A SIFT or SURF matching is implemented between every matching window to find the homologous points. This is followed by Least Square Matching LSM to refine the correspondences for a sub-pixel localization and to avoid inaccurate matches. Image matching is followed by a bundle adjustment to orient the images automatically to finally have a sparse 3D model. We used the commercial software Photomodeler Scanner 2010 for implementing the bundle adjustment since it reports a number of accuracy indices which are necessary for the evaluation purposes. The experimental test of comparing the automated image matching of four pre-designed streopairs shows that our approach can provide a high accuracy and effective orientation when compared to the results of commercial and open source software which does not exploit the pre-knowledge about the scene.
C1 [Alsadik, Bashar; Gerke, Markus; Vosselman, George] Univ Twente, ITC Fac, EOS Dept, NL-7500 AE Enschede, Netherlands.
   [Remondino, Fabio; Menna, Fabio] Bruno Kessler Fdn FBK, Opt Metrol Unit 3D, Trento, Italy.
   [Alsadik, Bashar] Univ Baghdad, Coll Engn, Surveying Dept, Baghdad, Iraq.
RP Alsadik, B (corresponding author), Univ Twente, ITC Fac, EOS Dept, POB 217, NL-7500 AE Enschede, Netherlands.
EM alsadik@itc.nl; remondino@fbk.eu; fmenna@fbk.eu; gerke@itc.nl;
   vosselman@itc.nl
RI Remondino, Fabio/C-5503-2018; Alsadik, Bashar/E-2052-2018; Vosselman,
   George/D-3985-2009; Gerke, Markus/A-8791-2012
OI Remondino, Fabio/0000-0001-6097-5342; Alsadik,
   Bashar/0000-0002-9425-9325; Vosselman, George/0000-0001-8813-8028;
   Gerke, Markus/0000-0002-2221-6182; menna, fabio/0000-0002-5365-8813
CR ALSADIK B, 2012, ISPRS ANN PHOTOGRAMM, V3, P7
   Alsadik B., 2013, J CULTURAL IN PRESS
   Barazzetti L, 2010, PHOTOGRAMM REC, V25, P356, DOI 10.1111/j.1477-9730.2010.00599.x
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Cho W., 1992, 418 OH STAT U
   Fraser C.S., 1996, CLOSE RANGE PHOTOGRA, P256
   Gruen A.W., 1985, S AFRICAN J PHOTOGRA, V14, P176
   Kosecka J., 2010, COMPUTER VISION IMAG, V100, P274
   Lourakis M., 2004, DESIGN IMPLEMENTATIO
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MASON S, 1995, ISPRS J PHOTOGRAMM, V50, P13, DOI 10.1016/0924-2716(95)90117-W
   Nister D., 2001, AUTOMATIC DENSE RECO
   PhotoModeler, 2009, PHOTOMODELER QUICK S
   Photoscan A., 2011, AGISOFT STEREOSCAN
   Pierrot-Deseilligny M., 2012, MICMAC SOFTWARE AUTO
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Snavely N., 2010, BUNDLER STRUCTURE MO
   Snavely N., 2008, P COMP VIS PATT REC
   Troisi, 2012, LECT NOTES COMPUTER, V7616, P40, DOI [10.1007/978-3-642-34234-9_5, DOI 10.1007/978-3-642-34234-9_5]
   Wu C. C., 2012, VISUALSFM VISUAL STR
   YANG YB, 1995, ARTIF INTELL, V78, P121, DOI 10.1016/0004-3702(95)00028-3
NR 22
TC 10
Z9 10
U1 0
U2 1
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PY 2013
VL 40-5-W1
BP 1
EP 7
PG 7
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Physical Geography; Remote Sensing; Imaging Science & Photographic
   Technology
GA BD1RO
UT WOS:000358300800001
DA 2022-02-10
ER

PT C
AU Xiang, WH
   Cao, Y
   Wang, ZF
AF Xiang, Wenhui
   Cao, Yang
   Wang, Zengfu
GP IEEE
TI Automatic Take-Off and Landing of a Quad-rotor Flying Robot
SO PROCEEDINGS OF THE 2012 24TH CHINESE CONTROL AND DECISION CONFERENCE
   (CCDC)
SE Chinese Control and Decision Conference
LA English
DT Proceedings Paper
CT 24th Chinese Control and Decision Conference (CCDC)
CY MAY 23-25, 2012
CL Taiyuan, PEOPLES R CHINA
SP NE Univ, IEEE Ind Elect (IE) Chapter, Ind Elect Soc (IES), Taiyuan Univ Sci & Technol, Taiyuan Univ Technol, IEEE, IEEE Harbin Sect Control Syst Soc Chapter, IEEE Control Syst Soc (CSS), Syst Engn Soc China, Chinese Assoc Artificial Intelligence, Tech Comm Control Theory, Chinese Assoc Automat, Chinese Assoc Aeronaut, Automat Control Soc, Chinese Assoc Syst Simulat, Simulat Methods & Modeling Soc, Chinese Assoc Artificial Intelligence, Intelligent Control & Management Soc
DE quad-rotor UVA; Wii remote; Visual tracking; Infrared camera
AB In recent years, miniature quad-rotor unmanned aerial vehicles developed rapidly. Compared with the helicopter, the quad-rotor aircraft has more compact structure and greater lift force. One of the key aspects in autonomous behavior is take-off and landing. To achieve autonomy for this kind of aircraft, novel sensors are required. Those sensors need to cope with strictly limited onboard processing power. Our visual tracking system not using expensive cameras but a Wii remote camera, it belongs to commodity consumer hardware. The camera is capable of tracking infrared blobs in the case of no direct sunlight. The rate of the sensor's data refresh can reach 250Hz. Combined infrared cameras with IMU, our system can completely tracking and positioning of the aircraft. Then, we choose PID algorithm to control the aircraft, and the Quad-rotor flying robot successfully achieved automatic takeoff and landing at last.
C1 [Xiang, Wenhui; Cao, Yang; Wang, Zengfu] Univ Sci & Technol China, Hefei 230027, Peoples R China.
RP Xiang, WH (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM forrest@ustc.edu.cn
CR Bouabdallah S., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2451
   Guenard N, 2007, IEEE INT CONF ROBOT, P1342, DOI 10.1109/ROBOT.2007.363171
   Gurdan D, 2007, IEEE INT CONF ROBOT, P361, DOI 10.1109/ROBOT.2007.363813
   Herisse B, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P801, DOI 10.1109/IROS.2008.4650731
   Mak L. C., 2007, 2 INT C SENS TECHN I, P32
   Saripalli S, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P2799, DOI 10.1109/ROBOT.2002.1013656
   Schou T., 2007, P 19 AUSTR C COMP HU, P231
   Shakernia O., 1999, Asian Journal of Control, V1, P128
   Sharp CS, 2001, IEEE INT CONF ROBOT, P1720, DOI 10.1109/ROBOT.2001.932859
   Sreedharan Sreeram, 2007, P 19 AUSTR C COMP HU, P227
   Wenzel K., 2009, P AMS 09, P73
   Wenzel KE, 2010, J INTELL ROBOT SYST, V57, P297, DOI 10.1007/s10846-009-9355-5
NR 12
TC 5
Z9 5
U1 2
U2 6
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1948-9439
BN 978-1-4577-2072-7
J9 CHIN CONT DECIS CONF
PY 2012
BP 1251
EP 1255
PG 5
WC Automation & Control Systems; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Engineering
GA BGS33
UT WOS:000323966201056
DA 2022-02-10
ER

PT J
AU Mustapha, UF
   Alhassan, AW
   Jiang, DN
   Li, GL
AF Mustapha, Umar Farouk
   Alhassan, Abdul-Wadud
   Jiang, Dong-Neng
   Li, Guang-Li
TI Sustainable aquaculture development: a review on the roles of cloud
   computing, internet of things and artificial intelligence (CIA)
SO REVIEWS IN AQUACULTURE
LA English
DT Review
DE aquaculture; CIA (cloud computing internet of things and artificial
   intelligence); fish health; productivity; sustainability; technology
AB Each year, there is a significant rise in demand for global food production due to population increase and a rise in demand for protein food sources. This puts pressure on capture fishery as fish is a preferred protein source worldwide. However, the more resources we put into the capture fisheries to obtain maximum catch, the faster the fisheries stock becomes depleted. The best option left to produce enough fish to meet demand is relying on advanced aquaculture. Unfortunately, the impact of technological advancement in the aquaculture sector is not profound compared to the agricultural and manufacturing sectors. The advent of Cloud computing, Internet of Things, and Artificial Intelligence (CIA) has expanded numerous possibilities for applying and integrating information technology in all works of life. This article reviews the emergence of research development on CIA and the potential to revolutionize the aquaculture industry. The use of CIA techniques and tools such as drones, nano and micro-sensors, bionic robots, remote cameras, intelligent sorting, energy-saving processing equipment, statistical modules, and algorithms will reduce human intervention and increase aquaculture productivity. Also, the application of CIA in the aquaculture value chain to ensure effectiveness in traceability, feeding, disease detection, growth prediction, environmental monitoring, market information, and others is key to increasing aquaculture productivity and sustainability. Therefore, the future of aquaculture operations with less human labour, effective maintenance, and resource utilization largely depend on innovative technologies. Here, we outlined the need for adopting innovative technologies and the limiting factors that hinge on CIA adoption in the aquaculture industry.
C1 [Mustapha, Umar Farouk; Jiang, Dong-Neng; Li, Guang-Li] Guangdong Ocean Univ, Guangdong Prov Famous Fish Reprod Regulat & Breed, Fisheries Coll, Zhanjiang, Peoples R China.
   [Alhassan, Abdul-Wadud] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China.
RP Li, GL (corresponding author), Guangdong Ocean Univ, Fisheries Coll, Zhanjiang 524088, Peoples R China.
EM ligl@gdou.edu.cn
RI Mustapha, Umar Farouk/AAA-3306-2022
OI Mustapha, Umar Farouk/0000-0003-4193-6617
FU Key Project of "Blue Granary Science and Technology Innovation" of the
   Ministry of Science and Technology [2018YFD0901203]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   (NSFC) [31702326]; Natural Science Foundation of Guangdong
   ProvinceNational Natural Science Foundation of Guangdong Province
   [2018B030311050, 2019A1515012042, 2019A1515010958]; Department of
   Education of Guangdong ProvinceNational Natural Science Foundation of
   Guangdong Province [2018KTSCX090]; Program for Scientific Research
   Start-Up Funds of Guangdong Ocean University
FX This study was supported by grants from the Key Project of "Blue Granary
   Science and Technology Innovation" of the Ministry of Science and
   Technology (2018YFD0901203); the National Natural Science Foundation of
   China (31702326); Natural Science Foundation of Guangdong Province
   (2018B030311050; 2019A1515012042 and 2019A1515010958); Department of
   Education of Guangdong Province (2018KTSCX090); Program for Scientific
   Research Start-Up Funds of Guangdong Ocean University.
CR Abdullahi HS, 2015, L N INST COMP SCI SO, V154, P388, DOI 10.1007/978-3-319-25479-1_29
   [Anonymous], 2020, SUSTAINABILITY ACTIO
   Atzori L, 2010, COMPUT NETW, V54, P2787, DOI 10.1016/j.comnet.2010.05.010
   Bae MJ, 2014, SCI TOTAL ENVIRON, V466, P635, DOI 10.1016/j.scitotenv.2013.07.075
   Bagul SY, 2017, WATER SCI TECHNOL, V76, P719, DOI 10.2166/wst.2017.223
   Barbedo JGA., 2014, REV INNOVER, V1, P19
   Barosa R, 2019, 2019 SECOND INTERNATIONAL CONFERENCE ON NEXT GENERATION COMPUTING APPLICATIONS 2019 (NEXTCOMP 2019)
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Beveridge MCM, 2013, J FISH BIOL, V83, P1067, DOI 10.1111/jfb.12187
   Bhattacharyya D.K., 2013, NETWORK ANOMALY DETE
   Blok V, 2018, ETHICS POLICY ENV, V21, P246, DOI 10.1080/21550085.2018.1509491
   Bossier P, 2017, MICROB BIOTECHNOL, V10, P1012, DOI 10.1111/1751-7915.12836
   Brown M., 2018, SMART FARMING AUTOMA
   Burton RJF, 2018, LAND USE POLICY, V70, P334, DOI 10.1016/j.landusepol.2017.10.014
   Chen YQ, 2019, INT CONF COMP SCI ED, P335, DOI 10.1109/ICCSE.2019.8845527
   CHEN YY, 2016, MATH PROBL ENG, V2016, DOI DOI 10.1155/2016/6564202
   Crab R, 2012, AQUACULTURE, V356, P351, DOI 10.1016/j.aquaculture.2012.04.046
   Cruz EF, 2020, ICSOFT: PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON SOFTWARE TECHNOLOGIES, P501, DOI 10.5220/0009889705010508
   dos Santos AA, 2019, ECOL INFORM, V53, DOI 10.1016/j.ecoinf.2019.100977
   Dupont C., 2018, IOT AQUACULTURE 40 G, P180
   Edwards P, 2019, MAR POLICY, V106, DOI 10.1016/j.marpol.2019.103547
   Emerenciano M, 2013, BIOMASS NOW CULTIV U, DOI [10.5772/53902, DOI 10.5772/53902]
   Emerenciano M. G. C., 2017, Water quality, P91
   Engle CR, 2020, J WORLD AQUACULT SOC, V51, P847, DOI 10.1111/jwas.12706
   FAO, 2016, FISHERIES AQUACULTUR
   FAO, 2018, STAT WORLDS FISH AQ
   Feamster N., 2017, FREEDOM TO TINKER
   Fearghal O., 2019, DATA DRIVEN AQUACULT
   FOC, 2019, STATE SALMON AQUACUL
   Fore M, 2018, BIOSYST ENG, V173, P176, DOI 10.1016/j.biosystemseng.2017.10.014
   Freitas J, 2020, AQUACULTURE, V518, DOI 10.1016/j.aquaculture.2019.734857
   Gao GD, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.105013
   Govindaraju K, 2020, J CLUST SCI, V31, P1163, DOI 10.1007/s10876-019-01724-3
   Gukelberger E, 2020, INTEGR ENVIRON ASSES, V16, P942, DOI 10.1002/ieam.4281
   Gupta M, 2020, IEEE ACCESS, V8, P34564, DOI 10.1109/ACCESS.2020.2975142
   Hall, 2002, NEW EC HDB, V38, P1
   Hayes Mason  Curran, 2016, INTERNET THINGSLEGAL
   Hoel T., 2018, DATA SCI HELPS NORWA
   Hsu CL, 2016, COMPUT HUM BEHAV, V62, P516, DOI 10.1016/j.chb.2016.04.023
   IBM, 2016, WHAT IS INTERNET THI
   IPI Singapore, 2019, MONITORING TECHNOLOG
   Kariuki, 2015, J EC SUSTAINABLE DEV, V6, P208
   Katiha P. K., 2005, Aquaculture Economics & Management, V9, P237, DOI 10.1080/13657300590961573
   Klerkx L, 2019, NJAS-WAGEN J LIFE SC, V90-91, DOI 10.1016/j.njas.2019.100315
   Kuhn DD, 2010, AQUACULTURE, V303, P28, DOI 10.1016/j.aquaculture.2010.03.001
   Li DL, 2020, J WORLD AQUACULT SOC, V51, P808, DOI 10.1111/jwas.12736
   Li DL, 2020, AQUACULTURE, V528, DOI 10.1016/j.aquaculture.2020.735508
   Li DL, 2020, REV AQUACULT, V12, P1390, DOI 10.1111/raq.12388
   Liu ZY, 2014, AQUACULT ENG, V60, P20, DOI 10.1016/j.aquaeng.2014.03.005
   Lu Huanda, 2018, Zhejiang Daxue Xuebao Nongye Yu Shengming Kexue Ban, V44, P499
   Mahmood A, 2020, ICES J MAR SCI, V77, P1308, DOI 10.1093/icesjms/fsz223
   Martin H., 2019, AQUACULTURE 4 0 APPL
   Mattern F, 2010, LECT NOTES COMPUT SC, V6462, P242, DOI 10.1007/978-3-642-17226-7_15
   McGovern A, 2017, B AM METEOROL SOC, V98, P2073, DOI 10.1175/BAMS-D-16-0123.1
   Meng L, 2018, IEEE ACCESS, V6, P17880, DOI 10.1109/ACCESS.2018.2820326
   Michael H., 2019, 5 INNOVATIONS AQUACU
   Miranda M.Q., 2016, RAI REV ADM INOVACAO, V13, P48, DOI [10.1016/j.rai.2016.02.002, DOI 10.1016/J.RAI.2016.02.002]
   Moreno-Andres J, 2020, WATER RES, V181, DOI 10.1016/j.watres.2020.115928
   Mustafa Man, 2013, Galaxea - Tokyo, V15, P101
   Naddaf-Sh MM, 2018, COMPLEXITY, DOI 10.1155/2018/5298294
   Nordrum A, 2017, POPULAR INTERNET THI
   Ogunremi JB, 2012, LIFE SCI J, V9, P329
   Perera C, 2015, IEEE T EMERG TOP COM, V3, P585, DOI 10.1109/TETC.2015.2390034
   Piplani D., 2015, 7 INT C HCI, P95, DOI DOI 10.1145/2835966.2836277
   Purcell SW, 2018, MAR POLICY, V91, P58, DOI 10.1016/j.marpol.2018.02.005
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Rahim M., 2010, INT ARAB J INF TECHN, V19
   Rahman A, 2013, INT J COMPUT INTELL, V12, DOI 10.1142/S1469026813500089
   Raju KRSR, 2017, IEEE INT ADV COMPUT, P318, DOI [10.1109/IACC.2017.67, 10.1109/IACC.2017.0075]
   Rosado da Cruz A.M., 2019, 14 IB C INF SYST TEC, P1, DOI [10. 23919/CISTI.2019.8760891, DOI 10.23919/CISTI.2019.8760891]
   Russell S., 2002, ARTIFICIAL INTELLIGE
   Saberioon M, 2018, COMPUT ELECTRON AGR, V150, P484, DOI 10.1016/j.compag.2018.05.025
   Sanchez-Torres G, 2018, ENG LET, V26
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shahriar MS, 2014, COMPUT ELECTRON AGR, V102, P85, DOI 10.1016/j.compag.2014.01.011
   Shamshiri RR, 2018, INT J AGR BIOL ENG, V11, P1, DOI 10.25165/j.ijabe.20181104.4278
   Sharma H, 2018, J SENS ACTUAR NETW, V7, DOI 10.3390/jsan7030040
   Shetty S, 2018, COGENT ENG, V5, DOI 10.1080/23311916.2018.1542576
   Simbeye D. S., 2014, J NETWORK, V9, p840 , DOI DOI 10.4304/jnw.9.4.840-849
   Sourceforge, 2020, BEST IOT SOFTWARE 20
   Stergiou C, 2018, FUTURE GENER COMP SY, V78, P964, DOI 10.1016/j.future.2016.11.031
   TechTarget, WHAT IS IOT INTERNET
   Universitry CM, 2018, THE ONLY COKE MACHIN
   Van Henten IEJ., 2020, EVOLUTION AGR TECHNO
   Vermesan O, 2014, RIVER PUBL SER COMM, P7
   Vongsingthong S, 2014, SURANAREE J SCI TECH, V21, P359
   Wei YG, 2020, COMPUT ELECTRON AGR, V169, DOI 10.1016/j.compag.2019.105119
   XpertSea, FARMED SEAFOOD EVERY
   Yan B, 2012, INT J RF TECHNOL-RES, V4, P55, DOI 10.3233/RFT-2012-0035
   Yang L, 2017, RELIAB ENG SYST SAFE, V160, P201, DOI 10.1016/j.ress.2016.12.008
   Yang XT, 2021, REV AQUACULT, V13, P66, DOI 10.1111/raq.12464
   Zhang S, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10020364
   Zhou C, 2018, COMPUT ELECTRON AGR, V146, P114, DOI 10.1016/j.compag.2018.02.006
   Zou LL, 2015, AQUACULT REP, V2, P46, DOI 10.1016/j.aqrep.2015.07.001
NR 94
TC 2
Z9 2
U1 23
U2 50
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1753-5123
EI 1753-5131
J9 REV AQUACULT
JI Rev. Aquac.
PD SEP
PY 2021
VL 13
IS 4
BP 2076
EP 2091
DI 10.1111/raq.12559
EA MAR 2021
PG 16
WC Fisheries
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Fisheries
GA UK0RU
UT WOS:000634285200001
DA 2022-02-10
ER

PT C
AU Xu, MW
   Xu, TT
   Liu, YX
   Lin, FXZ
AF Xu, Mengwei
   Xu, Tiantu
   Liu, Yunxin
   Lin, Felix Xiaozhu
GP USENIX ASSOC
TI Video Analytics with Zero-streaming Cameras
SO PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE
LA English
DT Proceedings Paper
CT USENIX Annual Technical Conference / 15th USENIX Symposium on Operating
   Systems Design and Implementation (OSDI)
CY JUL 14-16, 2021
CL ELECTR NETWORK
SP USENIX Assoc, VMWare, Facebook, IBM, Google, Bloomberg, LinkedIn, Microsoft, NetApp, Salesforce, Amazon, ByteDance, Baidu, Two Sigma, Futurewei, Exotanium, Hewlett Packard Enterprise
AB Low-cost cameras enable powerful analytics. An unexploited opportunity is that most captured videos remain "cold" without being queried. For efficiency, we advocate for these cameras to be zero streaming: capturing videos to local storage and communicating with the cloud only when analytics is requested.
   How to query zero-streaming cameras efficiently? Our response is a camera/cloud runtime system called DIVA. It addresses two key challenges: to best use limited camera resource during video capture; to rapidly explore massive videos during query execution. DIVA contributes two unconventional techniques. (1) When capturing videos, a camera builds sparse yet accurate landmark frames, from which it learns reliable knowledge for accelerating future queries. (2) When executing a query, a camera processes frames in multiple passes with increasingly more expensive operators. As such, DIVA presents and keeps refining inexact query results throughout the query's execution. On diverse queries over 15 videos lasting 720 hours in total, DIVA runs at more than 100x video realtime and outperforms competitive alternative designs. To our knowledge, DIVA is the first system for querying large videos stored on low-cost remote cameras.
C1 [Xu, Mengwei] Peking Univ, Beijing, Peoples R China.
   [Xu, Mengwei] Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
   [Xu, Tiantu] Purdue ECE, W Lafayette, IN USA.
   [Liu, Yunxin] Tsinghua Univ, Inst AI Ind Res AIR, Beijing, Peoples R China.
   [Lin, Felix Xiaozhu] Univ Virginia, Charlottesville, VA 22903 USA.
   [Xu, Mengwei; Xu, Tiantu] Purdue Univ, W Lafayette, IN 47907 USA.
RP Xu, MW (corresponding author), Peking Univ, Beijing, Peoples R China.; Xu, MW (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
FU National Key R&D Program of China [2020YFB1805500]; Fundamental Research
   Funds for the Central UniversitiesFundamental Research Funds for the
   Central Universities; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China (NSFC) [62032003,
   61922017, 61921003]; NSFNational Science Foundation (NSF) [1846102,
   1919197]
FX Mengwei Xu was supported by National Key R&D Program of China under
   grant number 2020YFB1805500, the Fundamental Research Funds for the
   Central Universities, and National Natural Science Foundation of China
   under grant numbers 62032003, 61922017, and 61921003. Tiantu Xu and
   Felix Xiaozhu Lin were supported in part by NSF awards #1846102 and
   #1919197. We thank our shepherd, Michael Kozuch, and the anonymous ATC
   reviewers for their useful suggestions.
CR Augustin A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16091466
   Beymer D, 1997, PROC CVPR IEEE, P495, DOI 10.1109/CVPR.1997.609371
   Blu T, 2008, IEEE SIGNAL PROC MAG, V25, P31, DOI 10.1109/MSP.2007.914998
   Bohm C, 2001, SIGMOD RECORD, V30, P379
   Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384
   Canel C., 2019, P 2 SYSML C SYSML 19
   Chakrabarti Kaushik, 2000, ICDE C JAN
   Chen TYH, 2015, SenSys'15: Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, P155, DOI 10.1145/2809695.2809711
   Condie  T., 2010, P 7 USENIX C NETW SY
   Feng Boyuan, 2018, ABS180906691 CORR
   Feng ZQ, 2019, IEEE INTERNET COMPUT, V23, P35, DOI 10.1109/MIC.2019.2892941
   Feng Ziqiang, 2018, EVA EFFICIENT SYSTEM
   Han B, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P129, DOI 10.1145/2999572.2999606
   HAZEWINKEL M, 1988, ENCY MATH
   Hellerstein J. M., 1997, SIGMOD Record, V26, P171
   Hellerstein JM, 1999, COMPUTER, V32, P51, DOI 10.1109/2.781635
   Hsieh K, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P269
   Hung CC, 2018, 2018 THIRD IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC), P115, DOI 10.1109/SEC.2018.00016
   Ilyas I.F., 2004, P ACM SIGMOD INT C M, P203
   Jain Samvit, 2018, ABS181101268 CORR
   Jiang JC, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P253, DOI 10.1145/3230543.3230574
   Jin T, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P835, DOI 10.1145/3297858.3304038
   Kading C, 2017, LECT NOTES COMPUT SC, V10118, P588, DOI 10.1007/978-3-319-54526-4_43
   Kang D, 2017, PROC VLDB ENDOW, V10, P1586, DOI 10.14778/3137628.3137664
   Kang Daniel, 2018, ABS180501046
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Koudas N, 2000, IEEE T KNOWL DATA EN, V12, P3, DOI 10.1109/69.842246
   Krishnan Sanjay, 2019, CIDR 2019
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI [10.1007/s11263-019-01204-1, 10.1007/978-3-030-01264-9_45]
   Liao Mike, 2018, BENCHMARKING HARDWAR
   Lipton Alan J, 2015, US Patent, Patent No. [9,158,975, 9158975]
   Liu MS, 2018, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2018.00596
   Mengwei Xu, 2020, MobiSys '20: Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services, P191, DOI 10.1145/3386901.3388948
   NIST, 2019, SPECTR CRUNCH
   Pakha Chrisma, 2018, P 10 USENIX WORKSH H
   Pansare N, 2011, PROC VLDB ENDOW, V4, P1135
   Paz Ziv, INNOVATION SURVEILLA
   Poms A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201394
   Pu QF, 2015, ACM SIGCOMM COMP COM, V45, P421, DOI 10.1145/2829988.2787505
   Rabkin A., 2014, NSDI, P275
   Ran XK, 2018, IEEE INFOCOM SER, P1421, DOI 10.1109/INFOCOM.2018.8485905
   Redmon J., 2018, ARXIV PREPRINT ARXIV
   Saletan Slate William, 2013, CASE MASS SURVEILLAN
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Shalizi Cosma Rohilla, ADV DATA ANAL ELEMEN
   Shen HC, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P322, DOI 10.1145/3341301.3359658
   Shen Haichen, 2017, IEEE C COMP VIS PATT
   Shi HH, 2018, IEEE COMPUT SOC CONF, P116, DOI 10.1109/CVPRW.2018.00023
   Teng Ervin, 2018, ABS180310358 CORR
   Teng Ervin, 2017, ABS170905021 CORR
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viswanathan R, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P435
   Vulimiri A., 2015, NSDI, P323
   Vulimiri A, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1087, DOI 10.1145/2723372.2735365
   Wang HZ, 2010, APPL MATH MODEL, V34, P3376, DOI 10.1016/j.apm.2010.02.027
   Wang Hao, 2017, 9 USENIX WORKSH HOT
   Wang JJ, 2018, 2018 THIRD IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC), P159, DOI 10.1109/SEC.2018.00019
   Wang Yiding, 2019, 11 USENIX WORKSH HOT
   Xu MW, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P129, DOI 10.1145/3241539.3241563
   Xu TT, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303971
   Yi SH, 2017, INT CON DISTR COMP S, P2573, DOI 10.1109/ICDCS.2017.182
   Yuanqi Li, 2020, SIGCOMM '20: Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, P359, DOI 10.1145/3387514.3405874
   Zhang B, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P236, DOI 10.1145/3230543.3230554
   Zhang HY, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P377
   Zhang T, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P426, DOI 10.1145/2789168.2790123
   Zhu Hongwei, 2017, US Patent, Patent No. [9,639,747, 9639747]
   Zhu XZ, 2018, PROC CVPR IEEE, P7210, DOI 10.1109/CVPR.2018.00753
NR 68
TC 0
Z9 0
U1 1
U2 1
PU USENIX ASSOC
PI BERKELEY
PA SUITE 215, 2560 NINTH ST, BERKELEY, CA 94710 USA
BN 978-1-939133-23-6
PY 2021
BP 99
EP 115
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Information
   Systems; Computer Science, Software Engineering; Computer Science,
   Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS1WD
UT WOS:000696708600007
DA 2022-02-10
ER

PT J
AU Stoner, DC
   Wolfe, ML
   Rieth, WR
   Bunnell, KD
   Durham, SL
   Stoner, LL
AF Stoner, David C.
   Wolfe, Michael L.
   Rieth, Wendy R.
   Bunnell, Kevin D.
   Durham, Susan L.
   Stoner, Lisa L.
TI De facto refugia, ecological traps and the biogeography of anthropogenic
   cougar mortality in Utah
SO DIVERSITY AND DISTRIBUTIONS
LA English
DT Article
DE Colorado Plateau; ecological trap; exploitation; Great Basin; Puma
   concolor; range contraction; refuge
ID MOUNTAIN LIONS; CARNIVORE CONSERVATION; PROTECTED AREAS; PUMA-CONCOLOR;
   POPULATION; EXTINCTION; STRATEGIES; DYNAMICS; IMPACT; PREY
AB Aim Modern extirpations within the Carnivora have generally followed the human footprint. The contagion hypothesis predicts that range contractions should occur along gradients in human activity, leaving relict populations in remote areas at range edges. We evaluated this hypothesis for cougars (Puma concolor), a widely distributed and heavily exploited North American carnivore.
   Location Colorado Plateau and Great Basin ecoregions within Utah, USA.
   Methods We examined the spatial distribution of anthropogenic cougar mortality (n = 4217) using indices of remoteness and habitat quality within a GIS/ multiple-regression analytical framework. To identify areas of disproportionately high or low exploitation rates, we used break-points from the literature and local field studies. We defined de facto refugia as watersheds with mean annual harvest rates <= 24% of the predicted population, whereas ecological traps were those watersheds that exceeded this value.
   Results Cougar harvest rates were greater in the core and lower along the periphery of their statewide geographic range. The largest refugia were overrepresented in arid ecoregions with low human population densities, whereas ecological traps were concentrated in areas of low remoteness. Ecological traps were within mean cougar dispersal distances from refugia, highlighting the potential for source-sink dynamics. Patterns of anthropogenic cougar mortality generally followed the predictions of the contagion hypothesis, being spatially correlated with human access in high-quality habitats.
   Main conclusions Low-quality habitats on the range margins are likely to harbour carnivore populations in the event of widespread human-caused declines, and therefore may have greater conservation value than has previously been assumed. Resource managers may consider using the distribution of de facto refugia and ecological traps within a source-sink context to develop conservation strategies for cougars and other wide-ranging, low-density carnivores with high dispersal tendencies.
C1 [Stoner, David C.; Wolfe, Michael L.] Utah State Univ, Dept Wildland Resources, Logan, UT 84322 USA.
   [Stoner, David C.; Wolfe, Michael L.; Durham, Susan L.] Utah State Univ, Ctr Ecol, Logan, UT 84322 USA.
   [Rieth, Wendy R.; Stoner, Lisa L.] Utah State Univ, Dept Wildland Resources, RS GIS Lab, Logan, UT 84322 USA.
   [Bunnell, Kevin D.] Utah Div Wildlife Resources, Salt Lake City, UT 84114 USA.
RP Stoner, DC (corresponding author), Utah State Univ, Dept Wildland Resources, Logan, UT 84322 USA.
EM david.stoner@usu.edu
FU Utah Division of Wildlife Resources through the Federal Aid in Wildlife
   Restoration Program; African Safari Club of Florida; NASA Biodiversity
   and Ecological Forecasting Program (Climate and Biological Response)
   [NNH10ZDA001N]
FX Project Funding came from the Utah Division of Wildlife Resources
   through the Federal Aid in Wildlife Restoration Program, grant no.
   W-65-M. We gratefully acknowledge H. Bernales, K. Hersey, A. Aoude, B.
   Blackwell, B. Bates and T. Becker of the UDWR for logistical and
   technical support. We thank R. Larsen for unpublished camera-trap data
   from the Keg Mountains. The African Safari Club of Florida generously
   provided D. S. with subsidiary funding during the writing phase of the
   project. Additional support was provided by the NASA Biodiversity and
   Ecological Forecasting Program (Climate and Biological Response, grant
   no. NNH10ZDA001N). We thank F. Howe, J. du Toit, J. MacMahon, E. Gese,
   K. Logan and an anonymous reviewer for constructive criticism of the
   original manuscript.
CR Anderson CR, 2005, WILDLIFE SOC B, V33, P179, DOI 10.2193/0091-7648(2005)33[179:EEOPTA]2.0.CO;2
   Andreasen AM, 2012, MOL ECOL, V21, P5689, DOI 10.1111/j.1365-294X.2012.05740.x
   Balme GA, 2010, ANIM CONSERV, V13, P315, DOI 10.1111/j.1469-1795.2009.00342.x
   Balme Guy A., 2010, P341
   Banner R., 2009, RANGELAND RESOURCES
   Basille M, 2009, ECOGRAPHY, V32, P683, DOI 10.1111/j.1600-0587.2009.05712.x
   BECK T, 2005, COUGAR MANAGEMENT GU
   Beier P, 2010, URBAN CARNIVORES ECO
   BROWN JH, 1984, AM NAT, V124, P255, DOI 10.1086/284267
   Carbone C, 2002, SCIENCE, V295, P2273, DOI 10.1126/science.1067994
   Cardillo M, 2005, SCIENCE, V309, P1239, DOI 10.1126/science.1116030
   Caso A, 2008, IUCN RED LIST THREAT
   Channell R, 2000, NATURE, V403, P84, DOI 10.1038/47487
   Choate DM, 2006, WILDLIFE SOC B, V34, P782, DOI 10.2193/0091-7648(2006)34[782:EOCPEI]2.0.CO;2
   Cooley HS, 2009, ECOLOGY, V90, P2913, DOI 10.1890/08-1805.1
   Cougar Discussion Group, 2009, COUG DISC GROUP PUBL
   Davidson AD, 2009, P NATL ACAD SCI USA, V106, P10702, DOI 10.1073/pnas.0901956106
   De Angelo C, 2011, DIVERS DISTRIB, V17, P422, DOI 10.1111/j.1472-4642.2011.00746.x
   Delibes M, 2001, AM NAT, V158, P277, DOI 10.1086/321319
   GILBERT P F, 1970, Journal of Wildlife Management, V34, P15, DOI 10.2307/3799486
   Herfindal I., 2007, J ZOOL, V265, P63
   Jenks, 2011, MANAGING COUGARS N A, P111
   Jung TS, 2005, CAN FIELD NAT, V119, P580, DOI 10.22621/cfn.v119i4.192
   Kerley LL, 2002, CONSERV BIOL, V16, P97, DOI 10.1046/j.1523-1739.2002.99290.x
   Laliberte AS, 2004, BIOSCIENCE, V54, P123, DOI 10.1641/0006-3568(2004)054[0123:RCONAC]2.0.CO;2
   Laundre J, 2003, ANIM CONSERV, V6, P159, DOI 10.1017/S1367943003003202
   Leopold A., 1933, GAME MANAGEMENT
   Lindzey F., 1987, WILD FURBEARER MANAG, P657
   Logan K., 2001, DESERT PUMA
   Lomolino M, 2006, BIOGEOGRAPHY
   Loveridge Andrew J., 2010, P283
   Martin PS, 1999, CONSERV BIOL, V13, P36, DOI 10.1046/j.1523-1739.1999.97417.x
   McClure MF, 2005, EUR J WILDLIFE RES, V51, P170, DOI 10.1007/s10344-005-0086-z
   Minor ES, 2010, CONSERV BIOL, V24, P1549, DOI 10.1111/j.1523-1739.2010.01558.x
   Monteith KL, 2011, ECOSPHERE, V2, DOI 10.1890/ES10-00096.1
   Morrison JC, 2007, J MAMMAL, V88, P1363, DOI 10.1644/06-MAMM-A-124R2.1
   Naidu A, 2011, J FISH WILDL MANAG, V2, P106, DOI 10.3996/042010-JFWM-008
   Naves J, 2003, CONSERV BIOL, V17, P1276, DOI 10.1046/j.1523-1739.2003.02144.x
   Packer C, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005941
   Pettorelli N, 2011, CLIM RES, V46, P15, DOI 10.3354/cr00936
   Pierce BM, 2012, J MAMMAL, V93, P977, DOI 10.1644/12-MAMM-A-014.1
   Pierce BM, 2000, ECOLOGY, V81, P1533
   Rabinowitz A, 2010, BIOL CONSERV, V143, P939, DOI 10.1016/j.biocon.2010.01.002
   Rieth W. R., 2009, THESIS UTAH STATE U
   Robinson HS, 2008, ECOL APPL, V18, P1028, DOI 10.1890/07-0352.1
   Ruth TK, 2011, J WILDLIFE MANAGE, V75, P1381, DOI 10.1002/jwmg.190
   Seager R, 2007, SCIENCE, V316, P1181, DOI 10.1126/science.1139601
   Stein BA, 2008, BIOSCIENCE, V58, P339, DOI 10.1641/B580409
   Stoner D., 2011, THESIS UTAH STATE U
   Stoner DC, 2006, J WILDLIFE MANAGE, V70, P1588, DOI 10.2193/0022-541X(2006)70[1588:CELIUI]2.0.CO;2
   Sweanor LL, 2008, J WILDLIFE MANAGE, V72, P1076, DOI 10.2193/2007-024
   Sweanor LL, 2000, CONSERV BIOL, V14, P798, DOI 10.1046/j.1523-1739.2000.99079.x
   Tolon V, 2012, ECOL APPL, V22, P648, DOI 10.1890/11-0422.1
   Wilson S, 2010, WEST N AM NATURALIST, V70, P238, DOI 10.3398/064.070.0211
   Woodroffe R, 2001, CONSERV BIOL SER, V5, P61
   Woodroffe R, 1998, SCIENCE, V280, P2126, DOI 10.1126/science.280.5372.2126
NR 56
TC 18
Z9 18
U1 0
U2 97
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1366-9516
EI 1472-4642
J9 DIVERS DISTRIB
JI Divers. Distrib.
PD SEP
PY 2013
VL 19
IS 9
BP 1114
EP 1124
DI 10.1111/ddi.12035
PG 11
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA 201EK
UT WOS:000323123300003
DA 2022-02-10
ER

PT C
AU Tscharf, A
   Rumpler, M
   Fraundorfer, F
   Mayer, G
   Bischof, H
AF Tscharf, Alexander
   Rumpler, Markus
   Fraundorfer, Friedrich
   Mayer, Gerhard
   Bischof, Horst
BE Armenakis, C
TI ON THE USE OF UAVS IN MINING AND ARCHAEOLOGY - GEO-ACCURATE 3D
   RECONSTRUCTIONS USING VARIOUS PLATFORMS AND TERRESTRIAL VIEWS
SO ISPRS UAV-G2015
LA English
DT Proceedings Paper
CT International Conference on Unmanned Aerial Vehicles in Geomatics
CY AUG 30-SEP 02, 2015
CL Toronto, CANADA
DE Photogrammetric Computer Vision; Unmanned Aerial Vehicles; Image-based
   3D Reconstruction; Mapping; Georegistration; Accuracy Evaluation;
   Structure from Motion; Land Surveying; Mining; Archeology
AB During the last decades photogrammetric computer vision systems have been well established in scientific and commercial applications. Especially the increasing affordability of unmanned aerial vehicles (UAVs) in conjunction with automated multi-view processing pipelines have resulted in an easy way of acquiring spatial data and creating realistic and accurate 3D models. With the use of multi-copter UAVs, it is possible to record highly overlapping images from almost terrestrial camera positions to oblique and nadir aerial images due to the ability to navigate slowly, hover and capture images at nearly any possible position. Multi-copter UAVs thus are bridging the gap between terrestrial and traditional aerial image acquisition and are therefore ideally suited to enable easy and safe data collection and inspection tasks in complex or hazardous environments. In this paper we present a fully automated processing pipeline for precise, metric and geo-accurate 3D reconstructions of complex geometries using various imaging platforms. Our workflow allows for georeferencing of UAV imagery based on GPS-measurements of camera stations from an on-board GPS receiver as well as tie and control point information. Ground control points (GCPs) are integrated directly in the bundle adjustment to refine the georegistration and correct for systematic distortions of the image block. We discuss our approach based on three different case studies for applications in mining and archaeology and present several accuracy related analyses investigating georegistration, camera network configuration and ground sampling distance. Our approach is furthermore suited for seamlessly matching and integrating images from different view points and cameras (aerial and terrestrial as well as inside views) into one single reconstruction. Together with aerial images from a UAV, we are able to enrich 3D models by combining terrestrial images as well inside views of an object by joint image processing to generate highly detailed, accurate and complete reconstructions.
C1 [Tscharf, Alexander; Mayer, Gerhard] Univ Leoben, Chair Min Engn & Mineral Econ, Leoben, Austria.
   [Rumpler, Markus; Fraundorfer, Friedrich; Bischof, Horst] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
RP Tscharf, A (corresponding author), Univ Leoben, Chair Min Engn & Mineral Econ, Leoben, Austria.
EM alexander.tscharf@unileoben.ac.at; rumpler@icg.tugraz.at;
   fraundorfer@icg.tugraz.at; gerhard.mayer@unileoben.ac.at;
   bischof@icg.tugraz.at
OI Bischof, Horst/0000-0002-9096-6671
CR Agarwal S., 2012, CERES SOLVER
   Bischof, 2014, ISPRS ANN PHOTOGRAMM, V3, P135, DOI [DOI 10.5194/ISPRSANNALS-II-3-135-2014, 10.5194/isprsannals-II-3-135-2014]
   Daftry S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.19
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fraser C.S., 1996, CLOSE RANGE PHOTOGRA, P256
   Furukawa Y., 2009, IEEE T PATTERN ANAL
   Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Henrik S, 2006, P IEEE COMPUTER SOC, P2161, DOI DOI 10.1109/CVPR.2006.264
   Hirschmueller H., 2005, IEEE C COMP VIS PATT
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Irschara A, 2012, ISPRS ANN PHOTOGRAMM
   Kraus K., 1994, PHOTOGRAMMETRIE
   Labatut P, 2007, IEEE I CONF COMP VIS, P504
   Leberl F., 2010, PHOTOGRAMMETRIC ENG
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Moser P., 2006, ISEE 2006, P80
   Nister D, 2003, PROC CVPR IEEE, P195
   Pfeifer Norbert, 2012, INT ARCH PHOTOGRAMME
   REHAK M, 2013, REMOTE SENSING SPATI, P317
   Rumpler M., 2011, 35 WORKSH AUSTR ASS
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Triggs Bill, 1999, P 1999 INT WORKSH VI, P298, DOI DOI 10.1007/3-540-44480-7_21
   Waechter M, 2014, LECT NOTES COMPUT SC, V8693, P836, DOI 10.1007/978-3-319-10602-1_54
   Watson GA, 2006, J COMPUT APPL MATH, V197, P387, DOI 10.1016/j.cam.2005.06.047
   Zeisl B, 2009, BRIT MACH VIS C BMVC
NR 26
TC 21
Z9 22
U1 1
U2 11
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
PY 2015
BP 15
EP 22
DI 10.5194/isprsannals-II-1-W1-15-2015
PG 8
WC Engineering, Aerospace; Geography, Physical; Remote Sensing; Imaging
   Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA BF4BN
UT WOS:000380615100003
OA Green Accepted, gold, Green Submitted
DA 2022-02-10
ER

PT J
AU Gaston, AJ
   Pelletier, M
   Eberl, C
   Mallory, ML
AF Gaston, Anthony J.
   Pelletier, Mia
   Eberl, Christine
   Mallory, Mark L.
TI Incubation shifts of northern fulmars Fulmarus glacialis in the Canadian
   high Arctic determined by digital photography
SO POLAR BIOLOGY
LA English
DT Article
DE Breeding; Incubation; Fulmarus glacialis; Northern fulmar; Monitoring
ID ECOLOGY; NUNAVUT; BIRDS
AB Determining the incubation pattern of northern fulmars (Fulmarus glacialis) at Canadian Arctic colonies is difficult, in part due to challenges with accessing nests and due, in part, to similarities in colour morphs which can be hard to distinguish. We used digital photographs of incubating birds taken similar to daily at 49 breeding sites during the second half of the incubation period and compared inter-observer variation in distinguishing incubation shifts by each member of the pair. Using this method gave similar results when different subsets of sites were included and the assessment of incubation shifts by three different people produced similar results, yielding a mean shift duration of 6.4 days. Provided pairs with suitable variation in plumage or bill colour are selected, our results demonstrate that a remote camera technique can provide precise data to assess fulmar responses to annual environmental conditions for feeding and breeding.
C1 [Gaston, Anthony J.; Eberl, Christine] Carleton Univ, Natl Wildlife Res Ctr, Environm Canada, Ottawa, ON K1A 0H3, Canada.
   [Pelletier, Mia] Canadian Wildlife Serv, Iqaluit, NU, Canada.
   [Mallory, Mark L.] Acadia Univ, Dept Biol, Wolfville, NS B0P 1X0, Canada.
RP Gaston, AJ (corresponding author), Carleton Univ, Natl Wildlife Res Ctr, Environm Canada, Ottawa, ON K1A 0H3, Canada.
EM tony.gaston@ec.gc.ca
RI Mallory, Mark L/A-1952-2017
OI Mallory, Mark L/0000-0003-2744-3437
FU Environment Canada (Canadian Wildlife Service, Science & Technology
   Branch); Aboriginal Affairs and Northern Development Canada (Northern
   Contaminants Program); Natural Resources Canada (Polar Continental Shelf
   Project)Natural Resources Canada; National Science and Engineering
   Research CouncilNatural Sciences and Engineering Research Council of
   Canada (NSERC)
FX We thank Paul Woodard for assistance in 2012 and members of the PLI
   field crews in previous years for their great work. Assistance with
   photograph editing and sorting was provided by Justin Buller. Financial
   and logistic support were provided by Environment Canada (Canadian
   Wildlife Service, Science & Technology Branch), Aboriginal Affairs and
   Northern Development Canada (Northern Contaminants Program), Natural
   Resources Canada (Polar Continental Shelf Project), and the National
   Science and Engineering Research Council. Research was conducted under
   permits NUN-MBS-12-03, NUN-SCI-12-04, GN WRP 2012-040, N2003J0014 and
   3BCPLI1217.
CR Edwards EWJ, 2013, DEEP-SEA RES PT II, V98, P438, DOI 10.1016/j.dsr2.2013.04.011
   Falk K, 1997, IBIS, V139, P270, DOI 10.1111/j.1474-919X.1997.tb04625.x
   Gaston AJ, 2006, ARCTIC, V59, P165
   Gaston AJ, 2005, ECOGRAPHY, V28, P331, DOI 10.1111/j.0906-7590.2005.04179.x
   HATCH SA, 1991, CONDOR, V93, P409, DOI 10.2307/1368957
   HATCH SA, 1990, CAN J ZOOL, V68, P1664, DOI 10.1139/z90-247
   Mallory ML, 2008, MAR BIOL, V154, P1031, DOI 10.1007/s00227-008-0994-z
   Mallory ML, 2007, CONDOR, V109, P894, DOI 10.1650/0010-5422(2007)109[894:DSICTB]2.0.CO;2
   Mallory ML, 2006, ENVIRON REV, V14, P187, DOI [10.1139/A06-003, 10.1139/a06-003]
   Mallory ML, 2009, J ORNITHOL, V150, P175, DOI 10.1007/s10336-008-0332-8
   Mallory ML, 2012, BIRDS N AM ONLINE, DOI [10.2073/bna.361, DOI 10.2073/BNA.361]
   Mougin J. L., 1967, Oiseau et la Revue Francaise d'Ornithologie, V37, P57
   STATSOFT Inc, 2005, STAT DAT AN SOFTW SY
   WELCH HE, 1992, ARCTIC, V45, P343
   WILLIAMSON KENNETH, 1952, SCOTTISH NAT, V64, P138
NR 15
TC 2
Z9 2
U1 0
U2 22
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0722-4060
EI 1432-2056
J9 POLAR BIOL
JI Polar Biol.
PD FEB
PY 2014
VL 37
IS 2
BP 261
EP 267
DI 10.1007/s00300-013-1429-y
PG 7
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA 290YN
UT WOS:000329794700010
DA 2022-02-10
ER

PT J
AU Xu, BB
   Wang, WS
   Falzon, G
   Kwan, P
   Guo, LF
   Chen, GP
   Tait, A
   Schneider, D
AF Xu, Beibei
   Wang, Wensheng
   Falzon, Greg
   Kwan, Paul
   Guo, Leifeng
   Chen, Guipeng
   Tait, Amy
   Schneider, Derek
TI Automated cattle counting using Mask R-CNN in quadcopter vision system
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
LA English
DT Article
DE Object detection; Deep learning; Remote monitoring; Livestock
   management; Quadcopter vision system
ID OBJECT DETECTION; AERIAL IMAGES; ANIMALS; DETECT
AB The accurate and reliable counting of animals in quadcopter acquired imagery is one of the most promising but challenging tasks in intelligent livestock management in the future. In this paper we demonstrate the application of the cutting-edge instance segmentation framework, Mask R-CNN, in the context of cattle counting in different situations such as extensive production pastures and also in intensive housing such as feedlots. The optimal IoU threshold (0.5) and the full-appearance detection for the algorithm in this study are verified through performance evaluation. Experimental results in this research show the framework's potential to perform reliably in offline quadcopter vision systems with an accuracy of 94% in counting cattle on pastures and 92% in feedlots. Compared with the existing typical competing algorithms, Mask R-CNN outperforms both in the counting accuracy and average precision especially on the datasets with occlusion and overlapping. Our research shows promising steps towards the incorporation of artificial intelligence using quadcopters for enhanced management of animals.
C1 [Xu, Beibei; Wang, Wensheng; Guo, Leifeng] Chinese Acad Agr Sci, Agr Informat Inst, Beijing 100086, Peoples R China.
   [Falzon, Greg; Schneider, Derek] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia.
   [Falzon, Greg; Schneider, Derek] Univ New England, Precis Agr Res Grp, Armidale, NSW 2351, Australia.
   [Kwan, Paul] Melbourne Inst Technol, Sch Informat Technol & Engn, Melbourne, Vic 3000, Australia.
   [Chen, Guipeng] Jiangxi Acad Agr Sci, Agr Econ & Informat Inst, Nanchang 330200, Jiangxi, Peoples R China.
   [Tait, Amy] Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2351, Australia.
RP Wang, WS (corresponding author), Chinese Acad Agr Sci, Agr Informat Inst, Beijing 100086, Peoples R China.
EM wangwensheng@caas.cn
RI Falzon, Greg A/A-2657-2012
OI Falzon, Greg A/0000-0002-1989-9357; , Beibei/0000-0001-5804-2906
FU Beijing Aokemei Technical Service Company Limited; Fundamental Research
   Funds of Agricultural Information Institute of Chinese Academy of
   Agriculture Sciences, China [JBYW-AII-2019-19]; General Project of
   Jiangxi Province Key Research and Development Plan [20192BBF60053];
   Jiangxi Province Science Foundation for Youths [20192ACBL21023]; Meat
   and Livestock Australia (MLA) (University of New England Animal Ethics)
   [AEC18-308]
FX This research was funded by Beijing Aokemei Technical Service Company
   Limited and also was supported by Fundamental Research Funds of
   Agricultural Information Institute of Chinese Academy of Agriculture
   Sciences, China (JBYW-AII-2019-19), General Project of Jiangxi Province
   Key Research and Development Plan (20192BBF60053) and Jiangxi Province
   Science Foundation for Youths (20192ACBL21023). Imagery of the feedlot
   animals was provided by a University of New England project funded by
   Meat and Livestock Australia (MLA) (University of New England Animal
   Ethics Approval Number AEC18-308) and we are grateful to three private
   farmlands in New England in Australia for their kindly support with data
   collection (University of New England Standard Operating Procedure W14
   Camera Traps and Animal Ethics Approval Number AEC19-009).
CR Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
   Andrew W, 2017, IEEE INT CONF COMP V, P2850, DOI 10.1109/ICCVW.2017.336
   Barbedo JGA, 2018, OUTLOOK AGR, V47, P214, DOI 10.1177/0030727018781876
   Bengio Yoshua, 2009, P 26 ANN INT C MACH, P41, DOI [10.1145/1553374.1553380, DOI 10.1145/1553374.1553380]
   Buric M, 2018, 2018 41ST INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1034, DOI 10.23919/MIPRO.2018.8400189
   Chabot, 2009, SYSTEMATIC EVALUATIO
   Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chamoso P., 2014, AMBIENT INTELLIGENCE, P71, DOI DOI 10.1007/978-3-319-07596-9_8
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   CHRETIEN L.-P., 2015, INT ARCH PHOTOGRAMM, P241, DOI DOI 10.5194/ISPRSARCHIVES-XL-1-W4-241-2015
   Condon J., 2015, DRONES HOLD PROMISE
   Dalal N., 2021, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Danish M., 2018, THESIS TECHNOLOGICAL, DOI DOI 10.21427/D7S51F
   Descamps S, 2011, BIRD STUDY, V58, P302, DOI 10.1080/00063657.2011.588195
   Dolecheck KA, 2015, J DAIRY SCI, V98, P8723, DOI 10.3168/jds.2015-9645
   Frost AR, 1997, COMPUT ELECTRON AGR, V17, P139, DOI 10.1016/S0168-1699(96)01301-4
   GAO CQ, 2016, NEUROCOMPUTING, V208, P108, DOI DOI 10.1016/J.NEUC0M.2016.01.097
   GIRSHICK R, 2014, CVPR, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gonzalez LF, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010097
   Grenzdorffer G.J., 2013, REMOTE SENSING SPATI, V1, P169, DOI DOI 10.5194/ISPRSARCHIVES-XL-1-W2-169-2013
   Handcock RN, 2009, SENSORS-BASEL, V9, P3586, DOI 10.3390/s90503586
   He K., 2016, P IEEE C COMPUTER VI, P770, DOI DOI 10.1109/CVPR.2016.90
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
   Hollings T, 2018, METHODS ECOL EVOL, V9, P881, DOI 10.1111/2041-210X.12973
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Jail S., 2018, INT J ENG COMPUT SCI, V7, P23908
   Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
   Koski William R., 2009, Aquatic Mammals, V35, P347, DOI 10.1578/AM.35.3.2009.347
   Lee A, 2015, COMP DEEP NEURAL NET
   Lhoest S, 2015, INT ARCH PHOTOGRAMM, V40-3, P355, DOI 10.5194/isprsarchives-XL-3-W3-355-2015
   Li JL, 2007, PROCEEDINGS OF UK-CHINA SPORTS ENGINEERING WORKSHOP, P1
   Liaghat S., 2010, American Journal of Agricultural and Biological Sciences, V5, P50
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
   Marsh J.R., 2008, ASSESSMENT INJECTABL, P1, DOI [10.13031/2013.24845, DOI 10.13031/2013.24845]
   Mejias L., 2013, OCEANS SAN DIEGO 201, DOI [10. 23919/oceans. 2013. 6741088, DOI 10.23919/OCEANS.2013.6741088]
   Neethirajan Suresh, 2017, Sensing and Bio-Sensing Research, V12, P15, DOI 10.1016/j.sbsr.2016.11.004
   Neethirajan S, 2017, BIOSENS BIOELECTRON, V98, P398, DOI 10.1016/j.bios.2017.07.015
   Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pathare S.P., 2015, DETECTION BLACK BACK
   Qiao Y., 2019, COMPUT ELECTRON AGR, V165, P54
   Radovic M, 2017, J IMAGING, V3, DOI 10.3390/jimaging3020021
   Redmon J., 2018, ARXIV PREPRINT ARXIV
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
   Rivas A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072048
   Ruiz-Garcia L, 2009, SENSORS-BASEL, V9, P4728, DOI 10.3390/s90604728
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sa I, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16081222
   Sadgrove EJ, 2018, COMPUT IND, V98, P183, DOI 10.1016/j.compind.2018.03.014
   Sadgrove EJ, 2017, COMPUT ELECTRON AGR, V139, P204, DOI 10.1016/j.compag.2017.05.017
   Schneider S., 2018, ARXIV180310842
   Sellier N., 2014, AM J AGR SCI TECHNOL, DOI 10.7726/ajast.2014.1008
   Sommer L, 2018, IEEE WINT CONF APPL, P635, DOI 10.1109/WACV.2018.00075
   Stein M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111915
   Sun, 2017, ARXIV171107264
   Tian F. Y., 2013, T CHIN SOC AGR MACHI, V44, P277
   Van Nuffel A, 2015, ANIMALS, V5, P861, DOI 10.3390/ani5030388
   Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
   Viazzi S, 2013, J DAIRY SCI, V96, P257, DOI 10.3168/jds.2012-5806
   Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
   ZHANG W, 1990, APPL OPTICS, V29, P4790, DOI 10.1364/AO.29.004790
   Zhang W., 1988, P ANN C JPN SOC APPL
   Zhou D., 2014, REAL TIME ANIMAL DET
NR 69
TC 31
Z9 32
U1 8
U2 13
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0168-1699
EI 1872-7107
J9 COMPUT ELECTRON AGR
JI Comput. Electron. Agric.
PD APR
PY 2020
VL 171
AR 105300
DI 10.1016/j.compag.2020.105300
PG 12
WC Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Computer Science
GA LC4VS
UT WOS:000525324500013
OA hybrid
DA 2022-02-10
ER

PT J
AU Mohan, L
   Menon, V
AF Mohan, Lakshmi
   Menon, Vivek
TI Modelling large scale camera networks for identification and tracking:
   an abstract framework
SO IET COMPUTER VISION
LA English
DT Article
ID SURVEILLANCE SYSTEMS
AB In this study, the authors discuss a novel approach for multi-camera-based unobtrusive identification and tracking of occupants in wide-area, multi-building scenarios. Considering the scalability issues in adopting a centralised approach to monitor wide-area scenarios, they proposed a distributed approach to occupant identification and tracking. The key technical idea underlying their approach is to abstract a wide-area indoor surveillance environment using a distributed state transition system (DSTS) model, which in turn is composed of independent building-specific state transition systems, coordinating and collaborating with each other. This study presents the details of their DSTS model and examines the temporal ordering of recognition events within the DSTS for ensuring accurate state information and responses to spatio-temporal queries. They also provide an experimental evaluation of the performance of their model using precision-recall metrics. Their conclusion is that the DSTS model serves as an efficient mechanism for tracking occupants in wide-area, multi-building scenarios monitored by camera networks.
C1 [Mohan, Lakshmi; Menon, Vivek] Amrita Vishwa Vidyapeetham, Dept Comp Sci & Engn, Kollam 690525, Kerala, India.
RP Mohan, L (corresponding author), Amrita Vishwa Vidyapeetham, Dept Comp Sci & Engn, Kollam 690525, Kerala, India.
EM lakshmimohan@am.amrita.edu
RI Menon, Vivek/ABF-1377-2020
OI Menon, Vivek/0000-0003-1305-633X
FU Department of Science & Technology (DST), Government of IndiaDepartment
   of Science & Technology (India) [SR/WOS-A/ET-96/2017]
FX The authors acknowledge the Department of Science & Technology (DST),
   Government of India for financial support (vide reference
   SR/WOS-A/ET-96/2017) under the Women Scientists Scheme (WOS-A) to carry
   out this work.
CR An L, 2014, WIDE AREA SURVEILL, P123
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Chellappa R, 2011, HDB FACE RECOGNITION, P323, DOI DOI 10.1007/978-0-85729-932-1_13
   Cho Y, 2010, IEEE T CONSUM ELECTR, V56, P1997, DOI 10.1109/TCE.2010.5606357
   Dey AK, 2001, PERS UBIQUIT COMPUT, V5, P4, DOI 10.1007/s007790170019
   Feris RS, 2010, INTELLIGENT VIDEO SURVEILLANCE: SYSTEMS AND TECHNOLOGY, P47
   Fleuret F, 2014, ADV COMPUT VIS PATT, P309, DOI 10.1007/978-1-4471-6296-4_15
   Hong K, 2013, IEEE INT CON DIS, P309, DOI 10.1109/ICDCSW.2013.44
   Hou L, 2017, EURASIP J ADV SIG PR, DOI 10.1186/s13634-017-0482-z
   Kamal A, 2014, WIDE AREA SURVEILLAN, P207
   King D, HIGH QUALITY FACE RE
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Layne R, 2014, ADV COMPUT VIS PATT, P93, DOI 10.1007/978-1-4471-6296-4_5
   Liu HH, 2013, IEEE T IND INFORM, V9, P1222, DOI 10.1109/TII.2013.2255616
   Menon V, 2011, COMPUTER, V44, P73, DOI 10.1109/MC.2011.59
   Nappi M, 2018, IMAGE VISION COMPUT, V76, P27, DOI 10.1016/j.imavis.2018.05.001
   Narayan Neeti, 2018, P IEEE C COMP VIS PA, P1438
   Ramachandran U, 2012, P IEEE, V100, P878, DOI 10.1109/JPROC.2011.2182093
   Rinner B, 2008, P IEEE, V96, P1565, DOI 10.1109/JPROC.2008.928742
   ROSEBROCK A, OPENCV FACE RECOGNIT
   Schilit B.N., 1994, 1 WORKSH MOB COMP SY, P85, DOI DOI 10.1109/WMCSA.1994.16
   Sharif M., 2017, J ENG SCI TECHNOL RE, V10, P166
   Singhal, 2008, DISTRIBUTED COMPUTIN
   Song B, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940441
   Tsakanikas V, 2018, COMPUT ELECTR ENG, V70, P736, DOI 10.1016/j.compeleceng.2017.11.011
   Valera M, 2005, IEE P-VIS IMAGE SIGN, V152, P192, DOI 10.1049/ip-vis:20041147
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Zablocki M., 2014, J THEORETICAL APPL C, V8, P13
NR 28
TC 0
Z9 0
U1 0
U2 0
PU INST ENGINEERING TECHNOLOGY-IET
PI HERTFORD
PA MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN 1751-9632
EI 1751-9640
J9 IET COMPUT VIS
JI IET Comput. Vis.
PD OCT
PY 2020
VL 14
IS 7
BP 426
EP 433
DI 10.1049/iet-cvi.2019.0959
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA PE9ON
UT WOS:000598689800003
DA 2022-02-10
ER

PT J
AU Su, K
   Liu, PY
   Gu, L
   Chen, WZ
   Hwang, K
   Yu, ZB
AF Su, Kui
   Liu, Peiyu
   Gu, Liang
   Chen, Wenzhi
   Hwang, Kai
   Yu, Zhibin
TI vMobiDesk: Desktop Virtualization for Mobile Operating Systems
SO IEEE ACCESS
LA English
DT Article
DE Servers; Smart phones; Virtualization; Operating systems; Cloud
   computing; Security; Mobile applications; Mobile cloud computing; mobile
   device; BYOD; virtual desktop; android
ID SECURITY
AB Smart mobile devices have significantly increased the popularity of Bring-Your-Own-Device (BYOD) at work, as they benefit people's daily lives. However, BYOD comes with several challenging issues such as limited hardware capacity, frequent upgrades of applications, and security and privacy concerns. Virtual Mobile Infrastructure (VMI), a general framework that provides more reliable and secure solution for BYOD, has therefore been proposed. The key of VMI is to host a mobile Operating System (OS) on a remote cloud data center, and run mobile applications on it. However, VMI faces performance challenge as it needs to display the entire virtualized desktop on a mobile device while the real content of the desktop is on a remote server. To address the performance challenge, we design and implement a VMI named vMobiDesk on top of Android with optimized network transfer mechanisms and display virtualization. In particular, vMobiDesk focuses on virtualizing the display of Android desktops, redirecting users' input events, providing audio support and remote camera. The experimental results show that vMobiDesk has low virtualization overhead, as well as enables mobile users to obtain good experiences with BYOD applications.
C1 [Su, Kui; Yu, Zhibin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Su, Kui; Chen, Wenzhi] Sangfor Technol Inc, Shenzhen 518055, Peoples R China.
   [Liu, Peiyu] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou 310058, Peoples R China.
   [Gu, Liang] Zhejiang Univ, Hangzhou 310058, Peoples R China.
   [Hwang, Kai] Chinese Univ Hong Kong, Comp Sci & Engn, Shenzhen 518172, Peoples R China.
RP Yu, ZB (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM zb.yu@siat.ac.cn
FU Shenzhen Institute of Artificial Intelligence and Robotics for Society
   at The Chinese University of Hong Kong, Shenzhen
FX This work was supported by the Shenzhen Institute of Artificial
   Intelligence and Robotics for Society at The Chinese University of Hong
   Kong, Shenzhen.
CR Andrus J, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P173
   [Anonymous], 2017, NUMB ANDR APPL
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Ballagas Rafael, 2004, P WORKSH UB DISPL EN
   Baratto R. A., 2004, P 10 ANN INT C MOB C, P1, DOI DOI 10.1145/1023720.1023722
   Baratto Ricardo A, 2005, ACM SIGOPS OPERATING, V39, P277, DOI DOI 10.1145/1095810.1095837
   Barham P., 2003, ACM SIGOPS OPERATING, V37, P164, DOI DOI 10.1145/1165389.945462
   Becher M, 2011, P IEEE S SECUR PRIV, P96, DOI 10.1109/SP.2011.29
   Cai W, 2013, 2013 IEEE SEVENTH INTERNATIONAL SYMPOSIUM ON SERVICE-ORIENTED SYSTEM ENGINEERING (SOSE 2013), P551, DOI 10.1109/SOSE.2013.30
   Casey K., 2013, TECH REP
   Chen WZ, 2015, IEEE T COMPUT, V64, P2741, DOI 10.1109/TC.2015.2389791
   Chen X, 2015, IEEE T PARALL DISTR, V26, P974, DOI 10.1109/TPDS.2014.2316834
   Dinh HT, 2013, WIREL COMMUN MOB COM, V13, P1587, DOI 10.1002/wcm.1203
   Ferrazzoli D, 2015, PARKINSONS DIS-US, V2015, DOI 10.1155/2015/520128
   Guan H., 2014, ACM T ARCHIT CODE OP, V11, P1
   Habib I., 2008, LINUX J, V2008, P8
   Helsley M., 2009, IBM DEVLOPERWORKS TE, P11
   Huang D, 2011, IEEE COMSOC MULTIMED, V6, P27
   Huang JJ, 2014, 36TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2014), P1036, DOI 10.1145/2568225.2568301
   La Polla M, 2013, IEEE COMMUN SURV TUT, V15, P446, DOI 10.1109/SURV.2012.013012.00028
   Li Q, 2013, IEEE SECUR PRIV, V11, P78, DOI 10.1109/MSP.2013.15
   Mell P, 2010, COMMUN ACM, V53, P50
   MILLER KW, 2012, IT PROF, V14, P53
   Morrow Bill, 2012, Network Security, V2012, P5, DOI 10.1016/S1353-4858(12)70111-3
   Ng A., 2017, TECH REP
   Protalinski E., 2017, TECH REP
   Richardson IE, 2004, H 264 MPEG 4 VIDEO C
   Richardson T, 1998, IEEE INTERNET COMPUT, V2, P33, DOI 10.1109/4236.656066
   Shi S., 2011, P 19 ACM INT C MULT, P103
   Shuja J, 2016, J NETW COMPUT APPL, V75, P335, DOI 10.1016/j.jnca.2016.08.021
   Shuja J, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2897164
   Song YJ, 2014, COMPUT EDUC, V74, P50, DOI 10.1016/j.compedu.2014.01.005
   Statt N., 2017, TECH REP
   Suzuki Y., 2014, PROC USENIX ANN TECH, P109
   Tian L, 2014, PROPERTY RIGHTS, LAND VALUES AND URBAN DEVELOPMENT: BETTERMENT AND COMPENSATION IN CHINA, P121
   Yang CT, 2014, J SUPERCOMPUT, V68, P183, DOI 10.1007/s11227-013-1034-4
NR 36
TC 0
Z9 0
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2020
VL 8
BP 213541
EP 213553
DI 10.1109/ACCESS.2020.3041304
PG 13
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA PC7WS
UT WOS:000597207400001
OA gold
DA 2022-02-10
ER

PT C
AU Murphy, FE
   Donovan, M
   Cunningham, J
   Jezequel, T
   Garcia, E
   Jaeger, A
   McCarthy, J
   Popovici, EM
AF Murphy, Fiona Edwards
   Donovan, Michelle
   Cunningham, James
   Jezequel, Tristan
   Garcia, Enrique
   Jaeger, Alex
   McCarthy, John
   Popovici, Emanuel M.
GP IEEE
TI i4Toys: Video Technology in Toys for Improved Access to Play,
   Entertainment, and Education
SO 2015 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGY AND SOCIETY (ISTAS)
SE IEEE International Symposium on Technology and Society
LA English
DT Proceedings Paper
CT IEEE International Symposium on Technology and Society (ISTAS)
CY NOV 11-12, 2015
CL Dublin, IRELAND
SP IEEE
DE Embedded Systems; Wireless Sensor Networks (WSN); Smart Toys; Low Power;
   Smart Homes; Assistive Technologies; Thermal Imaging; Internet of Things
   (IoT)
AB Toys have a very important role in society. Play is a vitally part of every child's life, providing not only entertainment but also influencing the psychological, physiological and social development of a child. Throughout the years, toys have reflected every generation's advances in technology. In this paper wireless technologies, low power computing and sensing are used to expand the capabilities of existing toys. The proposed framework is a first step towards vision enabled gesture recognition, emotion assessment, as well as providing useful physiological feedback such as temperature change or allergic reactions. The toys are equipped with versatile interfaces which allow children with a wide range of disabilities to interact with them. The solution retrofits low cost toy robots with state of the art sensors (PiR, temperature, and microphone), processing capabilities, and wireless technology. A computer vision system for location and control of the toys has also been also developed. This allows one or more toys to work in conjunction with a remote camera to interact with their environment. Two deployment scenarios are presented showing the feasibility of the platform. The first deployment involves using the thermal imaging and PiR sensors to detect "Santa" and capture a thermal recording of him. The second deployment involves using a fixed camera in a room for real time tracking and control of toys. The aim is to provide a platform which can provide robot-human and robot-robot interactions using vision sensors.
C1 [Murphy, Fiona Edwards; Donovan, Michelle; Cunningham, James; Jezequel, Tristan; Garcia, Enrique; Jaeger, Alex; Popovici, Emanuel M.] Univ Coll Cork, Dept Elect & Elect Engn, Cork, Ireland.
   [McCarthy, John] Univ Coll Cork, Sch Appl Psychol, Cork, Ireland.
RP Murphy, FE (corresponding author), Univ Coll Cork, Dept Elect & Elect Engn, Cork, Ireland.
CR Aghajan, 2006, P 4 ACM INT WORKSH V, P145, DOI DOI 10.1145/1178782.1178804
   Buchholz, 1999, EARLY CHILD DEV CARE, V155, P39
   Corbellini G, 2014, IEEE COMMUN MAG, V52, P72, DOI 10.1109/MCOM.2014.6852086
   Duc-Minh Pham, 2013, 2013 International Conference on Control, Automation and Information Sciences (ICCAIS), P257, DOI 10.1109/ICCAIS.2013.6720564
   Ebisch SJ, 2012, BIOL PSYCHOL, V89, P123, DOI 10.1016/j.biopsycho.2011.09.018
   Edwards F. Murphy, 2014, INT WORKSHOP ROBOTIC
   Edwards F. Murphy, 2014, ROYAL IRISH ACAD RES
   Fine G., 2009, P 1 ACM SIGMM INT WO, P19
   Frost J.L., 1992, PLAY AND PLAYSCAPES
   Guerrero P., 2006, 3 INT WORKSH PERV GA, P37
   Hoonhout H. C. M., 2006, P 1 FUN GAM C
   Larson E, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2565
   Rubin K.H., 1998, MULTIPLE PERSPECTIVE, P144
   Shirazi Alireza Sahami, 2014, CHI 14, P3483
   Smirnova EO, 2011, Psikhol Nauk Obrazov, P5
   Srivastava M., 2001, SMART KINDERGARTEN S, P132, DOI DOI 10.1145/381677..381690
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Zhang SW, 2012, IEEE INT C AUTOMAT L, P386, DOI 10.1109/ICAL.2012.6308240
NR 18
TC 4
Z9 4
U1 0
U2 12
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2158-3404
BN 978-1-4799-8283-7
J9 INT SYMP TECHNOL SOC
PY 2015
PG 6
WC Computer Science, Interdisciplinary Applications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BF1EF
UT WOS:000380381400028
DA 2022-02-10
ER

PT J
AU Shirazi, MS
   Morris, BT
AF Shirazi, Mohammad Shokrolah
   Morris, Brendan Tran
TI Investigation of safety analysis methods using computer vision
   techniques
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE safety analysis methods; vision-based tracking system; conflict events;
   real time
ID PEDESTRIAN SAFETY; INTERSECTIONS; BEHAVIOR; FRAMEWORK; TRACKING
AB This work investigates safety analysis methods using computer vision techniques. The vision-based tracking system is developed to provide the trajectory of road users including vehicles and pedestrians. Safety analysis methods are developed to estimate time to collision (TTC) and postencroachment time (PET) that are two important safety measurements. Corresponding algorithms are presented and their advantages and drawbacks are shown through their success in capturing the conflict events in real time. The performance of the tracking system is evaluated first, and probability density estimation of TTC and PET are shown for 1-h monitoring of a Las Vegas intersection. Finally, an idea of an intersection safety map is introduced, and TTC values of two different intersections are estimated for 1 day from 8:00 a.m. to 6:00 p.m. (C) 2017 SPIE and IS&T
C1 [Shirazi, Mohammad Shokrolah] Cleveland State Univ, Elect Engn & Comp Sci Dept, Cleveland, OH 44115 USA.
   [Morris, Brendan Tran] Univ Nevada, Dept Elect & Comp Engn, Las Vegas, NV 89154 USA.
RP Shirazi, MS (corresponding author), Cleveland State Univ, Elect Engn & Comp Sci Dept, Cleveland, OH 44115 USA.
EM m.shokrolahshirazi@csuohio.edu
OI Morris, Brendan/0000-0002-8592-8806
FU Nevada Department of Transportation
FX The authors thank the Nevada Department of Transportation for supporting
   the research project: "An Automated Camera-Based Pedestrian-Vehicle
   Conflict Evaluation System."
CR Alhajyaseen WKM, 2012, IATSS RES, V36, P66, DOI 10.1016/j.iatssr.2012.03.002
   Amundsen F, 1977, P 1 WORKSH TRAFF CON
   Buch N, 2011, IEEE T INTELL TRANSP, V12, P920, DOI 10.1109/TITS.2011.2119372
   Chin HC, 1997, SAFETY SCI, V26, P169, DOI 10.1016/S0925-7535(97)00041-6
   de Leur P, 2003, CAN J CIVIL ENG, V30, P711, DOI 10.1139/L03-034
   Hayward J.C., 1972, NEAR MISS DETERMINAT, P24
   Hu WM, 2004, IEEE T VEH TECHNOL, V53, P677, DOI 10.1109/TVT.2004.825772
   Hussein M, 2015, TRANSPORT RES REC, P17, DOI 10.3141/2519-03
   Ismail K, 2010, TRANSPORT RES REC, P52, DOI 10.3141/2198-07
   Ismail K, 2009, TRANSPORT RES REC, P44, DOI 10.3141/2140-05
   Li S, 2012, TRANSPORT RES REC, P121, DOI 10.3141/2299-13
   Morris B, 2009, PROC CVPR IEEE, P312, DOI 10.1109/CVPRW.2009.5206559
   Perkins S., 1966, CRITERIA TRAFFIC CON
   Pin C, 2015, TRANSPORT RES REC, P58, DOI 10.3141/2514-07
   Sacchi E, 2016, J TRANSP SAF SECUR, V8, P266, DOI 10.1080/19439962.2015.1030807
   Sayed T., 2013, TRANSPORTATION RES B
   Sayed T, 2012, TRANSPORT RES REC, P18, DOI 10.3141/2280-03
   Shen XH, 2013, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2013.444
   Shirazi MS, 2017, IEEE T INTELL TRANSP, V18, P4, DOI 10.1109/TITS.2016.2568920
   Shirazi MS, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.5.051203
   Shirazi MS, 2016, INT J ARTIF INTELL T, V25, DOI 10.1142/S0218213016400042
   Shirazi MS, 2015, LECT NOTES COMPUT SC, V9474, P752, DOI 10.1007/978-3-319-27857-5_67
   Shirazi MS, 2016, IEEE INTEL TRANSP SY, V8, P23, DOI 10.1109/MITS.2015.2477474
   Shirazi MS, 2015, IEEE INT VEH SYM, P1264, DOI 10.1109/IVS.2015.7225856
   Shirazi MS, 2015, IEEE INT VEH SYM, P1258, DOI 10.1109/IVS.2015.7225855
   Shirazi MS, 2014, LECT NOTES COMPUT SC, V8887, P708, DOI 10.1007/978-3-319-14249-4_68
   Shirazi MS, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P3100, DOI 10.1109/ITSC.2014.6958188
   Svensson A, 2006, ACCIDENT ANAL PREV, V38, P379, DOI 10.1016/j.aap.2005.10.009
   Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7
   Zaki MH, 2013, TRANSPORT RES REC, P75, DOI 10.3141/2393-09
   Zangenehpour S, 2016, ACCIDENT ANAL PREV, V86, P161, DOI 10.1016/j.aap.2015.10.025
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 32
TC 2
Z9 2
U1 0
U2 3
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD SEP
PY 2017
VL 26
IS 5
AR 051404
DI 10.1117/1.JEI.26.5.051404
PG 11
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Optics; Imaging Science & Photographic Technology
GA FL5DJ
UT WOS:000414251400005
DA 2022-02-10
ER

PT C
AU Gale, DM
   Leon-Huerta, A
   Cuevas, LC
   Santos, DC
   Rios, EH
   Alvarez, ML
   Sosa, ET
   Torres, CT
   Sanchez-Arguelles, D
   Narayanan, G
   Schloerb, FP
   Wilson, GW
   Smith, DR
AF Gale, David M.
   Leon-Huerta, Andrea
   Cabrera Cuevas, Lizeth
   Castro Santos, David
   Hernandez Rios, Emilio
   Lucero Alvarez, Maribel
   Tecuapetla Sosa, Esteban
   Tzile Torres, Carlos
   Sanchez-Arguelles, David
   Narayanan, Gopal
   Schloerb, F. Peter
   Wilson, Grant W.
   Smith, David R.
BE Navarro, R
   Burge, JH
TI Mapping the Large Millimeter Telescope primary reflector using
   photogrammetry; a first comparison with 12 GHz holography
SO ADVANCES IN OPTICAL AND MECHANICAL TECHNOLOGIES FOR TELESCOPES AND
   INSTRUMENTATION II
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Advances in Optical and Mechanical Technologies for
   Telescopes and Instrumentation II
CY JUN 26-JUL 01, 2016
CL Edinburgh, UNITED KINGDOM
SP SPIE
DE LMT; GTM; radio-telescope; primary surface measurement; photogrammetry;
   holography; laser tracker
AB The Large Millimeter Telescope (LMT) makes extensive use of 12 GHz holography during maintenance periods to finetune the alignment of primary reflector segments to the best-fit design parabola. Tracker measurements have also been used for this task, however the technique is severely limited by environmental noise and large data collection times, on the order of many hours for a single map. In 2015 we started photogrammetry trials as a complimentary measurement technique. Photogrammetry can offer reduced mapping times compared with laser trackers, and like holography, allows maps to be made at arbitrary elevation angles. Depending on the placement of reflecting targets, the technique can also provide higher spatial resolution than currently achieved using our holography system.
   Accurate photogrammetry requires a robust strategy for the incorporation of multiple camera stations, a task complicated by the size of the antenna, obstructions of the surface by the subreflector and tetrapod legs, and the practicability of using the site tower crane as a moving camera platform. Image scaling is also a major consideration, since photogrammetry lacks any inherent distance reference. Therefore appropriate scale bars must be fabricated and located within the camera field of view. Additional considerations relate to the size and placement of reflective targets, and the optimization of camera settings. In this paper we present some initial comparisons of laser tracker, holography and photogrammetry measurements taken in 2015, showing clearly the status of alignment for distinct zones of the currently operating 32.5 m primary collecting area.
C1 [Gale, David M.; Leon-Huerta, Andrea; Cabrera Cuevas, Lizeth; Castro Santos, David; Hernandez Rios, Emilio; Lucero Alvarez, Maribel; Tecuapetla Sosa, Esteban; Tzile Torres, Carlos; Sanchez-Arguelles, David] Inst Nacl Astrofis Opt & Electr, Luis Enrique Erro 1, Puebla 72840, Mexico.
   [Narayanan, Gopal; Schloerb, F. Peter; Wilson, Grant W.] Univ Massachusetts, Dept Astron, 710 North Pleasant St, Amherst, MA 01003 USA.
   [Smith, David R.] MERLAB, 357 S Candler St, Decatur, GA 30030 USA.
RP Gale, DM (corresponding author), Inst Nacl Astrofis Opt & Electr, Luis Enrique Erro 1, Puebla 72840, Mexico.
EM dgale@inaoep.mx
OI Sanchez-Arguelles, David/0000-0002-7344-9920
CR ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Iglewicz B., 1993, DETECT HANDLE OUTLIE, V16
   Leon-Huerta A, 2013, PROC SPIE, V8788, DOI 10.1117/12.2020747
NR 3
TC 0
Z9 0
U1 0
U2 5
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-5106-0203-8; 978-1-5106-0204-5
J9 PROC SPIE
PY 2016
VL 9912
AR 99124F
DI 10.1117/12.2233843
PN 1
PG 15
WC Instruments & Instrumentation; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Instruments & Instrumentation; Optics
GA BG2XH
UT WOS:000387747900131
DA 2022-02-10
ER

PT J
AU Wilson, KL
   Wong, MC
   Devred, E
AF Wilson, Kristen L.
   Wong, Melisa C.
   Devred, Emmanuel
TI Branching Algorithm to Identify Bottom Habitat in the Optically Complex
   Coastal Waters of Atlantic Canada Using Sentinel-2 Satellite Imagery
SO FRONTIERS IN ENVIRONMENTAL SCIENCE
LA English
DT Article
DE habitat mapping; machine learning; rockweed; satellite remote sensing;
   seaweed; submerged aquatic vegetation; kelp; seagrass
ID EELGRASS ZOSTERA-MARINA; RESOLUTION SATELLITE; COLUMN CORRECTION;
   SEAGRASS COVER; NOVA-SCOTIA; BEDS; CLASSIFICATION; LANDSAT; BAY;
   ADAPTATION
AB Sentinel-2 satellite imagery has been successfully used to map submerged seagrasses in clear waters, and surface-canopy forming seaweed habitats in a range of water types. We examined the ability to use Sentinel-2 remote sensing reflectance to classify fully submerged seagrass and seaweed habitats in optically complex, temperate waters within a high priority management region in Atlantic Canada. To do so, we determined the "best" Sentinel-2 image available between 2015 and 2019 based on tidal height, absence of sun glint and clouds, and water transparency. Using the full Sentinel-2 tile, we atmospherically corrected the image using ACOLITE's dark spectrum fitting method. Our classification goal was a two-class prediction of vegetation presence and absence. Using information obtained from drop-camera surveys, the image was first partially classified using simple band thresholds based on the normalized difference vegetation index (NDVI), red/green ratio and the blue band. A random forest model was built to classify the remaining areas to a maximum depth of 10 m, the maximum depth at which field surveys were performed. The resulting habitat map had an overall accuracy of 79% and similar to 231 km(2) of vegetated habitat were predicted to occur (total area 345.15 km(2)). As expected, the classification performed best in regions dominated by bright sandy bare substrate, and dense dark vegetated beds. The classification performed less well in regions dominated by dark bare muddy substrate, whose spectra were similar to vegetated habitat, in pixels where vegetation density was low and mixed with other substrates, and in regions impacted by freshwater input. The maximum depth that bottom habitat was detectable also varied across the image. Leveraging the full capacity of the freely available Sentinel-2 satellite series with its high spatial resolution and resampling frequency, provides a new opportunity to generate large scale vegetation habitat maps, and examine how vegetation extent changes over time in Atlantic Canada, providing essential data layers to inform monitoring and management of macrophyte dominated habitats and the resulting ecosystem functions and services.
C1 [Wilson, Kristen L.; Wong, Melisa C.; Devred, Emmanuel] Fisheries & Oceans Canada, Bedford Inst Oceanog, Dartmouth, NS, Canada.
RP Wilson, KL (corresponding author), Fisheries & Oceans Canada, Bedford Inst Oceanog, Dartmouth, NS, Canada.
EM kristen.wilson@dfo-mpo.gc.ca
FU Fisheries and Oceans Canada Strategic Program for Ecosystem-Based
   Research and Advice (SPERA)
FX This study was funded by Fisheries and Oceans Canada Strategic Program
   for Ecosystem-Based Research and Advice (SPERA). The study also
   benefited from in-kind support from the Oceans and Coastal Management
   Division, Ecosystem Management Branch.
CR Barbier EB, 2011, ECOL MONOGR, V81, P169, DOI 10.1890/10-1510.1
   Barille L, 2010, AQUAT BOT, V92, P185, DOI 10.1016/j.aquabot.2009.11.006
   Barrell J, 2015, INT J REMOTE SENS, V36, P4069, DOI 10.1080/01431161.2015.1076208
   Beca-Carretero P, 2020, AQUAT CONSERV, V30, P1098, DOI 10.1002/aqc.3312
   Bell TW, 2020, REMOTE SENS ENVIRON, V238, DOI [10.1016/j.rse.2018.06.03, 10.1016/j.rse.2018.06.039]
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Caballero I, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030451
   Casal G, 2011, ESTUAR COAST SHELF S, V91, P371, DOI 10.1016/j.ecss.2010.10.024
   Casal G, 2020, ESTUAR COAST SHELF S, V241, DOI 10.1016/j.ecss.2020.106814
   Dattola L, 2018, PROC SPIE, V10784, DOI 10.1117/12.2326798
   DFO, 2019, 2019016 DFO
   DFO, 2009, 2009018 DFO
   Dierssen HM, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11141664
   Drusch M, 2012, REMOTE SENS ENVIRON, V120, P25, DOI 10.1016/j.rse.2011.11.026
   Duarte CM, 2013, NAT CLIM CHANGE, V3, P961, DOI [10.1038/nclimate1970, 10.1038/NCLIMATE1970]
   Duffy JE, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00317
   Fauzan M.A., 2017, INDONESIAN J GEOGRAP, V49, P195, DOI [10.22146/ijg.28407, DOI 10.22146/IJG.28407]
   Fethers J. O., 2018, THESIS
   Filbee-Dexter K, 2018, BIOSCIENCE, V68, P64, DOI 10.1093/biosci/bix147
   Foody GM, 2002, REMOTE SENS ENVIRON, V80, P185, DOI 10.1016/S0034-4257(01)00295-4
   Gamer M., 2019, IRR VARIOUS COEFFICI
   Garcia RA, 2020, REMOTE SENS ENVIRON, V249, DOI 10.1016/j.rse.2020.112015
   Hedley JD, 2005, INT J REMOTE SENS, V26, P2107, DOI 10.1080/01431160500034086
   Hijmans R. J, 2019, RASTER GEOGRAPHIC DA
   Hogrefe KR, 2014, REMOTE SENS-BASEL, V6, P12447, DOI 10.3390/rs61212447
   Hossain MS, 2015, INT J REMOTE SENS, V36, P61, DOI 10.1080/01431161.2014.990649
   Islam KA, 2020, DATA SCI ENG, V5, P111, DOI 10.1007/s41019-020-00126-0
   JOHNSON CR, 1988, ECOL MONOGR, V58, P129, DOI 10.2307/1942464
   Knudby A, 2014, INT J APPL EARTH OBS, V28, P90, DOI 10.1016/j.jag.2013.11.015
   Kotta J, 2018, ECOL EVOL, V8, P9086, DOI 10.1002/ece3.4463
   Kovacs E, 2018, REMOTE SENS LETT, V9, P686, DOI 10.1080/2150704X.2018.1468101
   Kuhn M., 2019, CARET CLASSIFICATION
   Kutser T, 2020, REMOTE SENS ENVIRON, V240, DOI 10.1016/j.rse.2019.111619
   Kutser T, 2009, REMOTE SENS ENVIRON, V113, P2267, DOI 10.1016/j.rse.2009.06.016
   Lauer M, 2008, OCEAN COAST MANAGE, V51, P495, DOI 10.1016/j.ocecoaman.2008.04.006
   Lee ZP, 1999, APPL OPTICS, V38, P3831, DOI 10.1364/AO.38.003831
   Leon AZ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12101572
   Leon-Perez MC, 2019, J COASTAL RES, V35, P937, DOI 10.2112/JCOASTRES-D-18-00106.1
   Leutner B., 2019
   LYZENGA DR, 1978, APPL OPTICS, V17, P379, DOI 10.1364/AO.17.000379
   Macdonald C., 2012, REPORT SUBMITTED ACA
   MacQueen J., 1967, P 5 BERKELEY S MATH, P281
   Marcello J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081208
   McKinna LIW, 2015, J GEOPHYS RES-OCEANS, V120, P1741, DOI 10.1002/2014JC010224
   Milton G. R., 2009, NOVA SCO DEP NAT RES, V32
   Mora-Soto A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12040694
   Mumby PJ, 1998, INT J REMOTE SENS, V19, P203, DOI 10.1080/014311698216521
   Murphy GEP, 2019, FACETS, V4, P210, DOI 10.1139/facets-2018-0044
   Ha NT, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030355
   O'Neill JD, 2013, REMOTE SENS ENVIRON, V133, P152, DOI 10.1016/j.rse.2013.02.010
   Orth RJ, 2006, BIOSCIENCE, V56, P987, DOI 10.1641/0006-3568(2006)56[987:AGCFSE]2.0.CO;2
   Poursanidis D, 2019, INT J APPL EARTH OBS, V80, P58, DOI 10.1016/j.jag.2019.03.012
   Poursanidis D, 2018, INT J REMOTE SENS, V39, P8670, DOI 10.1080/01431161.2018.1490974
   R Core Team, 2019, R LANG ENV STAT COMP
   Richards JA, 1986, REMOTE SENSING DIGIT
   Roelfsema CM, 2009, J SPAT SCI, V54, P45, DOI 10.1080/14498596.2009.9635166
   Roelfsema CM, 2014, REMOTE SENS ENVIRON, V150, P172, DOI 10.1016/j.rse.2014.05.001
   Schmidt AL, 2011, MAR ECOL PROG SER, V437, P51, DOI 10.3354/meps09276
   Schroeder SB, 2019, GLOB ECOL CONSERV, V19, DOI 10.1016/j.gecco.2019.e00683
   Simms EL, 2001, INT J REMOTE SENS, V22, P2083, DOI 10.1080/01431160116919
   St-Pierre AP, 2020, J EXP MAR BIOL ECOL, V522, DOI 10.1016/j.jembe.2019.151246
   Su LH, 2019, J MAR SCI ENG, V7, DOI 10.3390/jmse7040098
   Teagle H, 2017, J EXP MAR BIOL ECOL, V492, P81, DOI 10.1016/j.jembe.2017.01.017
   Traganos D, 2018, INT J REMOTE SENS, V39, P9428, DOI 10.1080/01431161.2018.1519289
   Traganos D, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10081227
   Traganos D, 2018, MAR POLLUT BULL, V134, P197, DOI 10.1016/j.marpolbul.2017.06.075
   Vahtmae E, 2020, J APPL REMOTE SENS, V14, DOI 10.1117/1.JRS.14.016504
   Vahtmae E, 2013, REMOTE SENS-BASEL, V5, P2451, DOI 10.3390/rs5052451
   Vandermeulen H, 2014, HELGOLAND MAR RES, V68, P559, DOI 10.1007/s10152-014-0412-5
   Vanhellemont Q., 2016, P 2016 ESA LIV PLAN, P9
   Vanhellemont Q, 2019, REMOTE SENS ENVIRON, V225, P175, DOI 10.1016/j.rse.2019.03.010
   Vapnick, 1995, NATURE STAT LEARNING
   Vercaemer B., 2018, Canadian Technical Report of Fisheries and Aquatic Sciences, V3249, P1
   Wang Y, 2010, AD HOC SENS WIREL NE, V10, P1
   Waycott M, 2009, P NATL ACAD SCI USA, V106, P12377, DOI 10.1073/pnas.0905620106
   Webster T., 2018, FINAL REPORT
   Webster T, 2020, BOT MAR, V63, P43, DOI 10.1515/bot-2018-0080
   Webster T, 2016, J COASTAL RES, P31, DOI 10.2112/SI76-004
   Wicaksono P, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111279
   Wicaksono P, 2018, INT J REMOTE SENS, V39, P5739, DOI 10.1080/01431161.2018.1506951
   Wickham H, 2019, **DATA OBJECT**
   Wilson K. L., 2019, CAN TECH REP FISH AQ, V3337
   Wilson KL, 2019, ESTUAR COAST SHELF S, V226, DOI 10.1016/j.ecss.2019.106292
   Wong MC, 2019, MAR BIOL, V166, DOI 10.1007/s00227-019-3488-2
   Wong MC, 2016, ESTUAR COAST, V39, P1785, DOI 10.1007/s12237-016-0121-1
   Yucel-Gier G, 2020, TURK J FISH AQUAT SC, V20, P571, DOI 10.4194/1303-2712-v20_7_07
   Zoffoli ML, 2014, SENSORS-BASEL, V14, P16881, DOI 10.3390/s140916881
NR 87
TC 2
Z9 2
U1 6
U2 8
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
EI 2296-665X
J9 FRONT ENV SCI-SWITZ
JI Front. Environ. Sci.
PD NOV 12
PY 2020
VL 8
AR 579856
DI 10.3389/fenvs.2020.579856
PG 19
WC Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA OV4YQ
UT WOS:000592217800001
OA gold
DA 2022-02-10
ER

PT J
AU Lee, CY
   Lin, SJ
   Lee, CW
   Yang, CS
AF Lee, Chao-Yang
   Lin, Shou-Jen
   Lee, Chen-Wei
   Yang, Chu-Sing
TI AN EFFICIENT CAMERA HAND-OFF FILTER IN REAL-TIME SURVEILLANCE TRACKING
   SYSTEM
SO INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL
LA English
DT Article
DE Seamless tracking; Handoff task; Camera assignment; Surveillance system;
   PTZ camera
ID FEATURES; MOTION
AB Recently, an automatic system extensively used for surveillance in wide area environments has been developed to continually track an individual while keep the individual centered in the field of view (FOV). There have long been some knotty problems in the tracking of moving object in computer vision. In a multi-camera surveillance system, both the handoff and assignment among cameras play an important role in generating an automated and persistent object tracking, a condition typical of most surveillance requirements. There has been a lack of studies in camera hand-off issue in PTZ-camera system. This paper investigates the application of automatic methods for tracking individual across cameras via a surveillance network. We propose a camera hand-off filter which can automatically provide an optimal capacity and a solution to perform the camera assignment efficiently. It comprises three fundamental components. Firstly, each camera determines time to trigger camera handoff process. Then, the execution of camera rotation is applied. Finally, the optimal camera should be selected. Moreover, our approach of individual tracking activates surveillance in cameras and achieves seamless tracking of high quality image. We develop an innovative low computational complexity handoff filter that can automatically carry out the camera assignment and handoff task. In the experiment, with a feasible solution for seamless tracking and real-time surveillance, our algorithm can efficiently and automatically perform the hand-off task in a multiple active camera surveillance systems.
C1 [Lee, Chao-Yang; Lin, Shou-Jen; Yang, Chu-Sing] Natl Cheng Kung Univ, Dept Elect Engn, NCKU Res Ctr Energy Technol & Strategy, Inst Comp & Commun Engn, Tainan 701, Taiwan.
   [Lee, Chen-Wei] Jinwen Univ Sci & Technol, Dept Elect Engn, Inst Comp & Commun Engn, Taipei, Taiwan.
RP Lee, CY (corresponding author), Natl Cheng Kung Univ, Dept Elect Engn, NCKU Res Ctr Energy Technol & Strategy, Inst Comp & Commun Engn, 1 Univ Rd, Tainan 701, Taiwan.
EM q3897109@mail.ncku.edu.tw; lsr@tn.edu.tw; chenwei@just.edu.tw;
   csyang@ee.ncku.edu.tw
RI Lee, Chao-Yang/Y-4558-2019
OI Lee, Chao-Yang/0000-0003-3898-3551
FU Research Center for energy Technology and Strategy, National Cheng Kung
   University under Ministry of Education; Research Center for energy
   Technology and Strategy, National Cheng Kung University under National
   Science Council of Taiwan [D100-23003, NSC 99-2221-E-228-003]
FX The authors would like to acknowledge the valuable comments and
   suggestions of the reviewers, which have improved the quality of this
   paper. Funding from the Research Center for energy Technology and
   Strategy, National Cheng Kung University, under projects from the
   Ministry of Education and the National Science Council (D100-23003 and
   NSC 99-2221-E-228-003) of Taiwan is gratefully acknowledged.
CR [Anonymous], 2008, OP COMP VIS LIB
   Araki S, 2000, IEICE T INF SYST, VE83D, P1583
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Cai Q, 1999, IEEE T PATTERN ANAL, V21, P1241, DOI 10.1109/34.809119
   Guha P, 2005, 2005 12TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS, P621
   Hu JS, 2007, IEEE-ASME T MECH, V12, P339, DOI 10.1109/TMECH.2007.897280
   Jae Moung Kim, 2008, 3rd International Conference on Cognitive Radio Oriented Wireless Networks and Communications (CrownCom 2008), P1, DOI 10.1109/CROWNCOM.2008.4562484
   JO Y, 2006, P ACM INT WORKSH VID, P195
   KOTECHA J, 2003, IEEE T SIGNAL PROCES, V51
   Li JB, 2010, INT J INNOV COMPUT I, V6, P4055
   Li YK, 2009, GEOMORPHOLOGY AND PLATE TECTONICS, P1
   Lu Y, 2008, CAN J ELECT COMPUT E, V33, P145, DOI 10.1109/CJECE.2008.4721631
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   MURRAY D, 1994, IEEE T PATTERN ANAL, V16, P449, DOI 10.1109/34.291452
   Orwell J, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P14, DOI 10.1109/VS.1999.780264
   Pan P, 2008, IEEE T CIRC SYST VID, V18, P1268, DOI 10.1109/TCSVT.2008.928889
   Ren J, 2008, IEEE T CIRC SYST VID, V18, P350, DOI 10.1109/TCSVT.2008.918276
   Sebe N, 2000, IEEE T PATTERN ANAL, V22, P1132, DOI 10.1109/34.879793
   Shibata M, 2008, AMC '08: 10TH INTERNATIONAL WORKSHOP ON ADVANCED MOTION CONTROL, VOLS 1 AND 2, PROCEEDINGS, P62
   Shiu LC, 2010, INT J INNOV COMPUT I, V6, P2941
   Song KT, 2006, IEEE T SYST MAN CY B, V36, P1091, DOI 10.1109/TSMCB.2006.872271
   Valera M, 2005, IEE P-VIS IMAGE SIGN, V152, P192, DOI 10.1049/ip-vis:20041147
   Varcheie PDZ, 2011, IEEE T INSTRUM MEAS, V60, P354, DOI 10.1109/TIM.2010.2084210
   Wan G, 2006, Proceedings of the 2006 IEEE International Conference on Vehicular Electronics and Safety, P9
   Wei XP, 2010, INT J INNOV COMPUT I, V6, P4651
   Yang CS, 2008, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON SENSING TECHNOLOGY, P142, DOI 10.1109/ICSENST.2008.4757089
   Yilmaz A, 2004, IEEE T PATTERN ANAL, V26, P1531, DOI 10.1109/TPAMI.2004.96
NR 27
TC 3
Z9 4
U1 0
U2 5
PU ICIC INTERNATIONAL
PI KUMAMOTO
PA TOKAI UNIV, 9-1-1, TOROKU, KUMAMOTO, 862-8652, JAPAN
SN 1349-4198
EI 1349-418X
J9 INT J INNOV COMPUT I
JI Int. J. Innov. Comp. Inf. Control
PD FEB
PY 2012
VL 8
IS 2
BP 1397
EP 1417
PG 21
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 902GH
UT WOS:000301026100027
DA 2022-02-10
ER

PT J
AU Loos, A
   Ernst, A
AF Loos, Alexander
   Ernst, Andreas
TI An automated chimpanzee identification system using face detection and
   recognition
SO EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING
LA English
DT Article
DE Wildlife monitoring; African great apes; Face and facial feature
   detection; Individual identification
ID FOREST MAMMALS; CLASSIFICATION; EIGENFACES
AB Due to the ongoing biodiversity crisis, many species including great apes like chimpanzees are on the brink of extinction. Consequently, there is an urgent need to protect the remaining populations of threatened species. To overcome the catastrophic decline of biodiversity, biologists and gamekeepers recently started to use remote cameras and recording devices for wildlife monitoring in order to estimate the size of remaining populations. However, the manual analysis of the resulting image and video material is extremely tedious, time consuming, and cost intensive. To overcome the burden of timeaEuroconsuming routine work, we have recently started to develop computer vision algorithms for automated chimpanzee detection and identification of individuals. Based on the assumption that humans and great apes share similar properties of the face, we proposed to adapt and extend face detection and recognition algorithms, originally developed to recognize humans, for chimpanzee identification. In this paper we do not only summarize our earlier work in the field, we also extend our previous approaches towards a more robust system which is less prone to difficult lighting situations, various poses, and expressions as well as partial occlusion by branches, leafs, or other individuals. To overcome the limitations of our previous work, we combine holistic global features and locally extracted descriptors using a decision fusion scheme. We present an automated framework for photo identification of chimpanzees including face detection, face alignment, and face recognition. We thoroughly evaluate our proposed algorithms on two datasets of captive and freeaEuroliving chimpanzee individuals which were annotated by experts. In three experiments we show that the presented framework outperforms previous approaches in the field of great ape identification and achieves promising results. Therefore, our system can be used by biologists, researchers, and gamekeepers to estimate population sizes faster and more precisely than the current frameworks. Thus, the proposed framework for chimpanzee identification has the potential to open up new venues in efficient wildlife monitoring and can help researches to develop innovative protection schemes in the future.
C1 [Loos, Alexander] Audio Visual Syst Fraunhofer IDMT, D-98693 Ilmenau, Germany.
   [Ernst, Andreas] Fraunhofer IIS, Elect Imaging, D-91058 Erlangen, Germany.
RP Loos, A (corresponding author), Audio Visual Syst Fraunhofer IDMT, D-98693 Ilmenau, Germany.
EM alexander.loos@idmt.fraunhofer.de
FU German Federal Ministry of Education and Research (BMBF)Federal Ministry
   of Education & Research (BMBF); Swiss Science FoundationSwiss National
   Science Foundation (SNSF); Max Planck SocietyMax Planck
   SocietyFoundation CELLEX
FX This work was funded by the German Federal Ministry of Education and
   Research (BMBF) under the 'Pact for research and innovation'. We thank
   the Ivorian authorities for the long-term support, especially the
   Ivorian Ministere de l'Environnement, des Eaux et Forets and the
   Ministere de l'Enseignement Superieur et de la Recherche Scientifique,
   the directorship of the Tai National Park, the OIPR, and the CSRS in
   Abidjan. Financial support is gratefully acknowledged from the Swiss
   Science Foundation. We would like to thank especially Dr. Tobias
   Deschner for collecting videos and pictures over the last years and for
   providing invaluable assistance during the data collection. We thank all
   the numerous field assistants and students for their work on the Tai
   Chimpanzee Project. We thank the Zoo Leipzig and the Wolfgang Kohler
   Primate Research Center (WKPRC), especially Josep Call and all the
   numerous research assistants, zoo-keepers, and Josefine Kalbitz for
   their support and collaboration. We also thank Laura Aporius for
   providing the videos and pictures in 2010. This work was supported by
   the Max Planck Society. We also thank Laura Aporius for the annotation
   of data.
CR Ahumada JA, 2011, PHILOS T R SOC B, V366, P2703, DOI 10.1098/rstb.2011.0115
   Araabi BN, 2000, ANN BIOMED ENG, V28, P1269, DOI 10.1114/1.1317532
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Burghardt T., 2008, THESIS U BRISTOL
   Burghardt T, 2006, NEUREL 2006: EIGHT SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL ENGINEERING, PROCEEDINGS, P27
   Burghardt Tilo, 2007, 5 INT C COMP VIS SYS
   Campbell G, 2008, CURR BIOL, V18, pR903, DOI 10.1016/j.cub.2008.08.015
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Ernst A, 2009, GI JAHRESTAGUNG, P3941
   Ernst A., 2011, 8 IEEE INT C ADV VID, P279, DOI DOI 10.1109/AVSS.2011.6027337
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Froba B, 2003, VERFAHREN ECHTZEIT G
   Gao Y, 2006, INT C PATT RECOG, P351
   Gokberk B, 2005, LECT NOTES COMPUT SC, V3546, P1019
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Head J, 2013, ECOL EVOL, DOI [10.1002/ece3.670, DOI 10.1002/ECE3.670.HTTP://0NLINELIBRARY.WILEY.C0M/D0I/10.1002/ECE3.670/ABSTRACT]
   Hoppe-Dominik B, 2011, AFR J ECOL, V49, P450, DOI 10.1111/j.1365-2028.2011.01277.x
   Lahiri M., 2011, ACM INT C MULT RETR
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Loos A, 2013, IEEE INT C AC SPEECH
   Loos A, 2011, 19 EUR SIGN PROC C E
   Loos A, 2012, IEEE INT S MULT DET
   Loos A., 2012, 1 ACM INT WORKSH MUL
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Phillips P. J., 2011, HDB FACE RECOGNITION, P551, DOI DOI 10.1007/978-0-85729-932-1_21.
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   PHILLIPS PJ, 2003, 6965 NAT I STAND TEC
   Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Spampinato C, 2012, MULTIMED TOOLS APPL, DOI [10.1007/s11042-012-1101-5, DOI 10.1007/S11042-012-1101-5.HTTP://SCH0LAR.G00GLE.C0M/CITATI0NS?]
   Spampinato C, 2010, ACM INT WORKSH AN RE, P40
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Vie JC, 2009, WILDLIFE CHANGING WO
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Walsh PD, 2003, NATURE, V422, P611, DOI 10.1038/nature01566
   Wawerla J, 2009, MACH VISION APPL, V20, P303, DOI 10.1007/s00138-008-0128-0
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu B, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P79
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Xie S, 2010, IEEE T IMAGE P, V19
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151
NR 49
TC 36
Z9 37
U1 2
U2 20
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1687-5176
EI 1687-5281
J9 EURASIP J IMAGE VIDE
JI EURASIP J. Image Video Process.
PY 2013
AR 49
DI 10.1186/1687-5281-2013-49
PG 17
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Imaging Science & Photographic Technology
GA 229JL
UT WOS:000325259000001
OA gold
DA 2022-02-10
ER

PT J
AU Rumpler, M
   Tscharf, A
   Mostegel, C
   Daftry, S
   Hoppe, C
   Prettenthaler, R
   Fraundorfer, F
   Mayer, G
   Bischof, H
AF Rumpler, Markus
   Tscharf, Alexander
   Mostegel, Christian
   Daftry, Shreyansh
   Hoppe, Christof
   Prettenthaler, Rudolf
   Fraundorfer, Friedrich
   Mayer, Gerhard
   Bischof, Horst
TI Evaluations on multi-scale camera networks for precise and geo-accurate
   reconstructions from aerial and terrestrial images with user guidance
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Photogrammetric computer vision; Unmanned aerial vehicles; Image-based
   3D reconstruction; Mapping Camera calibration; Image acquisition; Online
   feedback; Structure-from-motion; Georeferencing; Fiducial markers;
   Accuracy evaluation
AB During the last decades photogrammetric computer vision systems have been well established in scientific and commercial applications. Recent developments in image-based 3D reconstruction systems have resulted in an easy way of creating realistic, visually appealing and accurate 3D models. We present a fully automated processing pipeline for metric and geo-accurate 3D reconstructions of complex geometries supported by an online feedback method for user guidance during image acquisition. Our approach is suited for seamlessly matching and integrating images with different scales, from different view points (aerial and terrestrial), and with different cameras into one single reconstruction. We evaluate our approach based on different datasets for applications in mining, archaeology and urban environments and thus demonstrate the flexibility and high accuracy of our approach. Our evaluation includes accuracy related analyses investigating camera self-calibration, georegistration and camera network configuration. (C) 2016 Elsevier Inc. All rights reserved.
C1 [Rumpler, Markus; Mostegel, Christian; Hoppe, Christof; Prettenthaler, Rudolf; Fraundorfer, Friedrich; Bischof, Horst] Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria.
   [Tscharf, Alexander; Mayer, Gerhard] Univ Leoben, Chair Min Engn & Mineral Econ, Leoben, Austria.
   [Daftry, Shreyansh] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
RP Rumpler, M (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria.
EM rumpler@icg.tugraz.at; alexander.tscharf@unileoben.ac.at;
   mostegel@icg.tugraz.at; daftry@cmu.edu; hoppe@icg.tugraz.at;
   rudolf.prettenthaler@tugraz.at; fraundorfer@icg.tugraz.at;
   gerhard.mayer@unileoben.ac.at; bischof@icg.tugraz.at
RI Daftry, Shreyansh/AAI-1710-2019
OI Bischof, Horst/0000-0002-9096-6671
FU Austrian Research Promotion Agency (FFG) BRIDGE programme [841298]; EC
   FP7 project 3D-PITOTI [ICT-2011-600545]
FX This work has been supported by the Austrian Research Promotion Agency
   (FFG) BRIDGE programme under grant 841298 and EC FP7 project 3D-PITOTI
   (ICT-2011-600545). We further thank Ute Lohner-Urban, Peter Scherrer and
   the Institute of Archaeology, University of Graz.
CR Agarwal S., 2000, CERES SOLVER
   Bischof, 2014, ISPRS ANN PHOTOGRAMM, V3, P135, DOI [DOI 10.5194/ISPRSANNALS-II-3-135-2014, 10.5194/isprsannals-II-3-135-2014]
   Daftry S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.19
   Daftry S, 2015, IEEE INT CONF ROBOT, P3487, DOI 10.1109/ICRA.2015.7139681
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fraser C.S., 1996, CLOSE RANGE PHOTOGRA, P256
   Furukawa Y., 2009, IEEE T PATTERN ANAL
   Haralick R. M., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P592, DOI 10.1109/CVPR.1991.139759
   Hartley R., 2004, MULTIPLE VIEW GEOMET
   Henrik S, 2006, P IEEE COMPUTER SOC, P2161, DOI DOI 10.1109/CVPR.2006.264
   Hirschmueller H., 2005, IEEE C COMP VIS PATT
   Hoppe C, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.94
   Hoppe C, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.70
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Irschara Arnold, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2599, DOI 10.1109/CVPRW.2009.5206587
   Irschara A, 2012, ISPRS ANN PHOTOGRAMM
   Irschara A, 2007, IEEE I CONF COMP VIS, P2996
   KRAUS K, 1994, PHOTOGRAMMETRIE, V1
   Labatut P, 2007, IEEE I CONF COMP VIS, P504
   Leberl F, 2010, PHOTOGRAMM ENG REM S, V76, P1123, DOI 10.14358/PERS.76.10.1123
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nilosek D., 2012, GEOACCURATE DENSE PO
   Nister D, 2003, PROC CVPR IEEE, P195
   Pfeifer N, 2012, INT ARCH PHOTOGRAMM, V39, P487
   Rehak M, 2013, INT ARCH PHOTOGRAMM, P317
   Rumpler M., 2011, 35 WORKSH AUSTR ASS
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Strecha C., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587706
   Triggs Bill, 1999, P 1999 INT WORKSH VI, P298, DOI DOI 10.1007/3-540-44480-7_21
   Tscharf A, 2015, ISPRS UAV-G2015, P15, DOI 10.5194/isprsannals-II-1-W1-15-2015
   Waechter M, 2014, LECT NOTES COMPUT SC, V8693, P836, DOI 10.1007/978-3-319-10602-1_54
   Watson GA, 2006, J COMPUT APPL MATH, V197, P387, DOI 10.1016/j.cam.2005.06.047
   Zeisl B, 2009, BRIT MACH VIS C BMVC
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 35
TC 13
Z9 13
U1 1
U2 16
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD APR
PY 2017
VL 157
SI SI
BP 255
EP 273
DI 10.1016/j.cviu.2016.04.008
PG 19
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA EQ9UI
UT WOS:000398430300018
OA Green Accepted
DA 2022-02-10
ER

PT J
AU Allen, ML
   Taylor, AP
AF Allen, Maximilian L.
   Taylor, A. Preston
TI First Record of Scavenging by a Western Screech-Owl (Megascops
   kennicottii)
SO WILSON JOURNAL OF ORNITHOLOGY
LA English
DT Article
DE diet; Megascops kennicottii; motion-triggered cameras; owl; scavenging;
   Virginia opossum; Western Screech-Owl
ID PREDATORS
AB Scavenging is a common behavior in many animal species and can increase the fitness of individuals and populations. Scavenging has been documented more commonly in recent years with advances in technology, including in difficult to observe avian species. Four North American owl species have been documented scavenging, and here we relate the first documented scavenging by a Western Screech-Owl (Megascops kennicottii), which was video-documented with a motion-triggered camera. The screech-owl visited a Virginia Opossum (Didelphus virginiana) carcass seven times over an 11-hr period, with the longest visit lasting 7 mins (mean = 3.14 +/- SD 2.34 mins); during the visits the owl actively fed on the carcass. The most common prey of Western Screech-Owls are small mammals, insects, arthropods, and small birds. No mesocarnivore or large mammal has ever been detected in diet studies of screech-owls, though screech-owls have been shown to attack and kill prey larger than themselves. The importance of carrion in owl diets is largely unexplored, and the scavenging behavior of owls could be more significant than currently believed depending on its availability and the presence of competing scavengers. Further research is needed to establish the commonality of scavenging by owls, and its various ecological effects.
C1 [Allen, Maximilian L.] Victoria Univ Wellington, Sch Biol Sci, Wellington 6140, New Zealand.
   [Taylor, A. Preston] Humboldt State Univ, Dept Wildlife, Arcata, CA 95521 USA.
RP Allen, ML (corresponding author), Victoria Univ Wellington, Sch Biol Sci, POB 600, Wellington 6140, New Zealand.
EM maximilian.allen@vuw.ac.nz
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
FU California Department of Fish and Wildlife; University of
   CaliforniaUniversity of California System
FX M. Allen and P. Taylor contributed equally to the writing of this
   article. We would like to thank J. Gayner for the generous use of his
   land in Mendocino County, California. Funding for the study was
   generously provided by The California Department of Fish and Wildlife
   and University of California, Davis. M. Johnson, M. Colwell, and two
   anonymous reviewers provided comments on previous drafts that greatly
   increased the quality of the manuscript.
CR Allen Maximilian L., 2013, Northwestern Naturalist, V94, P79
   Bauer JW, 2005, SOUTHWEST NAT, V50, P466, DOI 10.1894/0038-4909(2005)050[0466:SBIP]2.0.CO;2
   BENT A. C., 1983, LIFE HIST N AM BIRDS
   BROCKMANN HJ, 1979, ANIM BEHAV, V27, P487, DOI 10.1016/0003-3472(79)90185-4
   CAMPBELL BERRY, 1934, CONDOR, V36, P201, DOI 10.2307/1363855
   CRAIGHEAD J. J., 1916, HAWKS OWLS WILDLIFE
   DeVault TL, 2003, OIKOS, V102, P225, DOI 10.1034/j.1600-0706.2003.12378.x
   Duncan J.R., 1998, BIRDS N AM
   HAYWARD GD, 1988, OECOLOGIA, V75, P253, DOI 10.1007/BF00378606
   JOHNSGARD P.A., 2002, N AM OWLS
   Kapfer JM, 2011, WILSON J ORNITHOL, V123, P646, DOI 10.1676/11-015.1
   Kostecke RM, 2001, J FIELD ORNITHOL, V72, P439, DOI 10.1648/0273-8570-72.3.439
   Munro J. A., 1925, Canadian Field Naturalist, V39, P166
   Patterson J. Micheal, 2007, Northwestern Naturalist, V88, P12, DOI 10.1898/1051-1733(2007)88[12:AAOSOB]2.0.CO;2
   Selva N, 2005, CAN J ZOOL, V83, P1590, DOI 10.1139/Z05-158
   SMITH D G, 1971, Great Basin Naturalist, V31, P83
   SUTTON G. M., 1927, WILSON B, V39, P171
   Wilmers CC, 2003, J ANIM ECOL, V72, P909, DOI 10.1046/j.1365-2656.2003.00766.x
NR 18
TC 9
Z9 13
U1 1
U2 21
PU WILSON ORNITHOLOGICAL SOC
PI WACO
PA 5400 BOSQUE BLVD, STE 680, WACO, TX 76710 USA
SN 1559-4491
J9 WILSON J ORNITHOL
JI Wilson J. Ornithol.
PD JUN
PY 2013
VL 125
IS 2
BP 417
EP 419
DI 10.1676/12-176.1
PG 3
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA 170ZG
UT WOS:000320893300027
DA 2022-02-10
ER

PT J
AU Govil, K
   Welch, ML
   Ball, JT
   Pennypacker, CR
AF Govil, Kinshuk
   Welch, Morgan L.
   Ball, J. Timothy
   Pennypacker, Carlton R.
TI Preliminary Results from a Wildfire Detection System Using Deep Learning
   on Remote Camera Images
SO REMOTE SENSING
LA English
DT Article
DE smoke detection; fire detection; machine learning
ID CLIMATE-CHANGE
AB Pioneering networks of cameras that can search for wildland fire signatures have been in development for some years (High Performance Wireless Research & Education Network-HPWREN cameras and the ALERT Wildfire camera). While these cameras have proven their worth in monitoring fires reported by other means, we have developed a functioning prototype system that can detect smoke from fires usually within 15 min of ignition, while averaging less than one false positive per day per camera. This smoke detection system relies on machine learning-based image recognition software and a cloud-based work-flow capable of scanning hundreds of cameras every minute. The system is operating around the clock in Southern California and has already detected some fires earlier than the current best methods-people calling emergency agencies or satellite detection from the Geostationary Operational Environmental Satellite (GOES) satellites. This system is already better than some commercial systems and there are still many unexplored methods to further improve accuracy. Ground-based cameras are not going to be able to detect every wildfire, and so we are building a system that combines the best of terrestrial camera-based detection with the best approaches to satellite-based detection.
C1 [Govil, Kinshuk; Welch, Morgan L.] Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA.
   [Ball, J. Timothy] Fireball LLC, Reno, NV 89509 USA.
   [Pennypacker, Carlton R.] Univ Calif Berkeley, Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA.
   [Pennypacker, Carlton R.] Univ Calif Berkeley, Dept Phys, Berkeley, CA 94720 USA.
RP Govil, K (corresponding author), Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA.
EM kinshuk@lbl.gov; mwelch@lbl.gov; tim@fireballit.com; pennypacker@lbl.gov
CR Alkhatib AAA, 2014, INT J DISTRIB SENS N, DOI 10.1155/2014/597368
   Anjum MDN, 2019, DIGIT COMMUN NETW, V5, P84, DOI 10.1016/j.dcan.2018.03.001
   Climate Council, 2019, THIS IS NOT NORM CLI
   Davies D., 2019, NASA TECHNICAL REPOR
   Filonenko A., 2017, P 2017 10 INT C HUM, DOI [10.1109/HSI.2017.8004998, DOI 10.1109/HSI.2017.8004998]
   Frizzi S., 2016, P IEEE IND EL SOC C, DOI [10.1109/IECON.2016.7793196, DOI 10.1109/IECON.2016.7793196]
   Hohberg S. P., 2015, THESIS
   Holden ZA, 2018, P NATL ACAD SCI USA, V115, pE8349, DOI 10.1073/pnas.1802316115
   Koltunov A, 2016, REMOTE SENS ENVIRON, V184, P436, DOI 10.1016/j.rse.2016.07.021
   Langner R, 2013, PSYCHOL BULL, V139, P870, DOI 10.1037/a0030694
   Liu YQ, 2013, FOREST ECOL MANAG, V294, P120, DOI 10.1016/j.foreco.2012.06.049
   Matthews S, 2012, FIRE SAFETY J, V47, P54, DOI 10.1016/j.firesaf.2011.11.001
   Nagy RC, 2018, FIRE-BASEL, V1, DOI 10.3390/fire1010004
   Pennypacker CR, 2013, REMOTE SENS-BASEL, V5, P5173, DOI 10.3390/rs5105173
   Pomerleau M., FUTURE UNMANNED CAPA
   RAND JL, 1994, ADV SPACE RES-SERIES, V14, P183, DOI 10.1016/0273-1177(94)90088-4
   Schmit TJ, 2018, J OPER METEOROL, V6, P33, DOI 10.15191/nwajom.2018.0604
   Schoennagel T, 2017, P NATL ACAD SCI USA, V114, P4582, DOI 10.1073/pnas.1617464114
   Schroeder D., 2005, OPERATIONAL TRIAL FO, V6
   Stephens SL, 2009, ECOL APPL, V19, P305, DOI 10.1890/07-1755.1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vodacek A, 2002, INT J REMOTE SENS, V23, P2721, DOI 10.1080/01431160110109633
   Williams AP, 2019, EARTHS FUTURE, V7, P892, DOI 10.1029/2019EF001210
   Zhang XY, 2012, J GEOPHYS RES-ATMOS, V117, DOI 10.1029/2012JD017459
   Zhao XZ, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101526
NR 25
TC 10
Z9 10
U1 3
U2 8
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD JAN
PY 2020
VL 12
IS 1
AR 166
DI 10.3390/rs12010166
PG 15
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging
   Science & Photographic Technology
GA KO2PM
UT WOS:000515391700166
OA gold
DA 2022-02-10
ER

PT J
AU Elbroch, LM
   Allen, ML
AF Elbroch, L. Mark
   Allen, Maximilian L.
TI PREY INDICES AND BEHAVIORS AT A GRAY FOX DEN IN SAN MATEO COUNTY,
   CALIFORNIA
SO WESTERN NORTH AMERICAN NATURALIST
LA English
DT Article
ID HABITS; RED
AB The gray fox (Urocyon cinereoargenteus) is an under-studied species in North America. Here we provide data gathered through observations over 17 d at a gray fox den in San Mateo County, California. We recorded prey indices through direct observation, as well as through indirect observation with photos recorded by motion-triggered cameras. The largest prey was a mule deer fawn, which we determined was killed by a gray fox. This finding is the first record of gray fox predation on mule deer. Lagomorphs and rodents formed the majority of prey items. We also recorded behavior that both contradicted and corroborated previous literature. We observed the male bringing food items to the den, a behavior that previous researchers have disagreed about. We also observed allogrooming between the adult pair, as well as one instance among pups where leg-lifting accompanied by presentation of the genitalia was clearly used as an aggressive dominant behavior rather than a submissive behavior, as reported in previous literature.
C1 [Elbroch, L. Mark] Panthera, New York, NY 10018 USA.
   [Allen, Maximilian L.] Victoria Univ Wellington, Sch Biol Sci, Wellington 6140, New Zealand.
RP Allen, ML (corresponding author), Victoria Univ Wellington, Sch Biol Sci, Box 600, Wellington 6140, New Zealand.
EM maximilian.allen@vuw.ac.nz
RI Allen, Maximilian/ABG-9307-2020
OI Allen, Maximilian/0000-0001-8976-889X
CR Acorn R. C., 1990, METHODS INVESTIGATIN
   Caro T, 1999, TRENDS ECOL EVOL, V14, P366, DOI 10.1016/S0169-5347(99)01663-8
   Elbroch M., 2003, MAMMAL TRACKS SIGN G
   ERRINGTON PAUL L., 1935, JOUR MAMMAL, V16, P192, DOI 10.2307/1374445
   FOX M W, 1971, Zeitschrift fuer Tierpsychologie, V28, P185
   Fuller TK., 2004, CANIDS FOXES WOLVES, P92
   Grinnell J., 1937, FUR BEARING MAMMALS, V2, P421
   HALFORD DK, 1978, AM MIDL NAT, V100, P493, DOI 10.2307/2424855
   HECKROTTE C, 1967, COPEIA, P759
   HOCKMAN JG, 1983, AM MIDL NAT, V110, P276, DOI 10.2307/2425269
   Jameson E.W., 2004, CALIFORNIA NATURAL H
   Kuenzi A. J., 1998, NW NATURALIST, V79, P64
   Mayer K. E., 1988, GUIDE WILDLIFE HABIT
   Neale JCC, 2001, CAN J ZOOL, V79, P1794, DOI 10.1139/cjz-79-10-1794
   SCOTT TG, 1955, ECOLOGY, V36, P366, DOI 10.2307/1933254
   TAYLOR W. P., 1943, J TEXAS GAME FISH, V1, P19
   TAYLOR W. P., 1943, J TEXAS GAME FISH, V1, P12
   Wade D., 1982, US FISH WILDLIFE S B
   [No title captured], DOI DOI 10.2307/3503957
NR 19
TC 0
Z9 1
U1 6
U2 35
PU BRIGHAM YOUNG UNIV
PI PROVO
PA 290 LIFE SCIENCE MUSEUM, PROVO, UT 84602 USA
SN 1527-0904
EI 1944-8341
J9 WEST N AM NATURALIST
JI West. North Am. Naturalist
PD JUL
PY 2013
VL 73
IS 2
BP 240
EP 243
DI 10.3398/064.073.0215
PG 4
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA AM6GB
UT WOS:000339960300014
OA Green Published
DA 2022-02-10
ER

PT C
AU Itu, R
   Danescu, R
AF Itu, Razvan
   Danescu, Radu
BE Dubbert, J
   Muller, B
   Meyer, G
TI Machine Learning Based Automatic Extrinsic Calibration of an Onboard
   Monocular Camera for Driving Assistance Applications on Smart Mobile
   Devices
SO ADVANCED MICROSYSTEMS FOR AUTOMOTIVE APPLICATIONS 2018: SMART SYSTEMS
   FOR CLEAN, SAFE AND SHARED ROAD VEHICLES
SE Lecture Notes in Mobility
LA English
DT Proceedings Paper
CT 22nd International Forum on Advanced Microsystems for Automotive
   Applications (AMAA) - Smart Systems for Clean, Safe and Shared Road
   Vehicles
CY SEP 11-12, 2018
CL Berlin, GERMANY
SP European Technol Platform Smart Syst Integrat, VDI VDE Innovat + Technik GmbH, European Commiss
DE Automatic camera calibration; Monocular vision; Convolutional neural
   networks; Smart mobile devices
ID VANISHING POINTS
AB Smart mobile devices can be easily transformed into driving assistance tools or traffic monitoring systems. These devices are placed behind the windshield such that the camera is facing forward to observe the traffic. For the visual information to be useful, the camera must be calibrated, and a proper calibration is laborious and difficult to perform for the average user. In this paper, we propose a calibration technique that requires no input from the user and is able to estimate the extrinsic parameters of the camera: yaw, pitch and roll angles and the height of the camera above the road. The calibration algorithm is based on detecting vehicles using CNN based classifiers, and using statistics about their size and position in the image to estimate the extrinsic parameters via Extended Kalman filters.
C1 [Itu, Razvan; Danescu, Radu] Tech Univ Cluj Napoca, Str Memorandumului 28, Cluj Napoca, Romania.
RP Danescu, R (corresponding author), Tech Univ Cluj Napoca, Str Memorandumului 28, Cluj Napoca, Romania.
EM razvan.itu@cs.utcluj.ro; radu.danescu@cs.utcluj.ro
FU Ministry of Research and Innovation, CNCS - UEFISCDI within PNCDI
   IIIConsiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva
   pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si
   Inovarii (UEFISCDI) [PN-III-P1-1.1-TE-2016-0440]
FX This work was supported by a grant of Ministry of Research and
   Innovation, CNCS - UEFISCDI, project number PN-III-P1-1.1-TE-2016-0440,
   within PNCDI III.
CR Abadi Martin., 2016, ARXIV PREPRINT ARXIV
   Bazin JC, 2012, IEEE INT C INT ROBOT, P4282, DOI 10.1109/IROS.2012.6385802
   Bileschi Stanley, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1457, DOI 10.1109/ICCVW.2009.5457439
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Danescu R, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16101721
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Howard A.G., 2017, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/J.JAL.2014.11.010
   Itu R, 2017, INT C INTELL COMP CO, P273, DOI 10.1109/ICCP.2017.8117016
   Levinson J, 2013, P ROB SCI SYST, DOI DOI 10.15607/RSS.2013.IX.029
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   MAGEE MJ, 1984, COMPUT VISION GRAPH, V26, P256, DOI 10.1016/0734-189X(84)90188-9
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
NR 13
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 2196-5544
EI 2196-5552
BN 978-3-319-99762-9; 978-3-319-99761-2
J9 LECT N MOBIL
PY 2019
BP 16
EP 28
DI 10.1007/978-3-319-99762-9_2
PG 13
WC Automation & Control Systems; Computer Science, Theory & Methods;
   Engineering, Electrical & Electronic; Remote Sensing; Transportation
   Science & Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering; Remote
   Sensing; Transportation
GA BP5XP
UT WOS:000558588700002
DA 2022-02-10
ER

PT C
AU Konda, KR
   Rosani, A
   Conci, N
   De Natale, FGB
AF Konda, Krishna Reddy
   Rosani, Andrea
   Conci, Nicola
   De Natale, Francesco G. B.
BE Agapito, L
   Bronstein, MM
   Rother, C
TI Smart Camera Reconfiguration in Assisted Home Environments for Elderly
   Care
SO COMPUTER VISION - ECCV 2014 WORKSHOPS, PT IV
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 13th European Conference on Computer Vision (ECCV)
CY SEP 06-12, 2014
CL Zurich, SWITZERLAND
DE Elderly care; Real time video analysis; Automatic camera reconfiguration
ID FALL DETECTION; VIDEO
AB Researchers of different fields have been involved in human behavior analysis during the last years. The successful recognition of human activities from video analysis is still a challenging problem. Within this context, applications targeting elderly care are of considerable interest both for public and industrial bodies, especially considering the aging society we are living in. Ambient intelligence (AmI) technologies, intended as the possibility of automatically detecting and reacting to the status of the environment and of the persons, is probably the major enabling factor. AmI technologies require suitable networks of sensors and actuators, as well as adequate processing and communication technologies. In this paper we propose an innovative solution based on a real time analysis of video with application in the field of elderly care. The system performs anomaly detection and proposes the automatic reconfiguration of the camera network for better monitoring of the ongoing event. The developed framework is tested on a publicly available dataset and has also been deployed and evaluated in a real environment.
C1 [Konda, Krishna Reddy; Rosani, Andrea; Conci, Nicola; De Natale, Francesco G. B.] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
RP Rosani, A (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, Via Sommar 9, I-38123 Trento, Italy.
EM andrea.rosani@unitn.it
RI Conci, Nicola/AAH-4671-2020
OI Conci, Nicola/0000-0002-7858-0928
CR Anderson Derek, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P6388
   Chaaraoui AA, 2012, EXPERT SYST APPL, V39, P10873, DOI 10.1016/j.eswa.2012.03.005
   Auvinet E., 2010, DIRO U MONTR TECH RE 1350 DIRO U MONTR
   Borges PVK, 2013, IEEE T CIRC SYST VID, V23, P1993, DOI 10.1109/TCSVT.2013.2270402
   Cardinaux F, 2011, J AMB INTEL SMART EN, V3, P253, DOI 10.3233/AIS-2011-0110
   Chen CH, 2008, IEEE T CIRC SYST VID, V18, P1052, DOI 10.1109/TCSVT.2008.928223
   Cucchiara R, 2005, IEEE T SYST MAN CY A, V35, P42, DOI 10.1109/TSMCA.2004.838501
   Feng W., 2014, SIGNAL IMAGE VIDEO P, P1
   Foroughi Homa, 2008, 2008 11th International Conference on Computer and Information Technology (ICCIT), P219, DOI 10.1109/ICCITECHN.2008.4803020
   Hazelhoff L, 2008, LECT NOTES COMPUT SC, V5259, P298, DOI 10.1007/978-3-540-88458-3_27
   HHI, 2014, H 264 REF DEC H HERT
   KATZ S, 1970, GERONTOLOGIST, V10, P20, DOI 10.1093/geront/10.1_Part_1.20
   Lin CW, 2007, IEEE IC COMP COM NET, P1172
   Micheloni C, 2010, IEEE SIGNAL PROC MAG, V27, P78, DOI 10.1109/MSP.2010.937333
   Mubashir M, 2013, NEUROCOMPUTING, V100, P144, DOI 10.1016/j.neucom.2011.09.037
   MURRAY D, 1994, IEEE T PATTERN ANAL, V16, P449, DOI 10.1109/34.291452
   Nait-Charif H, 2004, INT C PATT RECOG, P323, DOI 10.1109/ICPR.2004.1333768
   Open source multiple contributions O. S., 2014, OPEN SOURCE MULTIPLE
   Quaritsch M, 2007, EURASIP J EMBED SYST, DOI 10.1155/2007/92827
   Rashidi P, 2013, IEEE J BIOMED HEALTH, V17, P579, DOI 10.1109/JBHI.2012.2234129
   Rougier C, 2007, 21ST INTERNATIONAL CONFERENCE ON ADVANCED NETWORKING AND APPLICATIONS WORKSHOPS/SYMPOSIA, VOL 2, PROCEEDINGS, P875, DOI 10.1109/ainaw.2007.181
   Scotti G, 2005, IEE P-VIS IMAGE SIGN, V152, P250, DOI 10.1049/ip-vis:20041302
   van Kasteren TLM, 2010, PERS UBIQUIT COMPUT, V14, P489, DOI 10.1007/s00779-009-0277-9
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yu XG, 2008, 2008 10TH IEEE INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATIONS AND SERVICES, P42, DOI 10.1109/HEALTH.2008.4600107
NR 25
TC 1
Z9 1
U1 0
U2 6
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-16220-1; 978-3-319-16219-5
J9 LECT NOTES COMPUT SC
PY 2015
VL 8928
BP 45
EP 58
DI 10.1007/978-3-319-16220-1_4
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BD5WP
UT WOS:000361842800004
DA 2022-02-10
ER

PT C
AU Hulens, D
   Goedeme, T
   Rumes, T
AF Hulens, Dries
   Goedeme, Toon
   Rumes, Tom
GP IEEE
TI Autonomous lecture recording with a PTZ camera while complying with
   cinematographic rules
SO 2014 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION (CRV)
LA English
DT Proceedings Paper
CT 11th Canadian Conference on Computer and Robot Vision (CRV)
CY MAY 07-09, 2014
CL Montreal, CANADA
SP Canadian Image Proc & Pattern Recognit Soc
DE automatic camera system; cinematographic rules; computer vision;
   tracking; real-time
AB Nowadays, many lectures and presentations are recorded and broadcasted for teleteaching applications. When no human camera crew is present, the most obvious choice is for static cameras. In order to enhance the viewing experience, more advanced systems automatically track and steer the camera towards the lecturer. In this paper we propose an even more advanced system that tracks the lecturer while taking cinematographic rules into account. On top of that, the lecturer can be filmed in different types of shots. Our system is able to detect and track the position of the lecturer, even with non-static backgrounds and in difficult illumination. We developed an action axis determination system, needed to apply cinematographic rules and to steer the Pan-Tilt-Zoom (PTZ) camera towards the lecturer.
C1 [Hulens, Dries; Goedeme, Toon] Katholieke Univ Leuven, EAVISE, St Katelijne Waver, Belgium.
   [Rumes, Tom] Thomas More, PRO media lab, Mechelen, Belgium.
RP Hulens, D (corresponding author), Katholieke Univ Leuven, EAVISE, St Katelijne Waver, Belgium.
EM dries.hulens@kuleuven.be; toon.goedeme@kuleuven.be;
   tom.rumes@thomasmore.be
OI Goedeme, Toon/0000-0002-7477-8961
CR Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Chaumette F, 2006, IEEE ROBOT AUTOM MAG, V13, P82, DOI 10.1109/MRA.2006.250573
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   De Smedt F, 2013, IEEE COMPUT SOC CONF, P622, DOI 10.1109/CVPRW.2013.94
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Geys T., 2004, 5 WORKSH OMN VIS CAM, P17
   Han-Ping Chou, 2010, 2010 International Conference on System Science and Engineering (ICSSE 2010), P167, DOI 10.1109/ICSSE.2010.5551811
   Lampi F, 2008, IEEE MULTIMEDIA, V15, P58, DOI 10.1109/MMUL.2008.43
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Qiong Liu, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P442
   Viola P., 2001, CVPR
   Welch G., 1995, INTRO KALMAN FILTER
   Winkler MB, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P471, DOI 10.1109/ISM.2012.96
NR 13
TC 8
Z9 8
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4799-4338-8
PY 2014
BP 371
EP 377
DI 10.1109/CRV.2014.57
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BB2CN
UT WOS:000341646300051
OA Green Accepted
DA 2022-02-10
ER

PT J
AU Fox, CH
   Paquet, PC
   Reimchen, TE
AF Fox, Caroline Hazel
   Paquet, Paul Charles
   Reimchen, Thomas Edward
TI Novel species interactions: American black bears respond to Pacific
   herring spawn
SO BMC ECOLOGY
LA English
DT Article
DE Species interactions; Clupea pallasii; Ursus americanus; Intertidal
   zone; Forage fish; Pacific Ocean; Spawn
ID BROWN BEARS; URSUS-ARCTOS; RIPARIAN FORESTS; CLUPEA-PALLASI; SALMON;
   MARINE; COASTAL; DIET; VEGETATION; CONSERVATION
AB Background: In addition to the decline and extinction of the world's species, the decline and eventual loss of species interactions is one of the major consequences of the biodiversity crisis. On the Pacific coast of North America, diminished runs of salmon (Oncorhynchus spp.) drive numerous marine-terrestrial interactions, many of which have been intensively studied, but marine-terrestrial interactions driven by other species remain relatively unknown. Bears (Ursus spp.) are major vectors of salmon into terrestrial ecosystems, but their participation in other cross-ecosystem interactions is similarly poorly described. Pacific herring (Clupea pallasii), a migratory forage fish in coastal marine ecosystems of the North Pacific Ocean and the dominant forage fish in British Columbia (BC), spawn in nearshore subtidal and intertidal zones. Spawn resources (eggs, milt, and spawning adults) at these events are available to coastal predators and scavengers, including terrestrial species. In this study, we investigated the interaction between American black bears (Ursus americanus) and Pacific herring at spawn events in Quatsino Sound, BC, Canada.
   Results: Using remote cameras to monitor bear activity (1,467 camera days, 29 sites, years 2010-2012) in supratidal and intertidal zones and a machine learning approach, we determined that the quantity of Pacific herring eggs in supratidal and intertidal zones was a leading predictor of black bear activity, with bears positively responding to increasing herring egg masses. Other important predictors included day of the year and Talitrid amphipod (Traskorchestia spp.) mass. A complementary analysis of black bear scats indicated that Pacific herring egg mass was the highest ranked predictor of egg consumption by bears. Pacific herring eggs constituted a substantial yet variable component of the early springtime diet of black bears in Quatsino Sound (frequency of occurrence 0-34%; estimated dietary content 0-63%). Other major dietary items included graminoids (grasses and sedges), Phaeophyta (brown algae), Zosteraceae (seagrasses), and Talitrid amphipods.
   Conclusion: This research represents the first scientific evidence of a cross-ecosystem interaction between Pacific herring and American black bears. Our findings also expand knowledge of the ecological roles of both species. Combined, evidence of anthropogenic constraints on both black bears and Pacific herring suggests that bear-herring interactions were potentially stronger and more widespread in the past.
C1 [Fox, Caroline Hazel; Paquet, Paul Charles] Raincoast Conservat Fdn, Sidney, BC V8L 3Y3, Canada.
   [Fox, Caroline Hazel; Reimchen, Thomas Edward] Univ Victoria, Dept Biol, Victoria, BC V8W 2Y2, Canada.
RP Fox, CH (corresponding author), Raincoast Conservat Fdn, POB 2429, Sidney, BC V8L 3Y3, Canada.
EM caroline@raincoast.org
FU National Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC) [NRC2354]; University of Victoria; Raincoast Conservation
   Foundation; Environment CanadaCGIAR
FX We thank the anonymous reviewers whose comments and suggestions greatly
   improved this manuscript. A human ethics waiver and an observational
   animal use permit were granted by the University of Victoria. A
   scientific license was obtained from Fisheries and Oceans Canada.
   Financial and logistical support was provided by the University of
   Victoria, Raincoast Conservation Foundation, Environment Canada, and the
   National Sciences and Engineering Research Council of Canada (NSERC
   operating grant NRC2354 to TER and NSERC PGS-D award to CHF). M. Adams,
   C. Darimont, R. Davey, J. Dower, D. Duffus, M. Fournier, J. and L. Fox,
   S. Insley, C. Kelly, K. Kezes, I. McKechnie, K. Morgan and others are
   thanked for their contributions. Quatsino First Nation is also
   acknowledged for granting permission to undertake research in their
   territory.
CR ALDERDICE DF, 1985, CAN J FISH AQUAT SCI, V42, P56, DOI 10.1139/f85-262
   Anderson EM, 2009, MAR ECOL PROG SER, V386, P287, DOI 10.3354/meps08048
   BALDWIN JR, 1994, MAR ECOL PROG SER, V103, P119, DOI 10.3354/meps103119
   Ben-David M, 1998, OIKOS, V83, P47, DOI 10.2307/3546545
   BenDavid M, 1997, CAN J ZOOL, V75, P803, DOI 10.1139/z97-102
   Bennetts RE, 1997, WILSON BULL, V109, P393
   Bishop MA, 2001, FISH OCEANOGR, V10, P149, DOI 10.1046/j.1054-6006.2001.00038.x
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Carlton JT, 2003, MAR ECOL PROG SER, V256, P271, DOI 10.3354/meps256271
   Christie KS, 2008, AUK, V125, P51, DOI 10.1525/auk.2008.125.1.51
   Dahle B, 1998, WILDLIFE BIOL, V4, P147, DOI 10.2981/wlb.1998.017
   Darimont CT, 2002, CAN J ZOOL, V80, P1638, DOI 10.1139/Z02-149
   Department of Fisheries and Oceans, 2011, Canadian Science Advisory Secretariat Science Advisory Report, V061, P1
   DFO-MPO, 2014, Canadian Science Advisory Secretariat Science Advisory Report, V3, P1
   Ellis DW, 1981, MONOGRAPH SERIES, V1
   Estes JA, 2011, SCIENCE, V333, P301, DOI 10.1126/science.1205106
   Fisheries and Oceans Canada, 2012, CAN SCI ADVIS SEC SC, V2012, P1
   Fox CH, 2014, MAR ECOL PROG SER, V495, P49, DOI 10.3354/meps10588
   Fox CH, 2010, URSUS, V21, P195, DOI 10.2192/09SC026.1
   Fox CH, 2013, THESIS U VICTORIA
   Fuiman LA, 2015, ECOLOGY, V96, P362, DOI 10.1890/14-0571.1
   Garman GC, 1998, J N AM BENTHOL SOC, V17, P277, DOI 10.2307/1468331
   Gray A, 2005, THESIS U WASHINGTON
   HAEGELE CW, 1981, CAN J FISH AQUAT SCI, V38, P381, DOI 10.1139/f81-053
   Haegele CW, 1991, P INT HERR S U AL FA, p[91, 309]
   HAY DE, 1985, CAN J FISH AQUAT SCI, V42, P111
   Helfield JM, 2006, ECOSYSTEMS, V9, P167, DOI 10.1007/s10021-004-0063-5
   Hewitt DG, 1996, WILDLIFE SOC B, V24, P547
   Hilderbrand GV, 1999, CAN J ZOOL, V77, P132, DOI 10.1139/cjz-77-1-132
   Iverson M, 2011, THESIS U TROMSO
   Keeling BE, 2013, THESIS S FRASER U
   KISTCHINSKI A A, 1972, IUCN (International Union for Conservation of Nature and Natural Resources) Publications New Series, V23, P67
   Laliberte AS, 2004, BIOSCIENCE, V54, P123, DOI 10.1641/0006-3568(2004)054[0123:RCONAC]2.0.CO;2
   Liaw A., 2002, R NEWS, V2, P18, DOI DOI 10.1177/154405910408300516
   LIVINGSTON PA, 1993, MAR ECOL PROG SER, V102, P205, DOI 10.3354/meps102205
   MATTSON DJ, 1991, CAN J ZOOL, V69, P1619, DOI 10.1139/z91-226
   McKechnie I, 2014, P NATL ACAD SCI USA, V111, pE807, DOI 10.1073/pnas.1316072111
   McLellan BN, 2011, CAN J ZOOL, V89, P546, DOI 10.1139/Z11-026
   Noyce KV, 1997, CAN J ZOOL, V75, P1671, DOI 10.1139/z97-794
   O'Keefe TC, 2003, AM FISH S S, V34, P99
   Orr M, 2005, ECOLOGY, V86, P1496, DOI 10.1890/04-1486
   Persson IL, 2001, WILDLIFE BIOL, V7, P27, DOI 10.2981/wlb.2001.006
   Polis GA, 1996, AM NAT, V147, P396, DOI 10.1086/285858
   Polis GA, 1997, ANNU REV ECOL SYST, V28, P289, DOI 10.1146/annurev.ecolsys.28.1.289
   Powell RA, 1997, CHAPMAN HALL WILDLIF
   PRITCHARD GT, 1990, CAN J ZOOL, V68, P1645, DOI 10.1139/z90-244
   PURCELL JE, 1989, CAN J FISH AQUAT SCI, V46, P1415, DOI 10.1139/f89-181
   Quinn T.P., 2018, BEHAV ECOLOGY PACIFI
   Quinn TP, 2014, CAN J ZOOL, V92, P893, DOI 10.1139/cjz-2014-0114
   Quinn TP, 2009, CAN J ZOOL, V87, P195, DOI 10.1139/Z09-004
   R Core Team, 2020, LANGUAGE ENV STAT CO
   Reimchen TE, 2003, AM FISH S S, V34, P59
   Reimchen TE, 1998, CAN FIELD NAT, V112, P446
   Reimchen TE, 2000, CAN J ZOOL, V78, P448, DOI 10.1139/cjz-78-3-448
   SCHWEIGERT J F, 1990, North American Journal of Fisheries Management, V10, P185, DOI 10.1577/1548-8675(1990)010<0185:EOSSFS>2.3.CO;2
   Schweigert JF, 2010, ICES J MAR SCI, V67, P1903, DOI 10.1093/icesjms/fsq134
   SCHWEIGERT JF, 1985, CAN J FISH AQUAT SCI, V42, P1806, DOI 10.1139/f85-226
   Shardlow TF, 2013, ECOL INDIC, V27, P97, DOI 10.1016/j.ecolind.2012.11.011
   Smith TS, 2012, URSUS, V23, P179, DOI 10.2192/URSUS-D-11-00020.1
   Smith TS, 2004, J WILDLIFE MANAGE, V68, P233, DOI 10.2193/0022-541X(2004)068[0233:DOIFBC]2.0.CO;2
   Soule ME, 2003, CONSERV BIOL, V17, P1238, DOI 10.1046/j.1523-1739.2003.01599.x
   Swenson JE, 1999, CAN J ZOOL, V77, P551, DOI 10.1139/cjz-77-4-551
   TANASICHUK RW, 1991, CAN J FISH AQUAT SCI, V48, P2118, DOI 10.1139/f91-251
   Taylor FCH, 1964, LIFE HIST PRESENT ST
   Thomas AC, 2011, MAR ECOL PROG SER, V441, P225, DOI 10.3354/meps09370
   Thornton T. F., 2010, J ECOLOGICAL ANTHR, V14, P81, DOI DOI 10.5038/2162-4593.14.1.7
   Van Daele LJ, 2012, URSUS, V23, P21
   Varpe O, 2005, OECOLOGIA, V146, P443, DOI 10.1007/s00442-005-0219-9
   Walters AW, 2009, CAN J FISH AQUAT SCI, V66, P439, DOI 10.1139/F09-008
   WARE DM, 1989, CAN J FISH AQUAT SCI, V46, P1776, DOI 10.1139/f89-225
   Willson MF, 2006, REV FISH BIOL FISHER, V16, P183, DOI 10.1007/s11160-006-9009-7
   Willson MF, 1998, BIOSCIENCE, V48, P455, DOI 10.2307/1313243
   Ziegltrum GJ, 2001, URSUS-SERIES, V12, P169
NR 73
TC 14
Z9 14
U1 2
U2 58
PU BMC
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
EI 1472-6785
J9 BMC ECOL
JI BMC Ecol.
PD MAY 26
PY 2015
VL 15
AR 14
DI 10.1186/s12898-015-0045-9
PG 18
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA CJ0JM
UT WOS:000355161600001
PM 26013706
OA Green Published, gold
DA 2022-02-10
ER

PT J
AU Yang, L
   Huang, QQ
   Song, XW
   Li, ML
   Hou, CP
   Xiong, ZX
AF Yang, Lei
   Huang, Qianqian
   Song, Xiaowei
   Li, Menglong
   Hou, Chunping
   Xiong, Zixiang
TI Girth Measurement Based on Multi-View Stereo Images for Garment Design
SO IEEE ACCESS
LA English
DT Article
DE Girth measurement; semantic segmentation; PSPNet; multi-view; stereo
   matching
ID PARAMETRIC DESIGN; BODY; PRECISION
AB In this paper, we propose a novel girth measurement system based on multi-view stereo images for garment design. Our system is set in a fixed location to capture three pairs of stereo images for the subject by six calibrated and synchronously triggered cameras. An important feature of this system is the use of an optimized semantic segmentation network that can efficiently segment the girth region in the captured six-view stereo images. Another important feature of this system is the use of color subspace classification and coordinate clustering that can effectively constrain the stereo matching within the scope of markers. Then, the system performs only on the corresponding clusters to extract stereo matching point pairs of markers correctly. The space coordinates of 3D point corresponding to each stereo matching point pair are calculated in each coordinate system of stereo cameras. The unified coordinates of these 3D markers are transformed from three different coordinate systems into one unified coordinate system. Girth is measured by curve fitting of these markers and calculating the length of the fitting curve. Our proposed system performs passive and intelligent girth measurement in garment design, and overcomes the problem of too many invalid stereo matching point pairs in girth measurement. Experimental results demonstrate its accuracy. Our system has a maximum bust measurement error of 1.28cm for woman and 1.31cm for man and a maximum waist measurement error of 1.18cm for woman and 0.99cm for man, which are within the error limit regulated by China national standards GB/T 2664-2017, 2665-2017, 2666-2017 and textile industry standard FZ/T 81004-2012.
C1 [Yang, Lei; Huang, Qianqian; Song, Xiaowei; Li, Menglong] Zhongyuan Univ Technol, Sch Elect & Informat, Zhengzhou 450007, Peoples R China.
   [Hou, Chunping] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Xiong, Zixiang] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
RP Yang, L; Song, XW (corresponding author), Zhongyuan Univ Technol, Sch Elect & Informat, Zhengzhou 450007, Peoples R China.
EM annieyanglei@163.com; sxw-tju@163.com
OI Yang, Lei/0000-0002-4864-2702; Huang, Qianqian/0000-0003-1468-6982; Li,
   Menglong/0000-0002-2766-7329
FU Science and Technology Innovation Team of Colleges and Universities in
   Henan Province [18IRTSTHN013]; Key Research Project of Colleges and
   Universities in Henan Province [19A510005]
FX This work was supported in part by the Science and Technology Innovation
   Team of Colleges and Universities in Henan Province under Grant
   18IRTSTHN013, and in part by the Key Research Project of Colleges and
   Universities in Henan Province under Grant 19A510005.
CR [Anonymous], 2017, GBT26662017 AQSIQ
   [Anonymous], 2012, 810042012 FZT AQSIQ
   [Anonymous], 2017, CHINA NATL STANDARD
   [Anonymous], 2017, 26652017 GBT AQSIQ
   [Anonymous], 2017, GBT26642017 AQSIQ
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   BenAbdelkader C, 2008, IEEE INT C AUT FAC G, P1, DOI [10.1109/AFGR.2008.4813453, DOI 10.1109/AFGR.2008.4813453]
   Bold N, 2019, IEEE ACCESS, V7, P164194, DOI 10.1109/ACCESS.2019.2952157
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Braganca S, 2018, WORK, V59, P325, DOI 10.3233/WOR-182684
   Braganca S, 2017, WORK, V57, P9, DOI 10.3233/WOR-172532
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y, 2016, IEEE T VIS COMPUT GR, V22, P2000, DOI 10.1109/TVCG.2015.2478779
   Daanen H. A. M., 1997, P SOC PHOTO-OPT INS, V3023, P34
   Dasgupta B, 1996, J I EL TELECOM ENG, V42, P3
   Deane AS, 2005, AM J PHYS ANTHROPOL, V128, P630, DOI 10.1002/ajpa.20202
   Dekker L., 2000, THESIS
   Gu BF, 2019, TEXT RES J, V89, P3792, DOI 10.1177/0040517518821914
   Gu BF, 2017, TEXT RES J, V87, P669, DOI 10.1177/0040517516636001
   Gu BF, 2017, J TEXT I, V108, P140, DOI 10.1080/00405000.2016.1160756
   Han H, 2010, INT J IND ERGONOM, V40, P530, DOI 10.1016/j.ergon.2010.06.002
   Han SD, 2009, IEEE T IMAGE PROCESS, V18, P2289, DOI 10.1109/TIP.2009.2025560
   Hisham MB, 2015, IEEE ST CONF RES DEV, P100, DOI 10.1109/SCORED.2015.7449303
   Hu JP, 2009, I C COMM SOFTW NET, P16, DOI 10.1109/ICCSN.2009.167
   Jiang XY, 2020, IEEE T IMAGE PROCESS, V29, P736, DOI 10.1109/TIP.2019.2934572
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kpodjedo S, 2013, IEEE T SOFTWARE ENG, V39, P1090, DOI 10.1109/TSE.2013.9
   Lazaridis G, 2006, IEEE T IMAGE PROCESS, V15, P2343, DOI 10.1109/TIP.2006.877346
   Li ZX, 2013, IEEE ENG MED BIO, P366, DOI 10.1109/EMBC.2013.6609513
   LIN GS, 2016, PROC CVPR IEEE, P3194, DOI DOI 10.1109/CVPR.2016.348
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu KX, 2019, INT J IND ERGONOM, V72, P212, DOI 10.1016/j.ergon.2019.05.012
   Liu KX, 2019, IEEE ACCESS, V7, P48830, DOI 10.1109/ACCESS.2019.2906261
   Liu KX, 2018, COMPUT AIDED DESIGN, V104, P113, DOI 10.1016/j.cad.2018.07.003
   Liu KX, 2018, INT J IND ERGONOM, V65, P46, DOI 10.1016/j.ergon.2018.01.013
   Liu KX, 2017, KNOWL-BASED SYST, V133, P174, DOI 10.1016/j.knosys.2017.07.007
   Liu YY, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0195600
   Liu ZB, 2017, IEEE T CYBERNETICS, V47, P695, DOI 10.1109/TCYB.2016.2524406
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JM, 2010, IEEE T INSTRUM MEAS, V59, P2048, DOI 10.1109/TIM.2009.2031847
   Ma JY, 2020, IEEE T IND ELECTRON, V67, P8649, DOI 10.1109/TIE.2019.2950866
   Ma JY, 2019, IEEE T IMAGE PROCESS, V28, P4045, DOI 10.1109/TIP.2019.2906490
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI 10.1007/s11263-018-1117-z
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017
   Miao J Q., 2014, ADV MAT RES, V989-994, P2088
   Nae-Joung Kwak, 2004, Proceedings of 2004 International Symposium on Intelligent Signal Processing And Communication Systems ISPACS 2004 (IEEE Cat. No.04EX910), P555, DOI 10.1109/ISPACS.2004.1439118
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   [潘秋娟 Pan Qiujuan], 2010, [激光与红外, Laser and Infrared], V40, P821
   Penders B, 2015, J PEDIATR ENDOCR MET, V28, P1357, DOI 10.1515/jpem-2015-0172
   Peng CL, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107498
   Ran Q, 2020, COMPUT GRAPH-UK, V87, P43, DOI 10.1016/j.cag.2020.01.003
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Reddy BS, 1996, IEEE T IMAGE PROCESS, V5, P1266, DOI 10.1109/83.506761
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Song XW, 2013, PROC SPIE, V8908, DOI 10.1117/12.2034809
   Su JQ, 2019, TEXT RES J, V89, P2199, DOI 10.1177/0040517518790975
   Tao Y, 2017, CHIN CONTR CONF, P4288, DOI 10.23919/ChiCC.2017.8028032
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Treleaven P, 2007, COMPUTER, V40, P28, DOI 10.1109/MC.2007.225
   Vanne J, 2006, IEEE T CIRC SYST VID, V16, P876, DOI 10.1109/TCSVT.2006.877150
   WONG AKC, 1989, IEEE T SYST MAN CYB, V19, P866, DOI 10.1109/21.35351
   Xiao P, 2019, IEEE ACCESS, V7, P164152, DOI 10.1109/ACCESS.2019.2933538
   Xuan Guo, 1993, Computer Analysis of Images and Patterns. 5th International Conference, CAIP '93 Proceedings, P518
   Yang JF, 2017, PATTERN RECOGN, V66, P34, DOI 10.1016/j.patcog.2017.01.008
   Zhang G., 2008, VISION MEASUREMENTS
   Zhang J, 2008, 2008 INTERNATIONAL MULTISYMPOSIUMS ON COMPUTER AND COMPUTATIONAL SCIENCES (IMSCCS), P102, DOI 10.1109/IMSCCS.2008.14
   Zhang JJ, 2020, J TEXT I, V111, P1324, DOI 10.1080/00405000.2019.1694351
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289
   Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 74
TC 0
Z9 0
U1 15
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2020
VL 8
BP 160338
EP 160354
DI 10.1109/ACCESS.2020.3021019
PG 17
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Engineering; Telecommunications
GA NP4LN
UT WOS:000570149300001
OA gold
DA 2022-02-10
ER

PT J
AU Brehme, CS
   Tracey, JA
   Ewing, BAI
   Hobbs, MT
   Launer, AE
   Matsuda, TA
   Adelsheim, EMC
   Fisher, RN
AF Brehme, Cheryl S.
   Tracey, Jeff A.
   Ewing, Brittany A. I.
   Hobbs, Michael T.
   Launer, Alan E.
   Matsuda, Tritia A.
   Adelsheim, Esther M. Cole
   Fisher, Robert N.
TI Responses of migratory amphibians to barrier fencing inform the spacing
   of road underpasses: a case study with California tiger salamanders
   (Ambystoma californiense) in Stanford, CA, USA
SO GLOBAL ECOLOGY AND CONSERVATION
LA English
DT Article
DE Road ecology; Movement ecology; Connectivity; Amphibians; Road
   mortality; Passages; Roads; Tunnels
ID LIFE-HISTORY; BUFFER ZONES; HABITAT USE; ORIENTATION; POPULATION;
   MOVEMENT; DISPERSAL; DISTANCES; MORTALITY; PATTERNS
AB Migratory amphibians are at high risk of negative impacts when roads intersect their upland and breeding habitats. Road mortality can reduce population abundance, survivorship, breeding, recruitment, and probability of long-term persistence. Increasingly, environmental planners recommend installation of under-road tunnels with barrier fencing to reduce mortality and direct amphibians towards the passages. Often, the permeability of these barrier and passage systems to amphibian population movements are unknown. We studied the movements of California tiger salamanders (CTS: Ambystoma californiense) in relation to solid and mesh barrier fencing attached to a 3-tunnel system between upland and breeding habitats in Stanford, California. We deployed active-trigger cameras along the fencing, used pattern recognition software to identify individuals by their unique spot patterns, and calculated individual salamander movement distances, speed, direction changes, and "success" at reaching the tunnel system. We found that migrating adult CTS moved an average of 40 m along barrier fencing before turning back into the habitat or "giving-up". This short distance, in comparison to long migratory movements, may be explained by the orientation mechanisms salamanders use to reach their breeding sites. The probability CTS found a passage decreased rapidly with increasing distance from the tunnel system, particularly if individuals turned the "wrong" way after encountering the fence. Salamanders changed directions more often and spent more time along mesh fencing. Our results suggest that a maximum of 12.5 m between passages along CTS migration routes should allow approximately 90% of adult salamanders to encounter road crossings. Additionally, use of solid fencing or a visual barrier on mesh fencing may help to lead salamanders to passages most efficiently. These considerations can assist those seeking to design effective road mitigation for CTS and other migratory amphibians.
C1 [Brehme, Cheryl S.; Tracey, Jeff A.; Ewing, Brittany A. I.; Matsuda, Tritia A.; Fisher, Robert N.] US Geol Survey, Western Ecol Res Ctr, 4165 Spruance Rd, San Diego, CA 92101 USA.
   [Hobbs, Michael T.] Hobbs Ecol, 4316 Bayne Pl, San Jose, CA 95130 USA.
   [Launer, Alan E.; Adelsheim, Esther M. Cole] Stanford Univ, Land Use & Environm Planning, 3160 Porter Dr, Stanford, CA 94305 USA.
RP Brehme, CS (corresponding author), US Geol Survey, Western Ecol Res Ctr, 4165 Spruance Rd, San Diego, CA 92101 USA.
EM cbrehme@usgs.gov; jatracey@usgs.gov; bewing@usgs.gov;
   mhobbs2928@gmail.com; aelauner@stanford.edu; tmatsuda@usgs.gov;
   ecolea@stanford.edu; rfisher@usgs.gov
RI Fisher, Robert/AAA-7301-2022
OI Fisher, Robert/0000-0002-2956-3240; Brehme, Cheryl/0000-0001-8904-3354
FU California Department of Transportation, Division of Research,
   Innovation and System Information [65A0553]
FX This work was supported by the California Department of Transportation,
   Division of Research, Innovation and System Information [Agreement
   65A0553]. Stanford University Conservation Program; and the Ecosystems
   Mission Area of the United States Geological Survey.
CR Andrews KM, 2015, WILDL MANAGE CONSERV, P1
   Bain TK, 2017, HERPETOL CONSERV BIO, V12, P192
   BARRY SJ, 1994, J HERPETOL, V28, P159, DOI 10.2307/1564615
   Beebee TJC, 2013, CONSERV BIOL, V27, P657, DOI 10.1111/cobi.12063
   Bolster B.C., 2010, 20104 FISH GAM COMM
   Boyle Sean P., 2019, Canadian Field-Naturalist, V133, P43, DOI 10.22621/cfn.v133i1.2076
   Brehm K., 1989, P29
   Brehme C.S., 2021, RES INFORM CALTRANS
   Brehme C.S., RES INFORM CALTRANS
   Brehme CS, 2018, LANDSCAPE ECOL, V33, P911, DOI 10.1007/s10980-018-0640-1
   Brehme CS, 2013, CONSERV BIOL, V27, P710, DOI 10.1111/cobi.12081
   Carr JA, 2011, HORMONES AND REPRODUCTION OF VERTEBRATES, VOL 2: AMPHIBIANS, P99
   City of Menlo Park, 2017, TRAFF VOL DAT
   Clevenger A.P., 2011, FHWACFLTD11003
   Congdon P, 2006, BAYESIAN STAT MODELI, V2nd, P221
   Cook David G., 2006, Northwestern Naturalist, V87, P215, DOI 10.1898/1051-1733(2006)87[215:DABPOT]2.0.CO;2
   Denwood MJ, 2016, J STAT SOFTW, V71, P1, DOI 10.18637/jss.v071.i09
   Eggert C, 2002, HERPETOL J, V12, P69
   Eigenbrod F, 2008, BIOL CONSERV, V141, P35, DOI 10.1016/j.biocon.2007.08.025
   Ezell E., OFF RES SPONS PROGR
   Gunson K, 2016, BEST MANAGEMENT PRAC
   Hamer AJ, 2008, BIOL CONSERV, V141, P2432, DOI 10.1016/j.biocon.2008.07.020
   Hamer AJ, 2015, HANDBOOK OF ROAD ECOLOGY, P261
   Helldin JO, 2019, PEERJ, V7, DOI 10.7717/peerj.7518
   Hobbs MT, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185026
   Huijser M.P, 2019, 2525 NCHRP AASHTO CO
   Iuell B., 2003, COST 341 WILDLIFE TR
   Jackson S.D., 1989, P93
   Jaeger JAG, 2004, CONSERV BIOL, V18, P1651, DOI 10.1111/j.1523-1739.2004.00304.x
   Jarvis LE, 2019, EUR J WILDLIFE RES, V65, DOI 10.1007/s10344-019-1263-9
   Jochimsen D.M., 2004, LIT REV EFFECTS ROAD
   Kovar R, 2009, AMPHIBIA-REPTILIA, V30, P367, DOI 10.1163/156853809788795236
   Langton T.E, 1989, AMPHIBIANS ROADS
   Langton T.E.S, 2017, AMPHIBIAN REPTILE HI
   Langton T.E.S, 2021, BEST MANAGEMENT PRAC
   Launer A.E., CALIFORNIA TIGER SAL
   Launer A.E., COMMUNICATION
   Loredo I, 1996, J HERPETOL, V30, P282, DOI 10.2307/1565527
   Macpherson MR, 2021, GLOB ECOL CONSERV, V26, DOI 10.1016/j.gecco.2021.e01471
   Maida JR, 2020, CAN J ZOOL, V98, P1, DOI 10.1139/cjz-2019-0110
   Markle CE, 2017, WILDLIFE SOC B, V41, P342, DOI 10.1002/wsb.767
   Matos C, 2019, ANIM CONSERV, V22, P285, DOI 10.1111/acv.12467
   Milburn-Rodriguez J.C, 2016, ROAD MORTALITY MITIG
   Navas Carlos A., 2016, P155
   Orloff SG, 2011, HERPETOL CONSERV BIO, V6, P266
   Orlowski G, 2007, AMPHIBIA-REPTILIA, V28, P25, DOI 10.1163/156853807779799045
   Ottburg FGWA, 2019, FRONT ECOL EVOL, V7, DOI 10.3389/fevo.2019.00023
   Pagnucco KS, 2012, COPEIA, P331, DOI 10.1643/CE-10-128
   Petrovan SO, 2019, BIOL CONSERV, V236, P252, DOI 10.1016/j.biocon.2019.05.023
   Plummer M., 2003, P 3 INT WORKSH DISTR, P125
   Rasband W.S., IMAGEJ
   Roth TC, 2015, CURR BIOL, V25, P333, DOI 10.1016/j.cub.2014.11.048
   Ruby Douglas E., 1994, Herpetological Monographs, V8, P144, DOI 10.2307/1467078
   Russell Anthony P., 2005, P151, DOI 10.1007/3-540-26604-6_7
   Schmidt Benedikt R., 2008, P157
   Searcy CA, 2014, ECOLOGY, V95, P68, DOI 10.1890/13-0120.1
   Searcy CA, 2013, BIOL CONSERV, V158, P80, DOI 10.1016/j.biocon.2012.08.033
   Semlitsch RD, 2008, J WILDLIFE MANAGE, V72, P260, DOI 10.2193/2007-082
   Semlitsch RD, 1998, CONSERV BIOL, V12, P1113, DOI 10.1046/j.1523-1739.1998.97274.x
   Semlitsch RD, 2003, CONSERV BIOL, V17, P1219, DOI 10.1046/j.1523-1739.2003.02177.x
   SHOOP CR, 1965, SCIENCE, V149, P558, DOI 10.1126/science.149.3683.558
   SINSCH U, 1990, ETHOL ECOL EVOL, V2, P65, DOI 10.1080/08927014.1990.9525494
   Sinsch U, 2006, MAR FRESHW BEHAV PHY, V39, P65, DOI 10.1080/10236240600562794
   Trenham PC, 2005, ECOL APPL, V15, P1158, DOI 10.1890/04-1150
   Trenham PC, 2000, COPEIA, P365, DOI 10.1643/0045-8511(2000)000[0365:LHADVI]2.0.CO;2
   Trenham PC, 2003, ECOL APPL, V13, P1522, DOI 10.1890/02-5206
   Trenham PC, 2001, ECOLOGY, V82, P3519, DOI 10.2307/2680169
   Twitty V. C., 1941, Copeia Ann Arbor, V1941, P1, DOI 10.2307/1437692
   U.S. Fish and Wildlife (USFWS), 2009, CAL TIG SAL AMB CAL
   van der Ree R, 2015, HANDBOOK OF ROAD ECOLOGY, P159
   van der Ree R, 2015, HANDBOOK OF ROAD ECOLOGY, P71
   Van Tienhoven AM, 2007, J APPL ECOL, V44, P273, DOI 10.1111/j.1365-2664.2006.01273.x
   Waye HL, 2013, HERPETOL CONSERV BIO, V8, P419
   Woltz HW, 2008, BIOL CONSERV, V141, P2745, DOI 10.1016/j.biocon.2008.08.010
NR 75
TC 0
Z9 0
U1 8
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
EI 2351-9894
J9 GLOB ECOL CONSERV
JI Glob. Ecol. Conserv.
PD NOV
PY 2021
VL 31
AR e01857
DI 10.1016/j.gecco.2021.e01857
PG 14
WC Biodiversity Conservation; Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation; Environmental Sciences & Ecology
GA WI8YR
UT WOS:000708641300013
OA gold
DA 2022-02-10
ER

PT J
AU Loredo, AI
   Rudd, JL
   Foley, JE
   Clifford, DL
   Cypher, BL
AF Loredo, Ariel Irene
   Rudd, Jamie Leann
   Foley, Janet Elizabeth
   Clifford, Deana Louise
   Cypher, Brian Leslie
TI CLIMATIC SUITABILITY OF SAN JOAQUIN KIT FOX (VULPES MACROTIS MUTICA)
   DENS FOR SARCOPTIC MANGE (SARCOPTES SCABIEI) TRANSMISSION
SO JOURNAL OF WILDLIFE DISEASES
LA English
DT Article
DE Climate suitability; dens; San Joaquin kit fox; sarcoptic mange;
   Sarcoptes scabiei; Vulpes macrotis mutica
ID BURROWS
AB More than 460 cases of sarcoptic mange (Sarcoptes scabiei) in endangered San Joaquin kit foxes (SJKF; Vulpes macrotis mutica) have been reported in Bakersfield, California, US. Because SJKF are a den-obligate species, their dens have been proposed as a route of transmission. We determined whether SJKF den temperatures and humidities could support mite off-host survival based on previously published estimates of off-host mite survival times. We monitored SJKF dens for 6 d in summer and winter of 2017 and 2018 using temperature- and humidity-sensing data loggers placed within the dens. Motion-triggered cameras monitored animal use of and entrances into the dens. Linear regression models were fitted to the published mite survival data to predict estimated mite survival time (EMST) in SJKF dens based on observed mean temperature and humidity of the den. Den covariates including irrigation, type of den, and season were then fitted to a mixed effects linear model to predict EMST. The average EMST across various habitats in Bakersfield was 4.8 d; the longest EMST was 7.1 d for dens in habitats with irrigated grass in the winter. Den climatic conditions in Bakersfield may support off-host mite survival through a timeframe adequate for revisitation by another fox. The finding that irrigation may enhance EMST suggested that risk to foxes varied with den type and that mitigation strategies may need to vary with den types.
C1 [Loredo, Ariel Irene; Rudd, Jamie Leann; Foley, Janet Elizabeth; Clifford, Deana Louise] Univ Calif Davis, Sch Vet Med, 1320 Tupper Hall, Davis, CA 95616 USA.
   [Rudd, Jamie Leann; Clifford, Deana Louise] Calif Dept Fish & Wildlife, 1701 Nimbus Rd, Rancho Cordova, CA 95670 USA.
   [Cypher, Brian Leslie] Calif State Univ Stanislaus, Endangered Species Recovery Program, One Univ Circle, Turlock, CA 95382 USA.
RP Foley, JE (corresponding author), Univ Calif Davis, Sch Vet Med, 1320 Tupper Hall, Davis, CA 95616 USA.
EM jefoley@ucdavis.edu
FU Cooperative Endangered Species Conservation Fund (US Fish and Wildlife
   Service-Section 6) subaward (California State University, Stanislaus,
   Endangered Species Recovery Program); Hagman Fellowship through the
   Masters of Preventive Veterinary Medicine Program at University of
   California Davis; Morris Animal Foundation [D17ZO-066]
FX We thank Sarena and Shabrina Dickerson for their help in the field and
   placement of iButtons, and Larry Saslaw for his assistance with
   developing methods to place iButtons deeper into dens. Funding grants
   include a Cooperative Endangered Species Conservation Fund (US Fish and
   Wildlife Service-Section 6) subaward (California State University,
   Stanislaus, Endangered Species Recovery Program). Student fees were
   funded in part by the Hagman Fellowship through the Masters of
   Preventive Veterinary Medicine Program at University of California
   Davis. This work was funded in part by a grant (D17ZO-066) from the
   Morris Animal Foundation.
CR Arlian LG, 2017, PARASITE VECTOR, V10, DOI 10.1186/s13071-017-2234-1
   ARLIAN LG, 1984, J AM ACAD DERMATOL, V11, P210, DOI 10.1016/S0190-9622(84)70151-4
   ARLIAN LG, 1989, ANNU REV ENTOMOL, V34, P139, DOI 10.1146/annurev.en.34.010189.001035
   BARNES AM, 1972, J MED ENTOMOL, V9, P330, DOI 10.1093/jmedent/9.4.330
   Burnham KP, 2004, SOCIOL METHOD RES, V33, P261, DOI 10.1177/0049124104268644
   Curtis C. F., 2012, UK Vet: Companion Animal, V17, P32, DOI 10.1111/j.2044-3862.2012.00222.x
   Cypher BL, 2017, J WILDLIFE DIS, V53, P46, DOI 10.7589/2016-05-098
   Cypher Brian L., 2013, Canid News, V16, P25
   Cypher Brian L., 2010, P49
   Koopman ME, 1998, J WILDLIFE MANAGE, V62, P373, DOI 10.2307/3802301
   Mellanby K., 1942, Bulletin of Entomological Research London, V33, P267, DOI 10.1017/S0007485300026584
   National Oceanic and Atmospheric Administration, 2018, CPC GLOB PREC DAT
   Old JM, 2018, TRANSBOUND EMERG DIS, V65, P399, DOI 10.1111/tbed.12770
   PENCE DB, 1994, J WILDLIFE MANAGE, V58, P624, DOI 10.2307/3809675
   Perez Jesus M., 1997, Journal of Wildlife Research, V2, P86
   Pinheiro J.C., 2000, MIXED EFFECTS MODELS
   R Development Core Team, 2018, R LANG ENG STAT COMP
   Reese EA, 1992, EGG106172156 US DEP
   Scott DW, 2001, MULLER KIRKS SMALL A, P476
   Seery DB, 2003, J MED ENTOMOL, V40, P718, DOI 10.1603/0022-2585-40.5.718
   Shibata F, 1999, MAMMALIA, V63, P281, DOI 10.1515/mamm.1999.63.3.281
   Shimmin GA, 2002, J ZOOL, V258, P469, DOI 10.1017/S0952836902001620
   Williams D.F., 1998, RECOVERY PLAN UPLAND
NR 23
TC 3
Z9 3
U1 3
U2 10
PU WILDLIFE DISEASE ASSOC, INC
PI LAWRENCE
PA 810 EAST 10TH ST, LAWRENCE, KS 66044-8897 USA
SN 0090-3558
EI 1943-3700
J9 J WILDLIFE DIS
JI J. Wildl. Dis.
PD JAN
PY 2020
VL 56
IS 1
BP 126
EP 133
DI 10.7589/2019-02-035
PG 8
WC Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Veterinary Sciences
GA KA7BG
UT WOS:000505950900012
PM 31295083
DA 2022-02-10
ER

PT J
AU Hereward, HFR
   Facey, RJ
   Sargent, AJ
   Roda, S
   Couldwell, ML
   Renshaw, EL
   Shaw, KH
   Devlin, JJ
   Long, SE
   Porter, B
   Henderson, JM
   Emmett, CL
   Astbury, L
   Maggs, L
   Rands, SA
   Thomas, RJ
AF Hereward, Hannah F. R.
   Facey, Richard J.
   Sargent, Alyssa J.
   Roda, Sara
   Couldwell, Matthew L.
   Renshaw, Emma L.
   Shaw, Katie H.
   Devlin, Jack J.
   Long, Sarah E.
   Porter, Ben J.
   Henderson, Jodie M.
   Emmett, Christa L.
   Astbury, Laura
   Maggs, Luke
   Rands, Sean A.
   Thomas, Robert J.
TI Raspberry Pi nest cameras: An affordable tool for remote behavioral and
   conservation monitoring of bird nests
SO ECOLOGY AND EVOLUTION
LA English
DT Article
DE Animal behavior; bespoke camera; burrow-nesting; interspecific
   interactions; nest box; Raspberry Pi; seabirds; storm-petrel
ID WILDLIFE; TECHNOLOGY; SYSTEM; TRAP
AB 1. Bespoke (custom-built) Raspberry Pi cameras are increasingly popular research tools in the fields of behavioral ecology and conservation, because of their comparative flexibility in programmable settings, ability to be paired with other sensors, and because they are typically cheaper than commercially built models.
   2. Here, we describe a novel, Raspberry Pi-based camera system that is fully portable and yet weatherproof-especially to humidity and salt spray. The camera was paired with a passive infrared sensor, to create a movement-triggered camera capable of recording videos over a 24-hr period. We describe an example deployment involving "retro-fitting" these cameras into artificial nest boxes on Praia Islet, Azores archipelago, Portugal, to monitor the behaviors and interspecific interactions of two sympatric species of storm-petrel (Monteiro's storm-petrel Hydrobates monteiroi and Madeiran storm-petrel Hydrobates castro) during their respective breeding seasons.
   3. Of the 138 deployments, 70% of all deployments were deemed to be "Successful" (Successful was defined as continuous footage being recorded for more than one hour without an interruption), which equated to 87% of the individual 30-s videos. The bespoke cameras proved to be easily portable between 54 different nests and reasonably weatherproof (similar to 14% of deployments classed as "Partial" or "Failure" deployments were specifically due to the weather/humidity), and we make further trouble-shooting suggestions to mitigate additional weather-related failures.
   4. Here, we have shown that this system is fully portable and capable of coping with salt spray and humidity, and consequently, the camera-build methods and scripts could be applied easily to many different species that also utilize cavities, burrows, and artificial nests, and can potentially be adapted for other wildlife monitoring situations to provide novel insights into species-specific daily cycles of behaviors and interspecies interactions.
C1 [Hereward, Hannah F. R.; Facey, Richard J.; Sargent, Alyssa J.; Roda, Sara; Couldwell, Matthew L.; Renshaw, Emma L.; Shaw, Katie H.; Devlin, Jack J.; Long, Sarah E.; Porter, Ben J.; Henderson, Jodie M.; Emmett, Christa L.; Astbury, Laura; Thomas, Robert J.] Cardiff Univ, Cardiff Sch Biosci, Cardiff, Wales.
   [Sargent, Alyssa J.] Univ Washington, Dept Biol, Seattle, WA 98195 USA.
   [Roda, Sara] A Rocha, Cruzhina, Alvor, Portugal.
   [Couldwell, Matthew L.] Gypseywood Cottage, York, N Yorkshire, England.
   [Shaw, Katie H.] Univ Cambridge, Cambridge, England.
   [Devlin, Jack J.] Univ Kentucky, Lexington, KY USA.
   [Porter, Ben J.] Tan & Garn, Rhiw, Wales.
   [Emmett, Christa L.] Univ West England, Dept Appl Sci, Bristol, Avon, England.
   [Maggs, Luke] D3 Data Driven Decis, Cardiff, Wales.
   [Rands, Sean A.] Univ Bristol, Sch Biol Sci, Bristol, Avon, England.
RP Hereward, HFR (corresponding author), Cardiff Univ, Cardiff Sch Biosci, Cardiff, Wales.
EM hannah.hereward@gmail.com
FU Natural Environment Research Council Great Western Four+ Doctoral
   Training Partnership studentship [NE/L002434/1]; Cardiff University;
   Project CASE partner - Eco-explore Community Interest Company
FX Natural Environment Research Council Great Western Four+ Doctoral
   Training Partnership studentship, Grant/Award Number: NE/L002434/1;
   Cardiff University; Project CASE partner -Eco-explore Community Interest
   Company
CR Alarcon-Nieto G, 2018, METHODS ECOL EVOL, V9, P1536, DOI 10.1111/2041-210X.13005
   Allan BM, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2163
   Bolton M, 2004, BIOL CONSERV, V116, P73, DOI 10.1016/S0006-3207(03)00178-2
   Bolton M, 2008, IBIS, V150, P717, DOI 10.1111/j.1474-919X.2008.00854.x
   Bried Joel, 2009, Ecological Restoration, V27, P27, DOI 10.3368/er.27.1.27
   Camacho L, 2017, IEEE SENS J, V17, P8000, DOI 10.1109/JSEN.2017.2760254
   Caravaggi A, 2020, CONSERV SCI PRACT, V2, DOI 10.1111/csp2.239
   Cox WA, 2012, STUD AVIAN BIOL, P185
   Cutler TL, 1999, WILDLIFE SOC B, V27, P571
   Edney AJ, 2021, IBIS, V163, P317, DOI 10.1111/ibi.12871
   Greene A, 2020, HARDWAREX, V7, DOI 10.1016/j.ohx.2019.e00089
   Greenville AC, 2016, SCIENCE, V353, P1360, DOI 10.1126/science.aag3057
   Hereward H.F.R., VIDEO MONITORI UNPUB
   Hereward H.F.R., SEABIRD
   Johnston SJ, 2017, ELECTRONICS-SWITZ, V6, DOI 10.3390/electronics6030051
   Jolles JW, 2021, METHODS ECOL EVOL, V12, P1562, DOI 10.1111/2041-210X.13652
   Jolles JW, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2017.2629
   Kallmyer NE, 2019, PLOS BIOL, V17, DOI 10.1371/journal.pbio.3000406
   Maldague X., 2019, PROCEEDINGS, V27, P11, DOI [10.3390/proceedings2019027011, DOI 10.3390/PROCEEDINGS2019027011]
   McBride WJ, 2019, ECOL INFORM, V54, DOI 10.1016/j.ecoinf.2019.101016
   Meek PD, 2014, BIODIVERS CONSERV, V23, P2321, DOI 10.1007/s10531-014-0712-8
   Meek PD, 2012, WILDLIFE RES, V39, P649, DOI 10.1071/WR12138
   Monteiro LR, 1998, PHILOS T ROY SOC B, V353, P945, DOI 10.1098/rstb.1998.0259
   Mouy X, 2020, HARDWAREX, V8, DOI 10.1016/j.ohx.2020.e00110
   Nazir S, 2017, INT J SATELL COMM N, V35, P201, DOI 10.1002/sat.1176
   Nazir S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0169758
   O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
   Phillips BT, 2019, DEEP-SEA RES PT I, V153, DOI 10.1016/j.dsr.2019.103136
   Prinz ACB, 2016, J FIELD ORNITHOL, V87, P427, DOI 10.1111/jofo.12182
   Reif V, 2006, EUR J WILDLIFE RES, V52, P251, DOI 10.1007/s10344-006-0039-1
   Scheibe KM, 2008, EUR J WILDLIFE RES, V54, P53, DOI 10.1007/s10344-007-0108-0
   Swann DE, 2004, WILDLIFE SOC B, V32, P357, DOI 10.2193/0091-7648(2004)32[357:ICFDWA]2.0.CO;2
   Thomas, 2021, SINGLE BOARD M UNPUB
   Thomas R.J., 2022, DRYAD DATASET, DOI [10.5061/dryad.9w0vt4bfb, DOI 10.5061/DRYAD.9W0VT4BFB]
   Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
   Youngblood M, 2020, RINGING MIGR, P1, DOI [10.1080/03078698.2019.1759908, DOI 10.1080/03078698.2019.1759908]
   Zarybnicka M, 2016, METHODS ECOL EVOL, V7, P483, DOI 10.1111/2041-210X.12509
NR 37
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2045-7758
J9 ECOL EVOL
JI Ecol. Evol.
PD NOV
PY 2021
VL 11
IS 21
BP 14585
EP 14597
DI 10.1002/ece3.8127
EA OCT 2021
PG 13
WC Ecology; Evolutionary Biology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology; Evolutionary Biology
GA WS3KI
UT WOS:000705967700001
PM 34765127
OA gold, Green Published, Green Accepted
DA 2022-02-10
ER

PT J
AU Edney, AJ
   Wood, MJ
AF Edney, Alice J.
   Wood, Matt J.
TI Applications of digital imaging and analysis in seabird monitoring and
   research
SO IBIS
LA English
DT Review
DE conservation photography; population ecology; remote sensing; seabird;
   videography
ID TIME-LAPSE PHOTOGRAPHY; UNMANNED AERIAL VEHICLES; INTRODUCED MAMMALS;
   SATELLITE IMAGERY; BREEDING SUCCESS; NESTING SEABIRD; VIDEO CAMERAS;
   PREDATOR; REVEAL; CENSUS
AB Rapid advances in digital imaging technology offer efficient and cost-effective methods for measuring seabird abundance, breeding success, phenology, survival and diet. These methods can facilitate understanding of long-term population trends, and the design and implementation of successful conservation strategies. This paper reviews the suitability of satellites, manned aircraft, unmanned aerial vehicles (UAVs), and fixed-position, handheld and animal-borne cameras for recording digital photographs and videos used to measure seabird demographic and behavioural parameters. It considers the disturbance impacts, accuracy of results obtained, cost-effectiveness and scale of monitoring possible compared with 'traditional' fieldworker methods. Given the ease of collecting large amounts of imagery, image processing is an important step in realizing the potential of this technology. The effectiveness of manual, semi-automated and automated image processing is also reviewed. Satellites, manned aircraft and UAVs have most commonly been used for population counts. Spatial resolution is lowest in satellites, limiting monitoring to large species and those with obvious signs of presence, such as penguins. Conversely, UAVs have the highest spatial resolution, which has allowed fine-scale measurements of foraging behaviour. Time-lapse cameras are more cost-effective for collecting time-series data such as breeding success and phenology, as human visits are only required infrequently for maintenance. However, the colony of interest must be observable from a single vantage point. Handheld, animal-borne and motion-triggered cameras have fewer cost-effective uses but have provided information on seabird diet, foraging behaviour and nest predation. The last of these has been important for understanding the impact of invasive mammals on seabird breeding success. Advances in automated image analysis are increasing the suitability of digital photography and videography to facilitate and/or replace traditional seabird monitoring methods. Machine-learning algorithms, such asPengbot, have allowed rapid identification of birds, although training requires thousands of pre-annotated photographs. Digital imaging has considerable potential in seabird monitoring, provided that appropriate choices are available for both image capture technology and image processing. These technologies offer opportunities to collect data in remote locations and increase the number of sites monitored. The potential to include such solutions in seabird monitoring and research will develop as the technology evolves, which will be of benefit given funding challenges in monitoring and conservation.
C1 [Edney, Alice J.; Wood, Matt J.] Univ Gloucestershire, Sch Nat & Social Sci, Francis Close Hall, Cheltenham GL50 4AZ, Glos, England.
RP Edney, AJ (corresponding author), Univ Gloucestershire, Sch Nat & Social Sci, Francis Close Hall, Cheltenham GL50 4AZ, Glos, England.
EM aliceedney4@gmail.com
RI Edney, Alice/AAB-1345-2021; Wood, Matt/ABD-6183-2020
OI Edney, Alice/0000-0003-1021-1533; Wood, Matt/0000-0003-0920-8396
FU University of Gloucestershire's Environmental Dynamics and Governance
   RPA
FX We would like to thank Sarah Money and Peter Fretwell for providing
   photographs to include in the paper. We would also like to thank the
   University of Gloucestershire's Environmental Dynamics and Governance
   RPA for awarding A.J.E. a bursary to undertake an MSc by Research. Two
   anonymous reviewers provided helpful feedback on the manuscript.
CR Ancel A, 2017, GLOB ECOL CONSERV, V9, P171, DOI 10.1016/j.gecco.2017.01.003
   Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150
   Anker-Nilssen T, 1996, WILDLIFE BIOL, V2, P17, DOI 10.2981/wlb.1996.004
   Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30
   Barrett RT, 2007, ICES J MAR SCI, V64, P1675, DOI 10.1093/icesjms/fsm152
   Bibby C.J., 2000, BIRD CENSUS TECHNIQU
   Bicknell AWJ, 2016, FRONT ECOL ENVIRON, V14, P424, DOI 10.1002/fee.1322
   Black C, 2018, ECOL EVOL, V8, P8286, DOI 10.1002/ece3.4160
   Black C, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0193532
   Black C, 2017, AUK, V134, P520, DOI 10.1642/AUK-16-69.1
   Black Caitlin E., 2018, Seabird, V31, P1
   Bluff LA, 2008, BIOL LETTERS, V4, P319, DOI 10.1098/rsbl.2008.0075
   Bolton M, 2007, J FIELD ORNITHOL, V78, P213, DOI 10.1111/j.1557-9263.2007.00104.x
   Borowicz A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22313-w
   Brisson-Curadeau E, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-18202-3
   Buckland ST, 2012, J APPL ECOL, V49, P960, DOI 10.1111/j.1365-2664.2012.02150.x
   Camphuysen K., 2004, STANDARDISED SEABIRD
   Chabot D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122588
   Collins PM, 2014, WILSON J ORNITHOL, V126, P158, DOI 10.1676/13-141.1
   Cook TR, 2020, CONSERV PHYSIOL, V8, DOI 10.1093/conphys/coz109
   COWARDIN LEWIS M., 1965, J WILDLIFE MANAGE, V29, P636, DOI 10.2307/3798066
   Croxall JP, 2012, BIRD CONSERV INT, V22, P1, DOI 10.1017/S0959270912000020
   Cutler TL, 1999, WILDLIFE SOC B, V27, P571
   Davies D, 2015, AVIAN CONSERV ECOL, V10, DOI 10.5751/ACE-00738-100105
   DODGE WENDELL E., 1960, JOUR WILDLIFE MANAGEMENT, V24, P340, DOI 10.2307/3797527
   Dolliver J.E., 2019, THESIS
   Ekanayake KB, 2015, WILDLIFE RES, V42, P509, DOI 10.1071/WR15108
   Evans P.G.H., 1986, NATO ASI Series Series G Ecological Sciences, V12, P179
   Field SA, 2005, J WILDLIFE MANAGE, V69, P473, DOI 10.2193/0022-541X(2005)069[0473:OAOMEU]2.0.CO;2
   Forys EA, 2017, SOUTHEAST NAT, V16, P317, DOI 10.1656/058.016.0301
   Frederiksen M., 2019, POPULATION SIZE HABI
   Fretwell PT, 2017, IBIS, V159, P481, DOI 10.1111/ibi.12482
   Fretwell PT, 2014, PLOS ONE, V9, DOI [10.1371/journal.pone.0085285, 10.1371/journal.pone.0088655]
   Fretwell PT, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033751
   Fretwell PT, 2009, GLOBAL ECOL BIOGEOGR, V18, P543, DOI 10.1111/j.1466-8238.2009.00467.x
   Gaglio D, 2017, METHODS ECOL EVOL, V8, P214, DOI 10.1111/2041-210X.12643
   Goebel ME, 2015, POLAR BIOL, V38, P619, DOI 10.1007/s00300-014-1625-4
   GREEN G. W., 1961, CANADIAN ENTOMOL, V93, P741
   Gregory RD, 2004, TECH ECOL CONSERVAT, P17
   Gremillet D, 2006, MAR BIOL, V148, P1081, DOI 10.1007/s00227-005-0130-2
   GRENZDORFFER G, 2013, INT ARCH PHOTOGRAMM, V2, P169
   Grishagin IV, 2015, ANAL BIOCHEM, V473, P63, DOI 10.1016/j.ab.2014.12.007
   GUINET C, 1995, POLAR BIOL, V15, P511
   Handley JM, 2018, ROY SOC OPEN SCI, V5, DOI 10.1098/rsos.171449
   Handley JM, 2016, POLAR BIOL, V39, P391, DOI 10.1007/s00300-015-1772-2
   Harding AMA, 2007, ECOLOGY, V88, P2024, DOI 10.1890/06-1695.1
   HARRIS MP, 1982, BIRD STUDY, V29, P149, DOI 10.1080/00063658209476750
   Hart LA, 2016, J THERM BIOL, V60, P149, DOI 10.1016/j.jtherbio.2016.07.001
   Hays GC, 2015, J ANIM ECOL, V84, P587, DOI 10.1111/1365-2656.12355
   Hebblewhite M, 2010, PHILOS T R SOC B, V365, P2303, DOI 10.1098/rstb.2010.0087
   Hervias S, 2013, BIOL INVASIONS, V15, P143, DOI 10.1007/s10530-012-0274-1
   Hinke JT, 2018, METHODS ECOL EVOL, V9, P1853, DOI 10.1111/2041-210X.13015
   Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
   Hodgson JC, 2016, SCI REP-UK, V6, DOI 10.1038/srep22574
   Hooker SK, 2008, MAR TECHNOL SOC J, V42, P65, DOI 10.4031/002533208786829179
   Horswill C, 2018, ECOL INDIC, V94, P218, DOI 10.1016/j.ecolind.2018.06.035
   Huffeldt NP, 2013, WATERBIRDS, V36, P330, DOI 10.1675/063.036.0310
   Hughes BJ, 2011, WILDLIFE BIOL, V17, P210, DOI 10.2981/10-106
   Hurford Clive, 2017, P249
   Hutchinson A.E., 1980, P COLONIAL WATERBIRD, V3, P235
   Irigoin-Lovera C, 2019, PEERJ, V7, DOI 10.7717/peerj.8129
   Israel M, 2017, INT CONF UNMAN AIRCR, P1199
   Joint Nature Conservation Committee, 2016, SEAB COUNT CENS BREE
   Joint Nature Conservation Committee, 2010, SEAD SURV DAT
   Jones FM, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-0442-6
   Jones L, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.248
   Kemper G, 2016, INT ARCH PHOTOGRAMM, V41, P689, DOI 10.5194/isprsarchives-XLI-B8-689-2016
   Korczak-Abshire M, 2016, CCAMLR SCI, V23, P1
   Korczak-Abshire M, 2019, POLAR BIOL, V42, P217, DOI 10.1007/s00300-018-2379-1
   Kucera TE, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P9, DOI 10.1007/978-4-431-99495-4_2
   Lieber L, 2019, COMMUN BIOL, V2, DOI 10.1038/s42003-019-0364-z
   Loarie SR, 2007, TRENDS ECOL EVOL, V22, P630, DOI 10.1016/j.tree.2007.08.018
   Lorentzen E., 2010, SEAPOP SHORT REP, V8, P1
   Luna N, 2018, TROP CONSERV SCI, V11, DOI 10.1177/1940082918785079
   Lynch HJ, 2012, ECOLOGY, V93, P1367
   Lynch HJ, 2012, POLAR BIOL, V35, P963, DOI 10.1007/s00300-011-1138-3
   Lyons M., 2019, PROTOCOL USING DRONE, DOI DOI 10.32942/OSF.IO/P9J3F
   McCafferty DJ, 2013, IBIS, V155, P4, DOI 10.1111/ibi.12010
   McClelland GTW, 2016, MAR ORNITHOL, V44, P215
   McLeay LJ, 2009, MAR BIOL, V156, P1765, DOI 10.1007/s00227-009-1211-4
   Mellor M., 2007, TRIAL HIGH DEFINITIO
   Mendez L, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-14478-7
   Merkel FR, 2016, J FIELD ORNITHOL, V87, P84, DOI 10.1111/jofo.12143
   Mitchell P.I., 2007, STRATEGIC REV UNPUB
   Moil RJ, 2007, TRENDS ECOL EVOL, V22, P660, DOI 10.1016/j.tree.2007.09.007
   Mosbech A, 2017, POLAR RES, V36, DOI 10.1080/17518369.2017.1374122
   MUDGE GP, 1987, BIRD STUDY, V34, P28, DOI 10.1080/00063658709476932
   Mulero-Pazmany M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178448
   Mullerova J, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.00887
   Nowak Maciej M., 2018, European Journal of Ecology, V4, P56, DOI 10.2478/eje-2018-0012
   Oosthuizen WC, 2020, POLAR BIOL, V43, P187, DOI 10.1007/s00300-019-02616-y
   Paleczny M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129342
   Pascalis F., 2018, PLOS ONE, V13
   Perkins AJ, 2018, IBIS, V160, P365, DOI 10.1111/ibi.12539
   Petersen A., 2008, 15 CAFF CBMP CAFF IN
   Pfeifer C, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3020039
   Ponganis PJ, 2000, J EXP BIOL, V203, P3275
   Radjawali I, 2017, J PEASANT STUD, V44, P753, DOI 10.1080/03066150.2016.1264937
   Robinson R.A., 2010, 526 BTO
   RSPB, 2020, PUFF
   Rummler MC, 2018, POLAR BIOL, V41, P2481, DOI 10.1007/s00300-018-2385-3
   Rummler MC, 2016, POLAR BIOL, V39, P1329, DOI 10.1007/s00300-015-1838-1
   Rush GP, 2018, ECOL EVOL, V8, P12322, DOI 10.1002/ece3.4495
   Sakamoto KQ, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0007322
   Sarda-Palomera F, 2012, IBIS, V154, P177, DOI 10.1111/j.1474-919X.2011.01177.x
   SCHWALLER MR, 1989, REMOTE SENS ENVIRON, V28, P199, DOI 10.1016/0034-4257(89)90113-2
   Sinclair Natalie C., 2017, Seabird, V30, P51
   Southwell C, 2015, J NAT CONSERV, V23, P1, DOI 10.1016/j.jnc.2014.11.002
   Stolpmann LM, 2019, EMU, V119, P391, DOI 10.1080/01584197.2019.1595661
   Takahashi A, 2004, P ROY SOC B-BIOL SCI, V271, pS281, DOI 10.1098/rsbl.2004.0182
   TEMPLE SA, 1972, J WILDLIFE MANAGE, V36, P944, DOI 10.2307/3799452
   Thaxter C.B., 2009, HIGH DEFINITION IMAG
   Thiebault A, 2014, BEHAV ECOL, V25, P1302, DOI 10.1093/beheco/aru132
   Thiebot JB, 2014, ORNITHOL SCI, V13, P41, DOI 10.2326/osj.13.41
   Tremblay Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088424
   Van Berkel T., 2014, CAMERA TRAPPING WILD
   Villegas P, 2018, 2018 IEEE ANDESCON
   Votier SC, 2003, POLAR BIOL, V26, P20, DOI 10.1007/s00300-002-0446-z
   Votier SC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0057376
   Waldron A, 2013, P NATL ACAD SCI USA, V110, P12144, DOI 10.1073/pnas.1221370110
   Walsh PM, 1995, SEABIRD MONITORING H
   Waluda CM, 2014, POLAR BIOL, V37, P1849, DOI 10.1007/s00300-014-1566-y
   Wang DL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111308
   Watanuki Y, 2008, MAR ECOL PROG SER, V356, P283, DOI 10.3354/meps07266
   Weimerskirch H, 2018, POLAR BIOL, V41, P259, DOI 10.1007/s00300-017-2187-z
   WELLER MW, 1972, AUK, V89, P196
   Whelan R., 2018, WILDLIFE BIOL, V2018, P1
   Wilhelm Sabina I., 2015, Marine Ornithology, V43, P211
   Williams HM, 2019, BEHAV ECOL SOCIOBIOL, V74, DOI 10.1007/s00265-019-2789-2
   Yoda K, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0019602
NR 130
TC 7
Z9 7
U1 4
U2 14
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0019-1019
EI 1474-919X
J9 IBIS
JI Ibis
PD APR
PY 2021
VL 163
IS 2
BP 317
EP 337
DI 10.1111/ibi.12871
EA SEP 2020
PG 21
WC Ornithology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA QR5JG
UT WOS:000569227700001
OA Green Accepted, Bronze
DA 2022-02-10
ER

PT J
AU Schoenecker, KA
   Doherty, PF
   Hourt, JS
   Romero, JP
AF Schoenecker, Kathryn A.
   Doherty, Paul F., Jr.
   Hourt, Jacob S.
   Romero, John P.
TI Testing Infrared Camera Surveys and Distance Analyses to Estimate Feral
   Horse Abundance in a Known Population
SO WILDLIFE SOCIETY BULLETIN
LA English
DT Article
DE Bureau of Land Management; distance sampling; feral horse; herd
   management area; sagebrush; US Forest Service
ID UNGULATE SURVEYS; AERIAL; DENSITY; BIAS
AB We tested the use of high-resolution infrared (IR) camera technology and distance sampling analyses to estimate abundance of feral horses (Equus caballus) during 2015-2016 in the McCullough Peaks Herd Management Area, Wyoming, USA. Infrared technology is becoming more common in ungulate population monitoring. The quality of IR cameras now allows ungulate species to be differentiated. Imperfect detection is a common problem in aerial surveys, so we tested the use of distance sampling analyses to account for imperfect detection probability. We conducted 2 aerial surveys in a sagebrush ecosystem with a demographically closed horse population. True abundance was known to within +/- 4 animals as a result of intensive, ground-based monitoring of each animal, all of which are uniquely identifiable. After truncation of our data, the most supported detection function was a uniform function with a detection probability equal to 1.0 out to 255 m. Our analyses yielded results that were within 10% of true abundance, but the coefficient of variation (CV) was large (36-58%) assuming a small sampling fraction. However, our truncated surveys covered approximately 95% of the herd management area. By including a finite population correction factor in our calculations of variance estimates, CVs (8-13%) were dramatically reduced. We found the combination of IR surveys and distance sampling analysis to be a useful method to estimate feral horse abundance in sagebrush vegetation type, which had limited cover to obscure horses. Repeated testing in sagebrush ecosystems as well as further testing in other habitat types and under differing conditions will inform how general our approach can be. (C) 2018 The Wildlife Society.
C1 [Schoenecker, Kathryn A.] US Geol Survey, Ft Collins Sci Ctr, 2150 Ctr Ave,Bldg C, Ft Collins, CO 80526 USA.
   [Schoenecker, Kathryn A.] Colorado State Univ, Ecosyst Sci & Sustainabil, Ft Collins, CO 80523 USA.
   [Doherty, Paul F., Jr.] Colorado State Univ, Warner Coll Nat Resources, Ft Collins, CO 80523 USA.
   [Hourt, Jacob S.; Romero, John P.] Owyhee Air Res, Nampa, ID 83687 USA.
   [Schoenecker, Kathryn A.] Colorado State Univ, Dept Ecosyst Sci & Sustainabil, Ft Collins, CO 80523 USA.
RP Schoenecker, KA (corresponding author), US Geol Survey, Ft Collins Sci Ctr, 2150 Ctr Ave,Bldg C, Ft Collins, CO 80526 USA.; Schoenecker, KA (corresponding author), Colorado State Univ, Ecosyst Sci & Sustainabil, Ft Collins, CO 80523 USA.; Schoenecker, KA (corresponding author), Colorado State Univ, Dept Ecosyst Sci & Sustainabil, Ft Collins, CO 80523 USA.
EM schoeneckerk@usgs.gov
FU Wyoming Department of Agriculture [144566]
FX We thank U.S. Bureau of Land Management (BLM) staff, particularly Dr. P.
   Griffin for input on the project study design and analyses and helpful
   comments on the manuscript. We are grateful to P. Hatle, BLM, and the
   nonprofit group Friends of a Legacy for monitoring individual horses and
   providing values for known abundance. We thank C. Wichman and the
   Wyoming Department of Agriculture (contract AG#144566) for funding this
   study, and the U.S. Geological Survey, Fort Collins Science Center for
   in-kind support. We are very grateful to the Associate Editor and
   several anonymous reviewers for their excellent reviews and
   contributions to this manuscript. Any use of trade, firm, or product
   names is for descriptive purposes only and does not imply endorsement by
   the U.S. Government. Co-author J. Romero is owner of Owyhee Air Research
   that conducted aerial surveys and collected data, but was not involved
   in data analysis, data interpretation, or manuscript preparation.
CR Beaver JT, 2014, WILDLIFE SOC B, V38, P419, DOI 10.1002/wsb.410
   Bernatas S, 2004, WILDLIFE SOC B, V32, P638, DOI 10.2193/0091-7648(2004)032[0638:SMFCBS]2.0.CO;2
   Buckland S.T., 2001, pi
   CAUGHLEY G, 1974, J WILDLIFE MANAGE, V38, P921, DOI 10.2307/3800067
   Chretien LP, 2016, WILDLIFE SOC B, V40, P181, DOI 10.1002/wsb.629
   Cilulko J, 2013, EUR J WILDLIFE RES, V59, P17, DOI 10.1007/s10344-012-0688-1
   Dunn WC, 2002, WILDLIFE SOC B, V30, P963
   Franke U, 2012, ANIM BIODIV CONSERV, V35, P285
   GARNER DL, 1995, ENVIRON MANAGE, V19, P233, DOI 10.1007/BF02471993
   Garrott RA, 2013, SCIENCE, V341, P847, DOI 10.1126/science.1240280
   Gillette GL, 2013, J FISH WILDL MANAG, V4, P386, DOI 10.3996/032013-JFWM-025
   Hadley D. G., 1990, US GEOLOGICAL SURVEY, V1756, DOI [10. 3133/b1756F, DOI 10.3133/B1756F]
   Havens KJ, 2016, THERMAL IMAGING TECHNIQUES TO SURVEY AND MONITOR ANIMALS IN THE WILD: A METHODOLOGY, P1
   Kissell RE, 2011, WILDLIFE BIOL, V17, P85, DOI 10.2981/10-040
   Lubow BC, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154902
   Lubow BC, 2009, J WILDLIFE MANAGE, V73, P1420, DOI 10.2193/2008-538
   National Research Council, 2013, US SCI IMPR BLM WILD
   Naugle DE, 1996, WILDLIFE SOC B, V24, P37
   SAMUEL MD, 1981, J WILDLIFE MANAGE, V45, P993, DOI 10.2307/3808111
   Storm DJ, 2011, WILDLIFE BIOL, V17, P431, DOI 10.2981/10-062
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   U. S. Department of the Interior, 2010, 2010057 US DEP INT B
   White GC, 1996, WILDLIFE SOC B, V24, P50
   WIGGERS EP, 1993, WILDLIFE SOC B, V21, P263
   Witczuk J, 2018, INT J REMOTE SENS, V39, P5504, DOI 10.1080/01431161.2017.1390621
NR 25
TC 4
Z9 4
U1 2
U2 25
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1938-5463
J9 WILDLIFE SOC B
JI Wildl. Soc. Bull.
PD SEP
PY 2018
VL 42
IS 3
BP 452
EP 459
DI 10.1002/wsb.912
PG 8
WC Biodiversity Conservation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Biodiversity & Conservation
GA GU6UK
UT WOS:000445451900010
OA Bronze
DA 2022-02-10
ER

PT J
AU Zhang, M
   Hu, XY
   Yao, J
   Zhao, LK
   Li, JC
   Gong, JY
AF Zhang, Mi
   Hu, Xiangyun
   Yao, Jian
   Zhao, Like
   Li, Jiancheng
   Gong, Jianya
TI Line-Based Geometric Consensus Rectification and Calibration From Single
   Distorted Manhattan Image
SO IEEE ACCESS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 07-12, 2015
CL Boston, MA
SP IEEE
DE Calibration; Cameras; Parameter estimation; Estimation; Image
   segmentation; Deep learning; Three-dimensional displays; Manhattan
   image; line detection; geometric consensus rectification; camera
   calibration; single image undistortion
ID AUTOMATIC CAMERA CALIBRATION; OMNIDIRECTIONAL CAMERAS; SPHERE
AB Recent advances in single image rectification and intrinsic calibration has been addressed by employing line information on the distorted image. The core issues of this technique are the separation of rectification and calibration procedures, and the suffering of geometric nonconformity. In this work, we propose a novel Geometric Consensus Rectification and Calibration algorithm, which we refer to as GCRC framework. We show how the geometric consensus rectification and calibration can be performed in a unified framework and solve the above issues. The proposed GCRC not only guarantees the geometrical consensus on the rectified images, but allows us to perform the robust intrinsic parameters estimation with the grouped circular arcs. Through grouping by voting in a unified framework, the geometric consensus rectification and calibration are robustly conducted on single distorted Manhattan images. Experiments on a number of distorted images, including the simulated YorkUrbanDB dataset, Panoramic Fisheye dataset, checkerboard image, and Internet images, demonstrate that the GCRC significantly improve the performance of geometrically consensus rectification and intrinsic parameters estimation. In particular, the GCRC shows relatively small variations with a different number of lines, which outperforms various previous approaches.
C1 [Zhang, Mi; Hu, Xiangyun; Yao, Jian; Gong, Jianya] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
   [Zhang, Mi; Li, Jiancheng] Wuhan Univ, Sch Geodesy & Geomat, Wuhan 430072, Peoples R China.
   [Zhao, Like] Henan Univ Technol, Coll Informat Sci & Engn, Zhengzhou 450001, Henan, Peoples R China.
RP Hu, XY (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
EM huxy@whu.edu.cn
FU Open Research Fund of State Key Laboratory of Information Engineering in
   Surveying, Mapping and Remote Sensing, Wuhan University [18R01]; China
   Postdoctoral Science FoundationChina Postdoctoral Science Foundation
   [2018M642915]; National Key Research and Development Program of China
   [2016YFB0501403]; National Natural Science Foundation of ChinaNational
   Natural Science Foundation of China (NSFC) [41771363, 41901265]
FX This work was supported in part by the Open Research Fund of State Key
   Laboratory of Information Engineering in Surveying, Mapping and Remote
   Sensing, Wuhan University, under Project 18R01, in part by the China
   Postdoctoral Science Foundation under Project 2018M642915, in part by
   the National Key Research and Development Program of China under Project
   2016YFB0501403, and in part by the National Natural Science Foundation
   of China under Project 41771363 and Project 41901265.
CR Agrawal A, 2013, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2013.184
   Ahn SJ, 2001, PATTERN RECOGN, V34, P2283, DOI 10.1016/S0031-3203(00)00152-7
   Antunes M., 2017, P IEEE C COMP VIS PA, P4288
   Antunes M, 2013, PROC CVPR IEEE, P1336, DOI 10.1109/CVPR.2013.176
   Barreto J.P, 2004, THESIS
   Barreto JP, 2006, COMPUT VIS IMAGE UND, V101, P151, DOI 10.1016/j.cviu.2005.07.002
   Barreto JP, 2005, IEEE T PATTERN ANAL, V27, P1327, DOI 10.1109/TPAMI.2005.163
   BASU A, 1995, PATTERN RECOGN LETT, V16, P433, DOI 10.1016/0167-8655(94)00115-J
   Benligiray B, 2016, EUR SIGNAL PR CONF, P938, DOI 10.1109/EUSIPCO.2016.7760386
   Bermudez-Cameo J, 2015, INT J COMPUT VISION, V114, P16, DOI 10.1007/s11263-014-0792-7
   Blott G., 2018, P EUR C COMP VIS ECC, P0
   Bogdan O., 2018, P 15 ACM SIGGRAPH EU, P6
   Bukhari F, 2013, J MATH IMAGING VIS, V45, P31, DOI 10.1007/s10851-012-0342-2
   Chang H, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010063
   Courbon J, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P1689
   De Villiers J. P., 2008, P SOC PHOTO-OPT INS, V7266
   Deng ZD, 2018, IEEE T INTELL TRANSP, V19, P1485, DOI 10.1109/TITS.2017.2723902
   Denis P, 2008, LECT NOTES COMPUT SC, V5303, P197, DOI 10.1007/978-3-540-88688-4_15
   Deutscher J, 2002, LECT NOTES COMPUT SC, V2353, P175
   Devernay F, 2001, MACH VISION APPL, V13, P14, DOI 10.1007/PL00013269
   Drap P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060807
   Eichenseer A, 2016, INT CONF ACOUST SPEE, P1541, DOI 10.1109/ICASSP.2016.7471935
   Fitzgibbon Andrew W, 2001, COMP VIS PATT REC 20, V1, pI
   Fleer D, 2017, ROBOT AUTON SYST, V89, P51, DOI 10.1016/j.robot.2016.12.001
   Grammatikopoulos L., 2013, INT ARCH PHOTOGRAM R, V36, P1
   Guillou E, 2000, VISUAL COMPUT, V16, P396, DOI 10.1007/PL00013394
   Hartley R, 2007, IEEE T PATTERN ANAL, V29, P1309, DOI 10.1109/TPAMI.2007.1147
   Hold-Geoffroy Y, 2018, PROC CVPR IEEE, P2354, DOI 10.1109/CVPR.2018.00250
   Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807
   Lazic N., 2011, THESIS
   Lee H, 2012, PROC CVPR IEEE, P877, DOI 10.1109/CVPR.2012.6247761
   Martinez-Finkelshtein A, 2006, INT MATH RES NOTICES, V2006, DOI 10.1155/IMRN/2006/91426
   Mei C., 2007, THESIS
   Mei C, 2007, IEEE INT CONF ROBOT, P3945, DOI 10.1109/ROBOT.2007.364084
   Melo R, 2013, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2013.72
   PIOTRASCHKE M, 2016, PROC CVPR IEEE, P3418, DOI DOI 10.1109/CVPR.2016.372
   Pritts J, 2018, PROC CVPR IEEE, P1993, DOI 10.1109/CVPR.2018.00213
   Puig L, 2012, COMPUT VIS IMAGE UND, V116, P120, DOI 10.1016/j.cviu.2011.08.003
   Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847
   Saez A, 2018, IEEE INT VEH SYM, P1039, DOI 10.1109/IVS.2018.8500456
   Simon G., 2018, P EUR C COMP VIS ECC, P318
   Su D, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18061885
   Tateno Keisuke, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11220), P732, DOI 10.1007/978-3-030-01270-0_43
   Tulsiani S, 2017, P IEEE C COMP VIS PA, P2626
   Urban S, 2015, ISPRS J PHOTOGRAMM, V108, P72, DOI 10.1016/j.isprsjprs.2015.06.005
   Vanschoren J., 2018, ARXIV181003548
   Vasconcelos F, 2018, IEEE T PATTERN ANAL, V40, P791, DOI 10.1109/TPAMI.2017.2699648
   Wildenauer H., 2013, P BRIT MACH VIS C BM, V1, P2
   Workman S., 2016, ARXIV160402129
   Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22
   Yin XQ, 2018, LECT NOTES COMPUT SC, V11214, P475, DOI 10.1007/978-3-030-01249-6_29
   Ying XG, 2004, LECT NOTES COMPUT SC, V3021, P442
   Ying XH, 2004, INT C PATT RECOG, P839, DOI 10.1109/ICPR.2004.1333903
   Ying XH, 2004, IEEE T PATTERN ANAL, V26, P1260, DOI 10.1109/TPAMI.2004.79
   Ying XH, 2008, INT J COMPUT VISION, V78, P89, DOI 10.1007/s11263-007-0082-8
   Ying XH, 2013, IEEE T PATTERN ANAL, V35, P1206, DOI 10.1109/TPAMI.2012.195
   Zhai MH, 2016, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2016.610
   Zhang GP, 2019, APPL OPTICS, V58, P1467, DOI 10.1364/AO.58.001467
   Zhang M, 2015, PROC CVPR IEEE, P4137, DOI 10.1109/CVPR.2015.7299041
   Zhao RQ, 2018, IEEE T PATTERN ANAL, V40, P3059, DOI 10.1109/TPAMI.2017.2772922
   Zhao Y, 2018, ADV MULTIMED, V2018, DOI 10.1155/2018/6182953
   Zhou LJ, 2018, BOUND VALUE PROBL, DOI 10.1186/s13661-018-1110-z
   Zhu Z, 2018, PROCEEDINGS OF 2018 INTERNATIONAL CONFERENCE ON INFORMATION SYSTEMS AND COMPUTER AIDED EDUCATION (ICISCAE 2018), P416, DOI 10.1109/ICISCAE.2018.8666916
NR 63
TC 1
Z9 1
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2019
VL 7
BP 156400
EP 156412
DI 10.1109/ACCESS.2019.2947177
PG 13
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Telecommunications
GA JN8TY
UT WOS:000497165400087
OA gold
DA 2022-02-10
ER

PT J
AU Broker, KCA
   Hansen, RG
   Leonard, KE
   Koski, WR
   Heide-Jorgensen, MP
AF Broker, Koen C. A.
   Hansen, Rikke G.
   Leonard, Kathleen E.
   Koski, William R.
   Heide-Jorgensen, Mads Peter
TI A comparison of image and observer based aerial surveys of narwhal
SO MARINE MAMMAL SCIENCE
LA English
DT Article
DE abundance estimation; aerial surveys; Arctic; Melville Bay; Monodon
   monoceros; narwhal; line-transect survey; strip census; unmanned aerial
   systems; West Greenland
ID UNMANNED AIRCRAFT SYSTEM; MONODON-MONOCEROS; MARINE MAMMALS; ESTIMATING
   ABUNDANCE; MELVILLE BAY; BAFFIN-BAY; VEHICLES; WHALES; WINTER; UAS
AB From 25 to 30 August 2014 a double-observer line-transect survey was conducted over Melville Bay, home to one of two summering populations of narwhal (Monodon monoceros) off West Greenland. A total of 1,932 linear kilometers was surveyed along 33 transects. In addition to using observers, the aircraft was equipped with two oblique cameras to capture a comparable data set. Analysts reviewed the images for narwhal sightings, which were then matched to the observer sightings. The objectives of the study were to determine advantages and disadvantages of the detection capabilities of both methodologies, and to conduct a comparative analysis of population abundance estimates. Correcting for the truncated detection distance of the images (500 m), the image analysts recorded more sightings (62) and a lower mean group size (2.2) compared to aerial observers (36 and 3.5, respectively), resulting in comparable numbers of individuals detected by both platforms (135 vs. 126). The abundance estimate based on the image sightings was 2,536 (CV = 0.51, 95% CI: 1,003-6,406), which was not significantly different from the aerial observers estimate of 2,596 individuals (CV = 0.51; 95% CI: 961-7,008). This study supports the potential of using UAS for marine mammal abundance studies.
C1 [Broker, Koen C. A.] Shell Global Solut Int BV, Kesslerpk 1, NL-2288 GS Rijswijk, Netherlands.
   [Broker, Koen C. A.] Univ Groningen, Groningen Inst Evolutionary Life Sci, POB 11103, NL-9700 CC Groningen, Netherlands.
   [Hansen, Rikke G.; Heide-Jorgensen, Mads Peter] Greenland Inst Nat Resources, Greenland Representat, Strandgade 91,2, DK-1401 Copenhagen K, Denmark.
   [Leonard, Kathleen E.] LGL Alaska Res Associates Inc, 2000 West Int Airport Rd,STE C-1, Anchorage, AK 99502 USA.
   [Koski, William R.] LGL Ltd Environm Res Associates, 22 Fisher St,POB 280, King City, ON L7B 1A6, Canada.
RP Broker, KCA (corresponding author), Shell Global Solut Int BV, Kesslerpk 1, NL-2288 GS Rijswijk, Netherlands.; Broker, KCA (corresponding author), Univ Groningen, Groningen Inst Evolutionary Life Sci, POB 11103, NL-9700 CC Groningen, Netherlands.
EM koen.broker@shell.com
OI Broker, Koen/0000-0002-5965-5517
FU Greenland Bureau of Minerals and Petroleum; Shell Greenland A/S; Shell
   Alaska
FX The aerial survey was funded by the Greenland Bureau of Minerals and
   Petroleum. Shell Greenland A/S and Shell Alaska funded the installation
   and operation of the automated camera system and manual analysis of
   images. We thank Anders Mosbech (DCE) for managing the Baffin Bay
   Environmental Study Program. We are also grateful to Brad Boschetto
   (Shell), Dale Funk and Craig Reiser (LGL Alaska Research) for organizing
   logistics of the camera installation. Per PalsbOll (University of
   Groningen), Kees Camphuysen (Royal Netherlands Institute for Sea
   Research), Louis Brzuzy, Michael Macrander (Shell), and three anonymous
   reviewers provided useful feedback on this document. The excellent
   pilots were provided by Norlandair. Thanks are due to the aerial
   observers Rasmus Due Nielsen, Silje Rekdal Larsen, and Mikkel Sinding,
   and the image analysts Amy Baker, Cynthia Christman, Heather Patterson,
   Lauren Ackein, Lisa Barry, and Matthew O'Dell (LGL Alaska Research) for
   their dedicated efforts.
CR Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
   BAJZAK D, 1990, WILDLIFE SOC B, V18, P125
   Borchers DL, 2006, BIOMETRICS, V62, P372, DOI 10.1111/j.1541-0420.2005.00493.x
   BUCKLAND S, 2015, METH STAT ECOL
   Buckland S.T., 2001, pi
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Chabot D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122588
   Chabot D, 2012, WATERBIRDS, V35, P170, DOI 10.1675/063.035.0119
   EBERHARDT LL, 1978, J WILDLIFE MANAGE, V42, P1, DOI 10.2307/3800685
   Fewster RM, 2009, BIOMETRICS, V65, P225, DOI 10.1111/j.1541-0420.2008.01018.x
   Finley K.J., 1990, Canadian Bulletin of Fisheries and Aquatic Sciences, V224, P97
   Frouin-Mouy H, 2017, ARCTIC, V70, P59, DOI 10.14430/arctic4632
   Gerrodette T, 2019, MAR MAMMAL SCI, V35, P22, DOI 10.1111/mms.12506
   Goebel ME, 2015, POLAR BIOL, V38, P619, DOI 10.1007/s00300-014-1625-4
   Heide-Jorgensen M. P., 2015, Journal of Cetacean Research and Management, V15, P1
   Heide-Jorgensen MP, 2016, POLAR BIOL, V39, P1605, DOI 10.1007/s00300-015-1885-7
   Heide-Jorgensen MP, 2010, J MAMMAL, V91, P1135, DOI 10.1644/09-MAMM-A-198.1
   Heide-Jorgensen M. P., 2004, MAR MAMMAL SCI, V20, P58, DOI DOI 10.1111/J.1748-7692.2004.TB01154.X
   Heide-JOrgensen M. P., 2013, MONITORING ABUNDANCE
   Heide-JOrgensen M. P., 2014, ABUNDANCE DISTRIBUTI
   Heide-Jorgensen MP, 2013, BIOL CONSERV, V158, P50, DOI 10.1016/j.biocon.2012.08.005
   Heide-Jorgensen MP, 2003, CAN J ZOOL, V81, P1298, DOI 10.1139/Z03-117
   Heide-Jorgensen MP, 2002, POLAR BIOL, V25, P331, DOI 10.1007/s00300-001-0347-6
   Heide-Jorgensen MP, 2002, SCI PUBLICATIONS N A, V4, P191, DOI DOI 10.7557/3.2844
   HEIDEJORGENSEN MP, 1993, CAN J FISH AQUAT SCI, V50, P2323, DOI 10.1139/f93-257
   Henkel Laird A., 2007, Marine Ornithology, V35, P145
   Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
   Hodgson A, 2017, ECOL APPL, V27, P1253, DOI 10.1002/eap.1519
   Innes Stuart, 2002, NAMMCO Scientific Publications, V4, P169
   Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
   Koski WR, 2015, J UNMANNED VEH SYST, V3, P22, DOI 10.1139/juvs-2014-0014
   Koski WR, 2013, J UNMANNED VEH SYST, V1, P25, DOI 10.1139/juvs-2013-0015
   Laake JL, 2004, ADVANCED DISTANCE SAMPLING: ESTIMATING ABUNDANCE OF BIOLOGICAL POPULATIONS, P108
   Lowry MS, 1999, MAR MAMMAL SCI, V15, P143, DOI 10.1111/j.1748-7692.1999.tb00786.x
   Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
   Marques FFC, 2004, ADVANCED DISTANCE SAMPLING: ESTIMATING ABUNDANCE OF BIOLOGICAL POPULATIONS, P31
   Martin SB, 2017, J ACOUST SOC AM, V142, P3331, DOI 10.1121/1.5014049
   Mathews E. A., 1995, P 3 GLAC BAY SCI S U, P254
   Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
   Norton-Griffiths M., 1974, East African Wildlife Journal, V12, P245
   Patterson C, 2016, J UNMANNED VEH SYST, V4, P53, DOI 10.1139/juvs-2015-0014
   Pike D., 2015, 2015034 DFO CSAS
   Ratcliffe N, 2015, J UNMANNED VEH SYST, V3, P95, DOI 10.1139/juvs-2015-0006
   Richardson W.J., 1995, MARINE MAMMALS NOISE
   Scott M.D., 1985, Inter-American Tropical Tuna Commission Bulletin, V18, P381
   Smith CE, 2016, J UNMANNED VEH SYST, V4, P31, DOI 10.1139/juvs-2015-0017
   Stewart R. E., 2013, SCI PUBLICATIONS N A, V9, P95
   Thomas L, 2010, J APPL ECOL, V47, P5, DOI 10.1111/j.1365-2664.2009.01737.x
   Wichmann FA, 2010, J VISION, V10, DOI 10.1167/10.4.6
   Williams R, 2017, ENDANGER SPECIES RES, V34, P149, DOI 10.3354/esr00845
NR 50
TC 10
Z9 10
U1 0
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0824-0469
EI 1748-7692
J9 MAR MAMMAL SCI
JI Mar. Mamm. Sci.
PD OCT
PY 2019
VL 35
IS 4
BP 1253
EP 1279
DI 10.1111/mms.12586
PG 27
WC Marine & Freshwater Biology; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Marine & Freshwater Biology; Zoology
GA JH0SS
UT WOS:000492484700003
OA hybrid, Green Published
DA 2022-02-10
ER

PT J
AU Weinstein, BG
AF Weinstein, Ben G.
TI Scene-specific convolutional neural networks for video-based
   biodiversity detection
SO METHODS IN ECOLOGY AND EVOLUTION
LA English
DT Article
DE automated monitoring; computer vision; hummingbirds; neural networks;
   remote cameras
AB 1. Finding, counting and identifying animals is a central challenge in ecology. Most studies are limited by the time and cost of fieldwork by human observers. To increase the spatial and temporal breadth of sampling, ecologists are adopting passive image-based monitoring approaches. While passive monitoring can expand data collection, a remaining obstacle is finding the small proportion of images containing ecological objects among the majority of frames containing only background scenes.
   2. I proposed a scene-specific convolutional neural network for detecting animals of interest within long duration time-lapse videos. Convolutional neural networks are a type of deep learning algorithm that have recently made significant advances in image classification.
   3. The approach was tested on videos of floral visitation by hummingbirds. Despite low frame rates, poor image quality, and complex video conditions, the model correctly classified over 90% of frames containing hummingbirds. Combining motion detection and image classification can substantially reduce the time investment in scoring images from passive monitoring studies.
   4. These results underscore the promise of deep learning to lead ecology into greater automation using passive image analysis. To help facilitate future applications, I created a desktop executable that can be used to apply pre-trained models to videos, as well as reproducible scripts for training new models on local and cloud environments.
C1 [Weinstein, Ben G.] Oregon State Univ, Marine Mammal Inst, Dept Fisheries & Wildlife, Newport, OR 97365 USA.
RP Weinstein, BG (corresponding author), Oregon State Univ, Marine Mammal Inst, Dept Fisheries & Wildlife, Newport, OR 97365 USA.
EM weinsteb@oregonstate.edu
FU Segment
FX Thanks to Segment for supporting this work as part of their Open Data
   Fellowship. Thanks to the many Google engineers who shared code samples
   and answering questions on deploying machine learning models. The author
   declares no conflict of interest.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Anderson TM, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0314
   Babaee M, 2018, PATTERN RECOGN, V76, P635, DOI 10.1016/j.patcog.2017.09.040
   Bouwmans T, 2014, MACH VISION APPL, V25, P1101, DOI 10.1007/s00138-013-0578-x
   Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906
   Bradski G, 2000, DR DOBBS J, V25, P120
   Braham M, 2016, INT C SYST SIGN IM P, DOI [10. 1109/iwssip. 2016. 7502717, DOI 10.1109/IWSSIP.2016.7502717, 10.1109/IWSSIP.2016.7502717]
   Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
   Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
   Guirado E, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9121220
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Marburg A., 2016, OCEANS 2016 MTS IEEE, P1
   Nogueira K, 2017, PATTERN RECOGN, V61, P539, DOI 10.1016/j.patcog.2016.07.001
   Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Schmid K, 2017, HYDROBIOLOGIA, V784, P93, DOI 10.1007/s10750-016-2860-1
   Simonyan K., 2014, ARXIV14091556 ARXIV14091556, DOI DOI 10.1109/CVPR.2015.7298594
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
   Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
   Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Weinstein BG, 2017, ECOL LETT, V20, P326, DOI 10.1111/ele.12730
   Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 29
TC 14
Z9 14
U1 2
U2 38
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 2041-210X
EI 2041-2096
J9 METHODS ECOL EVOL
JI Methods Ecol. Evol.
PD JUN
PY 2018
VL 9
IS 6
BP 1435
EP 1441
DI 10.1111/2041-210X.13011
PG 7
WC Ecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA GJ0XF
UT WOS:000434977100007
OA Bronze
DA 2022-02-10
ER

EF