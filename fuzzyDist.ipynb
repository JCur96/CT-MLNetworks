{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fuzzyDist.py\n",
    "## edge list creation script\n",
    "## author: J Curry\n",
    "## date: 09/03/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fishc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#from fuzzywuzzy import fuzz\n",
    "#from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from alive_progress import alive_bar\n",
    "import time\n",
    "\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## might not need these??\n",
    "import random\n",
    "import gc \n",
    "import timeit\n",
    "import sys\n",
    "from datetime import datetime as dt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "#%pip install sparse_dot_topn \n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsDf = pd.read_csv('citationNetworks/Data/SplitRefs.csv')\n",
    "#refsDf['ref_title'] = str(refsDf['ref_title'])\n",
    "refsDf['ref_title'] = refsDf['ref_title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWORDS = set(STOPWORDS)\n",
    "    \n",
    "def text_prepare(text, STOPWORDS):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: a clean string\n",
    "    \"\"\"\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[\\n\\\"\\'/(){}\\[\\]\\|@,;#]')\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # delete stopwords from text\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) \n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'camera trap type many need? review camera features study designs range wildlife research applications 2013 hystrix 24 pp. 148-156'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prepare(\" 'Which camera trap type and how many do I need?'' A review of camera features and study designs for a range of wildlife research applications (2013) Hystrix, 24, pp. 148-156\", STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(refsDf['ref_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsDf['ProcessedRef'] = refsDf['ref_title'].apply(lambda x: text_prepare(x, STOPWORDS)) # not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5, token_pattern='(\\S+)')\n",
    "tf_idf_matrix = tfidf_vectorizer.fit_transform(refsDf['ProcessedRef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 74915)\t0.19817021902739246\n",
      "  (0, 121137)\t0.22633767489493806\n",
      "  (0, 13794)\t0.2657340095081818\n",
      "  (0, 34994)\t0.2976221757442021\n",
      "  (0, 118448)\t0.30921328958415467\n",
      "  (0, 139623)\t0.3280220568989637\n",
      "  (0, 184753)\t0.32516350339682326\n",
      "  (0, 58897)\t0.32382239307955824\n",
      "  (0, 82572)\t0.1793236015622716\n",
      "  (0, 74898)\t0.15777839564422713\n",
      "  (0, 121025)\t0.14485437836170506\n",
      "  (0, 13526)\t0.13331285966112455\n",
      "  (0, 34962)\t0.15160325899150712\n",
      "  (0, 118445)\t0.24516488320976343\n",
      "  (0, 139609)\t0.2034815705889608\n",
      "  (0, 184647)\t0.18174581596572859\n",
      "  (0, 58890)\t0.2883937906121029\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer-assisted system photographic mark-recapture analysis 2012 methods ecol. evol'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refsDf['ProcessedRef'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "    ct.sparse_dot_topn(\n",
    "            M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "            np.asarray(A.indices, dtype=idx_dtype),\n",
    "            A.data,\n",
    "            np.asarray(B.indptr, dtype=idx_dtype),\n",
    "            np.asarray(B.indices, dtype=idx_dtype),\n",
    "            B.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data)\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# adjust lower bound: 0.8\n",
    "# keep top 10 similar results\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)\n",
    "\n",
    "t = time.time()-t1\n",
    "print(\"finished in:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsDf = pd.read_csv('citationNetworks/Data/SplitRefs.csv')\n",
    "#print(refsDf.head())\n",
    "#fuzz.ratio(refsDf[\"Title\"], refsDf[\"Reference\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to unique \n",
    "# uniqueRefs = refsDf.drop_duplicates(subset=['ref_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(uniqueRefs)):\n",
    "#     title = uniqueRefs.iloc[i,6] # 6 or title\n",
    "#     refTitle = uniqueRefs.iloc[i,7] # 7 or ref title\n",
    "#     ID = uniqueRefs.iloc[i,4] # 4 or ID\n",
    "#     ratio = fuzz.ratio(title, refTitle)\n",
    "#     if ratio > 50:\n",
    "#         #append to a df with title, re_title, ID, ratio\n",
    "#         tmpDf = pd.DataFrame([[ID, title, refTitle, ratio]], columns=list('ID', 'Title', 'RefTitle', 'PercentMatch'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in uniqueRefs.iterrows():\n",
    "#     title = row['Title'] # 6 or title\n",
    "#     refTitle = row['ref_title'] # 7 or ref title\n",
    "#     ID = row['ID'] # 4 or ID\n",
    "#     ratio = fuzz.ratio(title, refTitle)\n",
    "#     # print(ratio)\n",
    "#     # if ratio > 50:\n",
    "#     #     #append to a df with title, re_title, ID, ratio\n",
    "#     #     tmpDf = pd.DataFrame([[ID, title, refTitle, ratio]], columns=list('ID', 'Title', 'RefTitle', 'PercentMatch'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "uniqueRefs = uniqueRefs.replace(\"\", nan_value)\n",
    "uniqueRefs = uniqueRefs.dropna(subset=[\"ref_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniUniqueRefs = uniqueRefs.iloc[0:10]\n",
    "# titleDf = uniqueRefs = refsDf.drop_duplicates(subset=['Title'])#\n",
    "# #tmpDf = pd.DataFrame([[ID, title, refTitle, ratio, bestMatch, bestMatchID]]) # add bits needed\n",
    "# tmpDf = pd.DataFrame(columns=('ID', 'Title', 'RefTitle', 'PercentMatch', 'BestMatchTitle', 'BestMatchID'))\n",
    "# columns = list(tmpDf)\n",
    "# newDF = []\n",
    "\n",
    "# for index, row in tqdm(miniUniqueRefs.iterrows()):\n",
    "#     title = row['Title'] # 6 or title\n",
    "#     refTitle = str(row['ref_title']) # 7 or ref title\n",
    "#     ID = row['ID'] # 4 or ID\n",
    "#     output = process.extractOne(refTitle, titleDf['Title'], scorer=fuzz.token_sort_ratio)\n",
    "#     # print(output)\n",
    "#     ratio = output[1]\n",
    "#     bestMatch = output[0]\n",
    "#     bestMatchID = output[2]\n",
    "#     values = [ID, title, refTitle, ratio, bestMatch, bestMatchID]\n",
    "#     zipped = zip(columns, values)\n",
    "#     dictionary = dict(zipped)\n",
    "#     newDF.append(dictionary)\n",
    "#     # tmpDf = pd.DataFrame([[ID, title, refTitle, ratio, bestMatch, bestMatchID]]) # add bits needed\n",
    "#     # tmpDf.columns=('ID', 'Title', 'RefTitle', 'PercentMatch', 'BestMatchTitle', 'BestMatchID')\n",
    "#     # tmpDf.append(tmpDf)\n",
    "# print(tmpDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd355fb90a7420f99789f7931751513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titleDf = uniqueRefs = refsDf.drop_duplicates(subset=['Title'])#\n",
    "#tmpDf = pd.DataFrame([[ID, title, refTitle, ratio, bestMatch, bestMatchID]]) # add bits needed\n",
    "tmpDf = pd.DataFrame(columns=('ID', 'Title', 'RefTitle', 'PercentMatch', 'BestMatchTitle', 'BestMatchID'))\n",
    "columns = list(tmpDf)\n",
    "newDF = []\n",
    "\n",
    "#with alive_bar(238148, force_tty=True) as bar:\n",
    "for index, row in tqdm(uniqueRefs.iterrows()):\n",
    "    title = row['Title'] # 6 or title\n",
    "    refTitle = str(row['ref_title']) # 7 or ref title\n",
    "    ID = row['ID'] # 4 or ID\n",
    "    output = process.extractOne(refTitle, titleDf['Title'], scorer=fuzz.token_sort_ratio)\n",
    "    # print(output)\n",
    "    ratio = output[1]\n",
    "    bestMatch = output[0]\n",
    "    bestMatchID = output[2]\n",
    "    values = [ID, title, refTitle, ratio, bestMatch, bestMatchID]\n",
    "    zipped = zip(columns, values)\n",
    "    dictionary = dict(zipped)\n",
    "    newDF.append(dictionary)\n",
    "    #tmpDf = pd.DataFrame([[ID, title, refTitle, ratio, bestMatch, bestMatchID]]) # add bits needed\n",
    "    #tmpDf.columns=('ID', 'Title', 'RefTitle', 'PercentMatch', 'BestMatchTitle', 'BestMatchID')\n",
    "    #tmpDf.append(tmpDf)\n",
    "#bar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Can community-protected areas conserve biodiversity in human-modified tropical landscapes? The case of terrestrial mammals in Southern Mexico [Aires protégées communautaires peuvent conserver la biodiversité dans les paysages tropicaux modifiés par l'homme? Le cas des mammifères terrestres dans le sud du Mexique.] [New população e extensão do alcance do macaco-aranha marrom-headed equatoriana Criticamente em Perigo (Ateles fusciceps fusciceps) no oeste do Equador] [¿Pueden las áreas protegidas en la comunidad conservar la biodiversidad en paisajes tropicales modificados por el hombre-? El caso de los mamíferos terrestres en el sur de México.]\", 38)\n"
     ]
    }
   ],
   "source": [
    "miniUniqueRefs = uniqueRefs.iloc[0:10]\n",
    "df0_names = list(miniUniqueRefs.ref_title.unique())\n",
    "df1_names = list(uniqueRefs.ref_title.unique())\n",
    "df2_names = list(uniqueRefs.Title.unique())\n",
    "print(process.extractOne(str(df0_names), df2_names, scorer=fuzz.token_sort_ratio))\n",
    "#print(process.extractOne(str(df1_names), df2_names, scorer=fuzz.token_sort_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_names(name, list_names, min_score=0):\n",
    "    max_score = -1\n",
    "    max_name = ''\n",
    "    for x in list_names:\n",
    "        score = fuzz.ratio(name, x)\n",
    "        if (score > min_score) & (score > max_score):\n",
    "            max_name = x\n",
    "            max_score = score\n",
    "    return (max_name, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Can community-protected areas conserve biodiversity in human-modified tropical landscapes? The case of terrestrial mammals in Southern Mexico [Aires protégées communautaires peuvent conserver la biodiversité dans les paysages tropicaux modifiés par l'homme? Le cas des mammifères terrestres dans le sud du Mexique.] [New população e extensão do alcance do macaco-aranha marrom-headed equatoriana Criticamente em Perigo (Ateles fusciceps fusciceps) no oeste do Equador] [¿Pueden las áreas protegidas en la comunidad conservar la biodiversidad en paisajes tropicales modificados por el hombre-? El caso de los mamíferos terrestres en el sur de México.]\", 38)\n"
     ]
    }
   ],
   "source": [
    "df1_names = list(uniqueRefs.ref_title.unique())\n",
    "titlesList = list(uniqueRefs.Title.unique())\n",
    "refsAsArray = uniqueRefs.to_numpy()\n",
    "miniUniqueRefs = uniqueRefs.iloc[0:10]\n",
    "miniArray = miniUniqueRefs.to_numpy()\n",
    "\n",
    "print(process.extractOne(str(miniArray[:,5]), titlesList, scorer=fuzz.token_sort_ratio))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b0f069635f502c080d2c86df588aca401f45023c6eba452c7d287578147dabf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
